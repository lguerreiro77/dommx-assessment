{
  "meta": {
    "built_at_utc": "2026-02-25T15:22:41Z",
    "pdf_count": 5
  },
  "pdfs": {
    "CMMI.pdf": {
      "fingerprint": "CMMI.pdf|1771999044|2284122",
      "pages": [
        "Pittsburgh, PA 15213-3890 CMMI® for Development, Version 1.2 CMMI-DEV, V1.2 CMU/SEI-2006-TR-008 ESC-TR-2006-008 Improving processes for better products CMMI Product Team August 2006 Unlimited distribution subject to the copyright.",
        "This work is sponsored by the U.S. Department of Defense. The Software Engi neering Institute is a federally funded research and development center s ponsored by the U.S. De partment of Defense. Copyright 2006 by Carnegie Mellon University. NO WARRANTY THIS CARNEGIE MELLON® UNIVERSITY AND SOFTWA RE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN “AS-IS” BASIS. CARNEGIE MELLON UNIVERSITY MAKE S NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTAB ILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOE S NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATEN T, TRADEMARK, OR COPYRIGHT INFRINGEMENT. Use of any trademarks in this report is not intended in any way to infringe on the rights of the trademark holder. Internal use. Permission to reproduce this document and to pr epare derivative works from this document for internal use is granted, provided the copyright and “No Warranty” statements are included with all reproductions and derivative works. External use. Requests for permission to reproduce this document or prepare derivative works of this document for external and commercial use should be addre ssed to the SEI Licensing Agent. This work was created in the performance of Federal Government Contract Number FA8721-05-C-0003 with Carnegie Mellon University for the operation of the Software Engineer ing Institute, a federally funded research and development center. The Government of the United States has a royalty-free government-purpose li cense to use, duplicat e, or disclose the work, in whole or in part and in any manner, and to have or permit others to do so, for gove rnment purposes pursuant to the copyright license under the clause at 252.227-7013. For information about purchasing paper copies of SEI repor ts, please visit the publications portion of our Web site (http://www.sei.cmu.edu/ publications/pubweb.html). The following service marks and register ed marks are used in this document: Capability Maturity Model® CMM® CMM IntegrationSM CMMI® IDEALSM SCAMPISM CMMI, CMM, and Capability Maturity Model are registered in the U.S. Patent and Trademark Office. CMM Integration, SCAMPI, and IDEAL are service marks of Carnegie Mellon University.",
        "CMMI for Development Version 1.2 Preface iPreface CMMI® (Capability Maturity Model® Integration) is a process improvement maturity model for the development of products and services. It consists of best practices that address development and maintenance activities that cover t he product lifecycle from conception through delivery and maintenance. This latest iteration of the model as represented herein integrates bodies of knowledge that are essential for development and maintenance, but that have been ad dressed separately in the past, such as software engineering, systems engineering, hardware and design engineering, the engineering “-i lities,” and acquisition. The prior designations of CMMI for systems engineering and software engineering (CMMI-SE/SW) are superseded by the title “CMMI for Development” to truly reflect the comprehensive integration of these bodies of knowledge and the applic ation of the model within the organization. CMMI for Development (CMMI-DEV) provides a comprehensive integrated solution for development and maintenance activities applied to products and services. CMMI for Development, Version 1. 2 is a continuat ion and update of CMMI version 1.1 and has been facilit ated by the concept of CMMI “constellations” wherein a set of core components can be augmented by additional material to provide app lication-specific models with highly common content. CMMI-DEV is the fi rst of such constellations and represents the development area of interest. Purpose The purpose of CMMI for Development is to help organizations improve their development and maintenance processes for both products and services. CMMI for Development is a collection of best practices that is generated from the CMMI Framework.1 The CMMI Framework supports the CMMI Product Suite by allowing mu ltiple models, training courses, and appraisal methods to be generated that support spec ific areas of interest. 1 The CMMI Framework is the basic structure that or ganizes CMMI components and combines them into CMMI constellations and models.",
        "CMMI for Development Version 1.2 Preface ii A constellation is a collection of CMMI components that includes a model, its training materials, and appraisal-related documents for an area of interest. Currently there are three planned constellations supported by the versi on 1.2 model framework: development, services, and acquisition. “Additions” are us ed to expand constellations for specific additional content. This document contains the CMMI for Development constellation and contains both the base CMMI-DEV as well as CMMI-DEV with the IPPD addition (CMMI-DEV+IPPD). If you are not using IPPD, ignore the information that is marked “IPPD Addition,” and you will be using t he CMMI for Development model. Unlike CMMI version 1.1, there is but a single model document that describes both the staged and cont inuous approaches to process improvement versus the prior use of two representations of staged and continuous in separate documents. This consolidated presentation of model material for both approac hes was first used in the book, CMMI: Guidelines for Process Integr ation and Product Improvement . Thanks to Peter Gordon, publishing partner at Addison-Wesley Professional, and the book’s authors, Mary Beth Ch rissis, Mike Konrad, and Sandy Shrum, we were able to use the book’s manuscript as the basis for developing CMMI version 1.2 [Chrissis 2003]. Acknowledgments Many talented people were involved as part of the product team for the CMMI v1.2 Product Suite. Three pr imary groups involved in this development were the Steering Group, Product Team, and Configuration Control Board. The Steering Group guides and approv es the plans of the Product Team, provides consultation on signi ficant CMMI project issues, and ensures involvement from a vari ety of interested communities. The Product Team writes, reviews, revises, discusses, and agrees on the structure and technical content of the CMMI Product Suite, including the framework, models, training, and appraisal materials. Development activities are based on multiple inputs. These inputs include an A- Specification and guidanc e specific to each re lease provided by the Steering Group, source models, change requests received from the user community, and input received fr om pilots and other stakeholders [SEI 2004]. The Configuration Control Board is th e official mechanism for controlling changes to the CMMI models and Introduction to CMMI training. As such, this group ensures integrity over the life of the product suite by",
        "CMMI for Development Version 1.2 Preface iiireviewing all proposed changes to the baseline and approving only those changes that sa tisfy the identified issues and meet the criteria for the upcoming release. Members of these groups that were involved in developing CMMI v1.2, are listed in Appendix C. Audience The audience for this model includes anyone interested in process improvement in a development and maintenance environment. Whether you are familiar with the concept of Capability Maturity Models or whether you are seeking informat ion to get started on your improvement efforts, this doc ument will be useful to you. This model is also intended for people who want to use an appraisal2 to see where they are, those who al ready know what they want to improve, and those who are just ge tting started and want to develop a general understanding of the CMMI for Development constellation. Organization of This Document This document is avail able on the SEI Web site3 and serves as a guide for improvement of organizational proc esses. It is organized into three main parts: • Part One—About CMMI for Development • Part Two—Generic Goals and Gene ric Practices, and the Process Areas • Part Three—The Appendices and Glossary Part One, “About CMMI for Developm ent,” consists of five chapters: • Chapter 1, “Introduction,” offe rs a broad view of CMMI and the CMMI for Development constella tion. It introduces you to the concepts of process improvemen t, and describes the history of models used for process improv ement and different process improvement approaches. • Chapter 2, “Process Area Components,” describes all of the components of the CMMI for Development process areas. 2 An appraisal is an examination of one or more processes by a trained team of professionals using a reference model (e.g., CMMI) as the basis for determining strengths and weaknesses. 3 The SEI Web site is located at http://www.sei.cmu.edu.",
        "CMMI for Development Version 1.2 Preface iv • Chapter 3, “Tying It All T ogether,” assembles the model components and explains the conc epts of maturity levels and capability levels. • Chapter 4, “Relationships among Process Areas,” provides insight into the meaning and interactions of the CMMI for Development process areas. • Chapter 5, “Using CMMI Models, ” describes paths to adoption and use of CMMI for process im provement and benchmarking. Part Two, “Generic Goals and Gene ric Practices, and the Process Areas,” contains all of the CMMI for Development constellation’s required and expected components. It al so contains related informative components, including component na mes, subpractices, notes, and typical work products. Part Two contains 23 sections. The first section contains the generic goals and practices, including a descr iption of how they are used and how they relate to the process ar eas. The remaining 22 sections each represent one of the CMMI fo r Development process areas.4 To make these process areas easy to find, t hey are organized alphabetically by process area acronym. Each secti on contains descriptions of goals, best practices, and examples. Part Three, “The Appendices and Glossary ,” consists of four information resources: • Appendix A, “References ,” contains referenc es you can use to locate documented sources of inform ation such as reports, process improvement models, industry standards, and books that are related to CMMI for Development. • Appendix B, “Acronyms,” defines the acronyms used herein. • Appendix C, “CMMI for Development Project Partici pants,” contains lists of people and their organizati ons who participated in the development of CMMI for Development, Version 1.2. • The “Glossary” defines many of the terms used in CMMI. How to Use This Document Whether you are new to process improvement, new to CMMI, or already familiar with CMMI, Part One can help you understand why CMMI for Development is the best model to use for improving your development and maintenance processes. 4 A “process area” is a cluster of related best practices in an area, which when implemented collectively, satisfy a set of goals considered important for making signif icant improvement in that area. We will cover this concept in detail in Chapter 2.",
        "CMMI for Development Version 1.2 Preface vReaders New to Process Improvement If you are new to process improvement or new to the CMM® concept, we suggest that you read Chapter 1, “Introduction,” firs t. Chapter 1 will give you an overview of process im provement and explain what CMMI is all about. Next, skim Part Two, including gener ic goals and practices as well as specific goals and practices, to get a feel for the scope of the best practices contained in the model. Pa y closest attention to the purpose and introductory notes at t he beginning of each section. In Part Three, look through the references in Appendix A and select additional sources you think would be beneficial to read before moving forward with using CMMI for Development. Read through the acronyms and glossary to become familiar wi th the language of CMMI. Then, go back and read the details of Part Two. Readers Experienced with Process Improvement If you are new to CMMI but have experience with other process improvement models, such as the So ftware CMM (version 1.1) or the Systems Engineering Capability Model (i.e., EIA 731), you will immediately recognize many similarities [EIA 1998]. We recommend that you read Part One to understand how CMMI is different from other process improv ement models, but you may want to read some of the sections more quickly than others. Read Part Two with an eye open for best practices y ou recognize from the models you have already tried. Identifying familiar material gives you a feel for what is new and what has been carried ov er from the model you already know. Next, review the glossary to understand how some terminology may differ from that used in the proc ess improvement model you know. Many concepts will be repeated, but they may be called something different. Readers Familiar with CMMI If you have reviewed or used a CMMI model before, you will quickly recognize the CMMI concepts di scussed and the best practices presented. The differences between version 1.2 and version 1.1 are explained in detail on the SEI Web site in the version 1.2 release notes. These differences reflect the enhanc ements suggested by the users of version 1.1.",
        "CMMI for Development Version 1.2 Preface vi The following improvements were made to version 1.2: • Both representations are presented together. • The advanced practice and comm on feature concepts have been removed. • The generic goal and practice descriptions were moved to Part Two. • Hardware amplifications were added. • All definitions were cons olidated in the glossary. • IPPD practices were consolidat ed and simplified. There are no longer any separate IPPD process areas. • Supplier Agreement Management (SAM) and Integrated Supplier Management (ISM) were consoli dated and Supplier Sourcing was removed. • Generic practice (GP) elaborations were added to the level 3 GPs. • An explanation of how process areas support the im plementation of GPs was added. • Material was added to ensure that standard processes are deployed to projects at their startup. Additional Information and Reader Feedback You can find additional information from various other sources about CMMI, such as the background and hist ory of the CMMI models, as well as the benefits of using CMMI models. Many of these sources are listed in Appendix A and are also published on the CMMI Web site— http://www.sei.cmu.edu/cmmi/ [SEI 2]. Suggestions for improving CMMI are welcome. For information on how to provide feedback, see the CMMI Web site at http://www.sei.cmu.edu/cmmi/models/c hange-requests.html. If you have questions about CMMI, send an email to cmmi- comments@sei.cmu.edu.",
        "CMMI for Development Version 1.2 Table of Contents vii Table of Contents Preface i Purpose i Acknowledgments ii Audience iii Organization of This Document iii How to Use This Document iv Readers New to Process Improvement v Readers Experienced with Process Improvement v Readers Familiar with CMMI v Additional Information and Reader Feedback vi About CMMI for Development 1 1 Introduction 3 About Capability Maturity Models 4 Evolution of CMMI 5 CMMI for Development 7 The Scope of CMMI for Development 8 The Group of IPPD Additions 9 Resolving Different Approaches of CMMs 9 Choosing a Representation 10 Continuous Representation 10 Staged Representation 10 Comparison of the Continu ous and Staged Representations 11 Factors in Your Decision 11 Why Not Both Representations? 12 Your Approach to Process Improvement 13 Scenario 1 13 Scenario 2 14 2 Process Area Components 16 Required, Expected, and Informative Components 16 Required Components 16 Expected Components 16 Informative Components 16 Components Associated with Part Two 17 Process Areas 18 Purpose Statements 19 Introductory Notes 19",
        "CMMI for Development Version 1.2 viii Table of Contents Related Process Areas 19 Specific Goals 19 Generic Goals 19 Specific Goal and Practice Summaries 20 Specific Practices 20 Typical Work Products 20 Subpractices 21 Generic Practices 21 Generic Practice Elaborations 21 Supporting Informative Components 22 Notes 22 Examples 22 Amplifications 22 References 23 Numbering Scheme 23 Typographical Conventions 24 Representation-Specific Content 27 Additions 28 3 Tying It All Together 29 Understanding Levels 29 Structures of the Continuous and Staged Representations 30 Understanding Capability Levels 32 Capability Level 0: Incomplete 33 Capability Level 1: Performed 33 Capability Level 2: Managed 33 Capability Level 3: Defined 33 Capability Level 4: Quantitatively Managed 34 Capability Level 5: Optimizing 34 Advancing through Capability Levels 34 Understanding Maturity Levels 35 Maturity Level 1: Initial 36 Maturity Level 2: Managed 36 Maturity Level 3: Defined 37 Maturity Level 4: Quantitatively Managed 37 Maturity Level 5: Optimizing 38 Advancing through Maturity Levels 39 Process Areas 41 Generic Goals and Practices 45 Representation Comparison 46 Equivalent Staging 47 4 Relationships Among Process Areas 51 Four Categories of CMMI Process Areas 51 Process Management 52",
        "CMMI for Development Version 1.2 Table of Contents ix Basic Process Management Process Areas 52 Advanced Process Management Process Areas 54 Project Management 55 Basic Project Management Process Areas 55 Advanced Project Management Process Areas 57 Engineering 58 Recursion and Iteration of Engineering Processes 61 Support 62 Basic Support Process Areas 62 Advanced Support Process Areas 64 5 Using CMMI Models 65 Adopting CMMI 65 Your Process Improvement Program 66 Selections That Influence Your Program 66 CMMI Models 67 Using CMMI Appraisals 68 Appraisal Requirements for CMMI 68 SCAMPI Appraisal Methods 69 Appraisal Considerations 69 CMMI-Related Training 70 Generic Goals and Generic Practices, and the Process Areas 73 Generic Goals and Generic Practices 75 Overview 75 Process Institutionalization 75 Performed Process 76 Managed Process 76 Defined Process 77 Quantitatively Managed Process 78 Optimizing Process 79 Relationships among Processes 80 Generic Goals and Generic Practices 81 Applying Generic Practices 94 Process Areas That Support Generic Practices 94 Causal Analysis and Resolution 101 Configuration Management 114 Decision Analysis and Resolution 131 Integrated Project Management +IPPD 145 Measurement and Analysis 178 Organizational Innovation and Deployment 198 Organizational Process Definition +IPPD 219 Organizational Process Focus 241 Organizational Process Performance 261",
        "CMMI for Development Version 1.2 x Table of Contents Organizational Training 275 Product Integration 293 Project Monitoring and Control 313 Project Planning 327 Process and Product Quality Assurance 353 Quantitative Project Management 364 Requirements Development 388 Requirements Management 408 Risk Management 420 Supplier Agreement Management 439 Technical Solution 456 Validation 483 Verification 496 The Appendices and Glossary 515 A. References 517 Publicly Available Sources 517 Regularly Updated Sources 521 B. Acronyms 522 C. CMMI for Development Project Participants 526 Product Team 526 Model Team Members 526 SCAMPI Upgrade Team Members 527 Training Team Members 527 Architecture Team Members 527 Hardware Team Members 528 Piloting Team Members 528 Quality Team Members 528 Sponsors 529 Steering Group 529 Steering Group Members 529 Ex-Officio Steering Group Members 530 Steering Group Support: Acquisition 530 Steering Group Support: CCB 530 Configuration Control Board 530 CCB Members 530 Non-Voting CCB Members 531 D. Glossary 532",
        "CMMI for Development Version 1.2 About CMMI for Development 1PART ONE About CMMI for Development",
        "CMMI for Development Version 1.2 2 About CMMI for Development",
        "CMMI for Development Version 1.2 Introduction 31 Introduction Now, more than ever, companies want to deliver products and services better, faster, and cheaper. At the same time, in the high-technology environment of the twenty-first cent ury, nearly all organizations have found themselves building increasi ngly complex products and services. Today, a single company usually does not develop all the components that compose a product or service. More commonly, some components are built in-house and some are acqui red; then all the components are integrated into the final product or service. Organizations must be able to manage and control this comp lex development and maintenance process. The problems these organizations address today involve enterprise- wide solutions that require an integrated approach. Effective management of organizational assets is critical to business success. In essence, these organizations are pr oduct and service developers that need a way to manage an integrated a pproach to their development activities as part of achiev ing their business objectives. In the current marketplace, ther e are maturity models, standards, methodologies, and guidelines that can help an organization improve the way it does business. Howeve r, most available improvement approaches focus on a specific part of the business and do not take a systemic approach to the problems th at most organizations are facing. By focusing on improving one area of a business, these models have unfortunately perpetuated the stovepi pes and barriers that exist in organizations. Capability Maturity Model® Integration (CMMI®) provides an opportunity to avoid or eliminate these stovepipes and barriers through integrated models that transcend disciplines. CM MI for Development consists of best practices that address devel opment and maintenance activities applied to products and services. It addresses practices that cover the product’s lifecycle from concept ion through delivery and maintenance. The emphasis is on the work necessary to build and maintain the total product.",
        "CMMI for Development Version 1.2 Introduction 4 About Capability Maturity Models In its research to help organizations develop and maintain quality products and services, the Softwar e Engineering Institute (SEI) has found several dimensions that an or ganization can focus on to improve its business. Figure 1.1 illustrates t he three critical dimensions that organizations typically focus on: people, procedures and methods, and tools and equipment. Procedures and methods defining the relationship of tasks Tools and equipmentPeople with skills, training, and motivationAB CD PROCESS Figure 1.1: The Three Critical Dimensions But what holds everything together? It is the processes used in your organization. Processes allow you to align the way you do business. They allow you to address scalabilit y and provide a way to incorporate knowledge of how to do things bette r. Processes allow you to leverage your resources and to examine business trends. This is not to say that people and te chnology are not important. We are living in a world where technology is changing by an order of magnitude every ten years. Similarly, people ty pically work for many companies throughout their careers. We live in a dynamic world. A focus on process provides the infrastructure necessary to deal with an ever- changing world, and to maximize the productivity of people and the use of technology to be more competitive. Manufacturing has long recogniz ed the importance of process effectiveness and efficiency. Today, many organizations in manufacturing and service industrie s recognize the importance of quality processes. Process helps an organization’s workforce meet business objectives by helping them wo rk smarter, not harder, and with improved consistency. Effective processes also provide a vehicle for",
        "CMMI for Development Version 1.2 Introduction 5introducing and using new technology in a way that best meets the business objectives of the organization. In the 1930s, Walter Shewhart began wo rk in process improvement with his principles of statistical quality control [Shewhart 1931]. These principles were refined by W. Edwards Deming [Deming 1986], Phillip Crosby [Crosby 1979], and Joseph Juran [Juran 1988]. Watts Humphrey, Ron Radice, and others extended these principles even further and began applying them to so ftware in their work at IBM and the SEI [Humphrey 1989]. Humphrey’s book, Managing the Software Process , provides a description of the basic principles and concepts on which many of the capability maturity models (CMMs®) are based. The SEI has taken the process m anagement premise, “the quality of a system or product is highly influenced by the quality of the process used to develop and maintain it,” and defined CMMs that embody this premise. The belief in this prem ise is seen worldwide in quality movements, as evidenced by t he International Organization for Standardization/International Elec trotechnical Commission (ISO/IEC) body of standards. CMMs focus on improving processes in an organization. They contain the essential elements of effect ive processes for one or more disciplines and describe an evolutio nary improvement path from ad hoc, immature processes to disciplined, mature processes with improved quality and effectiveness. The SEI created the first CMM desi gned for software organizations and published it in a book, The Capability Maturity Mo del: Guidelines for Improving the Software Process [SEI 1995]. The SEI’s book applied the principl es introduced almost a century ago to this never-ending cycle of proce ss improvement. The value of this process improvement approach has been confirmed over time. Organizations have experienced increa sed productivity and quality, improved cycle time, and more accurate and predictable schedules and budgets [Gibson 2006]. Evolution of CMMI Since 1991, CMMs have been developed for myriad disciplines. Some of the most notable include models for systems engineering, software engineering, software acquisiti on, workforce management and development, and integrated product and process development (IPPD). Although these models have proven useful to many organizations in different industries, the use of multiple models has been problematic. Many organizations would like t heir improvement efforts to span",
        "CMMI for Development Version 1.2 Introduction 6 different groups in their organizati ons. However, the differences among the discipline-specific models us ed by each group, including their architecture, content, and approach, have limited these organizations’ capabilities to broaden their impr ovements successfully. Further, applying multiple models that are not integrated within and across an organization is costly in terms of training, appraisals, and improvement activities. The CMM IntegrationSM project was formed to sort out the problem of using multiple CMMs. The CMMI Pro duct Team’s initial mission was to combine three source models: 1. The Capability Maturity Model for Software (SW-CMM) v2.0 draft C [SEI 1997b] 2. The Systems Engineering Capability Model (SECM) [EIA 1998]5 3. The Integrated Product Development Capability Maturity Model (IPD-CMM) v0.98 [SEI 1997a] The combination of these models in to a single improvement framework was intended for use by organizations in their pursuit of enterprise-wide process improvement. These three source models were selected because of their widespread adoption in the software and syst ems engineering communities and because of their different approaches to improving processes in an organization. Using information from these popul ar and well-regarded models as source material, the CMMI Produc t Team created a cohesive set of integrated models that can be adopted by those currently using the source models, as well as by thos e new to the CMM concept. Hence, CMMI is a result of the evoluti on of the SW-CMM, the SECM, and the IPD-CMM. Developing a set of integrated models involved more than simply combining existing model materials. Using processes that promote consensus, the CMMI Product T eam built a framework that accommodates multiple disciplines and is flexible enough to support the different approaches of t he source models [Ahern 2003]. 5 The Systems Engineering Capability Model is also k nown as Electronic Industries Alliance 731 (EIA 731).",
        "CMMI for Development Version 1.2 Introduction 7v1.02 (2000)v1.02 (2000) v1.1 (2002)v1.1 (2002)History of CMMs CMM for Software v1.1 (1993)CMM for Softwarev1.1 (1993) Systems Engineering CMM v1.1 (1995)Systems Engineering CMM v1.1 (1995) EIA 731 SECM (1998)EIA 731 SECM (1998)INCOSE SECAM (1996)INCOSE SECAM (1996) Integrated Product Development CMM (1997)Integrated Product Development CMM (1997)Software CMM v2, draft C (1997)Software CMM v2, draft C (1997) CMMI for Development v1.2 (2006)CMMI for Acquisition v1.2 (2007)CMMI for Acquisition v1.2 (2007) CMMI for Services v1.2 (2007)CMMI for Services v1.2 (2007) Figure 1.2: The History of CMMs Since the release of CMMI v1.1, we have seen that this improvement framework can be applied to other ar eas of interest [SEI 2002a, SEI 2002b]. To apply to multiple areas of interest, the framework groups best practices into what we call “c onstellations.” A constellation is a collection of CMMI components that are used to build models, training materials, and appraisal documents. Recently, the CMMI model archit ecture was improved to support multiple constellations and the sharing of best practices among constellations and their member m odels. Work has begun on two new constellations: one for services (CMMI for Services) and the other for acquisition (CMMI for Acquisition). Although CMMI for Development incorporates the development of se rvices, including the combination of components, consumables, and peopl e intended to meet service requirements, it differs from t he planned CMMI for Services (CMMI- SVC), which focuses on the delivery of services. The CMMI models that have been available in the community prior to 2006 are now considered part of the CMMI for De velopment constellation. CMMI for Development The CMMI for Development constellati on consists of two models: CMMI for Development +IPPD and CMMI for Development (without IPPD). Both models share much of their ma terial and are identical in these shared areas. However, CMMI fo r Development +IPPD contains additional goals and practices that cover IPPD.",
        "CMMI for Development Version 1.2 Introduction 8 Currently, only one model is published since the CMMI for Development +IPPD model contains the full complem ent of practices available in this constellation, and you can derive the ot her model from this material. If you are not using IPPD, ignore the information that is marked “IPPD Addition,” and you will be using the CMMI for Development model. If the need arises or the development constellation is expanded, the architecture will allow other m odels to be generated and published. CMMI for Development is the designat ed successor of the three source models. The SEI has retired the So ftware CMM and the IPD-CMM. EIA has retired the SECM. All three of these models are succeeded by CMMI for Development. The best practices in the CMMI models have gone through an extensive review process. CMMI version 0.2 was publicly reviewed and used in pilot activities. The CMMI Product Team evaluated more than 3,000 change requests to create CMMI version 1.0. Shor tly thereafter, version 1.02 was released, which incorporated several minor improvements. Version 1.1 incorporated improvem ents guided by feedback from early use, with more than 1,500 change r equests submitted as part of the public review, and hundreds of comment s as part of the change control process. CMMI version 1.2 was developed us ing input from nearly 2,000 change requests submitted by CMMI users. More than 750 of those requests were directed at CMMI model cont ent. As you can see, not only is CMMI widely adopted, but it is improved based on the feedback received from the community. The Scope of CMMI for Development CMMI for Development is a refe rence model that covers the development and maintenance activiti es applied to both products and services. Organizations from many industries, including aerospace, banking, computer hardware, software, defense, automobile manufacturing, and telecommunicati ons, use CMMI for Development. Models in the CMMI for Developmen t constellation contain practices that cover project management, process management, systems engineering, hardware engineering, software engineering, and other supporting processes used in dev elopment and maintenance. The CMMI for Development +IPPD model al so covers the use of integrated teams for development and maintenance activities.",
        "CMMI for Development Version 1.2 Introduction 9The Group of IPPD Additions In CMMI, “additions” are used to include material that may be of interest to particular users. For the CMMI for Development constellation, additional material was included to address IPPD. The IPPD group of additions covers an IPPD approach that includes practices that help organizations ac hieve the timely collaboration of relevant stakeholders throughout t he life of the product to satisfy customers’ needs, expectations, and requirements [DoD 1996]. When using processes that support an I PPD approach, you should integrate these processes with other proce sses in the organization. To support those using IPPD-related processe s, the CMMI for Development constellation allows organizations to optionally select the IPPD group of additions. When you select CMMI for Developm ent +IPPD, you are selecting the CMMI for Development model plus all the IPPD additions. When you select CMMI for Development, you ar e selecting the model without the IPPD additions. In the text in Part One of this document, we may use “CMMI for Development” to refer to ei ther of these models, for the sake of brevity. Resolving Different Approaches of CMMs The definition of a CMM allows t he community to develop models supporting different approaches to pr ocess improvement. As long as a model contains the essential element s of effective processes for one or more disciplines and describes an ev olutionary improvement path from ad hoc, immature processes to disci plined, mature processes with improved quality and effectiveness, it is considered a CMM. CMMI enables you to approach process im provement and appraisals using two different representat ions: continuous and staged. The continuous repres entation enables an organization to select a process area (or group of process areas) and improve processes related to it. This representation us es capability levels to characterize improvement relative to an individual process area. The staged representation uses predefined sets of process areas to define an improvement path for an or ganization. This improvement path is characterized by maturity levels. Ea ch maturity level provides a set of process areas that characterize different organizational behaviors.",
        "CMMI for Development Version 1.2 Introduction 10 Choosing a Representation If you are new to process improvemen t and are not familiar with either the staged or the continuous representation, you cannot go wrong if you choose one representation or the other. There are many valid reasons to select either representation. If you have been using a CMM and you are familiar with a particular representation, we suggest that you continue to use that representation because it will make the transition to CMMI easier. Once you have become completely comfortable with CMMI, you might then decide to use the other r epresentation. Because each representation has advantages over the other, some organizations use both r epresentations to address particular needs at various times in their improvement programs. In the following sections, we provide the advantages and dis advantages of eac h representation to help you decide which representatio n is best for your organization. Continuous Representation The continuous represent ation offers maximum flexibility when using a CMMI model for process improvem ent. An organization may choose to improve the performance of a single process-related trouble spot, or it can work on several areas that are closely aligned to the organization’s business objectives. The continuous representation also allows an organization to improve different proc esses at different rates. There are some limitations on an organization’s choices because of the dependencies among some process areas. If you know the processes that need to be improved in your organization and you understand t he dependencies among the process areas described in CMMI, the cont inuous representation is a good choice for your organization. Staged Representation The staged representation offers a systematic, structured way to approach model-based process im provement one stage at a time. Achieving each stage ensures that an adequate process infrastructure has been laid as a foundat ion for the next stage. Process areas are organized by maturi ty levels that take some of the guess work out of process impr ovement. The staged representation prescribes an order for implementing process areas according to maturity levels, which define the improvement path for an organization from the initial level to the optimiz ing level. Achieving each maturity level ensures that an adequate impr ovement foundation has been laid",
        "CMMI for Development Version 1.2 Introduction 11for the next maturity level and allows for lasting, incremental improvement. If you do not know where to start and which processes to choose to improve, the staged representation is a good choice for you. It gives you a specific set of processes to improve at each stage that has been determined through more than a dec ade of research and experience with process improvement. Comparison of the Continuous and Staged Representations Table 1.1 compares the advantages of each representation and may assist you with determining which representation is right for your organization. Table 1.1 Comparative Advant ages of Continuous and Staged Representations Continuous Representation Staged Representation Grants explicit freedom to select the order of improvement that best meets the organization’s business objec-tives and mitigates the organization’s areas of risk Enables organizations to have a pre-defined and proven improvement path Enables increased visibility of the capability achieved in each individual process area Focuses on a set of processes that provide an organization with a spe-cific capability that is characterized by each maturity level Allows improvements of different processes to be performed at differ-ent rates Summarizes process improvement results in a simple form—a single maturity level number Reflects a newer approach that does not yet have the data to demonstrate its ties to return on investment Builds on a relatively long history of use that includes case studies and data that demonstrate return on in-vestment Factors in Your Decision Three categories of factors that may influence your decision when selecting a representation are bus iness, culture, and legacy. Business Factors An organization with mature knowle dge of its own business objectives is likely to have a strong mapping of its processes to its business objectives. Such an organiza tion may find the continuous",
        "CMMI for Development Version 1.2 Introduction 12 representation useful to appraise its processes and in determining how well the organization’s processe s support and meet its business objectives. If an organization with a product-line focus decides to improve processes across the entire organization, it might be served best by the staged representation. The st aged representation will help an organization select the critical proc esses to focus on for improvement. The same organization may opt to improve processes by product line. In that case, it might select the continuous representation—and a different appraised rating of c apability might be achieved for each product line. Both approaches are valid. The most important consideration is which business objectives you would like your process improvement program to support and how these business objectives align with the two representations. Cultural Factors Cultural factors to consider when selecting a representation have to do with an organization’s capability to deploy a process improvement program. For instance, an organiza tion might select the continuous representation if the corporat e culture is process based and experienced in process improvement or has a specific process that needs to be improved quickly. An orga nization that has little experience in process improvement may choos e the staged repres entation, which provides additional guidance on t he order in which changes should occur. Legacy If an organization has experience with another model that has a staged representation, it may be wise to continue with the staged representation when using CMMI, especi ally if it has invested resources and deployed processes across the organization that are associated with a staged representation. The same is true for the continuous representation. Why Not Both Representations? Whether used for process improvement or appraisals, both representations are designed to offe r essentially equivalent results. Nearly all of the CMMI model content is common to both representations. Therefore, an organization need not select one representation over another. In fact, an organization may find utility in both representat ions. It is rare that an organization will implement either representation exactly as prescribed. Organizations that are successful in process improvement",
        "CMMI for Development Version 1.2 Introduction 13often define an improvem ent plan that focuses on the unique needs of that organization and therefore use th e principles of both the staged and the continuous representations. For example, organizations that select the staged representation and are at maturity level 1 often impl ement the maturity level 2 process areas but also the Organizational Proc ess Focus process area, which is included at maturity level 3. Another example is an organization that chooses the conti nuous representation for guidi ng its internal process improvement effort and then choos es the staged representation to conduct an appraisal. Your Approach to Process Improvement To demonstrate how to use this model , let us look at two different scenarios. Scenario 1 is an electroni c systems developer that wants to improve its product development processes using a continuous approach. Scenario 2 is a software development company that uses IPPD, has been using the Software C MM, and now wants to use CMMI. This company most recently has been ra ted at maturity level 3 using the Software CMM (version 1.1). Scenario 1 In this scenario, you are using a continuous approach and, therefore, you select the processes that are im portant to your business objectives. Since there are 22 process areas to choose from, this is usually too many to focus on when starting ou t. You may need to narrow your focus. For example, you may find that your competitor always releases its product before yours. You may choose to focus on improving your engineering and project management processes. Building on this decision, you select all the Engineering process areas as a starting point: Product Integration, Requirements Development, Requirements Management, Technica l Solution, Validation, and Verification. You also select Pr oject Planning and Project Monitoring and Control. You may at this point dec ide that eight process areas are still too many to focus on initially, and you decide that the requirements process is really where the problems are. Consequently, you select the Requirements Development and R equirements Management process areas to begin your improvement efforts. Next you decide how much improv ement is needed in the requirements area. Do you have any processes in place already? If you do not, your process improvement objective may be to get to capability level 1.",
        "CMMI for Development Version 1.2 Introduction 14 Do you have your requirements development and management processes in place for each project, but they are not managed processes? For example, policies, training, and tools are not implemented to support the processes. If your requirements processes are in place but there is no suppor ting infrastructure, your process improvement objective may be to get to capability level 2. Do you have all your requirements development and management processes and their management in place, but each project performs these processes differently? For exam ple, your requirements elicitation process is not performed consistently across the organization. If this is the case, your process improvem ent objective may be to get to capability level 3. Do you consistently manage and perform your requirements development and management processes but do not have an objective way to control and improve these proce sses? If this is the case, your process improvement objective may be to get to capability level 4. Do you want to ensure that you are selecting the right subprocesses to improve based on quantitativ e objectives to maximi ze your business? If so, your process improvement objecti ve may be to get to capability level 5 for selected processes. In t he description of each process area, remember to look for amplificati ons introduced by the phrases “For Hardware Engineering,” “For Systems Engineering,” and “For Software Engineering.” Use all information that has no specific markings and the material in the boxes labeled “Continuous Only.” As you can see from this scenario, you need to understand which processes need improvement and how much you want to mature each process. This way of proceeding re flects the fundamental principle behind the continuous representation. Scenario 2 In the second scenario, you are a software development company using IPPD, using the Software CMM, and y ou want to use CMMI. You select the process areas at maturity le vels 2 and 3 and choose the CMMI for Development +IPPD model. This selection includes the following seven process areas at maturity level 2: Requirements Managemen t, Project Planning, Project Monitoring and Control, Supplier Agreement Management, Measurement and Analysis, Proces s and Product Quality Assurance, and Configuration Management. It also includes the following 11 process areas at maturity leve l 3: Requirements Development, Technical Solution, Product Integr ation, Verification, Validation, Organizational Process Focus, Or ganizational Process Definition +IPPD, Organizational Training, In tegrated Project Management +IPPD,",
        "CMMI for Development Version 1.2 Introduction 15Risk Management, and Decision Analysis and Resolution. You will also include the IPPD additions. Since you have already been rated at maturity level 3 for the Software CMM, look at the CMMI process areas that were not in the Software CMM. These process areas in clude Measurement and Analysis, Requirements Development, Technica l Solution, Product Integration, Verification, Validation, Risk Management, and Decision Analysis and Resolution. Determine if you have th ese processes in your organization even though they were not descri bed in the Software CMM. If any processes in place correspond to these process areas and the other process areas that were in the So ftware CMM, perform a gap analysis against the goals and practices to make sure you addressed the intent of each CMMI process area. Remember, in each process area you select, to look for information labeled “For Software Engineering” and “IPPD Addition.” Use all information that has no s pecific markings, as we ll as the material in boxes labeled “Staged Only.” As you can see, the information pr ovided in this document can be used in a variety of ways, depending on your improvement needs. The overall goal of CMMI is to provide a framework that can share consistent process im provement best practices and approaches, but can be flexible enough to address t he rapidly changing needs of the community.",
        "CMMI for Development Version 1.2 Process Area Components 16 2 Process Area Components This chapter describes the com ponents of each process area, generic goal, and generic practice. Underst anding the meaning of these components is critical to using the info rmation in Part Two effectively. If you are unfamiliar with Part Two, you may want to skim the Generic Goals and Generic Practices secti on and a couple of process area sections to get a general feel for the content and lay out before reading this chapter. Required, Expected, and Informative Components Model components are grouped into three categories—required, expected, and informative—that reflect how to interpret them. Required Components Required components describe what an organization must achieve to satisfy a process area. This achievement must be visibly implemented in an organization’s processes. The required components in CMMI are the specific and generic goals. Goal satisfaction is used in appraisals as the basis for deciding whether a process area has been achieved and satisfied. Expected Components Expected components describe what an organization may implement to achieve a required component. Expec ted components guide those who implement improvements or perform appraisals. Expected components include the specific and generic practices. Before goals can be considered satisf ied, either the practices as described, or acceptable alternativ es to them, are present in the planned and implemented processes of the organization. Informative Components Informative components provide det ails that help organizations get started in thinking about how to approach the required and expected components. Subpractices, typical work products, amplifications,",
        "CMMI for Development Version 1.2 Process Area Components 17generic practice elaborations, goal and practice titles, goal and practice notes, and references are examples of informative model components. The CMMI glossary of terms is not a required, expected, or informative component of CMMI models. You should interpret the terms in the glossary in the context of the model component in which they appear. Components Associated with Part Two The model components associated wi th Part Two can be summarized to illustrate their relationshi ps, as shown in Figure 2.1. Process Area Generic PracticesGeneric PracticesGeneric GoalsGeneric Goals Expected InformativeInformative Required KEY:Purpose StatementIntroductory NotesRelated Process Areas SubpracticesSubpracticesSpecific GoalsSpecific Goals Specific PracticesSpecific Practices Typical Work ProductsTypical Work ProductsSubpracticesSubpractices SubpracticesGeneric Practice Elaborations Figure 2.1: CMMI Model Components The following sections provide de tailed descriptions of the model components.",
        "CMMI for Development Version 1.2 Process Area Components 18 Process Areas A process area is a cluster of related practices in an area that, when implemented collectively, satisfy a set of goals considered important for making improvement in that area. There are 22 process areas, present ed here in alphabetical order by acronym: • Causal Analysis and Resolution (CAR) • Configuration Management (CM) • Decision Analysis and Resolution (DAR) • Integrated Project M anagement +IPPD (IPM+IPPD)6 • Measurement and Analysis (MA) • Organizational Innovation and Deployment (OID) • Organizational Process Definition +IPPD (OPD+IPPD)6 • Organizational Process Focus (OPF) • Organizational Process Performance (OPP) • Organizational Training (OT) • Product Integration (PI) • Project Monitoring and Control (PMC) • Project Planning (PP) • Process and Product Quality Assurance (PPQA) • Quantitative Project Management (QPM) • Requirements Development (RD) • Requirements Management (REQM) • Risk Management (RSKM) • Supplier Agreement Management (SAM) • Technical Solution (TS) • Validation (VAL) • Verification (VER) 6 This process area has \"+IPPD\" after its name because it contains a goal and practices that are specific to IPPD. The material specific to IPPD is called an \"IPPD a ddition.\" All process areas with IPPD additions have \"+IPPD\" after their name.",
        "CMMI for Development Version 1.2 Process Area Components 19Purpose Statements The purpose statement describes t he purpose of the process area and is an informative component. For example, the purpose statement of the Organizational Process Definition process area is “The purpose of Organizational Process Definition (OPD) is to establish and maintain a usable set of organizational process assets and work environment standards.” Introductory Notes The introductory notes section of t he process area describes the major concepts covered in the process area and is an informative component. An example from the introductory not es of the Project Planning process area is “Planning begins with require ments that define the product and project.” Related Process Areas The related process areas section li sts references to related process areas and reflects the high-level relationships among the process areas. The related process area se ction is an informative component. An example of a reference found in t he related process areas section of the Project Planning process area is “Refer to the Risk Management process area for more informat ion about identifying and managing risks.” Specific Goals A specific goal describes the uni que characteristics that must be present to satisfy the process area. A specific goal is a required model component and is used in appraisa ls to help determine whether a process area is satisfied. For example, a specific goal from the Configuration Management process area is “Integrity of base lines is establis hed and maintained.” Only the statement of the specific goal is a required model component. The title of a specific goal (preced ed by the goal number) and any notes associated with the goal are consi dered informative model components. Generic Goals Generic goals are called “generic” because the same goal statement applies to multiple process areas. A generic goal describes the characteristics that mu st be present to instit utionalize the processes",
        "CMMI for Development Version 1.2 Process Area Components 20 that implement a process area. A generic goal is a required model component and is used in appraisals to determine whether a process area is satisfied. (See the Generic Goals and Generic Practices section on page 75 for a more detailed description of generic goals.) An example of a generic goal is “The process is institutionalized as a defined process.” Only the statement of the generic goal is a required model component. The title of a generic goal (preceded by the goal number) and any notes associated with the goal are consi dered informative model components. Specific Goal and Practice Summaries The specific goal and practice summa ry provides a high-level summary of the specific goals, which are required components, and the specific practices, which are expected components. The specific goal and practice summary is an informative component. Specific Practices A specific practice is the descripti on of an activity t hat is considered important in achieving the associ ated specific goal . The specific practices describe the activities that are expected to result in achievement of the specif ic goals of a process area. A specific practice is an expected model component. For example, a specific practice fr om the Project Monitoring and Control process area is “Monitor commitment s against those identified in the project plan.” Only the statement of the specif ic practice is an expected model component. The title of a specific pr actice (preceded by the practice number) and any notes associated wi th the specific practice are considered informativ e model components. Typical Work Products The typical work products section li sts sample output from a specific practice. These examples are ca lled typical work products because there are often other work products that are just as effective but are not listed. A typical work product is an informative model component. For example, a typical work product for the specific practice “Monitor the actual values of t he project planning parameters against the project” in the Project Monitoring and Cont rol process area is “Records of significant deviations.”",
        "CMMI for Development Version 1.2 Process Area Components 21Subpractices A subpractice is a detailed descr iption that provides guidance for interpreting and implementing a specific or generic practice. Subpractices may be worded as if pr escriptive, but are actually an informative component meant only to provide ideas that may be useful for process improvement. For example, a subpractice for the specific practice “Take corrective action on identified issues” in t he Project Monitoring and Control process area is “Determine and document the appropriate actions needed to address the identified issues.” Generic Practices Generic practices are called “ge neric” because the same practice applies to multiple process areas. A generic practice is the description of an activity that is considered im portant in achieving the associated generic goal. A generic practice is an expected model component. For example, a generic practice fo r the generic goal “The process is institutionalized as a managed process” is “Provide adequate resources for performing the process, developi ng the work products, and providing the services of the process.” Only the statement of the generi c practice is an expected model component. The title of a generic practice (preceded by the practice number) and any notes associated wi th the practice are considered informative model components. To reduce the repetitiveness of this information and to conserve the number of pages required to present this information, only the generic practice title, statement, and elabo rations appear in the process areas. (See the Generic Goals and Generic Practices section on page 75 for a complete description of the generic practices.) Generic Practice Elaborations A generic practice elaboration appear s after a generic practice in a process area to provide guidance on how the generic practice should be applied uniquely to the process area. A generic practice elaboration is an informative model component. For example, a generic practice el aboration after the generic practice “Establish and maintain an organizational policy for planning and performing the project planning pr ocess” in the Project Planning process area is “This policy estab lishes organizational expectations for estimating the planning parameters, making internal and external commitments, and developing the plan for managing the project.”",
        "CMMI for Development Version 1.2 Process Area Components 22 Supporting Informative Components In many places, further informat ion is needed to describe a concept. This informative material is prov ided in the form of the following components: • Notes • Examples • Amplifications • References Notes A note is text that can accompan y nearly any other model component. It may provide detail, background, or rationale. A note is an informative model component. For example, a note that accompanies the specific prac tice “Implement the selected action proposals that we re developed in causal analysis” in the Causal Analysis and Resolution process area is “Only changes that prove to be of value should be cons idered for broad implementation.” Examples An example is a component comprising text and often a list of items, usually in a box, that can accompany nearly any other component and provides one or more examples to clarify a concept or described activity. An example is an informative model component. The following is an example that accompanies the subpractice “Document noncompliance issues when they cannot be resolved within the project” under the specific prac tice “Communicate quality issues and ensure resolution of noncomplia nce issues with the staff and managers” in the Process and Product Quality Assurance process area. Examples of ways to resolve noncompliance within the project include the following: • Fixing the noncompliance • Changing the process descriptions, standards, or procedures that were violated • Obtaining a waiver to cover the noncompliance issue Amplifications An amplification is a note or exampl e that is relevant to a particular discipline. The disciplines covered in this model are hardware engineering, systems engineering, and software engineering.",
        "CMMI for Development Version 1.2 Process Area Components 23Each amplification is labeled with a heading that indicates the discipline to which it applies. For exampl e, an amplification for software engineering is labeled “For Software Engineering.” An amplification is an informative model component. An example of an amplification is t he one that accompani es the specific practice “Establish and maintain the overall project plan content” in the Project Planning process area. The am plification states “For Hardware Engineering: For hardware, the planni ng document is often referred to as a hardware development plan. Deve lopment activities in preparation for production may be included in the hardware development plan or defined in a separat e production plan.” References A reference is a pointer to additional or more detailed information in related process areas and can accompany nearly any other model component. A reference is an informative model component. For example, a reference that accomp anies the specific practice “Select the subprocesses that compose t he project's defined process based on historical stability and capability dat a” in the Quantitative Project Management process area is “Refer to the Organizational Process Definition process area for more information about the organization's process asset library, which might include a process element of known and needed capability.” Numbering Scheme Specific and generic goals are number ed sequentially. Each specific goal begins with the prefix SG (e.g., SG 1). Each generic goal begins with the prefix GG (e.g., GG 2). Each specific practice begins with the prefix SP, followed by a number in the form x.y (e.g., SP 1.1). The x is the same number as the goal to which the specific prac tice maps. The y is t he sequence number of the specific practice under the specific goal. An example of specific practice numbering is in the Project Planning process area. The first specific practice is numbered SP 1.1 and the second is SP 1.2.",
        "CMMI for Development Version 1.2 Process Area Components 24 Each generic practice begins with t he prefix GP, followed by a number in the form x.y (e.g., GP 1.1). The x corresponds to the number of the generic goal. The y is the sequence number of the generic pr actice under the generic goal. For example, the first generic practice associated with GG 2 is numbered GP 2.1 and the second is GP 2.2. Typographical Conventions The typographical conventions used in this model were designed to enable you to select what you need an d use it effectively. We present model components in formats that allo w you to find them quickly on the page. Figures 2.2 through 2.4 are sample pages from process areas in Part Two; they show the different proc ess area components, labeled so that you can identify them. Notice that components differ typographically so that you can easily identify each one.",
        "CMMI for Development Version 1.2 Process Area Components 25 specific goal specific practice typical work product subpractice referenceexamplesspecific goal andpractice summary Figure 2.2: Sample Page from CAR",
        "CMMI for Development Version 1.2 Process Area Components 26 amplifications note addition subpracticetypical work products Figure 2.3: Sample Page from VER",
        "CMMI for Development Version 1.2 Process Area Components 27 generic practicegeneric goal additionGP elaborationcontinuous only box staged only box Figure 2.4: Sample Page from IPM+IPPD Representation-Specific Content In Part Two, you will notice that some components in the Generic Practices by Goal section of each process area are in a box and labeled “Staged Only,” “Continuous Only,” or “Continuous/Mat urity Levels 3–5.” Components that are not marked apply to both representations. Components marked “Staged Only” ap ply only if you are using the staged representation. Component s marked “Continuous Only” apply only if you are using t he continuous representati on. (See Figure 2.4 for an example.)",
        "CMMI for Development Version 1.2 Process Area Components 28 Components marked “Continuous/Maturit y Levels 3–5” apply if you are using the continuous r epresentation or if y ou are using the staged representation and are pursuing maturity level 3, 4, or 5. However, these components do not apply if you are pursuing a maturity level 2 rating using the st aged representation. Additions An addition can be informative material , a specific practice, a specific goal, or a process area that ex tends the scope of a model or emphasizes a particular aspect of its us e. In this document, all additions apply to IPPD. An example of an addition is the one from the Organizational Training process area that appears after specific goal 1, “Establish an Organizational Training Capabilit y.” The addition states “Cross- functional training, leadership traini ng, interpersonal skills training, and training in the skills needed to integrate appropriate business and technical functions is needed by integrated team members. The potentially wider range of require ments and participant backgrounds may require relevant stakeholders who were not involved in requirements development to take cross training in the disciplines involved in product design in order to commit to requirements with a full understanding of the range of require ments and their interrelationships.”",
        "CMMI for Development Version 1.2 Tying It All Together 293 Tying It All Together Now that you have been introduced to the components of CMMI models, you need to understand how they all fit together to meet your process improvement needs [Dym ond 2004]. In this chapter, we introduce the concept of levels and show how the process areas are organized and used. To do this, we need to revisit the discussion that began in Chapter 1. Understanding Levels Levels are used in CMMI to describe an evolutionary path recommended for an organization that wants to improve the processes it uses to develop and maintain its products and services. Levels can also be the outcome of the rating activity of appraisals.7 Appraisals can be performed for organizations that comprise entire (usually small) companies, or for smaller groups su ch as a group of projects or a division within a company. CMMI supports two improvem ent paths. One path enables organizations to incrementally im prove processes corresponding to an individual process area (or process ar eas) selected by the organization. The other path enables organizations to improve a set of related processes by incrementally addressi ng successive sets of process areas. These two improvement paths are associated with the two types of levels that correspond to the two representations discussed in Chapter 1. For the continuous repr esentation, we use the term “capability level.” For the staged representation, we use the term “maturity level.” Regardless of which representation you select, the concept of levels is the same. Levels characterize improv ement from an ill-defined state to a state that uses quantitative in formation to determine and manage improvements that are needed to meet an organization’s business objectives. To reach a particular level, an organization must satisfy all of the appropriate goals of the process area or set of process areas that are 7 For more information about appraisals, refer to Appraisal Requirements for CMMI and the Standard CMMI Appraisal Method for Process Improvement Method Definition Document [SEI 2006a, SEI 2006b].",
        "CMMI for Development Version 1.2 Tying It All Together 30 targeted for improvement, regardless of whether it is a capability or a maturity level. Both representations also prov ide ways to implement process improvement to achieve business obj ectives. Both representations provide the same essential c ontent and use the same model components. Structures of the Conti nuous and Staged Representations Figure 3.1 illustrates the struct ures of the c ontinuous and staged representations. The diffe rences jump out at y ou immediately when you look at the structure of both repr esentations. The st aged representation utilizes maturity levels, whereas t he continuous repres entation utilizes capability levels. Process Areas Capability LevelsContinuous Representation Generic PracticesGeneric Goals Specific Goals Specific PracticesMaturity LevelsStaged RepresentationGeneric PracticesGeneric Goals Specific Goals Specific Practices Process Areas Figure 3.1: Structure of the Cont inuous and Staged Representations",
        "CMMI for Development Version 1.2 Tying It All Together 31What may strike you as you compare these two represent ations is their similarity. Both have many of the same components (e.g., process areas, specific goals, and specific practice s), and these components have the same hierarchy and configuration. What is not readily apparent from t he high-level view in Figure 3.1 is that the continuous re presentation focuses on process area capability as measured by capability levels and the staged representation focuses on organizational maturity as meas ured by maturity levels. These dimensions (the capability/maturity dimensions) of CMMI are used for benchmarking and appraisal activi ties, as well as guiding an organization’s improvement efforts. • Capability levels, which belong to a continuous r epresentation, apply to an organization’s process improvement achievement in individual process areas. T hese levels are a means for incrementally improving the processes corresponding to a given process area. There are six capab ility levels, numbered 0 through 5. • Maturity levels, which belong to a staged representation, apply to an organization’s process improvement achievement across multiple process areas. These levels are a means of predicting the general outcomes of the next project underta ken. There are five maturity levels, numbered 1 through 5. Table 3.1 compares the si x capability levels to the five maturity levels. Notice that the names of four of the levels are the same in both representations. The differ ences are that there is no maturity level 0 for the staged represent ation, and at level 1, the capability level is Performed, whereas the maturity level is Initial. Therefore, the starting point is different for the two representations. Table 3.1 Comparison of Capa bility and Maturity Levels Level Continuous Representation Capability Levels Staged Representation Maturity Levels Level 0 Incomplete N/A Level 1 Performed Initial Level 2 Managed Managed Level 3 Defined Defined Level 4 Quantitatively M anaged Quantitatively Managed Level 5 Optimizing Optimizing",
        "CMMI for Development Version 1.2 Tying It All Together 32 The continuous repres entation is concerned with selecting both a particular process area to improve and the desired c apability level for that process area. In this contex t, whether a process is performed or incomplete is important. Therefore, the name “incomplete” is given to the continuous represent ation starting point. Because the staged repres entation is concerned with the overall maturity of the organization, whether individual processes are performed or incomplete is not the primary focus. Therefore, the name “initial” is given to the sta ged representation starting point. Both capability levels and maturity le vels provide a wa y to measure how well organizations can and do improve their processes. However, the associated approach to proce ss improvement is different. Understanding Capability Levels To support those using t he continuous representation, all CMMI models reflect capability levels in their design and content. A capability level consists of a generic goal and its re lated generic practices as they relate to a process area, whic h can improve the organization’s processes associated with that proc ess area. As you satisfy the generic goal and its generic practices at eac h capability level, you reap the benefits of process improvement for that process area. The six capability levels, designated by the numbers 0 through 5, are as follows: 0. Incomplete 1. Performed 2. Managed 3. Defined 4. Quantitatively Managed 5. Optimizing The fact that capability levels 2 through 5 use the same terms as generic goals 2 through 5 is intenti onal because each of these generic goals and practices reflects the meaning of the capab ility levels in terms of goals and practices you can impl ement. (See the Generic Goals and Generic Practices section on page 75 for more information about generic goals and practices.) A short description of each capability level follows.",
        "CMMI for Development Version 1.2 Tying It All Together 33Capability Level 0: Incomplete An “incomplete process” is a proces s that either is not performed or partially performed. One or more of the specific goal s of the process area are not satisfied, and no generic goals exist for this level since there is no reason to institutionaliz e a partially performed process. Capability Level 1: Performed A capability level 1 proce ss is characterized as a “performed process.” A performed process is a process that satisfies the specific goals of the process area. It supports and enables the work needed to produce work products. Although capability level 1 results in important improvements, those improvements can be lost over time if they are not institutionalized. The application of institutionalizati on (the CMMI generic practices at capability levels 2 through 5) helps to ensure that improvements are maintained. Capability Level 2: Managed A capability level 2 process is char acterized as a “m anaged process.” A managed process is a perf ormed (capability level 1) process that has the basic infrastructure in place to support the process. It is planned and executed in accordance with policy; employs skilled people who have adequate resources to produc e controlled outputs; involves relevant stakeholders; is monitored, controlled, and reviewed; and is evaluated for adherence to its pr ocess description. The process discipline reflected by capability leve l 2 helps to ensure that existing practices are retained during times of stress. Capability Level 3: Defined A capability level 3 proce ss is characterized as a “defined process.” A defined process is a managed (capabi lity level 2) process that is tailored from the organization’s set of standard processes according to the organization’s tailoring guidelines , and contributes work products, measures, and other process impr ovement information to the organizational process assets. A critical distinction between capabilit y levels 2 and 3 is the scope of standards, process descriptions, and pr ocedures. At capab ility level 2, the standards, process descriptions, and procedures may be quite different in each specif ic instance of the process (e.g., on a particular project). At capability level 3, t he standards, process descriptions, and procedures for a project are tailored from the organization’s set of standard processes to suit a particular project or organizational unit and",
        "CMMI for Development Version 1.2 Tying It All Together 34 therefore are more consistent, except for the differences allowed by the tailoring guidelines. Another critical distinction is that at capability level 3, processes are typically described more rigorously than at capability level 2. A defined process clearly states the purpose, inputs, entry crit eria, activities, roles, measures, verification steps, outputs, and exit criteria. At capability level 3, processes are managed more pr oactively using an understanding of the interrelationships of the process activities and deta iled measures of the process, its work products, and its services. Capability Level 4: Quantitatively Managed A capability level 4 process is c haracterized as a “quantitatively managed process.” A quantitativel y managed process is a defined (capability level 3) proces s that is controlled us ing statistical and other quantitative techniques. Q uantitative objectives for quality and process performance are established and us ed as criteria in managing the process. Quality and process perfo rmance is understood in statistical terms and is managed throughout the life of the process. Capability Level 5: Optimizing A capability level 5 proce ss is characterized as an “optimizing process.” An optimizing process is a quantitat ively managed (c apability level 4) process that is improved based on an understanding of the common causes of variation inher ent in the process. The focus of an optimizing process is on continually improvin g the range of process performance through both incremental and innovative improvements. Remember that capability levels 2 through 5 use the same terms as generic goals 2 through 5, and a detailed description of these terms appears in the Generic Goals and Generic Practices section on page 75. Advancing through Capability Levels The capability levels of a proce ss area are achieved through the application of generic practices or suit able alternatives to the processes associated with that process area. Reaching capability level 1 for a proc ess area is equivalent to saying that the processes associated with that process area are “performed processes.” Reaching capability level 2 for a proc ess area is equivalent to saying that there is a policy that indicate s you will perform the process. There is a plan for performing it, resource s are provided, responsibilities are assigned, training to perform it is provided, selected work products",
        "CMMI for Development Version 1.2 Tying It All Together 35related to performing the process ar e controlled, and so on. In other words, a capability level 2 proce ss can be planned and monitored just like any project or support activity. Reaching capability level 3 for a process area assumes that an organizational standard process exists associated with that process area, which can be tailored to the nee ds of the project. The processes in the organization are now more consistently def ined and applied because they are based on organi zational standard processes. Reaching capability level 4 for a process area assumes that this process area is a key business driv er that the organization wants to manage using quantitative and statistical techni ques. This analysis gives the organization more visibilit y into the performance of selected subprocesses that will make it more competitive in the marketplace. Reaching capability level 5 for a pr ocess area assumes that you have stabilized the selected subprocesses and that you want to reduce the common causes of variation within that process. Remember that variation is a natural occurrence in any process, so although it is conceptually feasible to improve all processes, it would not be economical to improve all processe s to level 5. Again, you would concentrate on those processes that would help you to meet your business objectives. Understanding Maturity Levels To support those using the staged re presentation, all CMMI models reflect maturity levels in their design and content. A maturity level consists of related spec ific and generic practices for a predefined set of process areas that improve the or ganization’s overall performance. The maturity level of an organization provides a way to predict an organization’s performance in a given discipline or set of disciplines. Experience has shown that organizati ons do their best when they focus their process improvement effort s on a manageable number of process areas at a time and that those areas require increasing sophistication as the organization improves. A maturity level is a defined evolutionary plateau for organizational process improvement. Each maturity level matures an important subset of the organization’s processes, pr eparing it to move to the next maturity level. The maturity levels are measured by the achievement of the specific and generic goals asso ciated with each predefined set of process areas.",
        "CMMI for Development Version 1.2 Tying It All Together 36 There are five maturity levels, eac h a layer in the foundation for ongoing process improvement, designated by the numbers 1 through 5: 1. Initial 2. Managed 3. Defined 4. Quantitatively Managed 5. Optimizing Remember that maturity levels 2 through 5 use the same terms as capability levels 2 through 5. This was intentional be cause the concepts of maturity levels and capability le vels are complementary. Maturity levels are used to characterize organi zational improvement relative to a set of process areas, and capability le vels characterize organizational improvement relative to an individual process area. Maturity Level 1: Initial At maturity level 1, processes are usually ad hoc and chaotic. The organization usually does not prov ide a stable environment to support the processes. Success in t hese organizations depends on the competence and heroics of the peopl e in the organization and not on the use of proven processes. In spit e of this chaos, maturity level 1 organizations often produce products and services that work; however, they frequently exceed their budget s and do not meet their schedules. Maturity level 1 organizations are c haracterized by a tendency to over commit, abandonment of processes in a time of crisis, and an inability to repeat their successes. Maturity Level 2: Managed At maturity level 2, the projects of the organization have ensured that processes are planned and executed in accordance with policy; the projects employ skilled people who have adequate resources to produce controlled outputs; invo lve relevant stakeholders; are monitored, controlled, and review ed; and are evaluated for adherence to their process descriptions. The process discipline reflected by maturity level 2 helps to ensure that existing practices are retained during times of stress. When these prac tices are in place, projects are performed and managed according to their documented plans. At maturity level 2, the status of the work products and the delivery of services are visible to management at defined points (e.g., at major milestones and at the completion of major tasks). Commitments are established among relev ant stakeholders and are revised as needed.",
        "CMMI for Development Version 1.2 Tying It All Together 37Work products are appropriately c ontrolled. The work products and services satisfy their specified process descriptions, standards, and procedures. Maturity Level 3: Defined At maturity level 3, processes are well characterized and understood, and are described in standards, proc edures, tools, and methods. The organization’s set of standard processe s, which is the basis for maturity level 3, is established and im proved over time. These standard processes are used to establish c onsistency across the organization. Projects establish thei r defined processes by tailoring the organization’s set of standard processes according to tailoring guidelines. (See the glossary for a definition of “organiza tion’s set of standard processes.”) A critical distinction between maturi ty levels 2 and 3 is the scope of standards, process descriptions, and pr ocedures. At maturity level 2, the standards, process descriptions, and procedures may be quite different in each specif ic instance of the process (e.g., on a particular project). At maturity level 3, t he standards, process descriptions, and procedures for a project are tailored from the organization’s set of standard processes to suit a particular project or organizational unit and therefore are more consistent, except for the differences allowed by the tailoring guidelines. Another critical distinction is that at maturity level 3, processes are typically described more rigorously t han at maturity level 2. A defined process clearly states the purpose, inputs, entry crit eria, activities, roles, measures, verification steps, outputs, and exit criteria. At maturity level 3, processes are managed more pr oactively using an understanding of the interrelationships of the process activities and deta iled measures of the process, its work products, and its services. At maturity level 3, the organizati on must further mature the maturity level 2 process areas. The generic pr actices associated with generic goal 3 that were not addressed at maturity level 2 are applied to achieve maturity level 3. Maturity Level 4: Quantitatively Managed At maturity level 4, the organizati on and projects es tablish quantitative objectives for quality and process perfo rmance and use them as criteria in managing processes. Quantitative objectives are based on the needs of the customer, end users, organi zation, and process implementers. Quality and process performance is understood in statistical terms and is managed throughout the life of the processes [SEI 2001].",
        "CMMI for Development Version 1.2 Tying It All Together 38 For selected subprocesses, detailed measures of process performance are collected and statistically analyzed. Quality and process- performance measures are incorporated into the organization’s measurement repository to s upport fact-based decision making [McGarry 2000]. Special causes of process variation are identified and, where appropriate, the sources of special causes are corrected to prevent future occurrences. (See t he definition of “special cause of process variation” in the glossary.) A critical distinction between maturity levels 3 and 4 is the predictability of process performance. At matu rity level 4, the performance of processes is controlled using st atistical and other quantitative techniques, and is quantitat ively predictable. At maturity level 3, processes are typically only qualitatively predictable. Maturity Level 5: Optimizing At maturity level 5, an organization continually improves its processes based on a quantitative understanding of the common causes of variation inherent in processes. (S ee the definition of “common cause of process variation” in the glossary.) Maturity level 5 focuses on conti nually improving process performance through incremental and innovat ive process and technological improvements. Quantitative process improvement objectives for the organization are established, conti nually revised to reflect changing business objectives, and used as criteria in managing process improvement. The effects of depl oyed process improvements are measured and evaluated against the quantitative process improvement objectives. Both the defined proce sses and the organization’s set of standard processes are targets of m easurable improvement activities. A critical distinction between maturi ty levels 4 and 5 is the type of process variation addressed. At matu rity level 4, the organization is concerned with addressing special ca uses of process variation and providing statistical predictability of the results. Al though processes may produce predictable results, the re sults may be insufficient to achieve the established objectives. At matu rity level 5, the organization is concerned with addressing common c auses of proces s variation and changing the process (to shift the m ean of the process performance or reduce the inherent process variati on experienced) to improve process performance and to achieve the established quantitative process improvement objectives.",
        "CMMI for Development Version 1.2 Tying It All Together 39Advancing through Maturity Levels Organizations can achieve progre ssive improvements in their organizational maturity by achieving control first at the project level and continuing to the most advanced le vel—organization-wide continuous process improvement—using both quant itative and qualitative data to make decisions. Since improved organizational maturity is associated with improvement in the range of expected result s that can be achieved by an organization, it is one way of pr edicting the general outcomes of the organization’s next project. For inst ance, at maturity level 2, the organization has been elevated from ad hoc to disciplined by establishing sound project management. As your organization achieves the generic and specific goals for the se t of process areas in a maturity level, you are increasing your organizational maturity and reaping the benefits of process improvement. Bec ause each maturity level forms a necessary foundation for the next level, trying to skip maturity levels is usually counterproductive. At the same time, you must recognize that process improvement efforts should focus on the needs of the orga nization in the context of its business environment and that process areas at higher maturity levels may address the current needs of an organization or project. For example, organizations seeking to move from maturity level 1 to maturity level 2 are frequently enc ouraged to establish a process group, which is addressed by the Organiza tional Process Focus process area that resides at maturity level 3. Although a process group is not a necessary characteristic of a maturi ty level 2 organization, it can be a useful part of the organization’s approach to achieving maturity level 2. This situation is sometimes charac terized as establishing a maturity level 1 process group to bootstrap the maturity level 1 organization to maturity level 2. Maturity level 1 process improvement activities may depend primarily on the insight and competence of the process group staff until an infrastructure to s upport more disciplined and widespread improvement is in place. Organizations can institut e specific process impr ovements at any time they choose, even before they are pr epared to advance to the maturity level at which the specif ic practice is recommende d. In such situations, however, organizations should under stand that the success of these improvements is at risk because the foundation for their successful institutionalization has not been co mpleted. Processes without the proper foundation may fail at the very point they are needed most— under stress.",
        "CMMI for Development Version 1.2 Tying It All Together 40 A defined process that is characteristic of a maturity level 3 organization can be placed at great risk if maturi ty level 2 management practices are deficient. For example, management may commit to a poorly planned schedule or fail to control changes to baselined requirements. Similarly, many organizations prematurely colle ct the detailed dat a characteristic of maturity level 4, only to find the data uninterpret able because of inconsistencies in processe s and measurement definitions. Another example of using processe s associated with higher maturity- level process areas is in the buildin g of products. Certainly, we would expect maturity level 1 organizations to perform requirements analysis, design, integration, and verification. These activities are not described until maturity level 3, however, where they are described as the coherent, well-integrated engineering processes that complement a maturing project management capability, put in place so that the engineering improvements are not lost by an ad hoc management process.",
        "CMMI for Development Version 1.2 Tying It All Together 41Process Areas Process areas are viewed differently in the two representations. Figure 3.2 compares views of how process areas are used in the continuous representation and the staged representation. Process Area 1 Process Area 2 Process Area 3 Process Area 4 Process Area N CL1 CL2 CL3 CL4 CL5Continuous Target ProfileSelected Process Areas Targeted Capability Levels Staged = Groups of process areas chosen for process improvement to achieve maturity level 3Selected Maturity Level Maturity Level 5 Maturity Level 4 Maturity Level 2Maturity Level 3 REQMPPPMCSAMMAPPQACM Figure 3.2: Process Areas in Cont inuous and Staged Representations",
        "CMMI for Development Version 1.2 Tying It All Together 42 The continuous repres entation enables the organi zation to choose the focus of its process improvement efforts by choosing those process areas, or sets of interrelated pr ocess areas, that best benefit the organization and its business objective s. Although there are some limits on what an organization can choos e because of the dependencies among process areas, the organization has considerable freedom in its selection. To support those using the continuous represent ation, process areas are organized into four categor ies: Process Management, Project Management, Engineering, and Suppor t. These categories emphasize the relationships that exist among the process areas and are discussed in Chapter 4. Once you select the process areas, yo u must also select how much you would like to mature the processes associated with those process areas (i.e., select the appropriate c apability level). Capability levels and generic goals and practices support t he improvement of processes associated with individual process areas. For example, an organization may wish to strive to reach capab ility level 2 in one process area and capability level 4 in another. As t he organization reac hes a capability level, it sets its sights on the next capability level for one of these same process areas or decides to widen its view and address a larger number of process areas. This selection is typically descri bed through a target profile. A target profile defines all of the process areas to be addressed and the targeted capability level for each. This pr ofile then governs which goals and practices the organization will addr ess in its process improvement efforts. Most organizations will, at minimum, target capability level 1, which requires that all specific goals of the process area be achieved. However, organizations t hat target capability levels higher than 1 will concentrate on the instit utionalization of the selected processes in the organization by implementing the associated generic goals and practices. Conversely, you will see that the staged representati on encourages you to always look at process areas in the context of the maturity level to which they belong. The process areas are organized by maturity levels to reinforce this concept.",
        "CMMI for Development Version 1.2 Tying It All Together 43The staged representation provi des a predetermined path of improvement from maturity level 1 to maturity level 5 that involves achieving the goals of the process ar eas at each maturity level. To support those using t he staged representation, process areas are grouped by maturity level, indicati ng which process areas to implement to achieve each maturity level. For ex ample, at maturity level 2, there is a set of process areas that an organization would use to guide its process improvement until it could ac hieve all the goals of all these process areas. Once maturity le vel 2 is achieved this way, the organization focuses its efforts on maturity level 3 process areas, and so on. The generic goals that apply to each process area are also predetermined. Generic goal 2 applies to maturity level 2 and generic goal 3 applies to maturity levels 3 through 5. Table 3.2 provides a list of all process areas and their associated categories and maturity levels. To explain how the components of the process areas are viewed in each r epresentation, we must discuss how the representations addre ss specific practices.",
        "CMMI for Development Version 1.2 Tying It All Together 44 Table 3.2 Process Areas and Their A ssociated Categories and Maturity Levels Process Area Category Maturity Level Causal Analysis and Resolution Support 5 Configuration Management Support 2 Decision Analysis and Resolution Support 3 Integrated Project Management +IPPD Project Management 3 Measurement and Analysis Support 2 Organizational Innovation and Deployment Process Management 5 Organizational Process Definition +IPPD Process Management 3 Organizational Process Focus Process Management 3 Organizational Process Performance Process Management 4 Organizational Training Process Management 3 Product Integration Engineering 3 Project Monitoring and Control Project Management 2 Project Planning Project Management 2 Process and Product Qual ity Assurance Support 2 Quantitative Project Management Project Management 4 Requirements Development Engineering 3 Requirements Management Engineering 2 Risk Management Project Management 3 Supplier Agreement Management Project Management 2 Technical Solution Engineering 3 Validation Engineering 3 Verification Engineering 3",
        "CMMI for Development Version 1.2 Tying It All Together 45Generic Goals and Practices Generic goals are required model com ponents that apply to all process areas. Figure 3.3 illustrates the generic goals and practices. All of the generic goals and practices are used in the continuous representation. (See the Generic Goals and Generic Practices section on page 75 for a more detailed description of generic goals and practices.) The capability level you are targeting for your im provement effort will determine which generic goals and practices you will appl y to the process area you have selected. Generic Goals and Practices GP 1.1 GP 2.1 GP 2.2 GP 2.3GP 3.1 GP 3.2 GP 2.4 GP 2.5 GP 2.6 GP 2.7 GP 2.8 GP 2.9 GP 2.10GP 4.1 GP 4.2GP 5.1 GP 5.2Goal 1 Goal 2 Goal 3 Goal 4 Goal 5 Generic practices used in the staged representation Figure 3.3: Generic Goals and Generic Practices In the staged representation, only generic goals 2 and 3 are used, as illustrated by the generic practices hi ghlighted in gray in Figure 3.3. When you try to reach maturity leve l 2, you use the process areas at maturity level 2 as well as generic goal 2 and its generic practices. Notice that generic goals 4 and 5 and their associated generic practices are not used. This is because not all processes will be “raised” above (i.e., matured beyond) a defined proc ess. Only select processes and subprocesses will be quantitatively managed and optimized, and which",
        "CMMI for Development Version 1.2 Tying It All Together 46 processes and subprocesses are sele cted is addressed by the process areas at maturity levels 4 and 5. When you reach maturity levels 3, 4, and 5, you use the process areas at the appropriate maturity levels as well as all of those at the lower maturity levels. In addition, generic goal 3 and its associated generic practices (which include the generic practices associated with generic goal 2) are applied to all of these process areas. This means that even though you have already achieved a matu rity level 2 rating, to achieve a maturity level 3 rating you must retu rn to the maturity level 2 process areas and apply generic goal 3 and its generic practices as well. Representation Comparison Table 3.3 summarizes the differences between the two representations. Table 3.3 Comparing Conti nuous and Staged Repr esentations Continuous Representation Staged Representation The organization selects process areas and capability levels based on its process improvement objectives. The organization selects process areas based on the maturity levels. Improvement is measured using capability levels. Capability levels • Measure maturity of a par- ticular process across an organization. • Range from 0 through 5. Improvement is measured using maturity levels. Maturity levels • Measure maturity of a set of processes across an or- ganization. • Range from 1 through 5. Capability level profiles are used to target and track process improvement performance. Maturity levels are used to target and track process improvement performance. Equivalent staging allows an organization using the continuous approach to process improvement to derive a maturity level as part of an appraisal. There is no need for an equivalence mechanism back to the continuous approach.",
        "CMMI for Development Version 1.2 Tying It All Together 47Equivalent Staging Equivalent staging is a way to compare results from using the continuous representatio n to those of the st aged representation. In essence, if you measured improvem ent relative to selected process areas using capability levels in t he continuous repr esentation, how would you compare that to matu rity levels? Is this possible? Up to this point, we have not di scussed process appraisals in much detail. The SCAMPISM method8 is used for appraising organizations using CMMI, and one result of an apprai sal is a rating [Ahern 2005]. If the continuous representat ion is used for an appraisal, the rating is a capability level profile. If the staged representation is used for an appraisal, the rating is a maturity le vel (e.g., maturity level 3) rating. A capability level profile is a list of process areas and the corresponding capability level achieved for each. This profile enables an organization to track its capability level by process area. The profile is an achievement profile when it r epresents the organization’s actual progress for each process area. Alter natively, the profile is a target profile when it represents the organization’s planned process improvement objectives. Figure 3.4 illustrates both a target profile and an achievement profile. The gray por tion of each bar represents what has been achieved. The unshaded porti on represents what remains to be accomplished to meet the target profile. Requirements Management Project Planning Project Monitoring and Control Supplier Agreement Management Measurement and Analysis Process and Product Quality Assurance Configuration Management Capability Level 1CapabilityLevel 2CapabilityLevel 3CapabilityLevel 4CapabilityLevel 5Requirements Management Project Planning Project Monitoring and Control Supplier Agreement Management Measurement and Analysis Process and Product Quality Assurance Configuration Management Capability Level 1CapabilityLevel 2CapabilityLevel 3CapabilityLevel 4CapabilityLevel 5 Figure 3.4: An Example of an Achiev ement Profile and a Target Profile 8 The SCAMPI method is described in chapter 5.",
        "CMMI for Development Version 1.2 Tying It All Together 48 An achievement profile, when compar ed with a target profile, enables an organization to plan and track its progress for each selected process area. Maintaining capability level pr ofiles is advisabl e when using the continuous representation. Target staging is a sequence of target profiles that describes the path of process improvement to be followed by the organization. When building target profiles, the organizati on should pay attention to the dependencies between generic prac tices and process areas. If a generic practice depends on a certain pr ocess area, either to carry out the generic practice or to provi de a prerequisite product, the generic practice may be much less effect ive when the process area is not implemented.9 Although there are many reasons to use the continuous representation, the ratings provided by capability leve l profiles are limited in their ability to provide organizations with a way to generally compare themselves with other organizations. Capability le vel profiles could be used if each organization selected the same proce ss areas; however, maturity levels have been used to compare organizations for years and already provide predefined sets of process areas. Because of this situat ion, equivalent staging wa s created. Equivalent staging enables an organization using t he continuous repr esentation for an appraisal to convert a capability level profile to the associated maturity level rating. The most effective way to depict equivalent staging is to provide a sequence of target profiles, each of which is equivalent to a maturity level rating of the staged representatio n. The result is a target staging that is equivalent to the maturity levels of the stag ed representation. 9 See Table 6.2 on page 95 in the Generic Goals and Generic Practices s ection for more information about the dependencies between generic practices and process areas.",
        "CMMI for Development Version 1.2 Tying It All Together 49Figure 3.5 shows a summary of t he target profiles that must be achieved when using the continuous representatio n to be equivalent to maturity levels 2 through 5. Each shaded area in the capability level columns represents a target profile that is equivalent to a maturity level. Name Abbr ML CL1 CL2 CL3 CL4 CL5 Requirements Management REQM 2 Project Planning PP 2 Project Monitoring and Control PMC 2 Supplier Agreement Manage- ment SAM 2 Measurement and Analysis MA 2 Process and Product Quality Assurance PPQA 2 Configuration Management CM 2 Target Profile 2 Requirements Development RD 3 Technical Solution TS 3 Product Integration PI 3 Verification VER 3 Validation VAL 3 Target Profile 3 Organizational Process Focus OPF 3 Organizational Process Definition +IPPD OPD +IPPD 3 Organizational Training OT 3 Integrated Project Manage-ment +IPPD IPM +IPPD 3 Risk Management RSKM 3 Decision Analysis and Resolu-tion DAR 3 Organizational Process Per- formance OPP 4 Quantitative Project Manage-ment QPM 4 Target Profile 4 Organizational Innovation and Deployment OID 5 Causal Analysis and Resolu-tion CAR 5 Target Profile 5 Figure 3.5: Target Prof iles and Equivalent Staging",
        "CMMI for Development Version 1.2 Tying It All Together 50 The following rules summarize equivalent staging: • To achieve maturity level 2, a ll process areas assigned to maturity level 2 must achieve capability level 2 or higher. • To achieve maturity level 3, a ll process areas assigned to maturity levels 2 and 3 must achieve capability level 3 or higher. • To achieve maturity level 4, a ll process areas assigned to maturity levels 2, 3, and 4 must achieve capability level 3 or higher. • To achieve maturity level 5, all process areas must achieve capability level 3 or higher. These rules and the table for equivalent staging are complete; however, you may ask why target profiles 4 and 5 do not extend into the CL4 and CL5 columns. The reason is that t he maturity level 4 process areas describe a selection of the subproc esses to be stabilized based, in part, on the quality and process-performance objectives of the organization and projects. Not every process area will be addressed in the selection and CMMI does not presume in advan ce which process areas might be addressed in the selection. So, the achievement of capability le vel 4 for process areas cannot be predetermined because the choices depend on the selections made by the organization in its implementation of the maturity level 4 process areas. Thus, Figure 3.5 does not show target profile 4 extending into the CL4 column, although some process areas will have achieved capability level 4. The situation for matu rity level 5 and target profile 5 is similar. The existence of equivale nt staging should not discourage users of the continuous representation from establishing target profiles that extend above capability level 3. Such a tar get profile would be determined in part by the selections made by the organization to meet its business objectives.",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 514 Relationships Among Process Areas In this chapter, we describe intera ctions among process areas to help you see the organization’s view of process improvement and which process areas build on the implem entation of other process areas. Relationships among process areas are presented in two dimensions. The first dimension comprises the in teractions of individual process areas that show how information and artifacts flow from one process area to another. Shown by the multiple figures and descriptions in this chapter, these interactions help y ou see a larger view of process improvement. The second dimension comprises the in teractions of groups of process areas. Shown by the classification of some process areas as Basic and others as Advanced, these classifi cations illustrate that the Basic process areas should be implement ed before the Advanced process areas to ensure that the prerequi sites are met to successfully implement the Advanced process areas. Successful process improvement init iatives must be driven by the business objectives of the organi zation. For example, a common business objective is to reduce the ti me it takes to get a product to market. The process improvement objec tive derived from that might be to improve the project management processes to ensure on-time delivery; those improvements rely on best practices in the Project Planning and Project Monitori ng and Control process areas. Four Categories of CMMI Process Areas Process areas can be grouped into four categories: • Process Management • Project Management • Engineering • Support Although we are grouping process areas this way to discuss their interactions, process areas oft en interact and have an effect on one another regardless of their defined gr oup. For example, the Decision Analysis and Resolution process area provides specific practices to",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 52 address the formal evaluation that is used in the Technical Solution process area for selecting a tec hnical solution from alternative solutions. Technical Solution is an Engineering process area and Decision Analysis and Resoluti on is a Support process area. Being aware of the interactions t hat exist among CMMI process areas and which process areas are Basi c and Advanced will help you apply CMMI in a useful and productive wa y. The following sections describe the interactions of process areas within the categories and only briefly describe the interactions among proc ess areas in other categories. Interactions among process areas t hat belong to different categories are described in references within the Related Process Areas section of the process areas in Part Two. Refe r to Chapter 2 for more information about references. Process Management Process Management process areas cont ain the cross-project activities related to defining, planning, deploy ing, implementing, monitoring, controlling, appraising, measuri ng, and improving processes. The Process Management process ar eas of CMMI are as follows: • Organizational Process Focus • Organizational Process Definition +IPPD10 • Organizational Training • Organizational Process Performance • Organizational Innovation and Deployment Basic Process Management Process Areas The Basic Process Management process areas provide the organization with a capability to document and share best practices, organizational process assets, and learning across the organization. Figure 4.1 provides a bird’s-eye vi ew of the interactions among the Basic Process Management process areas and with other process area categories. As illustrated in Figur e 4.1, the Organizational Process Focus process area helps the organization to plan, implement, and deploy organizational process improvements based on an understanding of the current st rengths and weaknesses of the organization’s processes and process assets. 10 Organizational Process Definition (OPD) has one goal that applies only when using CMMI with the IPPD group of additions.",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 53OPF OPD+IPPD Resources and coordinationOT Standard process and other assetsTraining for projects and support groups in standard process and assetsOrganization’s process needs and objectives Organization’s process needs and objectives Standard process, work environment standards, and other assets.Senior management Organization’s business objectives Project Management, Support, and Engineering process areasTraining needs Improvement information (e.g., lessons learned, data, and artifacts) Process-improvement proposals; participation in defining, assessing, and deploying processes OPF = Organizational Process Focus OT = Organizational Training OPD+IPPD = Organizational Process Definition (with the IPPD addition) Figure 4.1: Basic Process Management Process Areas Candidate improvements to the orga nization’s processes are obtained through various means. These include process improvement proposals, measurement of the processes, le ssons learned in implementing the processes, and results of proc ess appraisal and product evaluation activities. The Organizational Process Defini tion process area establishes and maintains the organization’s set of standard processes, work environment standards, and other a ssets based on the process needs and objectives of the organizati on. These other assets include descriptions of lifecycle models, process tailoring guidelines, and process-related documentation and data. Projects tailor the organization’s set of standard pr ocesses to create their defined processes. The other assets support tailoring as well as implementation of the defined processes. Exper iences and work products from performing these defined processe s, including measurement data, process descriptions, process arti facts, and lessons learned, are incorporated as appropriate into the organization’s set of standard processes and other assets. With the +IPPD addition, Organizational Process Definition +IPPD provides IPPD rules and guidelines to the projects. The Organizational Training proce ss area identifies the strategic training needs of the organization as well as the tactical training needs that are common across projects and support groups. In particular, training is developed or obtained to develop the skills required to perform the organization’s set of standard processes. The main",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 54 components of training include a managed training development program, documented plans, personn el with appropriate knowledge, and mechanisms for measuring the effectiveness of the training program. Advanced Process Management Process Areas The Advanced Process Management process areas provide the organization with an improved capab ility to achieve its quantitative objectives for quality and process performance. Figure 4.2 provides a bird’s-eye vi ew of the interactions among the Advanced Process Management proce ss areas and with other process area categories. Each of the Ad vanced Process Management process areas depends on the ability to develop and deploy processes and supporting assets. The Basic Pr ocess Management process areas provide this ability. Senior managementOPPProgress toward achieving business objectivesOID Quality and process - performance objectives, measures, baselines, and modelsCost and benefitdata from pilotedimprovementsCost and benefitdata from pilotedimprovements Quality and process- performance objectives, measures, baselines, and models Process performance and capability data Basic Process Management process areasProject Management, Support, and Engineering process areas Ability to develop and deploy standard process and other assetsOrganizationImprovements Common measuresCommon measures OID = Organizational Innovation and Deployment OPP = Organizational Process Performance Figure 4.2: Advanced Proc ess Management Process Areas As illustrated in Figure 4.2, t he Organizational Process Performance process area derives quantitative objectives for quality and process performance from the organization’s business objectives. The organization provides projects and support groups with common measures, process-performance bas elines, and process-performance models. These additional organization al assets support quantitative project management and statisti cal management of critical subprocesses for both projects and support groups. The organization analyzes the process-performance data collected from these defined processes to develop a quantitativ e understanding of product quality,",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 55service quality, and process perform ance of the organization’s set of standard processes. The Organizational Innovation and De ployment process area selects and deploys proposed incremental and innovative improvements that improve the organization’s ability to meet its quality and process- performance objectives. The identific ation of promising incremental and innovative improvements should in volve the participation of an empowered workforce aligned with th e business values and objectives of the organization. The selection of improvements to deploy is based on a quantitative understanding of t he likely benefits and predictable costs of deploying candidate impr ovements, and the funding available for such deployment. Project Management Project Management process areas cover the project management activities related to planning, m onitoring, and controlling the project. The Project Management process ar eas of CMMI are as follows: • Project Planning • Project Monitoring and Control • Supplier Agreement Management • Integrated Project Management +IPPD11 • Risk Management • Quantitative Project Management Basic Project Management Process Areas The Basic Project Management proce ss areas address the activities related to establishing and maintain ing the project plan, establishing and maintaining commitments, moni toring progress against the plan, taking corrective action, and managing supplier agreements. Figure 4.3 provides a bird’s-eye vi ew of the interactions among the Basic Project Management process areas and with other process area categories. As illustrated in Figur e 4.3, the Projec t Planning process area includes developing the projec t plan, involving stakeholders appropriately, obtaining commitment to the plan, and maintaining the plan. When using IPPD, stakeholders r epresent not just the technical expertise for product and process development, but also the business implications of product and process development. 11 Integrated Project Management (IPM) has one goal that applies only when using CMMI with the IPPD group of additions.",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 56 PPWhat to build What to do SAMPMC What to monitorReplan PlansStatus, issues, and results of reviews and monitoring Product component requirements, technical issues, completed product components, and acceptance reviews and testsEngineering and Support process areas Measurement needs SupplierSupplier agreementCorrective action PMC = Project Monitoring and Control PP = Project Planning SAM = Supplier Agreement ManagementCommitmentsCorrective actionStatus, issues, and results of process and product evaluations; measures and analyses Figure 4.3: Basic Project Management Process Areas Planning begins with requirements that define the product and project (“What to Build” in Figure 4.3). The project plan covers the various project management and development activities performed by the project. The project reviews other pl ans that affect the project from various relevant stakeholders and es tablish commitments with those stakeholders for their cont ributions to the projec t. For example, these plans cover configuration managemen t, verification, and measurement and analysis. The Project Monitoring and Contro l process area includes monitoring activities and taking corrective acti on. The project plan specifies the appropriate level of project monitoring, the frequency of progress reviews, and the measures used to monitor progress. Progress is determined primarily by comparing proj ect status to t he plan. When the actual status deviates significantly from the expected values, corrective actions are taken as appropria te. These actions may include replanning. The Supplier Agreement Management process area addresses the need of the project to acquire those portions of work that are produced by suppliers. Sources of products that may be used to satisfy project requirements are proactively identifi ed. The supplier is selected, and a supplier agreement is establis hed to manage the supplier. The supplier’s progress and performance ar e tracked by monitoring selected work products and processes, and the supplier agreement is revised as appropriate. Acceptance reviews and tests are conducted on the supplier-produced product component.",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 57Advanced Project Management Process Areas The Advanced Project Management pr ocess areas address activities such as establishing a defined proc ess that is tailored from the organization’s set of standard proce sses, establishing the project work environment from the organization’ s work environment standards, coordinating and collaborating with relevant stakeholders, managing risk, forming and sustaining int egrated teams for the conduct of projects, and quantitatively managing the project’s defined process. Figure 4.4 provides a bird’s-eye vi ew of the interactions among the Advanced Project Management proce ss areas and with other process area categories. Each Advanced Project Management process area depends on the ability to plan, moni tor, and control the project. The Basic Project Management process areas provide this ability. Process performance objectives, baselines, and models QPM Organization’s standard processes, work environment standards, and supporting assets IPM+IPPDRSKM Lessons learned, planning, and performance data Lessons learned, planning, and performance data Project performance dataStatistical management data Risk taxonomies and parameters, risk status, risk mitigation plans, and corrective actionProcess Management process areas Basic Project Management process areasRisk exposure due to unstable processes Quantitative objectives, subprocessesto statistically manage, project’s composed, and defined process Identified risks Engineering and Support process areasCoordination, commitments, and issues to resolveProduct architecture for structuring teamsProject’s composed and defined processProject’s shar ed vision IPM+IPPD = Integrated Project Management (with the IPPD addition) QPM = Quantitative Project Management RSKM = Risk ManagementProject’s defined process and work environmentIPPD rules and guidelines Integrated teams for performing engineering and support processes Figure 4.4: Advanced Proj ect Management Process Areas The Integrated Project Management process area establishes and maintains the project’s defined proc ess that is tailored from the organization’s set of standard proce sses. The project is managed using the project’s defined process. The pr oject uses and cont ributes to the organization’s process assets. The project’s work environment is established and maintained from the organization’s work environment standards. The management of the project ensur es that the relevant stakeholders associated with the project coordinate their efforts in a timely manner. It does this by providing for the m anagement of stakeholder involvement; the identification, negot iation, and tracking of critical dependencies; and",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 58 the resolution of coordination issues within the project and with relevant stakeholders. With the +IPPD addition, Int egrated Project Management +IPPD establishes and maintains the s hared vision of the project and an integrated team structure for the pr oject and then establishes integrated teams to perform the work of t he project, ensuring the appropriate collaboration across teams. Although risk identification and monito ring are covered in the Project Planning and Project Monitoring and Control process areas, the Risk Management process area takes a continuing, forward-looking approach to managing risks with activiti es that include identification of risk parameters, risk assessments, and risk mitigation. The Quantitative Project Management process area applies quantitative and statistical techniques to m anage process performance and product quality. Quality and process-performanc e objectives for the project are based on the objectives established by the organization. The project’s defined process comprises, in part, process elements and subprocesses whose process perfo rmance can be predicted. At a minimum, the process variation expe rienced by subprocesses critical to achieving the project’s quality and process-performance objectives is understood. Corrective action is take n when special causes of process variation are identified. (See the definition of “s pecial cause of process variation” in the glossary.) Engineering Engineering process areas cover the development and maintenance activities that are shared ac ross engineering disciplines. The Engineering process areas were written using general engineering terminology so that any technical discipline involved in the product development process (e.g., software engineering or mechanical engineering) can use them for process improvement. The Engineering process areas also integrate the processes associated with different engineering disciplines into a single product development process, supporting a product-orient ed process improvement strategy. Such a strategy targets essentia l business objectives rather than specific technical disciplines. This approach to processes effectively avoids the tendency toward an organi zational “stovepipe” mentality. The Engineering process areas appl y to the development of any product or service in the developm ent domain (e.g., software products, hardware products, services, or processes).",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 59The technical foundation for IPPD is grounded in a robust systems engineering approach that encompasses development in the context of the phases of the product’s life. T he Engineering process areas provide this technical foundation. The impl ementation of IPPD is further addressed through amplifications to specific practices in the Engineering process areas that em phasize concurrent development and focus on all phases of the product’s life. The Engineering process areas of CMMI are as follows: • Requirements Development • Requirements Management • Technical Solution • Product Integration • Verification • Validation Figure 4.5 provides a bird’s-eye view of the interactions among the six Engineering process areas. RD PI VALTS VERREQMRequirements Customer needsProduct and product component requirements Product components, work products, verification and validation reportsProductcomponents Alternative solutions RequirementsProduct Customer PI = Product Integration RD = Requirements DevelopmentREQM = Requirements Management TS = Technical Solution VAL = ValidationVER = Verification Figure 4.5: Engineering Process Areas The Requirements Development process area identifies customer needs and translates these needs into pr oduct requirements. The set of product requirements is analyzed to produce a high-level conceptual solution. This set of requirements is then allocated to establish an initial set of product component requirement s. Other requirements that help define the product are derived and allocated to product components.",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 60 This set of product and product component requirements clearly describes the product’s performanc e, design features, verification requirements, and so forth, in terms the developer understands and uses. The Requirements Development process area supplies requirements to the Technical Solution process ar ea, where the requirements are converted into the product architec ture, the product component design, and the product component itself (e.g., coding and fabrication). Requirements are also supplied to the Product Integration process area, where product components ar e combined and interfaces are verified to ensure that they meet the interface requirements supplied by Requirements Development. The Requirements Management process area maintains the requirements. It describes activi ties for obtaining and controlling requirement changes and ensuring th at other relevant plans and data are kept current. It provides traceab ility of requirements from customer to product to product component. Requirements Management ensures t hat changes to requirements are reflected in project plans, activities , and work products. This cycle of changes may affect all the other Engineering process areas; thus, requirements management is a dynam ic and often recursive sequence of events. The Requirements Managem ent process area is fundamental to a controlled and disciplined engineering design process. The Technical Solution process ar ea develops technical data packages for product components that will be us ed by the Product Integration or Supplier Agreement Management proc ess area. Alternative solutions are examined with the intent of selecting the optimum design based on established criteria. These criteria ma y be significantly different across products, depending on product ty pe, operational environment, performance requirements, support re quirements, and cost or delivery schedules. The task of selecting the final solution makes use of the specific practices in the Decision Analysis and Resolution process area. The Technical Solution process area re lies on the specific practices in the Verification process area to perform design verification and peer reviews during design and prior to final build. The Verification process area ensures that selected work products meet the specified requirements. The Veri fication process area selects work products and verification methods t hat will be used to verify work products against specifi ed requirements. Verification is generally an incremental process, starting wi th product component verification and usually concluding with verificati on of fully assembled products. Verification also addresses peer reviews. Peer reviews are a proven method for removing defects early and provide valuable insight into the",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 61work products and product components being developed and maintained. The Validation process area increm entally validates products against the customer’s needs. Validation ma y be performed in the operational environment or in a simulated operational environment. Coordination with the customer on the validat ion requirements is an important element of this process area. The scope of the Validation proc ess area includes validation of products, product components, select ed intermediate work products, and processes. These validated elements may often require reverification and revalidation. Iss ues discovered during validation are usually resolved in the Requirements Development or Technical Solution process area. The Product Integration process area contains the spec ific practices associated with generating the bes t possible integration sequence, integrating product components, and delivering the product to the customer. Product Integration uses the specific practices of both Verification and Validation in implementi ng the product integration process. Verification practices verify the interfaces and interface requirements of product components prior to product integrati on. This is an essential event in the integration process. During pr oduct integration in the operational environment, the spec ific practices of the Validation process area are used. Recursion and Iteration of Engineering Processes Most process standards agree that t here are two ways that processes can be applied. These two ways ar e called recursion and iteration. Recursion occurs when a process is applied to successive levels of system elements within a system structure. The outcomes of one application are used as inputs to the ne xt level in the system structure. For example, the verification proce ss is designed to apply to the entire assembled product, the major product components, and even components of components. How far into the product you apply the verification process depends entirely on the size and complexity of the end product. Iteration occurs when processes are r epeated at the same system level. New information is created by the implementation of one process that feeds back into a related process. Th is new information typically raises questions that must be resolved befor e completing the processes. For example, iteration will most li kely occur between requirements development and technical solution. R eapplication of the processes can",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 62 resolve the questions that are raised. Iteration can ensure quality prior to applying the next process. Engineering processes (e.g., requirem ents development or verification) are implemented repeatedly on a product to ensure that these engineering processes have been adequately addressed before delivery to the customer. Further, engineering processes are applied to components of the product. For exam ple, some questions that are raised by processes associated with the Verification and Validation process areas may be resolved by processes associated with the Requirements Development or Pr oduct Integration process area. Recursion and iteration of these pr ocesses enable the project to ensure quality in all components of the produc t before it is delivered to the customer. Support Support process areas cover the ac tivities that support product development and maintenance. The Support process areas address processes that are used in the contex t of performing other processes. In general, the Support process areas address processes that are targeted toward the project and may address processes that apply more generally to the organization. For example, Process and Product Quality Assurance can be used with all the process areas to provide an objective evaluation of the processe s and work products described in all the process areas. The Support process areas of CMMI are as follows: • Configuration Management • Process and Product Quality Assurance • Measurement and Analysis • Decision Analysis and Resolution • Causal Analysis and Resolution Basic Support Process Areas The Basic Support process ar eas address fundamental support functions that are used by all process areas. Although all Support process areas rely on the other pr ocess areas for input, the Basic Support process areas provide supp ort functions that also help implement several generic practices. Figure 4.6 provides a bird’s-eye vi ew of the interactions among the Basic Support process areas and with all other process areas.",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 63PPQA MA CMAll process areasMeasurements and analyses Information needs Configuration items and change requestsBaselines andaudit reportsProcesses and work products, and standards, and proceduresQuality and noncompliance issues MA = Measurement and Analysis CM = Configuration Management PPQA = Process and Product Quality Assurance Figure 4.6: Basic Support Process Areas The Measurement and A nalysis process area supports all process areas by providing specific pr actices that guide projects and organizations in aligning measur ement needs and objectives with a measurement approach that will provide objective re sults. These results can be used in making informed dec isions and taking appropriate corrective actions. The Process and Product Quality Assu rance process area supports all process areas by providing specific practices for objectively evaluating performed processes, work pr oducts, and services against the applicable process descriptions , standards, and procedures, and ensuring that any issues arising fr om these reviews are addressed. Process and Product Quality Assuranc e supports the delivery of high- quality products and services by provid ing the project staff and all levels of managers with appropriate visi bility into, and feedback on, the processes and associated work pro ducts throughout the life of the project. The Configuration Management pr ocess area supports all process areas by establishing and maintainin g the integrity of work products using configuration identification, c onfiguration contro l, configuration status accounting, and configurati on audits. The work products placed under configuration management in clude the products that are delivered to the customer, designated internal work products, acquired products, tools, and other items that are used in creating and describing these work products. Examples of work products that may be placed under configuration management in clude plans, process descriptions, requirements, design data, drawi ngs, product specif ications, code, compilers, product data files, and product technical publications.",
        "CMMI for Development Version 1.2 Relationships Among Process Areas 64 Advanced Support Process Areas The Advanced Support process ar eas provide the projects and organization with an improved support capability. Each of these process areas relies on specific inputs or pr actices from other process areas. Figure 4.7 provides a bird’s-eye vi ew of the interactions among the Advanced Support process areas and with all other process areas. DARAll process areasCAR Defects and other problems Selected issuesProcess improvement proposals Formal evaluations CAR = Causal Analysis and Resolution DAR = Decision Analysis and Resolution Figure 4.7: Advanced Support Process Areas Using the Causal Anal ysis and Resolution process area, project members identify causes of sele cted defects and other problems and take action to prevent them from occurring in the future. While the project’s defined processes are the pr incipal targets for identifying the cause of the defect, t he process improvement proposals they create target the organization’s set of st andard processes, which will prevent recurrence of the defect across the organization. The Decision Analysis and Resoluti on process area supports all the process areas by determining which issues should be subjected to a formal evaluation process and t hen applying a formal evaluation process to them.",
        "CMMI for Development Version 1.2 Using CMMI Models 655 Using CMMI Models The complexity of today’s products demands an integrated view of how organizations do business. CMMI can reduce the cost of process improvement across enterprises that depend on multiple functions or groups to produce products and services. To achieve this integrated view , the CMMI Framework includes common terminology, common model components, common appraisal methods, and common training material s. This chapter describes how organizations can use the CMMI Pro duct Suite not only to improve their quality, reduce their costs, and opt imize their schedules, but also to gauge how well their process impr ovement program is working. Adopting CMMI Research has shown that the most powerful initial step to process improvement is to build strong organizational support through strong senior management sponsorship. To gain senior management sponsorship, it is often beneficial to expose senior management to the performance results experienced by others who have used CMMI to improve their processes. For more information about CMMI performance results, see the SEI Web site at www.sei.cmu.edu/c mmi/results.html [SEI 3]. The senior manager, once committ ed as the process improvement sponsor, must be actively involv ed in the CMMI-based process improvement effort. Activities per formed by the senior management sponsor include (but are not limited to) the following: • Influence the organization to adopt CMMI. • Choose the best people to manage th e process improvement effort. • Monitor the process improvement effort personally. • Be a visible advocate and spokesperson for the process improvement effort. • Ensure that adequate resources ar e available to enable the process improvement effort to be successful.",
        "CMMI for Development Version 1.2 Using CMMI Models 66 Given sufficient senior management sponsorship, the next step is establishing a strong, technically competent process group that represents relevant stakeholders to guide process improvement efforts. For an organization with a mission to develop software-intensive systems, the process group might include engineers representing the different technical disciplines acro ss the organization and other selected members based on the business needs driving improvement. For example, a systems administrator may focus on information-technology support, whereas a marketing represen tative may focus on integrating customers’ needs. Both members coul d make powerful contributions to the process group. Once your organization has decided to adopt CMMI, planning can begin with an improvement approach such as the IDEALSM (Initiating, Diagnosing, Establishing, Acting, & Learning) model. For more information about the IDEAL model, see the SEI Web site at www.sei.cmu.edu/ideal/ideal.html [SEI 1]. Your Process Improvement Program Use the CMMI Product Suite to hel p establish your organization’s process improvement program. Using the product suite for this purpose can be a relatively informal process that involves understanding and applying CMMI best practices to your organization. Or, it can be a formal process that involves extens ive training, creation of a process improvement infrastructure, appraisals, and more. Selections That Influence Your Program You must make three selections to apply CMMI to your organization for process improvement: 1. Select a part of the organization. 2. Select a model. 3. Select a representation. Selecting the projects to be invo lved in your process improvement program is critical. If you select a gr oup that is too large, it may be too much for the initial improvement effort. The selection should also consider how homogeneous the group is (i.e., whether they all are software engineers, whether they all work on the same product or business line, and so on).",
        "CMMI for Development Version 1.2 Using CMMI Models 67Selecting the model to be used depends on the areas your organization is interested in improving. Not only mu st you select a constellation (e.g., Development, Acquisition, or Servic es), but you must also decide whether to include any additions (e.g., IPPD). The process of selecting the r epresentation to be used has some guidelines because of how CMMI model s are built. If your organization likes the idea of maturity levels and the staged representation, your improvement roadmap is already defin ed. If your organization likes the continuous representation, you can select nearly any process area or group of process areas to guide improvement, although dependencies among process areas should be cons idered when making such a selection. As the process improvement plans and activities progress, other important selections must be m ade, including which appraisal method should be used, which pr ojects should be appraised, how training for personnel should be secured, and which personnel should be trained. CMMI Models CMMI models describe what have been determined to be best practices that organizations have found to be pr oductive and useful to achieving their business objectives. Regardless of your type of organizati on, to apply CMMI best practices, you must use professional judgment when interpreting them for your situation, needs, and business objec tives. Although process areas depict the characteristics of an organization committed to process improvement, you must interpret t he process areas using an in-depth knowledge of CMMI, your organization, the business environment, and the specific circum stances involved. As you begin using a CMMI model to improve your organization’s processes, map your real-world proc esses to CMMI process areas. This mapping enables you to initially judge and later track your organization’s level of conformance to the CMMI model you are using and to identify opportunities for improvement. To interpret practices, it is important to consider the overall context in which these practices are used and to determine how well the practices satisfy the goals of a process area in that context. CMMI models do not explicitly prescribe nor imply particu lar processes that are right for any organization or project. Instead, C MMI describes minimal criteria necessary to plan and implement processes selected by the organization for improvement based on business objectives.",
        "CMMI for Development Version 1.2 Using CMMI Models 68 CMMI practices purposely use nonspeci fic phrases such as “relevant stakeholders,” “as appropriate,” and “as necessary” to accommodate the needs of different organizations and projects. The s pecific needs of a project may also differ at va rious points during its life. Using CMMI Appraisals Many organizations find value in measuring their progress by conducting an appraisal and thus earning a maturity level rating or a capability level achievement profile . These appraisals are typically conducted for one or more of the following reasons: • To determine how well the organization’s processes compare to CMMI best practices and identify areas where improvement can be made • To inform external customers and suppliers about how well the organization’s processes compare to CMMI best practices • To meet the contract require ments of one or more customers Appraisals of organizations using a CMMI model must conform to the requirements defined in the Apprai sal Requirements for CMMI (ARC) document. These appraisals focu s on identifying improvement opportunities and comparing the organi zation’s processes to CMMI best practices. Appraisal teams use a CMMI model and ARC-conformant appraisal method to guide their evaluat ion of the organization as well as how they report their conclusions . The appraisal results are then used (by a process group, for example) to plan improvements for the organization. Appraisal Requirements for CMMI The ARC document describes the re quirements for several types of appraisals. A full benchmarking class of appraisal is defined as a Class A appraisal. Less formal methods are defined as Class B or Class C methods. The ARC document wa s designed to help improve consistency across appraisal met hods, and to help appraisal method developers, sponsors, and users unde rstand the tradeoffs associated with various methods [SEI 2006a]. Depending on the purpose of the appraisal and the nature of the circumstances, one class may be prefe rred over the others. Sometimes self-assessments, initial appraisals , quick-look, or mini-appraisals, incremental appraisals, or external appraisals are appropriate, and other times a formal benchmarki ng appraisal is appropriate.",
        "CMMI for Development Version 1.2 Using CMMI Models 69A particular appraisal method is declared an ARC Class A, B, or C appraisal method based on the sets of ARC requirements that the method developer addressed when designing the method. More information about the ARC is available on the SEI Web site at www.sei.cmu.edu/cmmi/apprai sals/appraisals.html. SCAMPI Appraisal Methods The SCAMPI appraisal methods are the generally accepted methods used for conducting appraisals usi ng CMMI models. The SCAMPI Method Definition Document (MDD) defines rules for ensuring the consistency of appraisal ratings . For benchmarking against other organizations, appraisals must ensure consistent ratings. The achievement of a specific maturity le vel or the satisfaction of a process area must mean the same thing for different appraised organizations. The SCAMPI family of appraisals includes Class A, B, and C appraisal methods. SCAMPI A is the most rigorous method and the only method that can result in a rating. SCAMPI B provides options in model scope, but the characterization of practi ces is fixed to one scale and is performed on implemented practices. SCAMPI C provides a wide range of options, including characterizati on of planned approaches to process implementation according to a scale defined by the user. More information about SCAMPI met hods is available on the SEI Web site at www.sei.cmu.edu/cmmi/apprai sals/appraisals.html [SEI 2006b]. Appraisal Considerations Choices that affect a CMMI-bas ed appraisal include the following: • Which CMMI model to use for the appraisal (for this constellation, the choice would be between the CMMI for Development model and the CMMI for Development +IPPD model) • Establishing the appraisal scope, in cluding the organizational unit to be appraised, the CMMI process ar eas to be investigated, and the maturity level or capability level(s) to be appraised • Selecting the appraisal method • Selecting the appraisal team members • Selecting appraisal participants from the appraisal entities to be interviewed • Establishing appraisal outputs (e.g., ra tings or instantiation-specific findings) • Establishing appraisal constrai nts (e.g., time spent on site)",
        "CMMI for Development Version 1.2 Using CMMI Models 70 The SCAMPI MDD allows the selectio n of predefined options for use in an appraisal. These appraisal opt ions are designed to help organizations align CMMI with their business needs and objectives. Documentation of CMMI appraisal plans and results must always include a description of the appra isal options, model scope, and organizational scope selected. This documentation conf irms whether an appraisal meets the requirements for benchmarking. For organizations that wish to appr aise multiple functions or groups, CMMI’s integrated approach enables some economy of scale in model and appraisal training. One appraisal method can provide separate or combined results for multiple functions. The appraisal principles for the CMMI Product Suite12 remain the same as those used in appraisals for other process improvement models. Those principles are as follows: • Senior management sponsorship13 • A focus on the organization’s business objectives • Confidentiality for interviewees • Use of a documented appraisal method • Use of a process reference model (e.g., a CMMI model) as a base • A collaborative team approach • A focus on actions for process improvement CMMI-Related Training Whether your organization is new to process improvement or is already familiar with process improvement m odels, training is a key element in the ability of organizations to adopt C MMI. An initial set of courses is provided by the SEI and its Partners, but your organization may wish to supplement these courses with inte rnal instruction. This approach allows your organization to focus on the areas that provide the greatest business value. The SEI and its Partners offer the Introduction to CMMI course, which provides a basic overview of the C MMI models. The SEI also offers the Intermediate Concepts of CMMI course to those who plan to become more deeply involved in CMMI adoption or appraisal—for example, those who will guide improvement as part of a process group, those who will lead SCAMPI appraisals , and those who will teach the 12 See the glossary for the definition of CMMI Product Suite. 13 Experience has shown that the most critical factor in fluencing successful process improvement and appraisals is senior management sponsorship.",
        "CMMI for Development Version 1.2 Using CMMI Models 71Introduction to CMM course. Current information about CMMI-related training is available on the SEI Web site at www.sei.cmu.edu/cmmi/tra ining/training.html .",
        "CMMI for Development Version 1.2 Using CMMI Models 72",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices, and the Process Areas 73PART TWO Generic Goals and Generic Practices, and the Process Areas",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices, and the Process Areas 74",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 75GENERIC GOALS AND GENERIC PRACTICES Overview This section describes, in detail, all the generic goals and generic practices of CMMI—model components that directly address process institutionalization. In the process areas, generic goals and generic practices appear at the end of each process area. Generic practice elaborations appear after generic practices to show how t hese practices should uniquely be applied to the process area. The entire text of the generic goals and generic practices is not repeated in the process areas (i.e., subpractices, notes, examples, and references are omitted). Instead, only the generic goal and generic practice titles and statements appear. As you address each process area, refer to this section for t he details of all generic practices. Process Institutionalization Institutionalization is an important concept in process improvement. When mentioned in the generic goal a nd generic practice descriptions, institutionalization implies that the process is ingrained in the way the work is performed and there is commitment and consistency to performing the process. An institutionalized process is more likely to be retained during times of stress. When the requirements and obj ectives for the process change, however, the implementation of t he process may also need to change to ensure that it remains effect ive. The generic practices describe activities that address these as pects of institutionalization. The degree of instituti onalization is embodied in the generic goals and expressed in the names of the proc esses associated with each goal as indicated in Table 6.1.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 76 Table 6.1 Generic Goals and Process Names Generic Goal Progression of Processes GG 1 Performed process GG 2 Managed process GG 3 Defined process GG 4 Quantitatively managed process GG 5 Optimizing process The progression of process instituti onalization is char acterized in the following descriptions of each process. Performed Process A performed process is a proce ss that accomplishes the work necessary to produce work products. The specific goals of the process area are satisfied. Managed Process A managed process is a performed process that is planned and executed in accordance with polic y; employs skilled people who have adequate resources to produce contro lled outputs; involves relevant stakeholders; is monitored, contro lled, and reviewed; and is evaluated for adherence to its process description. The process may be instantiated by a project, group, or organizational function. Management of the process is concerned wi th institutionalization and the achievement of other s pecific objectives estab lished for the process, such as cost, schedule, and quality ob jectives. The control provided by a managed process helps to ensure that the establ ished process is retained during times of stress. The requirements and objectives for t he process are established by the organization. The status of the work products and delivery of the services are visible to management at defined points (e.g., at major milestones and completion of major tasks). Commitments are established among those performi ng the work and the relevant stakeholders and are revised as nec essary. Work products are reviewed with relevant stakeholde rs and are controlled. The work products and services satisfy their specified requirements. A critical distinction between a performed process and a managed process is the extent to whic h the process is managed. A managed process is planned (the plan may be pa rt of a more encompassing plan) and the performance of the proce ss is managed against the plan. Corrective actions are taken when the actual results and performance deviate significantly from the pl an. A managed process achieves the objectives of the plan and is institutionalized for consistent performance.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 77Defined Process A defined process is a managed process that is tailored from the organization’s set of standard processes according to the organization’s tailoring guidelines; has a maintained process description; and contributes work prod ucts, measures, and other process improvement information to the organizational process assets. The organizational process assets are ar tifacts that relate to describing, implementing, and improv ing processes. These artifacts are assets because they are developed or ac quired to meet the business objectives of the organization, and t hey represent investments by the organization that are expected to provide current and future business value. The organization’s set of standard proc esses, which are the basis of the defined process, are established and improved over time. Standard processes describe the fundamental process elements that are expected in the defined processes. Standard processes also describe the relationships (e.g., the orderi ng and the interfaces) among these process elements. The organization- level infrastructure to support current and future use of the organiza tion’s set of standard processes is established and improved over time . (See the definition of “standard process” in the glossary.) A project’s defined process provides a basis for planning, performing, and improving the project’s tasks and activities. A project may have more than one defined process (e.g., one for developing the product and another for testing the product). A defined process clearly states the following: • Purpose • Inputs • Entry criteria • Activities • Roles • Measures • Verification steps • Outputs • Exit criteria A critical distinction between a m anaged process and a defined process is the scope of application of t he process descriptions, standards, and procedures. For a managed proce ss, the process descriptions, standards, and procedures are applicabl e to a particular project, group,",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 78 or organizational function. As a re sult, the managed processes of two projects in one organization may be different. Another critical distinction is that a defined process is described in more detail and is performed more rigor ously than a managed process. This means that improvement information is easier to understand, analyze, and use. Finally, management of t he defined process is based on the additional insight provided by an und erstanding of the interrelationships of the process activities and detailed measures of the process, its work products, and its services. Quantitatively Managed Process A quantitatively managed process is a defined process that is controlled using statistical and ot her quantitative techniques . The product quality, service quality, and process-perform ance attributes are measurable and controlled throughout the project. Quantitative objectives are established based on the capability of the organization’s set of standard proc esses; the organization’s business objectives; and the needs of the cu stomer, end users, organization, and process implementers, subject to t he availability of resources. The people performing the process are dire ctly involved in quantitatively managing the process. Quantitative management is performed on the overall set of processes that produces a product. The subpr ocesses that are significant contributors to overall process per formance are statistically managed. For these selected subprocesses, detailed measures of process performance are collected and statisti cally analyzed. Special causes of process variation are identified and, where appropriate, the source of the special cause is addressed to prevent its recurrence. The quality and process-performance measures are incorporated into the organization’s measurement reposit ory to support future fact-based decision making. Activities for quantitatively managi ng the performance of a process include the following: • Identifying the subprocesses that are to be brought under statistical management • Identifying and measuring product and process attributes that are important contributors to quality and process performance • Identifying and addressing special ca uses of subprocess variations (based on the selected product and process attributes and subprocesses selected for statistical management)",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 79• Managing each of the selected subpr ocesses, with the objective of bringing their performance within natural bounds (i.e., making the subprocess performance statisti cally stable and predictable based on the selected product and process attributes) • Predicting the ability of the process to satisfy established quantitative quality and proce ss-performance objectives • Taking appropriate corrective actions when it is determined that the established quantitative quality and process-performance objectives will not be satisfied These corrective actions include chan ging the objectives or ensuring that relevant stakeholders have a quantitative understanding of, and have agreed to, the performance shortfall. A critical distinction between a defined process and a quantitatively managed process is the pr edictability of process pe rformance. The term quantitatively managed implies using appropriate statistical and other quantitative techniques to manage t he performance of one or more critical subprocesses so that t he performance of the process can be predicted. A defined process provides only qualitative predictability. Optimizing Process An optimizing process is a quantitat ively managed process that is changed and adapted to meet relevant current and projected business objectives. An optimizing process focuses on continually improving process performance through both incremental and innovative technological improvements. Proc ess improvements that address common causes of process variation, root causes of defects, and other problems; and those that would m easurably improve the organization’s processes are identified, evalua ted, and deployed as appropriate. These improvements are sele cted based on a quantitative understanding of their expected contribution to achieving the organization’s process improvement objectives versus the cost and impact to the organization. Selected incremental and innov ative technological process improvements are systematically managed and deployed into the organization. The effects of t he deployed process improvements are measured and evaluated against the quantitative process improvement objectives. In a process that is optimized, commo n causes of proc ess variation are addressed by changing the process in a way that will shift the mean or decrease variation when the process is restabilized. These changes are intended to improve process per formance and to achieve the organization’s established proc ess improvement objectives.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 80 A critical distinction between a quantitatively managed process and an optimizing process is that the optimizing pr ocess is continuously improved by addressing common caus es of process variation. A quantitatively managed process is c oncerned with addressing special causes of process variat ion and providing statisti cal predictability of the results. Although the process may produce predictable results, the results may be insufficient to achieve the organization’s process improvement objectives. Relationships among Processes The generic goals evolve so that each goal provides a foundation for the next. Therefore the followi ng conclusions can be made: • A managed process is a performed process. • A defined process is a managed process. • A quantitatively managed proc ess is a defined process. • An optimizing process is a quantitatively managed process. Thus, applied sequentially and in order, the generic goals describe a process that is increasingly instit utionalized from a performed process to an optimizing process. Achieving GG 1 for a process area is equivalent to saying you achieve the specific goals of the process area. Achieving GG 2 for a process area is equivalent to saying you manage the performance of processes associ ated with the process area. There is a policy that indicates you will perform it. There is a plan for performing it. There are resources provided, responsi bilities assigned, training on how to perform it, selected work products from performing the process are controlled, and so on. In other words, the process is planned and monitored just like any project or support activity. Achieving GG 3 for a process area assumes that an organizational standard process exists that can be tailo red to result in the process you will use. Tailoring might result in making no changes to the standard process. In other words, the process used and the standard process may be identical. Using the standard pr ocess “as is” is tailoring because the choice is made that no modification is required. Each process area describes multiple activities, some of which are repeatedly performed. You may need to tailor the way one of these activities is performed to account for new capabilities or circumstances. For example, you may have a standard for developing or obtaining organizational training that does not consider Web-based training. When preparing to develop or obtain a Web-based course, you may need to tailor the standard process to account for the particular challenges and benefits of Web-based training.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 81Achieving GG 4 or GG 5 for a process area is conceptually feasible but may not be economical except, perhaps, in situations where the product domain has become stable for an exte nded period or in situations in which the process area or domain is a critical business driver. Generic Goals and Generic Practices This section describes all of the generic goals and generic practices, as well as their associated subpractice s, notes, examples , and references. The generic goals are organized in numerical order, GG 1 through GG 5. The generic practices are also organized in numerical order under the generic goal they support. As mentioned earlier, the subpr actices, notes, examples, and references are not repeated in the process areas; the details of each generic goal and generic practice are found only here. GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the process area to develop work products and provide services to achieve the specific goals of the process area. The purpose of this generic practice is to produce the work products and deliver the services that are ex pected by performing the process. These practices may be done informally, without following a documented process description or pl an. The rigor with which these practices are performed depends on the individuals managing and performing the work and ma y vary considerably. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the process. The purpose of this generic practice is to define the organizational expectations for the process and make these expectations visible to those in the organization who ar e affected. In general, senior management is responsible for es tablishing and communicating guiding principles, direction, and expec tations for the organization.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 82 Not all direction from senior manag ement will bear the label “policy.” The existence of appropriate organiza tional direction is the expectation of this generic practice, regardless of what it is called or how it is imparted. GP 2.2 Plan the Process Establish and maintain the plan for performing the process. The purpose of this generic practice is to determine what is needed to perform the process and to achiev e the established objectives, to prepare a plan for performing the process, to prepare a process description, and to get agreement on the plan from relevant stakeholders. The practical implications of applyi ng a generic practice vary for each process area. For example, the planning described by this generic practice as applied to the Projec t Monitoring and Control process area may be carried out in full by the proc esses associated with the Project Planning process area. However, this generic practice, when applied to the Project Planning process area, se ts an expectation that the project planning process itself be planned. Ther efore, this generic practice may either reinforce expectations set elsewhere in CMMI or set new expectations that should be addressed. Refer to the Project Planning process area for more information on establishing and maintain ing a project plan. Establishing a plan includes doc umenting the plan and a process description. Maintaining the plan includes updating it to reflect corrective actions or changes in requirements or objectives. The plan for performing the process typically includes the following: • Process description • Standards and requirements for the work products and services of the process • Specific objectives for the perform ance of the process (e.g., quality, time scale, cycle time, and resource usage) • Dependencies among the activities, work products, and services of the process • Resources (including funding, people, and tools) needed to perform the process • Assignment of res ponsibility and authority • Training needed for performing and supporting the process • Work products to be controlled and the level of control to be applied",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 83• Measurement requirements to prov ide insight into the performance of the process, its work products, and its services • Involvement of identified stakeholders • Activities for monitoring and controlling the process • Objective evaluation acti vities of the process • Management review activities for the process and the work products Subpractices 1. Define and document the plan for performing the process. This plan may be a stand-alone document, embedded in a more comprehensive document, or distributed across multiple documents. In the case of the plan being distributed across multiple documents, ensure that a coherent picture of who does what is preserved. Documents may be hardcopy or softcopy. 2. Define and document the process description. The process description, which includes relevant standards and procedures, may be included as part of the plan for performing the process or may be included in the plan by reference. 3. Review the plan with rele vant stakeholders and get their agreement. This includes reviewing that the planned process satisfies the applicable policies, plans, requirements, and standards to provide assurance to relevant stakeholders. 4. Revise the plan as necessary. GP 2.3 Provide Resources Provide adequate resources for performing the process, developing the work products, and providing the services of the process. The purpose of this generic practice is to ensure that the resources necessary to perform the process as defined by the plan are available when they are needed. Resources include adequate funding, appropriate physical facilities, sk illed people, and appropriate tools. The interpretation of the term “adequate” depends on many factors and can change over time. Inadequate resources may be addressed by increasing resources or by removi ng requirements, constraints, and commitments.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 84 GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the process. The purpose of this generic practi ce is to ensure that there is accountability for performing the pr ocess and achieving the specified results throughout the life of the process. The people assigned must have the appropriate authority to per form the assigned responsibilities. Responsibility can be assigned using de tailed job descriptions or in living documents, such as the plan fo r performing the process. Dynamic assignment of responsibilit y is another legitimate way to perform this generic practice, as long as t he assignment and acceptance of responsibility are ensured throughout the life of the process. Subpractices 1. Assign overall responsibility and authority for performing the process. 2. Assign responsibility and authority for performing the specific tasks of the process. 3. Confirm that the people a ssigned to the responsibilities and authorities understand and accept them. GP 2.5 Train People Train the people performing or supporting the process as needed. The purpose of this generic practice is to ensure that the people have the necessary skills and expertise to perform or support the process. Appropriate training is provided to the people who will be performing the work. Overview training is provided to orient people who interact with those performing the work. Examples of methods for providi ng training include self-study; self- directed training; self-paced, progr ammed instruction; formalized on- the-job training; mentoring; and formal and classroom training. Training supports the successful performance of the process by establishing a common understanding of the process and by imparting the skills and knowledge needed to perform the process. Refer to the Organizational Training process area for more information about training the people performi ng or supporting the process.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 85GP 2.6 Manage Configurations Place designated work products of the process under appropriate levels of control. The purpose of this generic practice is to establish and maintain the integrity of the designat ed work products of the process (or their descriptions) throughout their useful life. The designated work products are specif ically identified in the plan for performing the process, along with a specification of the appropriate level of control. Different levels of control are appropr iate for different work products and for different points in time. For some work products, it may be sufficient to maintain version control (i.e., t he version of the work product in use at a given time, past or present, is known, and changes are incorporated in a controlled manner). Version control is usually under the sole control of the work produc t owner (which may be an individual, a group, or a team). Sometimes, it may be critical that work products be placed under formal or baseline configuration management. This type of control includes defining and establishi ng baselines at predet ermined points. These baselines are formally reviewed and agreed on, and serve as the basis for further development of the designated work products. Refer to the Configuration Management process area for more information about placing work products under configuration management. Additional levels of control bet ween version control and formal configuration management are possible. An identified work product may be under various levels of contro l at different points in time. GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the process as planned. The purpose of this generic practice is to establish and maintain the expected involvement of stakeholder s during the execution of the process. Involve relevant stakeholders as des cribed in an appropriate plan for stakeholder involvement. Involve stakehol ders appropriately in activities such as the following: • Planning • Decisions • Commitments",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 86 • Communications • Coordination • Reviews • Appraisals • Requirements definitions • Resolution of problems/issues Refer to the Project Planning proce ss area for information on the project planning for stakeholder involvement. The objective of planning stakeholder involvement is to ensure that interactions necessary to the pr ocess are accomplished, while not allowing excessive numbers of affected groups and individuals to impede process execution. Subpractices 1. Identify stakeholders relevant to this process and their appropriate involvement. Relevant stakeholders are identified among the suppliers of inputs to, the users of outputs from, and the performers of the activities within the process. Once the relevant stakeholders are identified, the appropriate level of their involvement in process activities is planned. 2. Share these identific ations with project planners or other planners as appropriate. 3. Involve relevant stakeholders as planned. GP 2.8 Monitor and Control the Process Monitor and control the process against the plan for performing the process and take appropriate corrective action. The purpose of this generic practice is to perform the direct day-to-day monitoring and controlling of the process. Appropriate visibility into the process is maintained so that approp riate corrective action can be taken when necessary. Monitoring and c ontrolling the process involves measuring appropriate attributes of the process or work products produced by the process. Refer to the Project Monitoring and Control process area for more information about monitoring and controlling the project and taking corrective action. Refer to the Measurement and Analysis process area for more information about measurement.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 87Subpractices 1. Measure actual performance against the plan for performing the process. The measures are of the process, its work products, and its services. 2. Review accomplishments and resu lts of the process against the plan for performing the process. 3. Review activities, status, and results of the process with the immediate level of management responsible for the process and identify issues. The reviews are intended to provide the immediate level of management with appropriate visibility into the process. The reviews can be both periodic and event driven. 4. Identify and evaluate the effects of significant deviations from the plan for performing the process. 5. Identify problems in the plan fo r performing the process and in the execution of the process. 6. Take corrective action when re quirements and objectives are not being satisfied, when issues are i dentified, or when progress differs significantly from the plan for performing the process. There are inherent risks that should be considered before any corrective action is taken. Corrective action may include the following: • Taking remedial action to repair defec tive work products or services • Changing the plan for performing the process • Adjusting resources, including people, tools, and other resources • Negotiating changes to the established commitments • Securing change to the requirements and objectives that have to be satisfied • Terminating the effort 7. Track corrective action to closure. GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the process against its process description, standards, and procedures, and address noncompliance. The purpose of this generic practice is to provide credible assurance that the process is implemented as planned and adheres to its process description, standards, and procedur es. This generic practice is implemented, in part, by evaluati ng selected work products of the process. (See the definition of objec tively evaluate in the glossary.)",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 88 Refer to the Process and Product Q uality Assurance process area for more information about object ively evaluating adherence. People not directly responsible for managing or performing the activities of the process typically evaluate adherence. In many cases, adherence is evaluated by people within the organization, but external to the process or project, or by people external to the organization. As a result, credible assurance of adherence can be provided even during times when the process is under stress (e.g., when the effort is behind schedule or over budget). GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the process with higher level management and resolve issues. The purpose of this generic practi ce is to provide higher level management with the appropriate vi sibility into the process. Higher level management includes thos e levels of management in the organization above the immediate le vel of management responsible for the process. In particular, higher level management includes senior management. These reviews are for managers who provide the policy and overall guidance for the process, and not for those who perform the direct day-to-day monitoring and controlling of the process. Different managers have different needs for information about the process. These reviews help ensur e that informed decisions on the planning and performing of the proc ess can be made. Therefore, these reviews are expected to be both periodic and event driven. GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined process. The purpose of this generic practice is to establish and maintain a description of the process that is ta ilored from the organization’s set of standard processes to address the needs of a specific instantiation. The organization should have standard proc esses that cover the process area, as well as have guidelines fo r tailoring these standard processes to meet the needs of a project or organizational function. With a defined process, variability in how the processes are performed across the organization is reduced and process assets, data, and learning can be effectively shared. Refer to the Organizational Process Definition process area for more information about the organizati on’s set of standard processes and tailoring guidelines.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 89Refer to the Integrated Project Management process area for more information on establishing and ma intaining the project’s defined process. The descriptions of the defined pr ocesses provide the basis for planning, performing, and managing t he activities, work products, and services associated with the process. Subpractices 1. Select from the organization’ s set of standard processes those processes that cover the proce ss area and best meet the needs of the project or organizational function. 2. Establish the defined process by tailoring the selected processes according to the organization’s tailoring guidelines. 3. Ensure that the organization’s process objectives are appropriately addressed in the defined process. 4. Document the defined process and the records of the tailoring. 5. Revise the description of t he defined process as necessary. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the process to support the future use and improvement of the organization’s processes and process assets. The purpose of this generic practi ce is to collect information and artifacts derived from planning and pe rforming the process. This generic practice is performed so that t he information and artifacts can be included in the organizational process assets and made available to those who are (or who will be) planning and performing the same or similar processes. The informati on and artifacts are stored in the organization’s measurement reposit ory and the organization’s process asset library. Examples of relevant information include the effort expended for the various activities, defects injected or removed in a particular activity, and lessons learned. Refer to the Organizational Process Definition process area for more information about the organizati on’s measurement repository and process asset library and for more information about the work products, measures, and improvement information that are incorporated into the organizational process assets.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 90 Refer to the Integrated Project Management process area for more information on contributing work products, measures, and documented experiences to the organizational process assets. Subpractices 1. Store process and product m easures in the organization’s measurement repository. The process and product measures are primarily those that are defined in the common set of measures for the organization’s set of standard processes. 2. Submit documentation for inclus ion in the organization’s process asset library. 3. Document lessons learned from the process for inclusion in the organization’s process asset library. 4. Propose improvements to the organizational process assets. GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the process, which address quality and process performance, based on customer needs and business objectives. The purpose of this generic prac tice is to determine and obtain agreement from relevant stak eholders about specif ic quantitative objectives for the process. T hese quantitative objectives can be expressed in terms of product qua lity, service quality, and process performance. Refer to the Quantitative Proj ect Management process area for information on how quantitative objec tives are set for subprocesses of the project’s defined process. The quantitative objectives may be spec ific to the process or they may be defined for a broader scope (e.g., for a set of processes). In the latter case, these quantitative objectiv es may be allocated to some of the included processes. These quantitative objectives are cr iteria used to judge whether the products, services, and process perfo rmance will satisfy the customers, end users, organization managemen t, and process implementers. These quantitative objectives go bey ond the traditional end-product objectives. They also cover interm ediate objectives that are used to manage the achievement of the objectives over time . They reflect, in part, the demonstrated performance of the organization’s set of standard processes. These quantitativ e objectives should be set to",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 91values that are likely to be achi eved when the processes involved are stable and within their natural bounds. Subpractices 1. Establish the quantitat ive objectives that pertain to the process. 2. Allocate the quantitative obje ctives to the process or its subprocesses. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the process to achieve the established quantitative quality and process-performance objectives. The purpose of this generic practice is to stabilize the performance of one or more subprocesses of the def ined process, which are critical contributors to overall performance, using appropriate statistical and other quantitative techniques. Stabilizing selected subprocesses supports predicting the ability of the process to achieve the established quantitative quality and proce ss-performance objectives. Refer to the Quantitative Proj ect Management process area for information on selecting subproce sses for statistical management, monitoring performance of subpr ocesses, and other aspects of stabilizing subprocess performance. A stable subprocess shows no significant indication of special causes of process variation. Stable subprocesse s are predictable within the limits established by the natural bounds of the subprocess. Variations in the stable subprocess are due to a consta nt system of chance causes, and the magnitude of the variati ons can be small or large. Predicting the ability of the proc ess to achieve the established quantitative objectives requires a quantitative understanding of the contributions of t he subprocesses that are crit ical to achieving these objectives and establishing and m anaging against interim quantitative objectives over time. Selected process and product measur es are incorporated into the organization’s measurement reposit ory to support process-performance analysis and future fact -based decision making. Subpractices 1. Statistically manage the performan ce of one or more subprocesses that are critical contributors to the overall performance of the process.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 92 2. Predict the ability of the pr ocess to achieve its established quantitative objectives consi dering the performance of the statistically managed subprocesses. 3. Incorporate selected process- performance measurements into the organization’s process-performance baselines. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the process in fulfilling the relevant business objectives of the organization. The purpose of this generic practice is to select and systematically deploy process and technology im provements that contribute to meeting established quality and proc ess-performance objectives. Refer to the Organizational Innov ation and Deployment process area for information about selecti ng and deploying incremental and innovative improvements that measurably improve the organization's processes and technologies. Optimizing the processes that are agile and innovative depends on the participation of an empowered work force aligned with the business values and objectives of the organi zation. The organization’s ability to rapidly respond to changes and opportunities is enhanced by finding ways to accelerate and share learning. Improvement of the processes is inherently part of everybody’s role, resulting in a cycle of continual improvement. Subpractices 1. Establish and maintain quant itative process improvement objectives that support the orga nization’s business objectives. The quantitative process improvement objectives may be specific to the individual process or they may be defined for a broader scope (i.e., for a set of processes), with the individual processes contributing to achieving these objectives. Objectives that are specific to the individual process are typically allocated from quantitative objectives established for a broader scope. These process improvement objectives are primarily derived from the organization’s business objectives and from a detailed understanding of process capability. These objectives are the criteria used to judge whether the process performance is quantitatively improving the organization’s ability to meet its business objectives. These process improvement objectives are often set to values beyond the current process performance, and both incremental and innovative technological improvements may be needed to achieve these objectives. These objectives may also be revised frequently to continue to drive",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 93the improvement of the process (i.e., when an objective is achieved, it may be set to a new value that is again beyond the new process performance). These process improvement objectives may be the same as, or a refinement of, the objectives established in the “Establish Quantitative Objectives for the Process” generic practice, as long as they can serve as both drivers and criteria for successful process improvement. 2. Identify process improvements t hat would result in measurable improvements to process performance. Process improvements include both incremental changes and innovative technological improvements. The innovative technological improvements are typically pursued as efforts that are separately planned, performed, and managed. Piloting is often performed. These efforts often address specific areas of the processes that are determined by analyzing process performance and identifying specific opportunities for significant measurable improvement. 3. Define strategies and manage d eployment of selected process improvements based on the quant ified expected benefits, the estimated costs and impacts, and the measured change to process performance. The costs and benefits of these improvements are estimated quantitatively, and the actual costs and benefits are measured. Benefits are primarily considered relative to the organization’s quantitative process improvement objectives. Improvements are made to both the organization’s set of standard processes and the defined processes. Managing deployment of the process improvements includes piloting changes and implementing adjustments where appropriate, addressing potential and real barriers to deployment, minimizing disruption to ongoing efforts, and managing risks. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the process. The purpose of this generic practi ce is to analyze defects and other problems that were encountered in a quantitatively managed process, to correct the root causes of t hese types of defects and problems, and to prevent these defects and problem s from occurring in the future. Refer to the Causal Analysis and Resolution process area for more information about identifying and corr ecting root causes of selected defects. Even though the Causal Analysis and Resolution process area has a project context, it can be applied to processes in other contexts as well.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 94 Root cause analysis can be applied beneficially to processes that are not quantitatively managed. However, the focus of this generic practice is to act on a quantitatively manag ed process, though the final root causes may be found outsi de of that process. Applying Generic Practices This section helps you to devel op a better understanding of the generic practices and provides informati on for interpreting and applying the generic practices in your organization. Generic practices are components that are common to all process areas. Think of generic practices as reminders. They serve the purpose of reminding you to do things right, and are expected model components. For example, when you are achieving the specific goals of the Project Planning process area, you are establ ishing and maintaining a plan that defines project activities. One of t he generic practices that applies to the Project Planning process area is “E stablish and maintain the plan for performing the project planning process” (GP 2.2). When applied to this process area, this generic practice reminds you to pl an the activities involved in creating the plan for the project. When you are satisfying the spec ific goals of the Organizational Training process area, you are de veloping the skills and knowledge of people in your project and organization so that they can perform their roles effectively and efficiently. When applying the same generic practice (GP 2.2) to the Organiza tional Training process area, this generic practice reminds you to plan the activities involved in developing the skills and knowledge of people in the organization. Process Areas That Support Generic Practices While generic goals and generic practices are the model components that directly address the instituti onalization of a process across the organization, many process areas li kewise address institutionalization by supporting the impl ementation of the generi c practices. Knowing these relationships will help you effectively implement the generic practices. Such process areas contain one or mo re specific practices that when implemented may also fully implement a generic practice or generate a work product that is used in the implementation of a generic practice.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 95An example is the Configurati on Management process area and GP 2.6, “Place designated work products of the process under appropriate levels of control.” To implement t he generic practice for one or more process areas, you might choose to implement the Configuration Management process area, all or in part, to implement the generic practice. Another example is the Organization al Process Definition process area and GP 3.1, “Establish and mainta in the description of a defined process.” To implement this generic practice for one or more process areas, you should first implement t he Organizational Process Definition process area, all or in part, to establish the organizational process assets that are needed to implement the generic practice. Table 6.2 describes (1) the pr ocess areas that support the implementation of generic practices, and (2) the recursive relationships between generic practices and their cl osely related process areas. Both types of relationships are import ant to remember during process improvement to take advantage of t he natural synergies that exist between the generic practices and their related process areas. Table 6.2 Generic Practice and Process Area Relationships Generic Practice Roles of Process Areas in Implementation of the Generic Practice How the Generic Practice Recursively Applies to its Related Process Area(s)14 GP 2.2 Plan the Process Project Planning: The project planning process can implement GP 2.2 in full for all project-related process areas (except for Project Planning itself). GP 2.2 applied to the project planning process can be characterized as “plan the plan” and covers planning project planning activities. GP 2.3 Provide Resources GP 2.4 Assign Responsibility Project Planning: The part of the project planning process that implements Project Planning SP 2.4, “Plan for necessary resources to perform the project,” supports the implementation of GP 2.3 and GP 2.4 for all project-related process areas (except perhaps init ially for Project Planning itself) by identifying needed processes, roles, and responsibilities to ensure the proper staffing, facilities, equipment, and other assets needed by the project are secured. 14 When the relationship between a generi c practice and a process area is less direct, the risk of confusion is reduced; therefore, we do not describe all recursive rela tionships in the table (e.g., for generic practices 2.3, 2.4, and 2.10).",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 96 Generic Practice Roles of Process Areas in Implementation of the Generic Practice How the Generic Practice Recursively Applies to its Related Process Area(s)14 GP 2.5 Train People Organizational Training: The organizational training process supports the implementation of GP 2.5 as applied to all process areas by making the training that addresses strategic or organization-wide training needs available to those who will perform or support the process. Project Planning: The part of the project planning process that implements Project Planning SP 2.5, “Plan for knowledge and skills needed to perform the project,” together with the organizational training process, supports the implementation of GP 2.5 in full for all project-related process areas. GP 2.5 applied to the organizational training process area covers training for performing the organizational training activities, which addresses the skills required to manage, create, and accomplish the training. GP 2.6 Manage Configurations Configuration Management: The configuration management process can implement GP 2.6 in full for all project- related process areas as well as some of the organizational process areas. GP 2.6 applied to the configuration management process covers change and version control for the work products produced by configuration management activities. GP 2.7 Identify and Involve Relevant Stakeholders Project Planning: The part of the project planning process that implements Project Planning SP 2.6, “Plan Stakeholder Involvement,” can implement the stakeholder identification part (first two subpractices) of GP 2.7 in full for all project-related process areas. Project Monitoring and Control: The part of the project monitoring and control process that implements Project Monitoring and Control SP 1.5, “Monitor Stakeholder Involvement,” can aid in implementing the third subpractice of GP 2.7 for all project-related process areas. Integrated Project Management: The part of the integrated project management process that implements Integrated Project Management SP 2.1, “Manage Stakeholder Involvement,” can aid in implementing the third subpractice of GP 2.7 for all project-related process areas. GP 2.7 applied to the project planning process covers the involvement of relevant stakeholders in project planning activities. GP 2.7 applied to the project monitoring and control process covers the involvement of relevant stakeholders in project monitoring and control activities. GP 2.7 applied to the integrated project management process covers the involvement of relevant stakeholders in integrated project management activities.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 97Generic Practice Roles of Process Areas in Implementation of the Generic Practice How the Generic Practice Recursively Applies to its Related Process Area(s)14 GP 2.8 Monitor and Control the Process Project Monitoring and Control: The project monitoring and control process can implement GP 2.8 in full for all project-related process areas. Measurement and Analysis: For all processes, not just project-related processes, the Measurement and Analysis process area provides general guidance about measuring, analyzing, and recording information that can be used in establishing measures for monitoring actual performance of the process. GP 2.8 applied to the project monitoring and control process covers the monitoring and controlling of the project’s monitor and control activities. GP 2.9 Objectively Evaluate Adherence Process and Product Quality Assurance: The process and product quality assurance process can implement GP 2.9 in full for all process areas (except perhaps for Process and Product Quality Assurance itself). GP 2.9 applied to the process and product quality assurance process covers the objective evaluation of quality assurance activities. GP 2.10 Review Status with Higher Level Management Project Monitoring and Control: The part of the project monitoring and control process that implements Project Monitoring and Control SP 1.6, “Conduct Progress Reviews,” and SP 1.7, “Conduct Milestone Reviews,” supports the implementation of GP 2.10 for all project-related process areas, perhaps in full, depending on higher level management involvement in these reviews. GP 3.1 Establish a Defined Process Integrated Project Management: The part of the integrated project management process that implements Integrated Project Management SP 1.1, “Establish and maintain the project’s defined process from project startup through the life of the project,” can implement GP 3.1 in full for all project- related process areas. Organizational Process Definition: For all processes, not just project- related processes, the organizational process definition process establishes the organizational process assets needed to implement GP 3.1. GP 3.1 applied to the integrated project management process covers establishing defined processes for integrated project management activities.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 98 Generic Practice Roles of Process Areas in Implementation of the Generic Practice How the Generic Practice Recursively Applies to its Related Process Area(s)14 GP 3.2 Collect Improvement Information Integrated Project Management: The part of the integrated project management process that implements Integrated Project Management SP 1.6, “Contribute work products, measures, and documented experiences to the organizational process assets,” can implement GP 3.2 in part or full for all project-related process areas. Organizational Process Focus: The part of the organizational process focus process that implements Organizational Process Focus SP 3.4, “Incorporate process-related work products, measures, and improvement information derived from planning and performing the process into the organizational process assets,” can implement GP 3.2 in part or full for all process areas. Organizational Process Definition: For all processes, the organizational process definition process establishes the organizational process assets needed to implement GP 3.2. GP 3.2 applied to the integrated project management process covers collecting improvement information derived from planning and performing integrated project management activities. GP 4.1 Establish Quantitative Objectives for the Process Quantitative Project Management: The part of the quantitative project management process that implements Quantitative Project Management SP 1.1, “Establish and maintain the project’s quality and process-performance objectives,” supports the implementation of GP 4.1 for all project-related process areas by providing objectives from which the objectives for each particular process can be derived. If these objectives become established as part of implementing subpractices 5 and 8 of Quantitative Project Management SP 1.1, then the quantitative project management process implements GP 4.1 in full. Organizational Process Performance: The part of the organizational process performance process that implements Organizational Process Performance SP 1.3, “Establish and maintain quantitative objectives for quality and process performance for the organization,” supports the implementation of GP 4.1 for all process areas. GP 4.1 applied to the quantitative project management process covers establishing quantitative objectives for quantitative project management activities. GP 4.1 applied to the organizational process performance process covers establishing quantitative objectives for organizational process-performance activities.",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 99Generic Practice Roles of Process Areas in Implementation of the Generic Practice How the Generic Practice Recursively Applies to its Related Process Area(s)14 GP 4.2 Stabilize Subprocess Performance Quantitative Project Management: The part of the quantitative project management process that implements Quantitative Project Management SG 2, “Statistically Manage Subprocess Performance,” can implement GP 4.2 in full for all project-related process areas to which a statistically managed subprocess can be mapped. Organizational Process Performance: For all processes, not just project- related processes, the organizational process performance process establishes organizational process assets that may be needed to implement GP 4.2. GP 4.2 applied to the quantitative project management process covers the stabilization of selected subprocesses within quantitative project management activities. GP 5.1 Ensure Continuous Process Improvement Organizational Innovation and Deployment: The organizational innovation and deployment process can implement GP 5.1 in full for all process areas providing that quality and process- performance objectives for the organization have been defined. (The latter would be the case, say, if the Organizational Process Performance process area has been implemented.) GP 5.1 applied to the organizational innovation and deployment process covers ensuring continuous process improvement of organizational innovation and deployment activities. GP 5.2 Correct Root Causes of Problems Causal Analysis and Resolution: The causal analysis and resolution process can implement GP 5.2 in full for all project-related process areas. GP 5.2 applied to the causal analysis and resolution process covers identifying root causes of defects and other problems in causal analysis and resolution activities. Given the dependencies that generic practices have on these process areas, and given the more “holistic” view that many of these process areas provide, these process areas are often implemented early, in whole or in part, before or concurr ent with implement ing the associated generic practices. There are also a few situations w here the result of applying a generic practice to a particular process area would seem to make a whole process area redundant, but, in fact, it does not. It may be natural to think that applying GP 3.1, Establis h a Defined Process, to the Project Planning and Project Monitoring and C ontrol process areas gives the same effect as the first specific goal of Integrated Project Management, “The project is conducted using a defined process that is tailored from the organization’s set of standard processes.”",
        "CMMI for Development Version 1.2 Generic Goals and Generic Practices 100 Although it is true that there is so me overlap, the application of the generic practice to these two proce ss areas provides defined processes covering project planning and project m onitoring and control activities. These defined processes do not nece ssarily cover support activities (such as configurat ion management), other project management processes (such as supplier agreement management), or the engineering processes. In contras t, the project’s defined process, provided by the Integrated Projec t Management process area, covers all appropriate project management, engineering, and support processes.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 101CAUSAL ANALYSIS AND RESOLUTION A Support Process Area at Maturity Level 5 Purpose The purpose of Causal Analysis and Resolution (CAR) is to identify causes of defects and other problem s and take action to prevent them from occurring in the future. Introductory Notes The Causal Analysis and Resolu tion process area involves the following: • Identifying and analyzin g causes of defects and other problems • Taking specific actions to re move the causes and prevent the occurrence of those types of defec ts and problems in the future Causal analysis and resolution improv es quality and productivity by preventing the introduction of defec ts into a product. Reliance on detecting defects after t hey have been introduced is not cost effective. It is more effective to prevent defects from being introduced by integrating causal analysis and resolution activities into each phase of the project. Since defects and problems may hav e been previously encountered on other projects or in earlier phases or tasks of the current project, causal analysis and resolution activities are a mechanism for communicating lessons learned among projects. The types of defects and other pr oblems encountered are analyzed to identify any trends. Based on an un derstanding of the defined process and how it is implemented, the root ca uses of the defects and the future implications of the defects are determined. Causal analysis may also be performed on problems unrelated to defects. For example, causal analysis may be used to improve quality attributes such as cycle time. Im provement proposals, simulations, dynamic systems models, engi neering analyses, new business directives, or other items may initiate such analysis. When it is impractical to perform c ausal analysis on a ll defects, defect targets are selected by tradeoffs on estimated investments and estimated returns of quality, productivity and cycle time.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 102 A measurement process should alre ady be in place. The defined measures can be used, though in so me instances new measures may be needed to analyze the effect s of the process change. Refer to the Measurement and Analysis process area for more information about establishing objectives for measurement and analysis, specifying the measur es and analyses to be performed, obtaining and analyzing measures , and reporting results. Causal Analysis and Resolution acti vities provide a mechanism for projects to evaluate their processe s at the local level and look for improvements that can be implemented. When improvements are judged to be e ffective, the information is extended to the organizational level. Refer to the Organizational Innov ation and Deployment process area for more information about improv ing organizational level processes through proposed improvements and action proposals. The informative material in this process area is written with the assumption that the specific practi ces are applied to a quantitatively managed process. The specific prac tices of this process area may be applicable, but with reduced value, if this assumption is not met. See the definitions of “stable process” and “co mmon cause of process variation” in the glossary. Related Process Areas Refer to the Quantitative Project Management process area for more information about the analysis of pr ocess performance and the creation of process capability measures fo r selected project processes. Refer to the Organizational Innov ation and Deployment process area for more information about the selection and deployment of improvements to organizational processes and technologies. Refer to the Measurement and Analysis process area for more information about establishing objectives for measurement and analysis, specifying the measur es and analyses to be performed, obtaining and analyzing measures , and reporting results.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 103Specific Goal and Practice Summary SG 1 Determine Causes of Defects SP 1.1 Select Defect Data for Analysis SP 1.2 Analyze Causes SG 2 Address Causes of Defects SP 2.1 Implement the Action Proposals SP 2.2 Evaluate the Effect of Changes SP 2.3 Record Data Specific Practices by Goal SG 1 Determine Causes of Defects Root causes of defects and other problems are systematically determined. A root cause is a source of a defect such that, if it is removed, the defect is decreased or removed. SP 1.1 Select Defect Data for Analysis Select the defects and other problems for analysis. Typical Work Products 1. Defect and problem data se lected for further analysis Subpractices 1. Gather relevant def ect or problem data. Examples of relevant defect data may include the following: • Defects reported by the customer • Defects reported by end users • Defects found in peer reviews • Defects found in testing Examples of relevant problem data may include the following: • Project management problem reports requiring corrective action • Process capability problems • Process duration measurements • Earned value measurements by process (e.g., cost performance index) • Resource throughput, utilization, or response time measurements Refer to the Verification process area for more information about work product verification.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 104 Refer to the Quantitative Proj ect Management process area for more information about statistical management. 2. Determine which defects and other problems will be analyzed further. When determining which defects to analyze further, consider the impact of the defects, their frequency of occurrence, the similarity between defects, the cost of analysis, the time and resources needed, the safety considerations, etc. Examples of methods for selecting defects and other problems include the following: • Pareto analysis • Histograms • Process capability analysis SP 1.2 Analyze Causes Perform causal analysis of selected defects and other problems and propose actions to address them. The purpose of this analysis is to develop solutions to the identified problems by analyzing the relevant data and producing action proposals for implementation. Typical Work Products 1. Action proposal Subpractices 1. Conduct causal analysis with the people who are responsible for performing the task. Causal analysis is performed, typically in meetings, with those people who have an understanding of the selected defect or problem under study. The people who have the best understanding of the selected defect are typically those responsible for performing the task. Examples of when to perform causal analysis include the following: • When a stable process does not meet its specified quality and process- performance objectives • During the task, if and when problems warrant a causal analysis meeting • When a work product exhibits an unexpected deviation from its requirements Refer to the Quantitative Proj ect Management process area for more information about achieving the project’s quality and process- performance objectives.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 1052. Analyze selected defects and ot her problems to determine their root causes. Depending on the type and number of defects, it may make sense to first group the defects before identifying their root causes. Examples of methods to determine root causes include the following: • Cause-and-effect (fishbone) diagrams • Check sheets 3. Group the selected defects and ot her problems based on their root causes. Examples of cause groups, or categories, include the following: • Inadequate training • Breakdown of communications • Not accounting for all details of a task • Making mistakes in manual procedures (e.g., typing) • Process deficiency 4. Propose and document actions that need to be taken to prevent the future occurrence of simila r defects or other problems. Examples of proposed actions include changes to the following: • The process in question • Training • Tools • Methods • Communications • Work products Examples of specific actions include the following: • Providing training in common problems and techniques for preventing them • Changing a process so that error-prone steps do not occur • Automating all or part of a process • Reordering process activities • Adding process steps to prevent defects, such as task kickoff meetings to review common defects and actions to prevent them",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 106 An action proposal usually documents the following: • Originator of the action proposal • Description of the problem • Description of the defect cause • Defect cause category • Phase when the problem was introduced • Phase when the defect was identified • Description of the action proposal • Action proposal category SG 2 Address Causes of Defects Root causes of defects and other problems are systematically addressed to prevent their future occurrence. Projects operating according to a well-defined process will systematically analyze the operati on where problems still occur and implement process changes to elim inate root causes of selected problems. SP 2.1 Implement the Action Proposals Implement the selected action proposals that were developed in causal analysis. Action proposals describe the tasks necessary to remove the root causes of the analyz ed defects or problems and avoid their reoccurrence. Only changes that prove to be of value should be considered for broad implementation. Typical Work Products 1. Action proposals sele cted for implementation 2. Improvement proposals Subpractices 1. Analyze the action proposals and determine their priorities. Criteria for prioritizing action proposals include the following: • Implications of not addressing the defects • Cost to implement process improvements to prevent the defects • Expected impact on quality 2. Select the action proposal s that will be implemented.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 1073. Create action items for impl ementing the action proposals. Examples of information provided in an action item include the following: • Person responsible for implementing it • Description of the areas affected by it • People who are to be kept informed of its status • Next date that status will be reviewed • Rationale for key decisions • Description of implementation actions • Time and cost for identifying the defect and correcting it • Estimated cost of not fixing the problem To implement the action proposals, the following tasks must be done: • Make assignments • Coordinate the persons doing the work • Review the results • Track the action items to closure Experiments may be conducted for particularly complex changes. Examples of experiments include the following: • Using a temporarily modified process • Using a new tool Action items may be assigned to members of the causal analysis team, members of the project team, or other members of the organization. 4. Identify and remove similar def ects that may exist in other processes and work products. 5. Identify and docum ent improvement proposals for the organization’s set of standard processes. Refer to the Organizational I nnovation and Deployment process area for more information about the selection and deployment of improvement proposals for the organization’s set of standard processes. SP 2.2 Evaluate the Effect of Changes Evaluate the effect of changes on process performance.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 108 Refer to the Quantitative Project Management process area for more information about analyzing proce ss performance and creating process capability measures for selected processes. Once the changed process is deployed across the project, the effect of the changes must be checked to gat her evidence that the process change has corrected the problem and improved performance. Typical Work Products 1. Measures of perform ance and performance change Subpractices 1. Measure the change in the perfo rmance of the project's defined process as appropriate. This subpractice determines whether the selected change has positively influenced the process performance and by how much. An example of a change in the performance of the project’s defined design process would be the change in the defect density of the design documentation, as statistically measured through peer reviews before and after the improvement has been made. On a statistical process control chart, this would be represented by a change in the mean. 2. Measure the capab ility of the project's defined process as appropriate. This subpractice determines whether the selected change has positively influenced the ability of the process to meet its quality and process-performance objectives, as determined by relevant stakeholders. An example of a change in the capability of the project’s defined design process would be a change in the ability of the process to stay within its process- specification boundaries. This can be statistically measured by calculating the range of the defect density of design documentation, as collected in peer reviews before and after the improvement has been made. On a statistical process control chart, this would be represented by lowered control limits. SP 2.3 Record Data Record causal analysis and resolution data for use across the project and organization. Data are recorded so that other projects and organizations can make appropriate process changes and achieve similar results.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 109Record the following: • Data on defects and other problems that were analyzed • Rationale for decisions • Action proposals from ca usal analysis meetings • Action items resulting from action proposals • Cost of the analysis and resolution activities • Measures of changes to the per formance of the defined process resulting from resolutions Typical Work Products 1. Causal analysis and resolution records Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the causal analysis and resolution process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the causal analysis and resolution process.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 110 Elaboration: This policy establishes organizati onal expectations for identifying and systematically addressing root caus es of defects and other problems. GP 2.2 Plan the Process Establish and maintain the plan for performing the causal analysis and resolution process. Elaboration: This plan for performing the causal analysis and resolution process can be included in (or referenced by) the pr oject plan, which is described in the Project Planning process area. Th is plan differs from the action proposals and associated action item s described in several specific practices in this process area. The plan called for in this generic practice would address the projec t’s overall causal analysis and resolution process (perhaps tailored from a standard process maintained by the organization). In contrast, t he process action proposals and associated action item s address the activities needed to remove a specific root cause under study. GP 2.3 Provide Resources Provide adequate resources for performing the causal analysis and resolution process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Database systems • Process modeling tools • Statistical analysis packages • Tools, methods, and analysis techniques (e.g., Ishikawa or fishbone diagram, Pareto analysis, histograms, process capability studies, or control charts) GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the causal analysis and resolution process. GP 2.5 Train People Train the people performing or supporting the causal analysis and resolution process as needed.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 111Elaboration: Examples of training topics include the following: • Quality management methods (e.g., root cause analysis) GP 2.6 Manage Configurations Place designated work products of the causal analysis and resolution process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Action proposals • Action proposals selected for implementation • Causal analysis and resolution records GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the causal analysis and resolution process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Conducting causal analysis • Assessing the action proposals GP 2.8 Monitor and Control the Process Monitor and control the causal analysis and resolution process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of root causes removed • Change in quality or process performance per instance of the causal analysis and resolution process • Schedule of activities for implementing a selected action proposal",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 112 GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the causal analysis and resolution process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Determining causes of defects • Addressing causes of defects Examples of work products reviewed include the following: • Action proposals selected for implementation • Causal analysis and resolution records GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the causal analysis and resolution process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined causal analysis and resolution process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the causal analysis and resolution process to support the future use and improvement of the organization’s processes and process assets.",
        "CMMI for Development Version 1.2 Causal Analysis and Resolution (CAR) 113Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Action proposals • Number of action proposals that are open and for how long • Action proposal status reports Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the causal analysis and resolution process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the causal analysis and resolution process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the causal analysis and resolution process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the causal analysis and resolution process.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 114 CONFIGURATION MANAGEMENT A Support Process Area at Maturity Level 2 Purpose The purpose of Configuration Manage ment (CM) is to establish and maintain the integrity of work products using configuration identification, configuration control, c onfiguration status acc ounting, and configuration audits. Introductory Notes The Configuration Management proc ess area involves the following: • Identifying the conf iguration of selected work products that compose the baselines at given points in time • Controlling changes to configuration items • Building or providing specifications to build work products from the configuration m anagement system • Maintaining the integrity of baselines • Providing accurate status and current configuration data to developers, end users, and customers The work products placed under c onfiguration management include the products that are delivered to the customer, designated internal work products, acquired products, tools, and other items that are used in creating and describing these work products. (See t he definition of “configuration management” in the glossary.) Acquired products may need to be placed under configuration management by both the supplier and the project. Provisions for conducting configuratio n management should be es tablished in supplier agreements. Methods to ensure that the data is complete and consistent should be es tablished and maintained. Refer to the Supplier Agreement Management process area for more information about establishing and maintaining agreements with suppliers.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 115Examples of work products that may be placed under configuration management include the following: • Plans • Process descriptions • Requirements • Design data • Drawings • Product specifications • Code • Compilers • Product data files • Product technical publications Configuration management of work products may be performed at several levels of granularity. C onfiguration items can be decomposed into configuration components and conf iguration units. Only the term “configuration item” is used in this process area. Ther efore, in these practices, “configuration item” ma y be interpreted as “configuration component” or “configuration unit” as appropriate. (See the definition of “configuration item” in the glossary.) Baselines provide a stable basis for continuing evolution of configuration items. An example of a baseline is an approved description of a product that includes internally consistent versions of requirements, requirement traceability matrices, design, discipline-specific items, and end-user documentation. Baselines are added to the configur ation management system as they are developed. Changes to baselines and the release of work products built from the confi guration management system are systematically controlled and monitored via t he configuration control, change management, and configuration auditing functions of configuration management. This process area applies not onl y to configuration management on projects, but also to configurati on management on organizational work products such as standards, procedures, and reuse libraries. Configuration management is focuse d on the rigorous control of the managerial and technical aspects of work products, including the delivered system.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 116 This process area covers the practi ces for performing the configuration management function and is applicable to all work products that are placed under confi guration management. Related Process Areas Refer to the Project Planning process area for information on developing plans and work breakdown st ructures, which may be useful for determining conf iguration items. Refer to the Project Monitoring and Control process area for more information about performance analyses and corrective actions. Specific Goal and Practice Summary SG 1 Establish Baselines SP 1.1 Identify Configuration Items SP 1.2 Establish a Configuration Management System SP 1.3 Create or Release Baselines SG 2 Track and Control Changes SP 2.1 Track Change Requests SP 2.2 Control Configuration Items SG 3 Establish Integrity SP 3.1 Establish Configuration Management Records SP 3.2 Perform Configuration Audits Specific Practices by Goal SG 1 Establish Baselines Baselines of identified work products are established. Specific practices to establish base lines are covered by this specific goal. The specific practices under the Track and Control Changes specific goal serve to maintain the baselines. The specific practices of the Establish Integrity specific goal document and audit the integrity of the baselines. SP 1.1 Identify Configuration Items Identify the configuration items, components, and related work products that will be placed under configuration management.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 117Configuration identificatio n is the selection, cr eation, and s pecification of the following: • Products that are delivered to the customer • Designated internal work products • Acquired products • Tools and other capital assets of the project's work environment • Other items that are used in creating and describing these work products Items under configuration managemen t will include specifications and interface documents that define the requirements for the product. Other documents, such as test result s, may also be included, depending on their criticality to defining the product. A “configuration item” is an ent ity designated for configuration management, which may consis t of multiple related work products that form a baseline. This logical groupi ng provides ease of identification and controlled access. The selection of work products for configuration management should be based on criteria established during planning. Typical Work Products 1. Identified conf iguration items Subpractices 1. Select the configuration items and the work produc ts that compose them based on documented criteria. Example criteria for selecting configuration items at the appropriate work product level include the following: • Work products that may be used by two or more groups • Work products that are expected to change ov er time either because of errors or change of requirements • Work products that are dependent on each other in that a change in one mandates a change in the others • Work products that are critical for the project",
        "CMMI for Development Version 1.2 Configuration Management (CM) 118 Examples of work products that may be part of a configuration item include the following: • Process descriptions • Requirements • Design • Test plans and procedures • Test results • Interface descriptions • Drawings • Source code • Tools (e.g., compilers) 2. Assign unique identifiers to configuration items. 3. Specify the im portant characteristics of each configuration item. Example characteristics of configuration items include author, document or file type, and programming language for software code files. 4. Specify when each conf iguration item is pl aced under configuration management. Example criteria for determining when to place work products under configuration management include the following: • Stage of the project lifecycle • When the work product is ready for test • Degree of control desired on the work product • Cost and schedule limitations • Customer requirements 5. Identify the owner responsibl e for each configuration item. SP 1.2 Establish a Configuration Management System Establish and maintain a configuration management and change management system for controlling work products. A configuration management system includes the storage media, the procedures, and the tools for acce ssing the configuration system. A change management system incl udes the storage media, the procedures, and tools for recordin g and accessing change requests.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 119Typical Work Products 1. Configuration management system with controlled work products 2. Configuration management syst em access control procedures 3. Change request database Subpractices 1. Establish a mechanism to m anage multiple control levels of configuration management. The level of control is typically selected based on project objectives, risk, and/or resources. Control levels may vary in relation to the project lifecycle, type of system under development, and specific project requirements. Example levels of control include the following: • Create – controlled by author • Engineering – notification to relevant stakeholders when changes are made • Development – lower level CCB control • Formal – higher level CCB control with customer involvement Levels of control can range from informal control that simply tracks changes made when the configuration items are being developed to formal configuration control using baselines that can only be changed as part of a formal configuration management process. 2. Store and retrieve configurat ion items in a configuration management system. Examples of configuration management systems include the following: • Dynamic (or author’s) systems contain components currently being created or revised. They are in the author’s wo rkspace and are controlled by the author. Configuration items in a dynamic system are under version control. • Master (or controlled) systems contain current baselines and changes to them. Configuration items in a master syst em are under full configuration management as described in this process area. • Static systems contain archives of vari ous baselines released for use. Static systems are under full configuration m anagement as described in this process area. 3. Share and transfer configurat ion items between control levels within the configurat ion management system. 4. Store and recover archived vers ions of configuration items. 5. Store, update, and retrieve c onfiguration management records.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 120 6. Create configurati on management reports from the configuration management system. 7. Preserve the contents of the configuration management system. Examples of preservation functions of the configuration management system include the following: • Backups and restoration of configuration management files • Archiving of configuration management files • Recovery from configuration management errors 8. Revise the configuration m anagement structure as necessary. SP 1.3 Create or Release Baselines Create or release baselines for internal use and for delivery to the customer. A baseline is a set of specificati ons or work products that has been formally reviewed and agreed on, that thereafter serves as the basis for further development or delivery, and that can be changed only through change control procedures. A baseline represents the assignment of an identifier to a configurat ion item or a collection of configuration items and associated entities. As a product evolves, several baselines may be used to control its dev elopment and testing. For Systems Engineering One common set of baselines includes the system-level requirements, system-element-level design requirement s, and the product definition at the end of development/beginning of producti on. These are typically referred to as the “functional baseline,” “alloca ted baseline,” and “product baseline.” For Software Engineering A software baseline can be a set of requirements, design, source code files and the associated executable code, bu ild files, and user documentation (associated entities) that have been assigned a unique identifier. Typical Work Products 1. Baselines 2. Description of baselines Subpractices 1. Obtain authorization from the configuration control board (CCB) before creating or releasing base lines of configuration items.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 1212. Create or release baselines only from configuration items in the configuration management system. 3. Document the set of configurat ion items that are contained in a baseline. 4. Make the current set of baselines readily available. SG 2 Track and Control Changes Changes to the work products under configuration management are tracked and controlled. The specific practices under this s pecific goal serve to maintain the baselines after they are established by the specif ic practices under the Establish Baselines specific goal. SP 2.1 Track Change Requests Track change requests for the configuration items. Change requests address not only ne w or changed requirements, but also failures and defects in the work products. Change requests are analyzed to det ermine the impact that the change will have on the work product, re lated work products, budget, and schedule. Typical Work Products 1. Change requests Subpractices 1. Initiate and record change requests in the change request database. 2. Analyze the impac t of changes and fixes proposed in the change requests. Changes are evaluated through activities that ensure that they are consistent with all technical and project requirements. Changes are evaluated for their impact beyond immediate project or contract requirements. Changes to an item used in multiple products can resolve an immediate issue while causing a problem in other applications. 3. Review change reques ts that will be addressed in the next baseline with the relevant stakeholders and get their agreement. Conduct the change request review with appropriate participants. Record the disposition of each change request and the rationale for the decision, including success criteria, a brief action plan if appropriate, and needs met or unmet by the",
        "CMMI for Development Version 1.2 Configuration Management (CM) 122 change. Perform the actions required in the disposition, and report the results to relevant stakeholders. 4. Track the status of c hange requests to closure. Change requests brought into the system need to be handled in an efficient and timely manner. Once a change request has been processed, it is critical to close the request with the appropriate approved action as soon as it is practical. Actions left open result in larger than necessary status lists, which in turn result in added costs and confusion. SP 2.2 Control Configuration Items Control changes to the configuration items. Control is maintained over the c onfiguration of the work product baseline. This control includes tracki ng the configurati on of each of the configuration items, approving a ne w configuration if necessary, and updating the baseline. Typical Work Products 1. Revision history of configuration items 2. Archives of the baselines Subpractices 1. Control changes to configurati on items throughout the life of the product. 2. Obtain appropriate authoriza tion before changed configuration items are entered into the c onfiguration management system. For example, authorization may come from the CCB, the project manager, or the customer. 3. Check in and check out configurat ion items from the configuration management system for incorporati on of changes in a manner that maintains the correctness and integrit y of the confi guration items. Examples of check-in and check-out steps include the following: • Confirming that the revisions are authorized • Updating the configuration items • Archiving the replaced baseline and retrieving the new baseline 4. Perform reviews to ensur e that changes have not caused unintended effects on the baselines (e.g., ensure that the changes have not compromised the safety and/ or security of the system).",
        "CMMI for Development Version 1.2 Configuration Management (CM) 1235. Record changes to configurat ion items and the reasons for the changes as appropriate. If a proposed change to the work product is accepted, a schedule is identified for incorporating the change into the work product and other affected areas. Configuration control mechanisms can be tailored to categories of changes. For example, the approval considerations could be less stringent for component changes that do not affect other components. Changed configuration items are released after review and approval of configuration changes. Changes are not official until they are released. SG 3 Establish Integrity Integrity of baselines is established and maintained. The integrity of the bas elines, established by processes associated with the Establish Baselines specific goal, and maintained by processes associated with the Track and Cont rol Changes specific goal, is provided by the specific practi ces under this specific goal. SP 3.1 Establish Configuration Management Records Establish and maintain records describing configuration items. Typical Work Products 1. Revision history of configuration items 2. Change log 3. Copy of the change requests 4. Status of configuration items 5. Differences between baselines Subpractices 1. Record configuration management actions in sufficient detail so the content and status of each conf iguration item is known and previous versions can be recovered. 2. Ensure that relevant stakehol ders have access to and knowledge of the configuration status of the configur ation items. Examples of activities for communicating configuration status include the following: • Providing access permissions to authorized end users • Making baseline copies readily available to authorized end users 3. Specify the latest ve rsion of the baselines.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 124 4. Identify the version of configuration item s that constitute a particular baseline. 5. Describe the differences between successive baselines. 6. Revise the status and history (i.e., changes and other actions) of each configuration it em as necessary. SP 3.2 Perform Configuration Audits Perform configuration audits to maintain integrity of the configuration baselines. Configuration audits confirm t hat the resulting baselines and documentation conform to a specified standard or requirement. Audit results should be recorded as approp riate. (See the glossary for a definition of “conf iguration audit.”) Examples of audit types include the following: • Functional Configuration Audits (FCA) – Audits conducted to verify that the as- tested functional characteristics of a configuration item have achieved the requirements specified in its functional baseline documentation and that the operational and support documentation is complete and satisfactory. • Physical Configuration Audit (PCA) – Audits conducted to verify that the as-built configuration item conforms to the technical documentation that defines it. • Configuration management audits – Audits conducted to confirm that configuration management records and configuration items are complete, consistent, and accurate. Typical Work Products 1. Configuration audit results 2. Action items Subpractices 1. Assess the integr ity of the baselines. 2. Confirm that the configurat ion management records correctly identify the confi guration items. 3. Review the structure and integrity of the items in the configuration management system. 4. Confirm the completeness and co rrectness of the items in the configuration management system. Completeness and correctness of the content is based on the requirements as stated in the plan and the disposition of approved change requests.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 1255. Confirm compliance with app licable configuration management standards and procedures. 6. Track action items from the audit to closure. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the configuration management process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the configuration management process. Elaboration: This policy establishes organizational expectations for establishing and maintaining baselines, tracking and controlling changes to the work products (under configuration management), and establishing and maintaining integrity of the baselines. GP 2.2 Plan the Process Establish and maintain the plan for performing the configuration management process. Elaboration: This plan for performing the confi guration management process can be included in (or referenced by) the proj ect plan, which is described in the Project Planning process area.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 126 GP 2.3 Provide Resources Provide adequate resources for performing the configuration management process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Configuration management tools • Data management tools • Archiving and reproduction tools • Database programs GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the configuration management process. GP 2.5 Train People Train the people performing or supporting the configuration management process as needed. Elaboration: Examples of training topics include the following: • Roles, responsibilities, and authority of the configuration management staff • Configuration management standards, procedures, and methods • Configuration library system GP 2.6 Manage Configurations Place designated work products of the configuration management process under appropriate levels of control. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.6 and the Configuration Management process area.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 127Examples of work products placed under control include the following: • Access lists • Change status reports • Change request database • CCB meeting minutes • Archived baselines GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the configuration management process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing baselines • Reviewing configuration management system reports and resolving issues • Assessing the impact of changes for the configuration items • Performing configuration audits • Reviewing the results of configuration management audits GP 2.8 Monitor and Control the Process Monitor and control the configuration management process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of changes to configuration items • Number of configuration audits conducted • Schedule of CCB or audit activities GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the configuration management process against its process description, standards, and procedures, and address noncompliance.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 128 Elaboration: Examples of activities reviewed include the following: • Establishing baselines • Tracking and controlling changes • Establishing and maintaining integrity of baselines Examples of work products reviewed include the following: • Archives of the baselines • Change request database GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the configuration management process with higher level management and resolve issues. Staged Only GG3 and its practices do not apply for a maturity level 2 rating, but do apply for a maturity level 3 rating and above. Continuous/Maturity Levels 3 - 5 Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined configuration management process.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 129Continuous/Maturity Levels 3 - 5 Only GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the configuration management process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Trends in the status of configuration items • Configuration audit results • Change request aging reports Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the configuration management process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the configuration management process to achieve the established quantitative quality and process- performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the configuration management process in fulfilling the relevant business objectives of the organization.",
        "CMMI for Development Version 1.2 Configuration Management (CM) 130 Continuous Only GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the configuration management process.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 131DECISION ANALYSIS AND RESOLUTION A Support Process Area at Maturity Level 3 Purpose The purpose of Decision Analysis and Resolution (DAR) is to analyze possible decisions using a formal evaluation process that evaluates identified alternatives agai nst established criteria. Introductory Notes The Decision Analysis and Reso lution process area involves establishing guidelines to determine which issues should be subjected to a formal evaluation process and then applying formal evaluation processes to these issues. A formal evaluation process is a structured approach to evaluating alternative solutions against est ablished criteria to determine a recommended solution to address an issue. A formal evaluation process involves the following actions: • Establishing the criteria for evaluating alternatives • Identifying alternative solutions • Selecting methods for evaluating alternatives • Evaluating the alternative soluti ons using the est ablished criteria and methods • Selecting recommended solutions fr om the alternatives based on the evaluation criteria Rather than using the phr ase “alternative soluti ons to address issues” each time it is needed, we will use one of two shorter phrases: “alternative solutions” or “alternatives.” A formal evaluation process reduces the subjective nature of the decision and has a higher probability of selecting a solution that meets the multiple demands of relevant stakeholders. While the primary application of th is process area is to technical concerns, formal evaluation proce sses can also be applied to many nontechnical issues, particularly wh en a project is being planned. Issues that have multiple alternat ive solutions and evaluation criteria lend themselves to a formal evaluation process.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 132 Trade studies of equipment or software are typical examples of formal evaluation processes. During planning, specific issues requiring a form al evaluation process are identified. Typical issues incl ude selection among architectural or design alternatives, use of reusable or commercial off-the-shelf (COTS) components, supplier selection, engi neering support environments or associated tools, test environments, delivery alternatives, and logistics and production. A formal evaluati on process can also be used to address a make-or-buy decision, the development of manufacturing processes, the selection of distri bution locations, and other decisions. Guidelines are created for decidi ng when to use formal evaluation processes to address unplanned issues. Guidelines often suggest using formal evaluation processes when is sues are associated with medium to high risks or when issues affe ct the ability to achieve project objectives. Formal evaluation processes can vary in formality, type of criteria, and methods employed. Less formal dec isions can be analyzed in a few hours, use only a few criteria (e.g., effectiveness and cost to implement), and result in a one- or two-page report. More formal decisions may require separate plans , months of effort, meetings to develop and approve criteria, simulations, prototypes, piloting, and extensive documentation. Both numeric and non-numeric crit eria can be used in a formal evaluation process. Numeric criteria use weights to reflect the relative importance of the criteria. Non-numeric criteria use a more subjective ranking scale (e.g., high, medium, or low). More formal decisions may require a full trade study. A formal evaluation process ident ifies and evaluates alternative solutions. The eventual selection of a solution may involve iterative activities of identific ation and evaluation. Po rtions of identified alternatives may be combined, emerging technologies may change alternatives, and the business sit uation of vendors may change during the evaluation period. A recommended alternative is acco mpanied by documentation of the selected methods, criteria, alternatives, and rationale for the recommendation. The documentation is distributed to relevant stakeholders; it provides a record of the formal evaluation process and rationale that are useful to other proj ects that encounter a similar issue. While some of the decisions made throughout the life of the project involve the use of a formal eval uation process, others do not. As mentioned earlier, guidelines shoul d be established to determine which issues should be subj ected to a formal evaluation process.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 133Related Process Areas Refer to the Project Planning proc ess area for more information about general planning for projects. Refer to the Integrated Project Management process area for more information about establishing t he project’s defined process. The project’s defined process includes a formal evaluation process for each selected issue and incorporates t he use of guidelines for applying a formal evaluation proce ss to unforeseen issues. Refer to the Risk Management process area for more information about identifying and mitigating risks. A fo rmal evaluation process is often used to address issues with identif ied medium or high risks. Selected solutions typically affe ct risk mitigation plans. Specific Goal and Practice Summary SG 1 Evaluate Alternatives SP 1.1 Establish Guidelines for Decision Analysis SP 1.2 Establish Evaluation Criteria SP 1.3 Identify Alternative Solutions SP 1.4 Select Evaluation Methods SP 1.5 Evaluate Alternatives SP 1.6 Select Solutions Specific Practices by Goal SG 1 Evaluate Alternatives Decisions are based on an evaluation of alternatives using established criteria. Issues requiring a formal evaluation process may be identified at any time. The objective should be to identif y issues as early as possible to maximize the time available to resolve them. SP 1.1 Establish Guidelines for Decision Analysis Establish and maintain guidelines to determine which issues are subject to a formal evaluation process. Not every decision is significant enough to require a formal evaluation process. The choice between the tr ivial and the truly important will be unclear without explicit guidance. Wh ether a decision is significant or not is dependent on the project and circumstances, and is determined by the established guidelines.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 134 Typical guidelines for determining wh en to require a formal evaluation process include the following: • When a decision is directly rela ted to topics assessed as being of medium or high risk • When a decision is related to changing work products under configuration management • When a decision would cause schedule delays over a certain percentage or specif ic amount of time • When a decision affects the abilit y to achieve project objectives • When the costs of the formal evaluation process are reasonable when compared to the decision’s impact • When a legal obligation ex ists during a solicitation Refer to the Risk Management process area for more information about determining which issues ar e medium or high risk. Examples of when to use a formal evaluation process include the following: • On decisions involving the procurement of material when 20 percent of the material parts constitute 80 percent of the total material costs • On design-implementation decisions when technical performance failure may cause a catastrophic failure (e.g., safety of flight item) • On decisions with the potential to significantly reduce design risk, engineering changes, cycle time, response time, and production costs (e.g., to use lithography models to assess form and fit capability before releasing engineering drawings and production builds) Typical Work Products 1. Guidelines for when to apply a formal evaluation process Subpractices 1. Establish guidelines. 2. Incorporate the use of the gui delines into the defined process where appropriate. Refer to the Integrated Project Management process area for more information about establishing the project’s defined process. SP 1.2 Establish Evaluation Criteria Establish and maintain the criteria for evaluating alternatives, and the relative ranking of these criteria.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 135The evaluation criteria provide t he basis for evaluating alternative solutions. The criteria are ranked so that the highest ranked criteria exert the most influence on the evaluation. This process area is referenced by many other process areas in the model, and there are many contexts in which a formal evaluation process can be used. Therefore, in some situations you may find that criteria have already been defined as part of another process. This specific practice does not suggest that a second development of criteria be conducted. Document the evaluation criteria to minimize the possibility that decisions will be second-guessed, or that the reason for making the decision will be forgotten. Decisions ba sed on criteria that are explicitly defined and established remove ba rriers to stakeholder buy-in. Typical Work Products 1. Documented evaluation criteria 2. Rankings of cr iteria importance Subpractices 1. Define the criteria for ev aluating alternative solutions. Criteria should be traceable to requirements, scenarios, business case assumptions, business objectives, or other documented sources. Types of criteria to consider include the following: • Technology limitations • Environmental impact • Risks • Total ownership and lifecycle costs 2. Define the range and scale for ranking the evaluation criteria. Scales of relative importance for evaluation criteria can be established with non- numeric values or with formulas that relate the evaluation parameter to a numeric weight. 3. Rank the criteria. The criteria are ranked according to the defined range and scale to reflect the needs, objectives, and priorities of the relevant stakeholders. 4. Assess the criteria and their relative importance. 5. Evolve the evaluation criter ia to improve their validity. 6. Document the rationale for the se lection and rejection of evaluation criteria.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 136 Documentation of selection criteria and rationale may be needed to justify solutions or for future reference and use. SP 1.3 Identify Alternative Solutions Identify alternative solutions to address issues. A wider range of alternatives c an surface by soliciting as many stakeholders as practical for input. I nput from stakeholders with diverse skills and backgrounds can help teams identify and address assumptions, constraints, and bias es. Brainstorming sessions may stimulate innovative alternatives through rapid interaction and feedback. Sufficient candidate solutions may not be furnished for analysis. As the analysis proceeds, other alternatives should be added to the list of potential candidate solutions. The gen eration and cons ideration of multiple alternatives early in a decision analysis and resolution process increases the likelihood that an a cceptable decision will be made, and that consequences of the decision w ill be understood. Typical Work Products 1. Identified alternatives Subpractices 1. Perform a literature search. A literature search can uncover what others have done both inside and outside the organization. It may provide a deeper understanding of the problem, alternatives to consider, barriers to implementation, existing trade studies, and lessons learned from similar decisions. 2. Identify alternatives for consideration in addi tion to those that may be provided with the issue. Evaluation criteria are an effective starting point for identifying alternatives. The evaluation criteria identify the priorities of the relevant stakeholders and the importance of technical, logistical, or other challenges. Combining key attributes of existing alternatives can generate additional and sometimes stronger alternatives. Solicit alternatives from relevant stakeholders. Brainstorming sessions, interviews, and working groups can be used effectively to uncover alternatives. 3. Document the proposed alternatives. SP 1.4 Select Evaluation Methods Select the evaluation methods. Methods for evaluating alternative so lutions against established criteria can range from simulations to the use of probabilistic models and",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 137decision theory. These me thods need to be carefully selected. The level of detail of a method should be commensurate with cost, schedule, performance, and risk impacts. While many problems may need only one evaluation method, some problems may require multiple met hods. For instance, simulations may augment a trade study to determine which design alternative best meets a given criterion. Typical Work Products 1. Selected evaluation methods Subpractices 1. Select the methods based on t he purpose for analyzing a decision and on the availability of the in formation used to support the method. For example, the methods used for evaluating a solution when requirements are weakly defined may be different from the methods used when the requirements are well defined. Typical evaluation methods include the following: • Modeling and simulation • Engineering studies • Manufacturing studies • Cost studies • Business opportunity studies • Surveys • Extrapolations based on field experience and prototypes • User review and comment • Testing • Judgment provided by an expert or group of experts (e.g., Delphi Method) 2. Select evaluation methods bas ed on their ability to focus on the issues at hand without being ov erly influenced by side issues. Results of simulations can be skewed by random activities in the solution that are not directly related to the issues at hand. 3. Determine the measures needed to support the evaluation method. Consider the impact on cost, schedule, performance, and risks.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 138 SP 1.5 Evaluate Alternatives Evaluate alternative solutions using the established criteria and methods. Evaluating alternative solutions involves analysis, discussion, and review. Iterative cycles of anal ysis are sometimes necessary. Supporting analyses, experimentati on, prototyping, piloting, or simulations may be needed to subs tantiate scoring and conclusions. Often, the relative importance of crit eria is imprecise and the total effect on a solution is not apparent until afte r the analysis is performed. In cases where the resulting scores diffe r by relatively small amounts, the best selection among alternative solutions may not be clear cut. Challenges to criteria and assumptions should be encouraged. Typical Work Products 1. Evaluation results Subpractices 1. Evaluate the proposed alternativ e solutions using the established evaluation criteria and selected methods. 2. Evaluate the assumptions related to the evaluation criteria and the evidence that supports the assumptions. 3. Evaluate whether uncertainty in t he values for alternative solutions affects the evaluation and address as appropriate. For instance, if the score can vary between two values, is the difference significant enough to make a difference in the final solution set? Does the variation in score represent a high risk? To address these concerns, simulations may be run, further studies may be performed, or evaluation criteria may be modified, among other things. 4. Perform simulations, modeling, pr ototypes, and pilots as necessary to exercise the evaluation cr iteria, methods, and alternative solutions. Untested criteria, their relative importance, and supporting data or functions may cause the validity of solutions to be questioned. Criteria and their relative priorities and scales can be tested with trial runs against a set of alternatives. These trial runs of a select set of criteria allow for the evaluation of the cumulative impact of the criteria on a solution. If the trials reveal problems, different criteria or alternatives might be considered to avoid biases. 5. Consider new alternative soluti ons, criteria, or methods if the proposed alternatives do not test well; repeat the evaluations until alternatives do test well. 6. Document the result s of the evaluation.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 139Document the rationale for the addition of new alternatives or methods and changes to criteria, as well as the results of interim evaluations. SP 1.6 Select Solutions Select solutions from the alternatives based on the evaluation criteria. Selecting solutions involves weighing the results from the evaluation of alternatives. Risks associated with im plementation of the solutions must be assessed. Typical Work Products 1. Recommended solutions to address significant issues Subpractices 1. Assess the risks associated with implementing the recommended solution. Refer to the Risk Management process area for more information about identifying and managing risks. Decisions must often be made with incomplete information. There can be substantial risk associated with the decision because of having incomplete information. When decisions must be made according to a specific schedule, time and resources may not be available for gathering complete information. Consequently, risky decisions made with incomplete information may require re-analysis later. Identified risks should be monitored. 2. Document the results and rationale for the recommended solution. It is important to record both why a solution is selected and why another solution was rejected. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 140 Continuous Only GP 1.1 Perform Specific Practices Perform the specific practices of the decision analysis and resolution process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the decision analysis and resolution process. Elaboration: This policy establishes organizati onal expectations for selectively analyzing possible decisi ons using a formal ev aluation process that evaluates identified alternatives agai nst established criteria. The policy should also provide guidance on wh ich decisions require a formal evaluation process. GP 2.2 Plan the Process Establish and maintain the plan for performing the decision analysis and resolution process. Elaboration: This plan for performing the decis ion analysis and resolution process can be included in (or referenced by ) the project plan, which is described in the Project Planning process area.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 141GP 2.3 Provide Resources Provide adequate resources for performing the decision analysis and resolution process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Simulators and modeling tools • Prototyping tools • Tools for conducting surveys GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the decision analysis and resolution process. GP 2.5 Train People Train the people performing or supporting the decision analysis and resolution process as needed. Elaboration: Examples of training topics include the following: • Formal decision analysis • Methods for evaluating alternative solutions against criteria GP 2.6 Manage Configurations Place designated work products of the decision analysis and resolution process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Guidelines for when to apply a formal evaluation process • Evaluation reports containing recommended solutions GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the decision analysis and resolution process as planned.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 142 Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing guidelines for which issues are subject to a formal evaluation process • Establishing evaluation criteria • Identifying and evaluating alternatives • Selecting evaluation methods • Selecting solutions GP 2.8 Monitor and Control the Process Monitor and control the decision analysis and resolution process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Cost-to-benefit ratio of using formal evaluation processes • Schedule for the execution of a trade study GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the decision analysis and resolution process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Evaluating alternatives using established criteria and methods Examples of work products reviewed include the following: • Guidelines for when to apply a formal evaluation process • Evaluation reports containing recommended solutions GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the decision analysis and resolution process with higher level management and resolve issues.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 143Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined decision analysis and resolution process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the decision analysis and resolution process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Number of alternatives considered • Evaluation results • Recommended solutions to address significant issues Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the decision analysis and resolution process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the decision analysis and resolution process to achieve the established quantitative quality and process-performance objectives.",
        "CMMI for Development Version 1.2 Decision Analysis and Resolution (DAR) 144 Continuous Only GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the decision analysis and resolution process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the decision analysis and resolution process.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 145INTEGRATED PROJECT MANAGEMENT +IPPD A Project Management Process Area at Maturity Level 3 Purpose The purpose of Integrated Project Management (IPM) is to establish and manage the project and the in volvement of the relevant stakeholders according to an integrat ed and defined process that is tailored from the organization’s set of standard processes. IPPD Addition For IPPD, Integrated Project M anagement +IPPD also covers the establishment of a shared vision for the project an d the establishment of integrated teams that will carry out objectives of the project. Introductory Notes Integrated Project Management involves the following: • Establishing the project’s defined process at project startup by tailoring the organization’s set of standard processes • Managing the project using the project’s defined process • Establishing the work environm ent for the project based on the organization's work environment standards • Using and contributing to the organizational process assets • Enabling relevant stakeholders’ concerns to be identified, considered, and, when appropri ate, addressed during the development of the product • Ensuring that the relevant stak eholders perform their tasks in a coordinated and timely manner (1) to address product and product component requirements, plans, objec tives, problems, and risks; (2) to fulfill their commitments; and (3 ) to identify, track, and resolve coordination issues IPPD Addition Integrated Project Management +IPPD also involves the following: • Establishing a shared vision for the project • Establishing integrated teams that are tasked to accomplish project objectives",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 146 The integrated and defined process that is tailored from the organization’s set of standard proce sses is called the project’s defined process. Managing the project’s effort, cost, schedule, staffing, risks, and other factors is tied to the tasks of the project’s defined process. The implementation and management of t he project’s defined process are typically described in the project plan. Certain activities may be covered in other plans that affect the projec t, such as the quality assurance plan, risk management strategy, and the c onfiguration management plan. Since the defined process for each project is tailored from the organization’s set of standard proce sses, variability among projects is typically reduced and projects can more easily share process assets, data, and lessons learned. This process area also addresses t he coordination of all activities associated with the project such as the following: • Development activities (e.g., requirements development, design, and verification) • Service activities (e.g., de livery, help desk, operations, and customer contact) • Acquisition activities (e.g., solic itation, contract monitoring, and transition to operation) • Support activities (e.g., confi guration managemen t, documentation, marketing, and training) The working interfaces and intera ctions among relevant stakeholders internal and external to the project are planned and managed to ensure the quality and integrity of the entire product. Relevant stakeholders participate, as appropriate, in defin ing the project’s defined process and the project plan. Reviews and exchang es are regularly conducted with the relevant stakeholders to ensure that coordination issues receive appropriate attention and everyone involved with the project is appropriately aware of the status , plans, and activities. (See the definition of “relevant stakeholder” in the glossary.) In defining the project’s defined process, formal inte rfaces are created as necessary to ensure that appropriate coordi nation and collaboration occurs. This process area applies in any or ganizational structure, including projects that are structured as li ne organizations, matrix organizations, or integrated teams. The term inology should be appropriately interpreted for the organizational structure in place.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 147Related Process Areas Refer to the Project Planning proc ess area for more information about planning the project, which includes identifying relevant stakeholders and their appropriate involvement in the project. Refer to the Project Monitoring and Control process area for more information about monitori ng and controlling the project. Refer to the Verification process area for more information about peer reviews. Refer to the Organizational Process Definition process area for more information about organizational pr ocess assets and work environment standards. Refer to the Measurement and Analysis process area for more information about defining a pr ocess for measuring and analyzing processes. IPPD Addition Refer to the Organizational Process Definition +IPPD process area for more information about creating the organizational rules and guidelines for IPPD. Specific Goal and Practice Summary SG 1 Use the Project’s Defined Process SP 1.1 Establish the Project’s Defined Process SP 1.2 Use Organizational Process Assets for Planning Project Activities SP 1.3 Establish the Project's Work Environment SP 1.4 Integrate Plans SP 1.5 Manage the Project Using the Integrated Plans SP 1.6 Contribute to the Organizational Process Assets SG 2 Coordinate and Collaborate with Relevant Stakeholders SP 2.1 Manage Stakeholder Involvement SP 2.2 Manage Dependencies SP 2.3 Resolve Coordination Issues IPPD Addition SG 3 Apply IPPD Principles SP 3.1 Establish the Project’s Shared Vision SP 3.2 Establish the Integrated Team Structure SP 3.3 Allocate Requirements to Integrated Teams SP 3.4 Establish Integrated Teams SP 3.5 Ensure Collaboration among Interfacing Teams",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 148 Specific Practices by Goal SG 1 Use the Project’s Defined Process The project is conducted using a defined process that is tailored from the organization's set of standard processes. The project’s defined process must include those processes from the organization’s set of standard processes that address all processes necessary to acquire or develop and maintain the product. The product- related lifecycle processes, such as the manufacturing and support processes, are developed c oncurrently with the product. SP 1.1 Establish the Project’s Defined Process Establish and maintain the project's defined process from project startup through the life of the project. Refer to the Organizational Process Definition process area for more information about the organizational process assets. Refer to the Organizational Proc ess Focus process area for more information about organizational process needs and objectives and deploying the organization’s set of standard processes on projects. The project’s defined process consists of defined processes that form an integrated, coherent lifecycle for the project. IPPD Addition The project’s defined process supports IPPD with processes that • Make the integrated project management environment more amenable to collocated or distributed teams • Select the project's integrated team structure • Allocate limited personnel resources • Implement cross-integrated team communication The project's defined process should satisfy the project's contractual and operational needs, opportunities, and constraints. It is designed to provide a best fit for the project’ s needs. A project's defined process is based on the following factors: • Customer requirements • Product and product component requirements • Commitments • Organizational process needs and objectives",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 149• Organization’s set of standard processes and tailoring guidelines • Operational environment • Business environment Establishing the project’ s defined process at project startup helps to ensure that project staff and stakehol ders implement a set of activities needed to efficiently establish an in itial set of requirements and plans for the project. As the project progre sses, the description of the project’s defined process is elaborated and revis ed to better meet the project’s requirements and the organization’ s process needs and objectives. Also, as the organization’s set of standard processes change, the project’s defined process may need to be revised. Typical Work Products 1. The project’s defined process Subpractices 1. Select a lifecycle model fr om those available from the organizational process assets. Examples of project characteristics that could affect the selection of lifecycle models include the following: • Size of the project • Experience and familiarity of staff in implementing the process • Constraints such as cycle time and acceptable defect levels 2. Select the standard processes from the organization's set of standard processes that best fi t the needs of the project. 3. Tailor the organization’s set of standard processes and other organizational process assets acco rding to the tailoring guidelines to produce the project’s defined process. Sometimes the available lifecycle models and standard processes are inadequate to meet a specific project’s needs. Sometimes the project will be unable to produce required work products or measures. In such circumstances, the project will need to seek approval to deviate from what is required by the organization. Waivers are provided for this purpose. 4. Use other artifacts from the orga nization's process asset library as appropriate.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 150 Other artifacts may include the following: • Lessons-learned documents • Templates • Example documents • Estimating models 5. Document the project's defined process. The project's defined process covers all of the activities for the project and its interfaces to relevant stakeholders. Examples of project activities include the following: • Project planning • Project monitoring • Requirements development • Requirements management • Supplier management • Configuration management • Quality assurance • Risk management • Decision analysis and resolution • Product development and support • Solicitation 6. Conduct peer reviews of t he project's defined process. Refer to the Verification process area for more information about conducting peer reviews. 7. Revise the project's def ined process as necessary. SP 1.2 Use Organizational Process Assets for Planning Project Activities Use the organizational process assets and measurement repository for estimating and planning the project’s activities. Refer to the Organizational Process Definition process area for more information about organizational pr ocess assets and the organization’s measurement repository. Typical Work Products 1. Project estimates 2. Project plans",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 151Subpractices 1. Use the tasks and work products of the project's defined process as a basis for estimating and plan ning the project's activities. An understanding of the relationships among the various tasks and work products of the project's defined process, and of the roles to be performed by the relevant stakeholders, is a basis for developing a realistic plan. 2. Use the organization’s measurem ent repository in estimating the project’s planning parameters. This estimate typically includes the following: • Using appropriate historical data from this project or similar projects • Accounting for and recording similarities and differences between the current project and those projects whos e historical data will be used • Independently validating the historical data • Recording the reasoning, assumptions, and rationale used to select the historical data Examples of parameters that are considered for similarities and differences include the following: • Work product and task attributes • Application domain • Design approach • Operational environment • Experience of the people Examples of data contained in the organization’s measurement repository include the following: • Size of work products or other work product attributes • Effort • Cost • Schedule • Staffing • Defects • Response time • Service capacity • Supplier performance",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 152 SP 1.3 Establish the Project's Work Environment Establish and maintain the project's work environment based on the organization's work environment standards. An appropriate work environment for a project comprises an infrastructure of facilities, t ools, and equipment t hat people need to perform their jobs effectively in support of business and project objectives. The work environment a nd its components are maintained at a level of performance and reliabilit y indicated by the organizational work environment standards. As required, the project’s work environment or some of its components can be developed internally or acquired from external sources. IPPD Addition An effective work environment helps projects employing IPPD to conduct work using collocated or distributed integrated teams. Two-way communications media should be readily accessible by all relevant stakeholders in the project. The project’s work environment might encompass environments for product integration, verification, and validation or they might be separate environments. Refer to the Establish Work Environm ent Standards specific practice in the Organizational Process Definition process area for more information about work environment standards. Refer to the Establish the Product Integration Environment specific practice of the Product Integration process area for more information about establishing and maintaining the product integration environment for the project. Refer to the Establish the Verificati on Environment specific practice of the Verification process area for mo re information about establishing and maintaining the verification environment for the project. Refer to the Establish the Validation Environment specific practice of the Validation process area for more information about establishing and maintaining the validation environment for the project. Typical Work Products 1. Equipment and tools for the project 2. Installation, operation, and ma intenance manuals for the project work environment 3. User surveys and results 4. Usage, performance, and maintenance records",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 1535. Support services for the project’s work environment Subpractices 1. Plan, design, and install a wo rk environment for the project. The critical aspects of the project work environment are, like any other product, requirements driven. Work environment functionality and operations are explored with the same rigor as is done for any other product development. It may be necessary to make tradeoffs among performance, costs, and risks. The following are examples of each: • Performance considerations may include timely interoperable communications, safety, security, and maintainability. • Costs may include capital outlays, training, support structure, disassembly and disposal of existing environments, and operation and maintenance of the environment. • Risks may include workflow and project disruptions. Examples of equipment and tools include the following: • Office software • Decision support software • Project management tools • Requirements management tools, design tools • Configuration management tools • Evaluation tools • Test and/or evaluation equipment 2. Provide ongoing maintenance and operational support for the project’s work environment. Maintenance and support of the work environment can be accomplished either with capabilities found inside the organization or hired from outside the organization. Examples of maintenance and support approaches include the following: • Hiring people to perform the maintenance and support • Training people to perform the maintenance and support • Contracting the maintenance and support • Developing expert users for selected tools 3. Maintain the qualification of t he components of the project’s work environment.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 154 Components include software, databases, hardware, tools, test equipment, and appropriate documentation. Qualification of software includes appropriate certifications. Hardware and test equipment qualification includes calibration and adjustment records and traceability to calibration standards. 4. Periodically review how well t he work environment is meeting the project’s needs and supporting colla boration, and take action as appropriate. Examples of actions that might be taken include the following: • Adding new tools • Acquiring additional networks, equipment, training, and support SP 1.4 Integrate Plans Integrate the project plan and the other plans that affect the project to describe the project’s defined process. Refer to the Project Planning proc ess area for more information about establishing and maintain ing a project plan. Refer to the Organizational Process Definition process area for more information about organizational proc ess assets and, in particular, the organization’s meas urement repository. Refer to the Measurement and Analysis process area for more information about defining measures and measurement activities and using analytic techniques. Refer to the Risk Management process area for more information about identifying and analyzing risks. Refer to the Organizational Proc ess Focus process area for more information about organizational process needs and objectives. This specific practice extends the specific prac tices for establishing and maintaining a project plan to address additional planning activities such as incorporating the project’s def ined process, coordinating with relevant stakeholders, using or ganizational process assets, incorporating plans for peer review s, and establishing objective entry and exit criteria for tasks. The development of the project plan should account for current and projected needs, objectives, and r equirements of the organization, customer, suppliers, and end users, as appropriate.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 155IPPD Addition The plans of the integrated teams are included in this integration. Developing a complete project plan and the project’s defined process may require an iterative effort if a comp lex, multi-layered, integrated team structure is being deployed. Typical Work Products 1. Integrated plans Subpractices 1. Integrate other plans that affect the project with the project plan. Other plans that affect the project may include the following: • Quality assurance plans • Configuration management plans • Risk management strategy • Documentation plans 2. Incorporate into the project pl an the definitions of measures and measurement activities for managing the project. Examples of measures that would be incorporated include the following: • Organization’s common set of measures • Additional project-specific measures 3. Identify and analyze product and project interface risks. Examples of product and project interface risks include the following: • Incomplete interface descriptions • Unavailability of tools or test equipment • Availability of COTS components • Inadequate or ineffective team interfaces 4. Schedule the tasks in a sequenc e that accounts for critical development factors and project risks.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 156 Examples of factors considered in scheduling include the following: • Size and complexity of the tasks • Integration and test issues • Needs of the customer and end users • Availability of critical resources • Availability of key personnel 5. Incorporate the plans for perfo rming peer reviews on the work products of the project's defined process. Refer to the Verification process area for more information about peer reviews. 6. Incorporate the training needed to perform the project’s defined process in the project’s training plans. This task typically involves negotiating with the organizational training group the support they will provide. 7. Establish objective ent ry and exit criteria to authorize the initiation and completion of the tasks described in the work breakdown structure (WBS). Refer to the Project Planning process area for more information about the WBS. 8. Ensure that the project plan is appropriately compatible with the plans of relevant stakeholders. Typically the plan and changes to the plan will be reviewed for compatibility. 9. Identify how conflicts will be resolved that arise among relevant stakeholders. SP 1.5 Manage the Project Using the Integrated Plans Manage the project using the project plan, the other plans that affect the project, and the project’s defined process. Refer to the Organizational Process Definition process area for more information about the organizational process assets. Refer to the Organizational Proc ess Focus process area for more information about organizational process needs and objectives and coordinating process improvement ac tivities with the rest of the organization. Refer to the Risk Management process area for more information about managing risks.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 157Refer to the Project Monitoring and Control process area for more information about monitori ng and controlling the project. Typical Work Products 1. Work products created by perform ing the project’s defined process 2. Collected measures (“actuals”) and progress records or reports 3. Revised requirements, plans, and commitments 4. Integrated plans Subpractices 1. Implement the project’s defined process using the organization's process asset library. This task typically includes the following: • Incorporating artifacts from the organization’ s process asset library into the project as appropriate • Using lessons learned from the organizati on’s process asset library to manage the project 2. Monitor and control the project’s activities and work products using the project’s defined process, pr oject plan, and other plans that affect the project. This task typically includes the following: • Using the defined entry and exit criteria to authorize the initiation and determine the completion of the tasks • Monitoring the activities that could signifi cantly affect the actual values of the project’s planning parameters • Tracking the project’s planning parameters using measurable thresholds that will trigger investigation and appropriate actions • Monitoring product and project interface risks • Managing external and internal commitments based on the plans for the tasks and work products of the project’s defined process An understanding of the relationships among the various tasks and work products of the project’s defined process, and of the roles to be performed by the relevant stakeholders, along with well-defined control mechanisms (e.g., peer reviews) achieves better visibility into the project’s performance and better control of the project. 3. Obtain and analyze the select ed measures to manage the project and support the organization’s needs. Refer to the Measurement and Analysis process area for more information about defining a pr ocess for obtaining and analyzing measures.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 158 4. Periodically review and align the project’s performance with the current and anticipated needs, objecti ves, and requirements of the organization, customer, and end users, as appropriate. This review includes alignment with the organizational process needs and objectives. Examples of actions that achieve alignment include the following: • Accelerating the schedule, with appropriate adjustments to other planning parameters and the project risks • Changing the requirements in response to a change in market opportunities or customer and end-user needs • Terminating the project SP 1.6 Contribute to the Organizational Process Assets Contribute work products, measures, and documented experiences to the organizational process assets. Refer to the Organizational Proc ess Focus process area for more information about process improvement proposals. Refer to the Organizational Process Definition process area for more information about the organizational process assets, the organization’s measurement repository, and the organ ization’s process asset library. This specific practice addresses co llecting information from processes in the project’s defined process. Typical Work Products 1. Proposed improvements to the organizational process assets 2. Actual process and product meas ures collected from the project 3. Documentation (e.g., exempl ary process descriptions, plans, training modules, checklists, and lessons learned) 4. Process artifacts associated with tailoring and implementing the organization’s set of standard processes on the project Subpractices 1. Propose improvements to the organizational process assets. 2. Store process and product m easures in the organization’s measurement repository. Refer to the Project Planning process area for more information about recording planning and replanning data.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 159Refer to the Project Monitoring and Control process area for more information about recording measures. This typically includes the following: • Planning data • Replanning data • Measures Examples of data recorded by the project include the following: • Task descriptions • Assumptions • Estimates • Revised estimates • Definitions of recorded data and measures • Measures • Context information that relates the measures to the activities performed and work products produced • Associated information needed to reconstruct the estimates, assess their reasonableness, and derive estimates for new work 3. Submit documentation for possibl e inclusion in the organization's process asset library. Examples of documentation include the following: • Exemplary process descriptions • Training modules • Exemplary plans • Checklists 4. Document lessons learned from the project for inclusion in the organization's process asset library. 5. Provide process artifacts associated with tailoring and implementing the organization’s set of standard processes in support of the organization’s process monitoring activities. Refer to the Monitor Implementa tion specific practice of the Organization Process Focus process area for more information about the organization’s activiti es to understand the extent of deployment of standard processes on new and existing projects.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 160 SG 2 Coordinate and Collaborate with Relevant Stakeholders Coordination and collaboration of the project with relevant stakeholders is conducted. SP 2.1 Manage Stakeholder Involvement Manage the involvement of the relevant stakeholders in the project. Stakeholder involvement is managed according to the project’s integrated and defined process. Refer to the Project Planning proc ess area for more information about identifying stakeholders and their appropriate involvement and about establishing and maintaining commitments. Typical Work Products 1. Agendas and schedules for collaborative activities 2. Documented issues (e.g., iss ues with customer requirements, product and product compone nt requirements, product architecture, and product design) 3. Recommendations for resolvi ng relevant stakeholder issues Subpractices 1. Coordinate with the relevant st akeholders who should participate in the project’s activities. The relevant stakeholders should already be identified in the project plan. 2. Ensure that work products that are produced to satisfy commitments meet the requirement s of the recipient projects. Refer to the Verification process area for more information about verifying work products a gainst their requirements. This task typically includes the following: • Reviewing, demonstrating, or testing, as appropriate, each work product produced by relevant stakeholders • Reviewing, demonstrating, or testing, as appropriate, each work product produced by the project for other projects with repr esentatives of the projects receiving the work product • Resolving issues related to the acceptance of the work products 3. Develop recommendations and coor dinate the actions to resolve misunderstandings and problems with the product and product component requirements, pr oduct and product component architecture, and product and product component design.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 161SP 2.2 Manage Dependencies Participate with relevant stakeholders to identify, negotiate, and track critical dependencies. Refer to the Project Planning proc ess area for more information about identifying stakeholders and their appropriate involvement and about establishing and maintaining commitments. Typical Work Products 1. Defects, issues, and action it ems resulting from reviews with relevant stakeholders 2. Critical dependencies 3. Commitments to address critical dependencies 4. Status of critical dependencies Subpractices 1. Conduct reviews with relevant stakeholders. 2. Identify each critical dependency. 3. Establish need dates and plan dates for each critical dependency based on the project schedule. 4. Review and get agreement on the commitments to address each critical dependency with the people responsible for providing the work product and the people re ceiving the work product. 5. Document the critical dependencies and commitments. Documentation of commitments typically includes the following: • Describing the commitment • Identifying who made the commitment • Identifying who is responsible for satisfying the commitment • Specifying when the commitment will be satisfied • Specifying the criteria for determining if the commitment has been satisfied 6. Track the critical dependencies and commitments and take corrective action as appropriate. Refer to the Project Monitoring and Control process area for more information about tracking commitments.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 162 Tracking the critical dependencies typically includes the following: • Evaluating the effects of late and early completion for impacts on future activities and milestones • Resolving actual and potential problems with the responsible people whenever possible • Escalating to the appropriate managers the actual and potential problems not resolvable with the responsible people SP 2.3 Resolve Coordination Issues Resolve issues with relevant stakeholders. Examples of coordination issues include the following: • Late critical dependencies and commitments • Product and product component requirements and design defects • Product-level problems • Unavailability of critical resources or personnel Typical Work Products 1. Relevant stakeholder coordination issues 2. Status of relevant st akeholder coordination issues Subpractices 1. Identify and doc ument issues. 2. Communicate issues to the relevant stakeholders. 3. Resolve issues with the relevant stakeholders. 4. Escalate to the appropriate managers those issues not resolvable with the relevant stakeholders. 5. Track the issues to closure. 6. Communicate with the relev ant stakeholders on the status and resolution of the issues. IPPD Addition SG 3 Apply IPPD Principles The project is managed using IPPD principles. The purpose of this specific goal and its practices is to create an IPPD environment that enables integrated teams to efficiently meet the project’s requirements and produce a quality product.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 163IPPD Addition SP 3.1 Establish the Project’s Shared Vision Establish and maintain a shared vision for the project. A project does not operate in isol ation. Understanding organizational mission, goals, expectations and constrai nts allows the project to align its direction, activi ties, and shared vision wi th the organization and helps create a common purpose within which project activities can be coordinated. To enable this, it is cr itical to understan d the interfaces between the project and stakeholders ex ternal to the project and the objectives and expectations of all relevant stakeholders (internal and external). When creating a shared vision, consider: • external stakeholder expe ctations and requirements • the aspirations and expe ctations of the project leader, team leaders, and team members • the project’s objectives • the conditions and outcome s the project will create • interfaces the project needs to maintain • the visions created by interfacing groups • the constraints imposed by outsi de authorities (e.g., environmental regulations) • project operation while working to achieve its objectives (both principles and behaviors) When creating a shared vision, a ll people in the project should be invited to participate. Although there may be a draft proposal, the larger population must have an opportunity to speak and be heard about what really matters to them. The shared vi sion is articulated in terms of both the core ideology (values, princi ples, and behaviors) and the desired future to which each member of the project can commit. An effective communica tions strategy is ke y to implementing and focusing the shared vision throughout the project. Promulgation of the shared vision is a public declaration of the commitment of the project to their shared vision and prov ides the opportunity for others to examine, understand, and align their activities in a common direction. The shared vision should be communicated, and agreement and commitment of the relevant stakeholders should be obtained.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 164 IPPD Addition Effective communications are al so especially important when incorporating new project members. New members of the project often need more or special attention to ensure that they understand the shared vision, have a stake in it, and are prepared to follow it in doing their work. Typical Work Products 1. Documented shared vision 2. Communications strategy 3. Published principles, shared vi sion statement, mission statement, and objectives (e.g., posters, wa llet cards, and presentations) Subpractices 1. Articulate the project’s shared vision in terms of purpose or mission, vision, values, and objectives. 2. Reach consensus on the project’s shared vision. 3. Establish a strategy to communi cate the project’s shared vision both externally and internally. 4. Create presentations suitable fo r the various audiences that need to be informed about the pr oject’s shared vision. 5. Ensure that project and individ ual activities and tasks are aligned with the project’s shared vision. SP 3.2 Establish the Integrated Team Structure Establish and maintain the integrated team structure for the project. Product requirements, cost, schedule, risk, resource projections, business processes, the project’s defined process, and organizational guidelines are evaluated to establis h the basis for defining integrated teams and their responsibilities, aut horities, and interrelationships. A typical integrated team stru cture may be based on the product- oriented hierarchy found in the WBS. More complex structuring occurs when the WBS is not product orient ed, product risks are not uniform, and resources are constrained. The integrated team structure is a dynamic entity that is adjusted to changes in people, requirements, and t he nature of tasks, and to tackle many difficulties. For small projects , the integrated team structure can",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 165IPPD Addition treat the whole project as an int egrated team. The integrated team structure should be continuously m onitored to detect malfunctions, mismanaged interfaces, and mismatches of the work to the staff. Corrective action should be taken when performance does not meet expectations. Refer to the Establish Rules and Gu idelines for Integrated Teams specific practice in the Organizati onal Process Definition +IPPD process area for more information about es tablishing organizational rules and guidelines for structuring and forming integrated teams. Typical Work Products 1. Assessments of the product and product architectures, including risk and complexity 2. Integrated team structure Subpractices 1. Establish an integr ated team structure. An integrated team structure is dependent on: • An assessment of product risk and complexity • Location and types of risks • Integration risks, including product component interfaces and inter-team communication • Resources, including availability of appropriately skilled people • Limitations on team size for effective collaboration • Need for team membership of stakeholders external to the project • Business processes • Organizational structure The integrated team structure should be based on an understanding of the project’s defined process and shared vision, the organization’s standard processes, and the organizational process assets applicable to teams and team structures. 2. Periodically evaluate and modify the integrated team structure to best meet project needs. Changes to the product requirements or architecture could affect the team structure.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 166 IPPD Addition Continuously monitor the integrated team structure to detect problems such as mismanaged interfaces, and mismatches between the work assigned and the staff performing the work. Take corrective action, including assessing the deployed teams and structures, when performance does not meet expectations. Changes in team structure can include the following: • Retiring a team for a period of time (e.g., while long-duration manufacturing or verifications are done) • Disbanding a team when it is no longer cost effective in serving the project • Combining teams to achieve operating efficiencies • Adding teams as new product component s are identified for development SP 3.3 Allocate Requirements to Integrated Teams Allocate requirements, responsibilities, tasks, and interfaces to teams in the integrated team structure. This allocation of requirements to integrated teams is done before any teams are formed to verify that t he integrated team structure is workable and covers all the necessa ry requirements, responsibilities, authorities, tasks, and interfaces. On ce the structure is confirmed, integrated team sponsors are chosen to establish the individual teams in the structure. Typical Work Products 1. Responsibilities allocat ed to each integrated team 2. Work product requirements, te chnical interfaces, and business (e.g., cost accounting and proj ect management) in terfaces each integrated team will be re sponsible for satisfying 3. List of integrated team sponsors Subpractices 1. Allocate the tasks, responsib ilities, and work products to be delivered, and the associated requi rements and interfaces to the appropriate integrated teams. Business, management, and other nontechnical responsibilities and authorities for each integrated team are necessary elements to proper team function. Integrated team responsibilities and authorities are normally developed by the project and are consistent with established organization practices.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 167IPPD Addition Example responsibilities and authorities, include the following: • Authority of teams to pick their own leader • Authority of teams to implement subteams (e.g., a product team forming an integration subteam) • Reporting chains • Reporting requirements (cost, schedule, and performance status) • Progress reporting measures and methods 2. Check that the distribution of requirements and interfaces covers all specified product require ments and other requirements. In the event that complete coverage of requirements is not achieved, corrective action should be taken to redistribute requirements or to alter the integrated team structure. 3. Designate the sponsor for each integrated team. An integrated team sponsor is a manager (individual or team) who is responsible for establishing and providing resources to an integrated team, monitoring its activities and progress, and taking corrective action when needed. A sponsor may manage one or many teams. Team sponsors can be project managers. SP 3.4 Establish Integrated Teams Establish and maintain integrated teams in the structure. The integrated teams within the integrated team structure are established by the team sponsors. This proc ess encompasses choosing team leaders and team members, and es tablishing the team charter for each integrated team based on the allo cation of requirements. It also involves providing the resources required to accomplish the tasks assigned to the team. Refer to the Establish Rules and Gu idelines for Integrated Teams specific practice in the Organizati onal Process Definition +IPPD process area for more information about es tablishing organizational rules and guidelines for structuring and forming integrated teams. Typical Work Products 1. List of team leaders 2. List of team members a ssigned to each integrated team 3. Integrated team charters 4. Measures for evaluating the performance of integrated teams",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 168 IPPD Addition 5. Periodic integrated team status reports Subpractices 1. Choose a leader for each integrated team. The extent of organizational and project direction in selecting the leader is often a function of product risk and complexity or an organization’s need to “grow” new leaders. Team sponsors may select the team leader or team members may vote on a leader from within the team, depending on organizational policies. 2. Allocate resources to each integrated team. The people and other resources are allocated to each integrated team. These items are discussed with the team to ensure that the resources are adequate and that the people are adequate to carry out the tasks and are compatible with other members of the team. 3. Charter each integrated team. The team charter is the contract among the team members and between the team and its sponsor for the expected work and level of performance. Charters establish the rights, guarantees, privileges, and permissions for organizing and performing the team’s assigned requirements and interfaces, responsibilities and tasks. The integrated team and its sponsor develop the team charter as a negotiation activity. When both approve it, the team charter constitutes a recognized agreement with management authority. Charters can include the following aspects: • How assignments are accepted • How resources and input are accessed • How work gets done • Who checks and reviews work • How work is approved • How work is delivered and communicated 4. Review the composition of an in tegrated team and its place in the integrated team structure when it s team leader changes or another significant change of membership occurs. A change of this kind may significantly affect the ability of the team to accomplish its objectives. A review of the match between the new composition and the current responsibilities should be made. If the match is not satisfactory, the team composition should be changed or the team’s responsibility should be modified. 5. Review the composition of a t eam and its tasking when a change in team responsibility occurs.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 169IPPD Addition Changes in responsibilities often occur as the project moves from one phase to the next. For example, less design expertise on teams may be needed when detailed design is completed and fabrication and integration of product components begins. 6. Manage the overall perfo rmance of the teams. The charter should specify how both team and individual performance will be measured and should include the critical success factors for the team within the project. SP 3.5 Ensure Collaboration among Interfacing Teams Ensure collaboration among interfacing teams. The success of an integrated team-bas ed project is a function of how effectively and successfully the in tegrated teams collaborate with one another to achieve project objecti ves. This collaboration may be accomplished using interface control working groups. See the Coordinate and Collaborate with Relevant Stakeholders specific goal of this process area for more information about managing stakeholder involvement, critic al dependencies, and resolving coordination issues. Refer to the Establish Rules and Gu idelines for Integrated Teams specific practice in the Organizati onal Process Definition +IPPD process area for more information about establishing organizational expectations and rules that will gui de how the integrated teams work collectively. Typical Work Products 1. Work product ownership agreements 2. Team work plans 3. Commitment lists Subpractices 1. Establish and maintain the boun daries of work product ownership among interfacing teams within the project or organization. 2. Establish and maintain interf aces and processes among interfacing teams for the exchange of inputs, outputs, or work products. 3. Develop, communicate, and dist ribute among interfacing teams the commitment lists and work plans that are related to work product or team interfaces.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 170 Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the integrated project management process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the integrated project management process. Elaboration: This policy establishes organizational expectations for establishing and maintaining the project’s defined proc ess from project startup through the life of the project, using the project’s def ined process in managing the project, and coordinating and collaborating with relevant stakeholders. IPPD Addition This policy also establishes organizational expectations for applying IPPD principles.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 171GP 2.2 Plan the Process Establish and maintain the plan for performing the integrated project management process. Elaboration: This plan for the integrated pr oject management process unites the planning for the project planning a nd monitor and control processes. The planning for performing the planning- related practices in Integrated Project Management is addressed as part of planning the project planning process. This plan for per forming the monitor-and-control- related practices in Integrated Project Management can be included in (or referenced by) the project plan, which is described in the Project Planning process area. Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.2 and project planning processes. GP 2.3 Provide Resources Provide adequate resources for performing the integrated project management process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Problem-tracking and trouble-reporting packages • Groupware • Video conferencing • Integrated decision database • Integrated product support environments GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the integrated project management process. GP 2.5 Train People Train the people performing or supporting the integrated project management process as needed.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 172 Elaboration: Examples of training topics include the following: • Tailoring the organization’s set of standard processes to meet the needs of the project • Procedures for managing the project based on the project’s defined process • Using the organization’s measurement repository • Using the organizational process assets • Integrated management • Intergroup coordination • Group problem solving IPPD Addition Examples of training topics also include the following: • Building the project's shared vision • Team building GP 2.6 Manage Configurations Place designated work products of the integrated project management process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • The project’s defined process • Project plans • Other plans that affect the project • Integrated plans • Actual process and product measures collected from the project IPPD Addition Examples of work products placed under control also include the following: • Project's shared vision • Integrated team structure • Integrated team charters",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 173GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the integrated project management process as planned. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.7 and the Manage Stakeholder Involvement practice in this process area. Examples of activities for stakeholder involvement include the following: • Resolving issues about the tailoring of the organizational process assets • Resolving issues among the project plan and the other plans that affect the project • Reviewing project performance to align with current and projected needs, objectives, and requirements IPPD Addition Examples of activities for stakeholder involvement also include the following: • Creating the project's shared vision • Defining the integrated team structure for the project • Populating the integrated teams GP 2.8 Monitor and Control the Process Monitor and control the integrated project management process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of changes to the project’s defined process • Schedule and effort to tailor the organization’s set of standard processes • Interface coordination issue trends (i.e., number identified and number closed) • Schedule for project tailoring activities",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 174 IPPD Addition Examples of measures and work products used in monitoring and controlling also include the following: • Project's shared vision usage and effectiveness • Integrated team-structure usage and effectiveness • Integrated team charters usage and effectiveness GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the integrated project management process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Establishing, maintaining, and using the project’s defined process • Coordinating and collaborating with relevant stakeholders IPPD Addition Examples of activities reviewed also include the following: • Using the project's shared vision • Organizing integrated teams Examples of work products reviewed include the following: • Project’s defined process • Project plans • Other plans that affect the project IPPD Addition Examples of work products reviewed also include the following: • Integrated team structure • Integrated team charters • Shared vision statements",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 175GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the integrated project management process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined integrated project management process. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 3.1 and the Integrated Project Management process area. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the integrated project management process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 3.2 and the Integrated Project Management process area.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 176 Examples of work products, measures, measurement results, and improvement information include the following: • Project’s defined process • Number of tailoring options exercised by the project to create its defined process • Interface coordination issue trends (i.e., number identified and number closed) • Number of times the PAL is accessed for assets related to project planning by project personnel • Records of expenses related to holding face-to-face meetings versus holding meetings using collaborative equipment such as teleconferencing and videoconferencing IPPD Addition Examples of work products, measures, measurement results, and improvement information also include the following: • Integrated team charters • Project shared vision Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the integrated project management process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the integrated project management process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the integrated project management process in fulfilling the relevant business objectives of the organization.",
        "CMMI for Development Version 1.2 Integrated Project Management +IPPD (IPM+IPPD) 177Continuous Only GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the integrated project management process.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 178 MEASUREMENT AND ANALYSIS A Support Process Area at Maturity Level 2 Purpose The purpose of Measurement and A nalysis (MA) is to develop and sustain a measurement capability that is used to support management information needs. Introductory Notes The Measurement and Anal ysis process area involves the following: • Specifying the objectives of m easurement and analysis such that they are aligned with identified information needs and objectives • Specifying the meas ures, analysis techni ques, and mechanism for data collection, data storage, reporting, and feedback • Implementing the collection, st orage, analysis, and reporting of the data • Providing objective results t hat can be used in making informed decisions, and taking appropriate corrective actions The integration of m easurement and analysis activities into the processes of the project supports the following: • Objective planning and estimating • Tracking actual performance against established plans and objectives • Identifying and resolving process-related issues • Providing a basis for incorporating measurement into additional processes in the future The staff required to implement a measurement capability may or may not be employed in a separate organization-wide program. Measurement capability ma y be integrated into individual projects or other organizational functions (e.g., quality assurance). The initial focus for measurement acti vities is at the project level. However, a measuremen t capability may prove useful for addressing organization- and/or enterprise-wide information needs. To support this capability, the measurement activities should s upport information needs at multiple levels including the bus iness, organizational unit, and project to minimize re-work as the organization matures.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 179Projects may choose to store proj ect-specific data and results in a project-specific reposit ory. When data are shared more widely across projects, the data may reside in the organization’s measurement repository. Measurement and analysis of the product components provided by suppliers is essential for effectiv e management of the quality and costs of the project. It is possible, with careful management of supplier agreements, to provide insight into the data that support supplier- performance analysis. Related Process Areas Refer to the Project Planning proc ess area for more information about estimating project attributes and other planning information needs. Refer to the Project Monitoring and Control process area for more information about monitoring projec t performance information needs. Refer to the Configuration Management process area for more information about managing me asurement work products. Refer to the Requirements Development process area for more information about meeting cust omer requirements and related information needs. Refer to the Requirements Management process area for more information about maintaining re quirements traceability and related information needs. Refer to the Organizational Process Definition process area for more information about establishing the organization’s measurement repository. Refer to the Quantitative Project Management process area for more information about understanding vari ation and the appropriate use of statistical analysis techniques. Specific Goal and Practice Summary SG 1 Align Measurement and Analysis Activities SP 1.1 Establish Measurement Objectives SP 1.2 Specify Measures SP 1.3 Specify Data Collection and Storage Procedures SP 1.4 Specify Analysis Procedures SG 2 Provide Measurement Results SP 2.1 Collect Measurement Data SP 2.2 Analyze Measurement Data SP 2.3 Store Data and Results SP 2.4 Communicate Results",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 180 Specific Practices by Goal SG 1 Align Measurement and Analysis Activities Measurement objectives and activities are aligned with identified information needs and objectives. The specific practices covered under this specific goal may be addressed concurrently or in any order: • When establishing measurement ob jectives, experts often think ahead about necessary criteria for specifying measures and analysis procedures. They also think concurrently about the constraints imposed by data co llection and storage procedures. • It often is important to specify the essential analyses that will be conducted before attending to details of measur ement specification, data collection, or storage. SP 1.1 Establish Measurement Objectives Establish and maintain measurement objectives that are derived from identified information needs and objectives. Measurement objectives docum ent the purposes for which measurement and analysis are done, a nd specify the kinds of actions that may be taken based on t he results of data analyses. The sources for measurement objectives may be management, technical, project, product, or process implem entation needs. The measurement objectives may be constrained by existing processes, available resources, or other measurement considerations. Judgments may need to be made about w hether the value of the results will be commensurate with the res ources devoted to doing the work. Modifications to identified informat ion needs and objectives may, in turn, be indicated as a consequence of the process and results of measurement and analysis. Sources of information needs and ob jectives may include the following: • Project plans • Monitoring of project performance • Interviews with managers and others who have information needs • Established management objectives • Strategic plans • Business plans",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 181• Formal requirements or contractual obligations • Recurring or other troublesome management or technical problems • Experiences of other projec ts or organizational entities • External industry benchmarks • Process improvement plans Example measurement objectives include the following: • Reduce time to delivery • Reduce total lifecycle cost • Deliver specified functionality completely • Improve prior levels of quality • Improve prior customer satisfaction ratings • Maintain and improve the acquirer/supplier relationships Refer to the Project Planning proc ess area for more information about estimating project attributes and other planning information needs. Refer to the Project Monitoring and Control process area for more information about project performance information needs. Refer to the Requirements Development process area for more information about meeting cust omer requirements and related information needs. Refer to the Requirements Management process area for more information about maintaining re quirements traceability and related information needs. Typical Work Products 1. Measurement objectives Subpractices 1. Document informati on needs and objectives. Information needs and objectives are documented to allow traceability to subsequent measurement and analysis activities. 2. Prioritize information needs and objectives. It may be neither possible nor desirable to subject all initially identified information needs to measurement and analysis. Priorities may also need to be set within the limits of available resources. 3. Document, review, and u pdate measurement objectives.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 182 It is important to carefully consider the purposes and intended uses of measurement and analysis. The measurement objectives are documented, reviewed by management and other relevant stakeholders, and updated as necessary. Doing so enables traceability to subsequent measurement and analysis activities, and helps ensure that the analyses will properly address identified information needs and objectives. It is important that users of measurement and analysis results be involved in setting measurement objectives and deciding on plans of action. It may also be appropriate to involve those who provide the measurement data. 4. Provide feedback for refining and clarifying information needs and objectives as necessary. Identified information needs and objectives may need to be refined and clarified as a result of setting measurement objectives. Initial descriptions of information needs may be unclear or ambiguous. Conflicts may arise between existing needs and objectives. Precise targets on an already existing measure may be unrealistic. 5. Maintain traceability of the meas urement objectives to the identified information needs and objectives. There must always be a good answer to the question, “Why are we measuring this?” Of course, the measurement objectives may also change to reflect evolving information needs and objectives. SP 1.2 Specify Measures Specify measures to address the measurement objectives. Measurement objectives are refi ned into precise, quantifiable measures. Measures may be either “base” or “derived.” Data for base measures are obtained by direct measurement. Data for derived measures come from other data, typically by comb ining two or more base measures. Examples of commonly used base measures include the following: • Estimates and actual measures of work product size (e.g., number of pages) • Estimates and actual measures of effort and cost (e.g., number of person hours) • Quality measures (e.g., number of defects by severity)",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 183Examples of commonly used derived measures include the following: • Earned Value • Schedule Performance Index • Defect density • Peer review coverage • Test or verification coverage • Reliability measures (e.g., mean time to failure) • Quality measures (e.g., number of defects by severity/total number of defects) Derived measures typically are expres sed as ratios, composite indices, or other aggregate summary measures. They are often more quantitatively reliable and meaningfu lly interpretable than the base measures used to generate them. Typical Work Products 1. Specifications of base and derived measures Subpractices 1. Identify candidate measures based on doc umented measurement objectives. The measurement objectives are refined into specific measures. The identified candidate measures are categorized and specified by name and unit of measure. 2. Identify existing m easures that already address the measurement objectives. Specifications for measures may already exist, perhaps established for other purposes earlier or elsewhere in the organization. 3. Specify operational defin itions for the measures. Operational definitions are stated in precise and unambiguous terms. They address two important criteria as follows: • Communication: What has been measured, how was it measured, what are the units of measure, and what has been included or excluded? • Repeatability: Can the measurement be repeated, given the same definition, to get the same results? 4. Prioritize, review, and update measures. Proposed specifications of the measures are reviewed for their appropriateness with potential end users and other relevant stakeholders. Priorities are set or changed, and specifications of the measures are updated as necessary.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 184 SP 1.3 Specify Data Collection and Storage Procedures Specify how measurement data will be obtained and stored. Explicit specificat ion of collection methods hel ps ensure that the right data are collected properly. It may also aid in further clarifying information needs and me asurement objectives. Proper attention to storage and retrie val procedures helps ensure that data are available and accessible for future use. Typical Work Products 1. Data collection and storage procedures 2. Data collection tools Subpractices 1. Identify existing sources of dat a that are generated from current work products, proces ses, or transactions. Existing sources of data may already have been identified when specifying the measures. Appropriate collection mechanisms may exist whether or not pertinent data have already been collected. 2. Identify measures fo r which data are needed, but are not currently available. 3. Specify how to collect and store the data for each required measure. Explicit specifications are made of how, where, and when the data will be collected. Procedures for collecting valid data are specified. The data are stored in an accessible manner for analysis, and it is determined whether they will be saved for possible reanalysis or documentation purposes. Questions to be considered typically include the following: • Have the frequency of collection and the points in the process where measurements will be made been determined? • Has the timeline that is required to move measurement results from the points of collection to repositories, other databases, or end users been established? • Who is responsible for obtaining the data? • Who is responsible for data storage, retrieval, and security? • Have necessary supporting tools been developed or acquired? 4. Create data collection me chanisms and process guidance. Data collection and storage mechanisms are well integrated with other normal work processes. Data collection mechanisms may include manual or automated forms and templates. Clear, concise guidance on correct procedures is available to those responsible for doing the work. Training is provided as necessary to",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 185clarify the processes necessary for collection of complete and accurate data and to minimize the burden on those who must provide and record the data. 5. Support automatic collection of the data where appropriate and feasible. Automated support can aid in collecting more complete and accurate data. Examples of such automated support include the following: • Time stamped activity logs • Static or dynamic analyses of artifacts However, some data cannot be collected without human intervention (e.g., customer satisfaction or other human judgments), and setting up the necessary infrastructure for other automation may be costly. 6. Prioritize, review, and update data collection and storage procedures. Proposed procedures are reviewed for their appropriateness and feasibility with those who are responsible for providing, collecting, and storing the data. They also may have useful insights about how to improve existing processes, or be able to suggest other useful measures or analyses. 7. Update measures and measurem ent objectives as necessary. Priorities may need to be reset based on the following: • The importance of the measures • The amount of effort required to obtain the data Considerations include whether new forms, tools, or training would be required to obtain the data. SP 1.4 Specify Analysis Procedures Specify how measurement data will be analyzed and reported. Specifying the analysis pr ocedures in advance ensures that appropriate analyses will be conducted and repo rted to address the documented measurement objectives (and t hereby the information needs and objectives on which they are based) . This approach also provides a check that the necessary dat a will in fact be collected. Typical Work Products 1. Analysis specific ations and procedures 2. Data analysis tools",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 186 Subpractices 1. Specify and prioritize the anal yses that will be conducted and the reports that will be prepared. Early attention should be paid to the analyses that will be conducted and to the manner in which the results will be reported. These should meet the following criteria: • The analyses explicitly address the documented measurement objectives • Presentation of the results is clearly understandable by the audiences to whom the results are addressed Priorities may have to be set within available resources. 2. Select appropriate data a nalysis methods and tools. Refer to the Select Measures and Analytic Techniques and Apply Statistical Methods to Understand Va riation specific practices of the Quantitative Project Manag ement process area for more information about the appropria te use of statistical analysis techniques and understanding va riation, respectively. Issues to be considered typically include the following: • Choice of visual display and other presentation techniques (e.g., pie charts, bar charts, histograms, radar charts, li ne graphs, scatter plots, or tables) • Choice of appropriate descriptive statisti cs (e.g., arithmetic mean, median, or mode) • Decisions about statistical sampling criter ia when it is impossible or unnecessary to examine every data element • Decisions about how to handle analysis in the presence of missing data elements • Selection of appropriate analysis tools Descriptive statistics are typically used in data analysis to do the following: • Examine distributions on the specified measures (e.g., central tendency, extent of variation, or data points exhibiting unusual variation) • Examine the interrelationships among the specified measures (e.g., comparisons of defects by phase of the product’s lifecycle or by product component) • Display changes over time 3. Specify administrative proc edures for analyzing the data and communicating the results.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 187Issues to be considered typically include the following: • Identifying the persons and groups responsible for analyzing the data and presenting the results • Determining the timeline to analyze the data and present the results • Determining the venues for communicating the results (e.g., progress reports, transmittal memos, written reports, or staff meetings) 4. Review and update the proposed content and format of the specified analys es and reports. All of the proposed content and format are subject to review and revision, including analytic methods and tools, administrative procedures, and priorities. The relevant stakeholders consulted should include intended end users, sponsors, data analysts, and data providers. 5. Update measures and measurem ent objectives as necessary. Just as measurement needs drive data analysis, clarification of analysis criteria can affect measurement. Specifications for some measures may be refined further based on the specifications established for data analysis procedures. Other measures may prove to be unnecessary, or a need for additional measures may be recognized. The exercise of specifying how measures will be analyzed and reported may also suggest the need for refining the measurement objectives themselves. 6. Specify criteria for evaluating the utility of the analysis results and for evaluating the conduct of the measurement and analysis activities. Criteria for evaluating the utility of the analysis might address the extent to which the following apply: • The results are (1) provided on a timely basis, (2) understandable, and (3) used for decision making. • The work does not cost more to perform than is justified by the benefits that it provides. Criteria for evaluating the conduct of the measurement and analysis might include the extent to which the following apply: • The amount of missing data or the number of flagged inconsistencies is beyond specified thresholds. • There is selection bias in sampling (e.g ., only satisfied end users are surveyed to evaluate end-user satisfaction, or only unsuccessful projects are evaluated to determine overall productivity). • The measurement data are repeatable (e.g., statistically reliable). • Statistical assumptions have been satisfied (e.g., about the distribution of data or about appropriate measurement scales).",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 188 SG 2 Provide Measurement Results Measurement results, which address identified information needs and objectives, are provided. The primary reason for doing measur ement and analysis is to address identified information needs and objectives. M easurement results based on objective evidence can help to monitor performance, fulfill contractual obligations, make informed management and technical decisions, and enable correcti ve actions to be taken. SP 2.1 Collect Measurement Data Obtain specified measurement data. The data necessary for analys is are obtained and checked for completeness and integrity. Typical Work Products 1. Base and derived measurement data sets 2. Results of data integrity tests Subpractices 1. Obtain the data for base measures. Data are collected as necessary for previously used as well as for newly specified base measures. Existing data are gathered from project records or from elsewhere in the organization. Note that data that were collected earlier may no longer be available for reuse in existing databases, paper records, or formal repositories. 2. Generate the data for derived measures. Values are newly calculated for all derived measures. 3. Perform data integrity checks as close to the source of the data as possible. All measurements are subject to error in specifying or recording data. It is always better to identify such errors and to identify sources of missing data early in the measurement and analysis cycle.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 189Checks can include scans for missing data, out-of-bounds data values, and unusual patterns and correlation across measures. It is particularly important to do the following: • Test and correct for inconsistency of classifications made by human judgment (i.e., to determine how frequently people make differing classification decisions based on the same information, otherwise known as “inter-coder reliability”). • Empirically examine the relationships among the measures that are used to calculate additional derived measures. Doing so can ensure that important distinctions are not overlooked and t hat the derived measures convey their intended meanings (otherwise known as “criterion validity”). SP 2.2 Analyze Measurement Data Analyze and interpret measurement data. The measurement data are analyzed as planned, additional analyses are conducted as necessary, result s are reviewed with relevant stakeholders, and necessary revisions for future analyses are noted. Typical Work Products 1. Analysis results and draft reports Subpractices 1. Conduct initial analyses, interpre t the results, and draw preliminary conclusions. The results of data analyses are rarely self-evident. Criteria for interpreting the results and drawing conclusions should be stated explicitly. 2. Conduct additional measurement and analysis as necessary, and prepare results for presentation. The results of planned analyses may suggest (or require) additional, unanticipated analyses. In addition, they may identify needs to refine existing measures, to calculate additional derived measures, or even to collect data for additional base measures to properly complete the planned analysis. Similarly, preparing the initial results for presentation may identify the need for additional, unanticipated analyses. 3. Review the initial result s with relevant stakeholders. It may be appropriate to review initial interpretations of the results and the way in which they are presented before disseminating and communicating them more widely. Reviewing the initial results before their release may prevent needless misunderstandings and lead to improvements in the data analysis and presentation.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 190 Relevant stakeholders with whom reviews may be conducted include intended end users and sponsors, as well as data analysts and data providers. 4. Refine criteria for future analyses. Valuable lessons that can improve future efforts are often learned from conducting data analyses and preparing results. Similarly, ways to improve measurement specifications and data collection procedures may become apparent, as may ideas for refining identified information needs and objectives. SP 2.3 Store Data and Results Manage and store measurement data, measurement specifications, and analysis results. Storing measurement-related inform ation enables the timely and cost- effective future use of historical dat a and results. The information also is needed to provide sufficient context for interpretation of the data, measurement criteria, and analysis results. Information stored typically includes the following: • Measurement plans • Specifications of measures • Sets of data that have been collected • Analysis reports and presentations The stored information contains or references the information needed to understand and interpret the measures and to assess them for reasonableness and applicability (e.g., measurement s pecifications used on different projects when comparing across projects). Data sets for derived measures typically can be recalculated and need not be stored. However, it may be appropriate to store summaries based on derived measures (e.g., charts , tables of results, or report prose). Interim analysis results need not be stored separately if they can be efficiently reconstructed. Projects may choose to store proj ect-specific data and results in a project-specific reposit ory. When data are shared more widely across projects, the data may reside in the organization’s measurement repository. Refer to the Establish the Organi zation’s Measurement Repository specific practice of the Organizati onal Process Definition process area for more information about establishi ng the organization’s measurement repository.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 191Refer to the Configuration Manageme nt process area for information about managing measurem ent work products. Typical Work Products 1. Stored data inventory Subpractices 1. Review the data to ensure their completeness, integrity, accuracy, and currency. 2. Store the data according to the data storage procedures. 3. Make the stored contents ava ilable for use only by appropriate groups and personnel. 4. Prevent the stored information from being used inappropriately. Examples of ways to prevent inappropriate use of the data and related information include controlling access to data and educating people on the appropriate use of data. Examples of inappropriate use include the following: • Disclosure of information that was provided in confidence • Faulty interpretations based on incomplete, out-of-context, or otherwise misleading information • Measures used to improperly evaluate the performance of people or to rank projects • Impugning the integrity of specific individuals SP 2.4 Communicate Results Report results of measurement and analysis activities to all relevant stakeholders. The results of the measurem ent and analysis process are communicated to relevant stakeholders in a timely and usable fashion to support decision making and assist in taking corrective action. Relevant stakeholders include intend ed users, sponsors, data analysts, and data providers. Typical Work Products 1. Delivered reports and related analysis results 2. Contextual information or guidanc e to aid in the interpretation of analysis results",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 192 Subpractices 1. Keep relevant stakeholders appris ed of measurement results on a timely basis. Measurement results are communicated in time to be used for their intended purposes. Reports are unlikely to be used if they are distributed with little effort to follow up with those who need to know the results. To the extent possible and as part of the normal way they do business, users of measurement results are kept personally involved in setting objectives and deciding on plans of action for measurement and analysis. The users are regularly kept apprised of progress and interim results. Refer to the Project Monitoring and Control process area for more information about the use of measurement results. 2. Assist relevant stakeholde rs in understanding the results. Results are reported in a clear and concise manner appropriate to the methodological sophistication of the relevant stakeholders. They are understandable, easily interpretable, and clearly tied to identified information needs and objectives. The data are often not self-evident to practitioners who are not measurement experts. Measurement choices should be explicitly clear about the following: • How and why the base and derived measures were specified • How the data were obtained • How to interpret the results based on the data analysis methods that were used • How the results address information needs Examples of actions to assist in understanding of results include the following: • Discussing the results with the relevant stakeholders • Providing a transmittal memo that provides background and explanation • Briefing users on the results • Providing training on the appropriate use and understanding of measurement results",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 193Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the measurement and analysis process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the measurement and analysis process. Elaboration: This policy establishes organiza tional expectations for aligning measurement objectives and activities with identified information needs and objectives and for providing measurement results. GP 2.2 Plan the Process Establish and maintain the plan for performing the measurement and analysis process. Elaboration: This plan for performing the measur ement and analysis process can be included in (or referenced by) the proj ect plan, which is described in the Project Planning process area. GP 2.3 Provide Resources Provide adequate resources for performing the measurement and analysis process, developing the work products, and providing the services of the process.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 194 Elaboration: Measurement personnel may be employed full time or part time. A measurement group may or may not exist to support measurement activities across multiple projects. Examples of other resources provided include the following tools: • Statistical packages • Packages that support data collection over networks GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the measurement and analysis process. GP 2.5 Train People Train the people performing or supporting the measurement and analysis process as needed. Elaboration: Examples of training topics include the following: • Statistical techniques • Data collection, analysis, and reporting processes • Development of goal-related measurements (e.g., Goal Question Metric) GP 2.6 Manage Configurations Place designated work products of the measurement and analysis process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Specifications of base and derived measures • Data collection and storage procedures • Base and derived measurement data sets • Analysis results and draft reports • Data analysis tools",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 195GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the measurement and analysis process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing measurement objectives and procedures • Assessing measurement data • Providing meaningful feedback to those responsible for providing the raw data on which the analysis and results depend GP 2.8 Monitor and Control the Process Monitor and control the measurement and analysis process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Percentage of projects using progress and performance measures • Percentage of measurement objectives addressed • Schedule for collection and review of measurement data GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the measurement and analysis process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Aligning measurement and analysis activities • Providing measurement results Examples of work products reviewed include the following: • Specifications of base and derived measures • Data collection and storage procedures • Analysis results and draft reports",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 196 GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the measurement and analysis process with higher level management and resolve issues. Staged Only GG3 and its practices do not apply for a maturity level 2 rating, but do apply for a maturity level 3 rating and above. Continuous/Maturity Levels 3 - 5 Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined measurement and analysis process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the measurement and analysis process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Data currency status • Results of data integrity tests • Data analysis reports Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process.",
        "CMMI for Development Version 1.2 Measurement and Analysis (MA) 197Continuous Only GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the measurement and analysis process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the measurement and analysis process to achieve the established quantitative quality and process- performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the measurement and analysis process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the measurement and analysis process.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 198 ORGANIZATIONAL INNOVATION AND DEPLOYMENT A Process Management Process Area at Maturity Level 5 Purpose The purpose of Organizational Innovat ion and Deployment (OID) is to select and deploy incremental and innovative improvements that measurably improve the organization’ s processes and technologies. The improvements support the or ganization’s quality and process- performance objectives as derived from the organization’s business objectives. Introductory Notes The Organizational Innovation and De ployment process area enables the selection and deployment of im provements that can enhance the organization’s ability to meet its quality and process-performance objectives. (See the definition of “quality and process-performance objectives” in the glossary.) The te rm “improvement,” as used in this process area, refers to all of t he ideas (proven and unproven) that would change the organization’s pr ocesses and technologies to better meet the organization’s quality and process-performance objectives. Quality and process-performance obje ctives that this process area might address include the following: • Improved product quality (e.g., functionality, performance) • Increased productivity • Decreased cycle time • Greater customer and end-user satisfaction • Shorter development or producti on time to change functionality or add new features, or adapt to new technologies • Reduce delivery time • Reduce time to adapt to new technologies and business needs Achievement of these objecti ves depends on the successful establishment of an infrastructure that enables and encourages all people in the organization to propose potential improvements to the organization’s processes and technol ogies. Achievement of these objectives also depends on being able to effectively evaluate and deploy proposed improvements to the organization’s processes and technologies. All members of the or ganization can participate in the",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 199organization’s process- and technol ogy-improvement activities. Their proposals are systematica lly gathered and addressed. Pilots are conducted to evaluate signi ficant changes involving untried, high-risk, or innovative improvements before they are broadly deployed. Process and technology improvements that will be deployed across the organization are selected from pr ocess- and technology-improvement proposals based on the following criteria: • A quantitative understanding of t he organization’s current quality and process performance • The organization’s quality and process-performance objectives • Estimates of the improvement in quality and process performance resulting from deploying the pr ocess and technology improvements • Estimated costs of deploy ing process and technology improvements, and the resources and funding available for such deployment The expected benefits added by the process and technology improvements are weighed agains t the cost and impact to the organization. Change and stability must be balanced carefully. Change that is too great or too rapid can overwhelm the organization, destroying its investment in organizational learning represented by organizational process assets. Rigid stability can result in stagnation, allowing the changing business environment to erode the organization’s business position. Improvements are deployed, as appropriate, to new and ongoing projects. In this process area, the term “process and technology improvements” refers to incremental and innovat ive improvements to processes and also to process or product tec hnologies (including project work environments). The informative material in this process area is written with the assumption that the specific practi ces are applied to a quantitatively managed process. The specific prac tices of this process area may be applicable, but with reduced value, if the assumption is not met. The specific practices in this process area complement and extend those found in the Organizational Pr ocess Focus process area. The focus of this process area is pr ocess improvement that is based on a quantitative knowledge of the organi zation’s set of standard processes and technologies and their expect ed quality and performance in predictable situations. In the Orga nizational Process Focus process area, no assumptions are made about the quantitat ive basis of improvement.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 200 Related Process Areas Refer to the Organizational Process Definition process area for more information about incorporating t he deployed process improvements into organizational process assets. Refer to the Organizational Proc ess Focus process area for more information about soliciting, collecting, and handling process improvement proposals and coordina ting the deployment of process improvement into the proj ect’s defined processes. Refer to the Organizational Training process area for more information about providing updated training to support deployment of process and technology improvements. Refer to the Organizational Proce ss Performance process area for more information about quality and process-performance objectives and process-performance models. Quality and process-performance objectives are used to analyze and select process- and technology- improvement proposals for deploym ent. Process-performance models are used to quantify the impac t and benefits of innovations. Refer to the Measurement and Analysis process area for more information about establishing objectives for measurement and analysis, specifying the measur es and analyses to be performed, obtaining and analyzing measures , and reporting results. Refer to the Integrated Project Management process area for more information about coordinating the deployment of process and technology improvements into the pr oject’s defined process and project work environment. Refer to the Decision Analysis and Resolution process area for more information about formal evaluations related to improvement proposals and innovations. Specific Goal and Practice Summary SG 1 Select Improvements SP 1.1 Collect and Analyze Improvement Proposals SP 1.2 Identify and Analyze Innovations SP 1.3 Pilot Improvements SP 1.4 Select Improvements for Deployment SG 2 Deploy Improvements SP 2.1 Plan the Deployment SP 2.2 Manage the Deployment SP 2.3 Measure Improvement Effects",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 201Specific Practices by Goal SG 1 Select Improvements Process and technology improvements, which contribute to meeting quality and process-performance objectives, are selected. SP 1.1 Collect and Analyze Improvement Proposals Collect and analyze process- and technology-improvement proposals. Each process- and technology-improvement proposal must be analyzed. Simple process and technology improvements, with well-understood benefits and effects, will not usually undergo detailed evaluations. Examples of simple process and technology improvements include the following: • Add an item to a peer review checklist. • Combine the technical review and management review for suppliers into a single technical/management review. Typical Work Products 1. Analyzed process- and tec hnology-improvement proposals Subpractices 1. Collect process- and technology-improvement proposals. A process- and technology-improvement proposal documents proposed incremental and innovative improvements to specific processes and technologies. Managers and staff in the organization, as well as customers, end users, and suppliers can submit process- and technology-improvement proposals. Process and technology improvements may be implemented at the local level before being proposed for the organization.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 202 Examples of sources for process- and technology-improvement proposals include the following: • Findings and recommendations from process appraisals • The organization’s quality and proc ess-performance objectives • Analysis of data about customer and end-us er problems as well as customer and end-user satisfaction • Analysis of data about project performanc e compared to quality and productivity objectives • Analysis of technical performance measures • Results of process and product benchmarking efforts • Analysis of data on defect causes • Measured effectiveness of process activities • Measured effectiveness of project work environments • Examples of process- and technology-improvement proposals that were successfully adopted elsewhere • Feedback on previously submitted process- and technology-improvement proposals • Spontaneous ideas from managers and staff Refer to the Organizational Proc ess Focus process area for more information about process- and technology-improvement proposals. 2. Analyze the costs and benefit s of process- and technology- improvement proposals as appropriate. Process- and technology-improvement proposals that have a large cost-to-benefit ratio are rejected. Criteria for evaluating costs and benefits include the following: • Contribution toward meeting the organization’s quality and process-performance objectives • Effect on mitigating identified project and organizational risks • Ability to respond quickly to changes in project requirements, market situations, and the business environment • Effect on related processes and associated assets • Cost of defining and collecting data that supports the measurement and analysis of the process- and technology-improvement proposal • Expected life span of the proposal Process- and technology-improvement proposals that would not improve the organization's processes are rejected.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 203Process-performance models provide insight into the effect of process changes on process capability and performance. Refer to the Organizational Proce ss Performance process area for more information about process-performance models. 3. Identify the proce ss- and technology-improvement proposals that are innovative. Innovative improvements are also identified and analyzed in the Identify and Analyze Innovations specific practice. Whereas this specific practice analyzes proposals that have been passively collected, the purpose of the Identify and Analyze Innovations specific practice is to actively search for and locate innovative improvements. The search primarily involves looking outside the organization. Innovative improvements are typically identified by reviewing process- and technology-improvement proposals or by actively investigating and monitoring innovations that are in use in other organizations or are documented in research literature. Innovation may be inspired by internal improvement objectives or by the external business environment. Innovative improvements are typically major changes to the process that represent a break from the old way of doing things (e.g., changing the lifecycle model). Innovative improvements may also include changes in the products that support, enhance, or automate the process (e.g., using off-the-shelf products to support the process). Examples of innovative improvements include the following: • Advances in computer and related hardware products • New support tools • New techniques, methodologies, processes, or lifecycle models • New interface standards • New reusable components • New management techniques • New quality-improvement techniques • New process development and deployment support tools 4. Identify potential barriers and ri sks to deploying each process- and technology-improvement proposal.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 204 Examples of barriers to deploying process and technology improvements include the following: • Turf guarding and parochial perspectives • Unclear or weak business rationale • Lack of short-term benefits and visible successes • Unclear picture of what is expected from everyone • Too many changes at the same time • Lack of involvement and support of relevant stakeholders Examples of risk factors that affect the deployment of process and technology improvements include the following: • Compatibility of the improvement with existing processes, values, and skills of potential end users • Complexity of the improvement • Difficulty implementing the improvement • Ability to demonstrate the value of the improvement before widespread deployment • Justification for large, up-front investm ents in areas such as tools and training • Inability to overcome “technology drag” where the current implementation is used successfully by a large and mature installed base of end users 5. Estimate the cost, effort, and schedule required for deploying each process- and technology-improvement proposal. 6. Select the process- and technol ogy-improvement proposals to be piloted before broadscale deployment. Since innovations, by definition, usually represent a major change, most innovative improvements will be piloted. 7. Document the results of t he evaluation of each process- and technology-improvement proposal. 8. Monitor the status of each pr ocess- and technology-improvement proposal. SP 1.2 Identify and Analyze Innovations Identify and analyze innovative improvements that could increase the organization’s quality and process performance. The specific practice, Collect and Analyze Improvement Proposals, analyzes proposals that are passively collected. The purpose of this specific practice is to actively s earch for, locate, and analyze innovative improvements. This search primar ily involves looking outside the organization.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 205Typical Work Products 1. Candidate innovative improvements 2. Analysis of proposed innovative improvements Subpractices 1. Analyze the organization's set of standard processes to determine areas where innovative improv ements would be most helpful. These analyses are performed to determine which subprocesses are critical to achieving the organization’s quality and process-performance objectives and which ones are good candidates to be improved. 2. Investigate innovative impr ovements that may improve the organization's set of standard processes. Investigating innovative improvements involves the following: • Systematically maintaining awareness of leading relevant technical work and technology trends • Periodically searching for commercially available innovative improvements • Collecting proposals for innovative im provements from the projects and the organization • Systematically reviewing processes and technologies used externally and comparing them to those used within the organization • Identifying areas where innovative im provements have been used successfully, and reviewing data and documentation of experience using these improvements • Identifying improvements that integrate new technology into products and project work environments 3. Analyze potential innovativ e improvements to understand their effects on process elements and predict their influence on the process. Process-performance models can provide a basis for analyzing possible effects of changes to process elements. Refer to the Organizational Proce ss Performance process area for more information about process-performance models. 4. Analyze the costs and benef its of potential innovative improvements. Innovative improvements that have a very large cost-to-benefit ratio are rejected. 5. Create process- and technology-improvement proposals for those innovative improvements that w ould result in improving the organization's processes or technologies.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 206 6. Select the innovative improvement s to be piloted before broadscale deployment. Since innovations, by definition, usually represent a major change, most innovative improvements will be piloted. 7. Document the results of the evaluations of innovative improvements. SP 1.3 Pilot Improvements Pilot process and technology improvements to select which ones to implement. Pilots are performed to assess new and unproven major changes before they are broadly deployed, as appropriate. The implementation of th is specific practice may overlap with the implementation of the Im plement the Action Proposal s specific practice in the Causal Analysis and Resolu tion process area (e.g., when causal analysis and resolution is implem ented organizationally or across multiple projects). Typical Work Products 1. Pilot evaluation reports 2. Documented lessons learned from pilots Subpractices 1. Plan the pilots. When planning pilots, it is critical to define quantitative criteria to be used for evaluating pilot results. 2. Review and get relevant stakehol der agreement on the plans for the pilots. 3. Consult with and assist the people performing the pilots. 4. Perform each pilot in an environment that is characteristic of the environment present in a broadscale deployment. 5. Track the pilots against their plans. 6. Review and document the results of pilots.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 207Pilot results are evaluated using the quantitative criteria defined during pilot planning. Reviewing and documenting the results of pilots usually involves the following: • Deciding whether to terminate the pilot, replan and continue the pilot, or proceed with deploying the process and technology improvement • Updating the disposition of process- and technology-improvement proposals associated with the pilot • Identifying and documenting new process- and technology-improvement proposals as appropriate • Identifying and documenting lessons learned and problems encountered during the pilot SP 1.4 Select Improvements for Deployment Select process and technology improvements for deployment across the organization. Selection of process and technol ogy improvements for deployment across the organization is based on qu antifiable criteria derived from the organization’s quality and proc ess-performance objectives. Typical Work Products 1. Process and technology improv ements selected for deployment Subpractices 1. Prioritize the candidate proc ess and technology improvements for deployment. Priority is based on an evaluation of the estimated cost-to-benefit ratio with regard to the quality and process-performance objectives. Refer to the Organizational Proce ss Performance process area for more information about quality and process-performance objectives. 2. Select the process and technol ogy improvements to be deployed. The selection of the process improvements is based on their priorities and the available resources. 3. Determine how each process and technology improvement will be deployed.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 208 Examples of where the process and technology improvements may be deployed include the following: • Organizational process assets • Project-specific or common work environments • Organization’s product families • Organization's capabilities • Organization’s projects • Organizational groups 4. Document the results of the selection process. The results of the selection process usually include the following: • The selection criteria for candidate improvements • The disposition of each improvement proposal • The rationale for the disposition of each improvement proposal • The assets to be changed for each selected improvement SG 2 Deploy Improvements Measurable improvements to the organization's processes and technologies are continually and systematically deployed. SP 2.1 Plan the Deployment Establish and maintain the plans for deploying the selected process and technology improvements. The plans for deploying each pr ocess and technology improvement may be included in the organization’s plan for organizational innovation and deployment or they ma y be documented separately. The implementation of th is specific practice complements the Deploy Organizational Process Assets specif ic practice in the Organizational Process Focus process area, and adds the use of quantitative data to guide the deployment and to determi ne the value of the improvements with respect to quality and pr ocess-performance objectives. Refer to the Organizational Proc ess Focus process area for more information about deploying or ganizational process assets. This specific practice plans the deployment of individual process and technology improvements. The Plan the Process generic practice addresses comprehensive planning that covers the specific practices in this process area.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 209Typical Work Products 1. Deployment plan for se lected process and technology improvements Subpractices 1. Determine how each process and technology improvement must be adjusted for organization-wide deployment. Process and technology improvements proposed within a limited context (e.g., for a single project) might have to be modified to work across the organization. 2. Determine the changes necessa ry to deploy each process and technology improvement. Examples of changes needed to deploy a process and technology improvement include the following: • Process descriptions, standards, and procedures • Work environments • Education and training • Skills • Existing commitments • Existing activities • Continuing support to end users • Organizational culture and characteristics 3. Identify strategies to address potential barriers to deploying each process and technology improvement. 4. Establish measures and objectiv es for determining the value of each process and technology improv ement with respect to the organization’s quality and process-performance objectives. Examples of measures for determining the value of a process and technology improvement include the following: • Return on investment • Time to recover the cost of the process or technology improvement • Measured improvement in the project’ s or organization’s process performance • Number and types of project and organiza tional risks mitigated by the process or technology improvement • Average time required to respond to changes in project requirements, market situations, and the business environment",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 210 Refer to the Measurement and Analysis process area for more information about establishing objectives for measurement and analysis, specifying the measur es and analyses to be performed, obtaining and analyzing measures , and reporting results. 5. Document the plan for depl oying each process and technology improvement. 6. Review and get agreement with relevant stakeholders on the plan for deploying each process and technology improvement. 7. Revise the plan for deploy ing each process and technology improvement as necessary. SP 2.2 Manage the Deployment Manage the deployment of the selected process and technology improvements. The implementation of th is specific practice may overlap with the implementation of the Im plement the Action Proposal s specific practice in the Causal Analysis and Resolu tion process area (e.g., when causal analysis and resolution is implem ented organizationally or across multiple projects). The primary differe nce is that in t he Causal Analysis and Resolution process area, planning is done to manage the removal of the root causes of defects or problems from the project’s defined processes. In the Organizational I nnovation and Deployment process area, planning is done to manage t he deployment of improvements to the organization’s processes and te chnologies that can be quantified against the organization’s business objectives. Typical Work Products 1. Updated training materials (to reflect deployed process and technology improvements) 2. Documented results of proc ess- and technology-improvement deployment activities 3. Revised process- and tec hnology-improvement measures, objectives, priorities , and deployment plans Subpractices 1. Monitor the deployment of the process and technology improvements using t he deployment plan. 2. Coordinate the deployment of process and technology improvements across the organization.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 211Coordinating deployment includes the following activities: • Coordinating the activities of projec ts, support groups, and organizational groups for each process and technology improvement • Coordinating the activities for deploying related process and technology improvements 3. Quickly deploy process and te chnology improvements in a controlled and disciplined manner, as appropriate. Examples of methods for quickly deploying process and technology improvements include the following: • Using red-lines, process change notic es, or other controlled process documentation as interim process descriptions • Deploying process and technology improvem ents incrementally, rather than as a single deployment • Providing comprehensive consulting to early adopters of the process and technology improvement in lieu of revised formal training 4. Incorporate the process and technology improvements into organizational process assets, as appropriate. Refer to the Organizational Process Definition process area for more information about organizational process assets. 5. Coordinate the deployment of the process and technology improvements into the projects' defined processes as appropriate. Refer to the Organizational Proc ess Focus process area for more information about deploying or ganizational process assets. 6. Provide consulting, as appropri ate, to support deployment of the process and technology improvements. 7. Provide updated training material s to reflect the improvements to the organizational process assets. Refer to the Organizational Training process area for more information about training materials. 8. Confirm that the deployment of all process and technology improvements is completed. 9. Determine whether the ability of the defined process to meet quality and process-performance obje ctives is adversely affected by the process and technology improvement, and take corrective action as necessary.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 212 Refer to the Quantitative Proj ect Management process area for more information about quantitat ively managing the project’s defined process to achieve the project’s established quality and process-performance objectives. 10. Document and review the resu lts of process- and technology- improvement deployment. Documenting and reviewing the results includes the following: • Identifying and documenting lessons learned • Identifying and documenting new process- and technology-improvement proposals • Revising process- and technology-improvem ent measures, objectives, priorities, and deployment plans SP 2.3 Measure Improvement Effects Measure the effects of the deployed process and technology improvements. Refer to the Measurement and Analysis process area for more information about establishing objectives for measurement and analysis, specifying the measur es and analyses to be performed, obtaining and analyzing measures , and reporting results. The implementation of th is specific practice may overlap with the implementation of the Eval uate the Effect of Chan ges specific practice in the Causal Analysis and Resolu tion process area (e.g., when causal analysis and resolution is implem ented organizationally or across multiple projects). Typical Work Products 1. Documented measures of the e ffects resulting from the deployed process and technology improvements Subpractices 1. Measure the actual cost, e ffort, and schedule for deploying each process and technology improvement. 2. Measure the value of each pr ocess and technology improvement. 3. Measure the progress toward achieving the organization's quality and process-performance objectives. 4. Analyze the progress toward achieving the organization's quality and process-performance objectives and take corrective action as needed.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 213Refer to the Organizational Proce ss Performance process area for more information about process-performance analyses. 5. Store the measures in the organization’s m easurement repository. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the organizational innovation and deployment process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the organizational innovation and deployment process. Elaboration: This policy establishes organizati onal expectations for identifying and deploying process and technology im provements that contribute to meeting quality and process- performance objectives.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 214 GP 2.2 Plan the Process Establish and maintain the plan for performing the organizational innovation and deployment process. Elaboration: This plan for performing the organi zational innovation and deployment process differs from the deployme nt plans described in a specific practice in this process area. The pl an called for in this generic practice would address the comprehensive pl anning for all of the specific practices in this process area, from collecting and analyzing improvement proposals all the way through to the measurement of improvement effects. In contrast, t he deployment plans called for in the specific practice would address the planning needed for the deployment of individual process and technology improvements. GP 2.3 Provide Resources Provide adequate resources for performing the organizational innovation and deployment process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Simulation packages • Prototyping tools • Statistical packages • Dynamic systems modeling • Subscriptions to online technology databases and publications • Process modeling tools GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the organizational innovation and deployment process. GP 2.5 Train People Train the people performing or supporting the organizational innovation and deployment process as needed.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 215Elaboration: Examples of training topics include the following: • Planning, designing, and conducting pilots • Cost/benefit analysis • Technology transition • Change management GP 2.6 Manage Configurations Place designated work products of the organizational innovation and deployment process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Documented lessons learned from pilots • Revised process- and technology-improvement measures, objectives, priorities, and deployment plans • Updated training material GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the organizational innovation and deployment process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Reviewing process- and technology-improvement proposals that may have major impacts on process performance or on customer and end-user satisfaction • Providing feedback to the organization on the status and results of the process- and technology-improvement deployment activities The feedback typically involves: • Informing the people who subm it process- and technology- improvement proposals about the di sposition of their proposals • Regularly informing relevant stakeholders about the plans and status for selecting and deploying process and technology improvements • Preparing and distributing a summa ry of process- and technology- improvement selection and deployment activities",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 216 GP 2.8 Monitor and Control the Process Monitor and control the organizational innovation and deployment process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Change in quality • Change in process performance • Schedule for activities to deploy a selected improvement GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the organizational innovation and deployment process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Selecting improvements • Deploying improvements Examples of work products reviewed include the following: • Deployment plans • Revised process- and technology-improvement measures, objectives, priorities, and deployment plans • Updated training material GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the organizational innovation and deployment process with higher level management and resolve issues.",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 217Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined organizational innovation and deployment process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the organizational innovation and deployment process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Lessons learned captured from relevant stakeholders that identify barriers to deployment from previous technology insertions • Documented measures of the costs and benefits resulting from deploying innovations • Report of a comparison of similar development processes to identify the potential for improving efficiency",
        "CMMI for Development Version 1.2 Organizational Innovation and Deployment (OID) 218 Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the organizational innovation and deployment process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the organizational innovation and deployment process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the organizational innovation and deployment process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the organizational innovation and deployment process.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 219ORGANIZATIONAL PROCESS DEFINITION +IPPD A Process Management Process Area at Maturity Level 3 Purpose The purpose of Organizational Process Definition (OPD) is to establish and maintain a usable set of organi zational process assets and work environment standards. IPPD Addition For IPPD, Organizational Process De finition +IPPD also covers the establishment of organizational rules and guidelines that enable conducting work using integrated teams. Introductory Notes Organizational process assets enabl e consistent process performance across the organization and provide a basis for cumulative, long-term benefits to the organization. (See the definition of “organizational process assets” in the glossary.) The organization’s process asset lib rary is a collection of items maintained by the organization for us e by the people and projects of the organization. This collection of item s includes descriptions of processes and process elements, descriptions of lifecycle models, process tailoring guidelines, process-relat ed documentation, and data. The organization’s process asset library supports organizational learning and process improvement by allowing the sharing of best practices and lessons learned across the organization. The organization’s set of standard processes is tailored by projects to create their defined processes. The ot her organizational process assets are used to support tailoring as we ll as the implementation of the defined processes. The work envir onment standards are used to guide creation of project work environments. A standard process is composed of other processes (i.e., subprocesses) or process element s. A process element is the fundamental (e.g., atomic) unit of pr ocess definition and describes the activities and tasks to consistently perform work. Process architecture provides rules for connecting t he process elements of a standard process. The organization’s set of standard processes may include multiple process architectures.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 220 (See the definitions of “standard pr ocess,” “process architecture,” “subprocess,” and “process element” in the glossary.) The organizational process assets may be organized in many ways, depending on the implementation of the Organizational Process Definition process area. Examples include the following: • Descriptions of lifecycle models may be documented as part of the organization’s set of standard processes, or they may be documented separately. • The organization’s set of standard processes may be stored in the organization’s process asset library, or they may be stored separately. • A single repository may contain both the measurements and the process-related documentation, or they may be stored separately. Related Process Areas Refer to the Organizational Proc ess Focus process area for more information about organizati onal process-related matters. Specific Goal and Practice Summary SG 1 Establish Organizational Process Assets SP 1.1 Establish Standard Processes SP 1.2 Establish Lifecycle Model Descriptions SP 1.3 Establish Tailoring Criteria and Guidelines SP 1.4 Establish the Organization’s Measurement Repository SP 1.5 Establish the Organization’s Process Asset Library SP 1.6 Establish Work Environment Standards IPPD Addition SG 2 Enable IPPD Management SP 2.1 Establish Empowerment Mechanisms SP 2.2 Establish Rules and Guidelines for Integrated Teams SP 2.3 Balance Team and Home Organization Responsibilities Specific Practices by Goal SG 1 Establish Organizational Process Assets A set of organizational process assets is established and maintained.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 221IPPD Addition Integrated processes that emphasize parallel rather than serial development are a cornerstone of IPPD implementation. The processes for developing the product and for developing product-related lifecycle processes, such as the manufacturing process and the support process, are integrated and conducted concurrently. Such integrated processes should accommodate the informat ion provided by stakeholders representing all phases of the product lifecycle from both business and technical functions. Processes for effective teamwork are also needed. SP 1.1 Establish Standard Processes Establish and maintain the organization's set of standard processes. Standard processes may be defined at mu ltiple levels in an enterprise and they may be related in a hierarchical manner. For example, an enterprise may have a set of standar d processes that is tailored by individual organizations (e.g., a division or site) in the enterprise to establish their set of standard processes. The set of standard processes may also be tailored for each of the organization’s business areas or product lines. Thus “t he organization’s set of standard processes” can refer to the st andard processes established at the organization level and standard proc esses that may be established at lower levels, although some organizations may only have a single level of standard processes. (See the defin itions of “standard process” and “organization’s set of standard processes” in the glossary.) Multiple standard processes may be needed to address the needs of different application domains, life cycle models, methodologies, and tools. The organization’s set of st andard processes contains process elements (e.g., a work product size -estimating element) that may be interconnected according to one or mo re process architectures that describe the relationships am ong these process elements. The organization’s set of standard processes typically includes technical, management, administr ative, support, and organizational processes. IPPD Addition In an IPPD environment, the organiza tion's set of standard processes includes a process that projects use to establish a shared vision. The organization’s set of standard pr ocesses should collectively cover all processes needed by the organization and projects, including those processes addressed by the proce ss areas at Maturity Level 2.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 222 Typical Work Products 1. Organization's set of standard processes Subpractices 1. Decompose each standard proce ss into constituent process elements to the detail needed to understand and describe the process. Each process element covers a bounded and closely related set of activities. The descriptions of the process elements may be templates to be filled in, fragments to be completed, abstractions to be refined, or complete descriptions to be tailored or used unmodified. These elements are described in sufficient detail such that the process, when fully defined, can be consistently performed by appropriately trained and skilled people. Examples of process elements include the following: • Template for generating work product size estimates • Description of work product design methodology • Tailorable peer review methodology • Template for conduct of management reviews 2. Specify the critical attri butes of each process element. Examples of critical attributes include the following: • Process roles • Applicable standards • Applicable procedures, methods, tools, and resources • Process-performance objectives • Entry criteria • Inputs • Product and process measures to be collected and used • Verification points (e.g., peer reviews) • Outputs • Interfaces • Exit criteria 3. Specify the relationships of the process elements.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 223Examples of relationships include the following: • Ordering of the process elements • Interfaces among the process elements • Interfaces with external processes • Interdependencies among the process elements The rules for describing the relationships among process elements are referred to as “process architecture.” The process architecture covers the essential requirements and guidelines. The detailed specifications of these relationships are covered in the descriptions of the defined processes that are tailored from the organization’s set of standard processes. 4. Ensure that the organization' s set of standard processes adheres to applicable policies, standards, and models. Adherence to applicable process standards and models is typically demonstrated by developing a mapping from the organization’s set of standard processes to the relevant process standards and models. In addition, this mapping will be a useful input to future appraisals. 5. Ensure that the organization’s set of standard processes satisfies the process needs and objectiv es of the organization. Refer to the Organizational Proc ess Focus process area for more information about establishing and maintaining the organization’s process needs and objectives. 6. Ensure that there is appropri ate integration among the processes that are included in the organizati on’s set of standard processes. 7. Document the organization's set of standard processes. 8. Conduct peer reviews on the organization's set of standard processes. Refer to the Verification process area for more information about peer review. 9. Revise the organization's set of standard processes as necessary. SP 1.2 Establish Lifecycle Model Descriptions Establish and maintain descriptions of the lifecycle models approved for use in the organization. Lifecycle models may be developed for a variety of customers or in a variety of situations, since one lif ecycle model may not be appropriate for all situations. Lifecycle models are often used to define the phases of the project. Also, the organizati on may define different lifecycle models for each type of produc t and service it delivers.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 224 Typical Work Products 1. Descriptions of lifecycle models Subpractices 1. Select lifecycle models based on the needs of projects and the organization. For example, project lifecycle models include the following: • Waterfall • Spiral • Evolutionary • Incremental • Iterative 2. Document the descriptions of the lifecycle models. The lifecycle models may be documented as part of the organization’s standard process descriptions or they may be documented separately. 3. Conduct peer reviews on the lifecycle models. Refer to the Verification process area for more information about conducting peer reviews. 4. Revise the descriptions of t he lifecycle models as necessary. SP 1.3 Establish Tailoring Criteria and Guidelines Establish and maintain the tailoring criteria and guidelines for the organization's set of standard processes. IPPD Addition In creating the tailoring criteria and guidelines, include considerations for concurrent development and operating wi th integrated teams. For example, how one tailors the manufacturing process will be diffe rent depending on whether it is developed serially after the product has been developed or in parallel with the development of the pr oduct, as in IPPD. Processes, such as resource allocation, will also be tailored differently if the project is operating with integrated teams.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 225The tailoring criteria and guide lines describe the following: • How the organization’s set of standard processes and organizational process assets are used to create the defined processes • Mandatory requirements that mu st be satisfied by the defined processes (e.g., the subset of the organizational process assets that are essential for any defined process) • Options that can be exercised and criteria for selecting among the options • Procedures that must be follow ed in performing and documenting process tailoring Examples of reasons for tailoring include the following: • Adapting the process for a new product line or work environment • Customizing the process for a specific application or class of similar applications • Elaborating the process description so that the resulting defined process can be performed Flexibility in tailoring and defining processes is balanced with ensuring appropriate consistency in the proc esses across the organization. Flexibility is needed to address cont extual variables such as the domain; nature of the customer; co st, schedule, and quality tradeoffs; technical difficulty of the wo rk; and experience of the people implementing the process. Consis tency across the organization is needed so that organizational standards , objectives, and strategies are appropriately addressed, and proce ss data and lessons learned can be shared. Tailoring criteria and guidelines may allow for using a standard process “as is,” with no tailoring. Typical Work Products 1. Tailoring guidelines for the organization's set of standard processes Subpractices 1. Specify the selection criter ia and procedures for tailoring the organization's set of standard processes.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 226 Examples of criteria and procedures include the following: • Criteria for selecting lifecycle model s from those approved by the organization • Criteria for selecting process element s from the organization’s set of standard processes • Procedures for tailoring the selected lif ecycle models and process elements to accommodate specific process characteristics and needs Examples of tailoring actions include the following: • Modifying a lifecycle model • Combining elements of different lifecycle models • Modifying process elements • Replacing process elements • Reordering process elements 2. Specify the standar ds for documenting t he defined processes. 3. Specify the procedures for s ubmitting and obtaining approval of waivers from the requirements of the organization’s set of standard processes. 4. Document the tailoring guideli nes for the organization's set of standard processes. 5. Conduct peer reviews on the tailoring guidelines. Refer to the Verification process area for more information about conducting peer reviews. 6. Revise the tailoring guidelines as necessary. SP 1.4 Establish the Organization’s Measurement Repository Establish and maintain the organization’s measurement repository. Refer to the Use Organizational Process Assets for Planning Project Activities specific practice of the Integrated Project Management process area for more information about the use of the organization’s measurement repository in pl anning project activities. The repository contains both product and proce ss measures that are related to the organization’s set of st andard processes. It also contains or refers to the information needed to understand and interpret the measures and assess them for reasonableness and applicability. For example, the definitions of the m easures are used to compare similar measures from different processes.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 227Typical Work Products 1. Definition of the common set of product and process measures for the organization’s set of standard processes 2. Design of the organizati on’s measurement repository 3. Organization's measurement r epository (that is , the repository structure and support environment) 4. Organization’s measurement data Subpractices 1. Determine the organization's needs for storing, retrieving, and analyzing measurements. 2. Define a common set of proce ss and product measures for the organization's set of standard processes. The measures in the common set are selected based on the organization’s set of standard processes. They are selected for their ability to provide visibility into process performance to support expected business objectives. The common set of measures may vary for different standard processes. Operational definitions for the measures specify the procedures for collecting valid data and the point in the process where the data will be collected. Examples of classes of commonly used measures include the following: • Estimates of work product size (e.g., pages) • Estimates of effort and cost (e.g., person hours) • Actual measures of size, effort, and cost • Quality measures (e.g., number of defects found or severity of defects) • Peer review coverage • Test coverage • Reliability measures (e.g., mean time to failure) Refer to the Measurement and Analysis process area for more information about defining measures. 3. Design and implement the measurement repository. 4. Specify the proc edures for storing, updating, and retrieving measures. 5. Conduct peer reviews on the def initions of the common set of measures and the procedures fo r storing and retrieving measures. Refer to the Verification process area for more information about conducting peer reviews.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 228 6. Enter the specified meas ures into the repository. Refer to the Measurement and Analysis process area for more information about collecting and analyzing data. 7. Make the contents of the measur ement repository available for use by the organization and pr ojects as appropriate. 8. Revise the measurement reposit ory, common set of measures, and procedures as the organization’s needs change. Examples of when the common set of measures may need to be revised include the following: • New processes are added • Processes are revised and new measures are needed • Finer granularity of data is required • Greater visibility into the process is required • Measures are retired SP 1.5 Establish the Organization’s Process Asset Library Establish and maintain the organization's process asset library. Examples of items to be stored in the organization’s process asset library include the following: • Organizational policies • Defined process descriptions • Procedures (e.g., estimating procedure) • Development plans • Acquisition plans • Quality assurance plans • Training materials • Process aids (e.g., checklists) • Lessons-learned reports Typical Work Products 1. Design of the organization’s process asset library 2. Organization's process asset library 3. Selected items to be included in the organization’s process asset library 4. Catalog of items in the or ganization’s process asset library",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 229Subpractices 1. Design and implement the organization’s process asset library, including the library structure and support environment. 2. Specify the criteria for including items in the library. The items are selected based primarily on their relationship to the organization’s set of standard processes. 3. Specify the proc edures for storing and retrieving items. 4. Enter the selected items into t he library and catalog them for easy reference and retrieval. 5. Make the items availabl e for use by the projects. 6. Periodically review the use of each item and use the results to maintain the library contents. 7. Revise the organization’s proc ess asset library as necessary. Examples of when the library may need to be revised include the following: • New items are added • Items are retired • Current versions of items are changed SP 1.6 Establish Work Environment Standards Establish and maintain work environment standards. Work environment standards allow the organization and projects to benefit from common tools, training, and maintenance, as well as cost savings from volume purchases. Work environment standards address the needs of all stakeholders and c onsider productivity, cost, availability, security, and workpl ace health, safety, and ergonomic factors. Work environment standards can include guidelines for tailoring and/or the use of waivers that allo w adaptation of the project’s work environment to meet specific needs. Examples of work environment standards include • Procedures for operation, safety, and security of the work environment • Standard workstation hardware and software • Standard application software and tailoring guidelines for it • Standard production and calibration equipment • Process for requesting and approving tailoring or waivers Typical Work Products 1. Work environment standards",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 230 Subpractices 1. Evaluate commercially-availabl e work environment standards appropriate for the organization. 2. Adopt existing work environm ent standards and develop new ones to fill gaps based on the or ganization’s process needs and objectives. IPPD Addition SG 2 Enable IPPD Management Organizational rules and guidelines, which govern the operation of integrated teams, are provided. An organizational infrastructure that supports and promotes IPPD concepts is critical if it is to be successfully sustained over the long term. These rules and guidelines prom ote concepts such as integrated teaming and allow for empowered decision making at many levels. Through its rules and guidelines, the organization demonstrates commitment to IPPD and the succe ss of its integrated teams. IPPD rules and guidelines become part of the organization’s set of standard processes and the project’s defined process. The organization’s standard processes enabl e, promote, and reinforce the behaviors expected from projects, integrated teams, and people. These expected behaviors are typically comm unicated in the form of policies, operating procedures, guidelines, and other organizational process assets. SP 2.1 Establish Empowerment Mechanisms Establish and maintain empowerment mechanisms to enable timely decision making. In a successful IPPD environmen t, clear channels of responsibility and authority must be established. Issues can arise at any level of the organization when integrated teams assume too much or too little authority and when it is unclear who is responsible for making decisions. Documenting and deployin g organizational guidelines that clearly define the empowerment of in tegrated teams c an prevent these issues.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 231IPPD Addition Implementing IPPD introduces chall enges to leadership because of the cultural changes required when people and integrated teams are empowered and decisions are driven to the lowest level appropriate. Effective and efficient communication mechanisms are critical to timely and sound decision making in the in tegrated work environment. Once an integrated team proj ect structure is est ablished and training is provided, mechanisms to handle empowerment, decision making, and issue resolution also need to be provided. Refer to the Decision Analysis and Resolution process area for more information about decision making. Typical Work Products 1. Empowerment rules and guidelines for people and integrated teams 2. Decision-making rules and guidelines 3. Issue resolution documentation Subpractices 1. Determine rules and guidelines for the degree of empowerment provided to people and integrated teams. Factors to consider regarding integrated team empowerment include the following: • Authority of teams to pick their own leader • Authority of teams to implement subteams (e.g., a product team forming an integration subteam) • The degree of collective decision making • The level of consensus needed for integrated team decisions • How conflicts and differences of opinion within the integrated teams are addressed and resolved 2. Determine rules and guidelines fo r the use of di fferent decision types in making various ki nds of team decisions. 3. Define the process for usi ng the decision-making rules and guidelines. 4. Define a process for issue resolution when an issue cannot be decided at the level at which it arose.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 232 IPPD Addition Refer to the Resolve Coordination Is sues specific practice in the Integrated Project Management process area for more information about resolving issues with relevant stakeholders. 5. Maintain the empowerment mechanisms and the rules and guidelines for decision making. SP 2.2 Establish Rules and Guidelines for Integrated Teams Establish and maintain organizational rules and guidelines for structuring and forming integrated teams. Operating rules and guidelines fo r the integrated teams define and control how teams interact to acco mplish objectives. These rules and guidelines also promote the effective leveraging of the teams’ efforts, high performance, and productivity. In tegrated team members must understand the standards for work and participate according to those standards. Typical Work Products 1. Rules and guidelines for the stru cturing and formation of integrated teams Subpractices 1. Establish rules and guidelines for structuring and forming integrated teams. Organizational process assets can help the project to structure and implement integrated teams. Such assets may include the following: • Team structure guidelines • Team formation guidelines • Team authority and responsibility guidelines • IPPD implementation techniques • Guidelines for managing risks in IPPD • Guidelines for establishing lines of communication and authority • Team leader selection criteria • Team responsibility guidelines",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 233IPPD Addition 2. Define the expectations, rules, and guidelines t hat will guide how the integrated teams work collectively. These rules and guidelines establish organizational practices for consistency across integrated teams and can include the following: • How interfaces among integrated teams are established and maintained • How assignments are accepted • How resources and input are accessed • How work gets done • Who checks, reviews, and approves work • How work is approved • How work is delivered and communicated • Reporting chains • Reporting requirements (cost, schedule, and performance status), measures, and methods • Progress reporting measures and methods 3. Maintain the rules and guidelines for structuring and forming integrated teams. SP 2.3 Balance Team and Home Organization Responsibilities Establish and maintain organizational guidelines to help team members balance their team and home organization responsibilities. A “home organization” is the part of the organization to which team members are assigned when they are not on an integrated team. A home organization may be called a “functional organization,” “home base,” “home office,” or “direct organization.” Home organizations are often responsible for the career growth of their members (e.g., performance appraisals and training to maintain functional and discipline expertise). In an IPPD environment, reporting procedures and rating systems assume that members’ responsibilit ies are focused on the integrated team, not on the home organization. However, the re sponsibility of integrated team members to their hom e organizations is also important, specifically for process implem entation and improvement. Workloads and responsibilities should be balanced between projects and functions, and career growth and advancement. Organizational mechanisms should exist that support the home organization while aligning the workforce to meet business objec tives in a teaming environment.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 234 IPPD Addition Sometimes teams persist beyond thei r productive life in organizations that do not have a home organization fo r the team members to return to after the integrated the team is di ssolved. Therefore, there should be guidelines for disbanding the integr ated teams and maintaining home organizations. Typical Work Products 1. Organizational guidelines for balancing team and home organization responsibilities 2. Performance review process that considers both functional supervisor and team leader input Subpractices 1. Establish guidelines for home or ganization respon sibilities that promote integrated team behavior. 2. Establish guidelines for team management responsibilities to ensure integrated team members r eport appropriately to their home organizations. 3. Establish a perform ance review process that considers input from both home organization and in tegrated team leaders. 4. Maintain the guidelines for balancing team and home organization responsibilities. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the organizational process definition process to develop work products and provide services to achieve the specific goals of the process area.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 235Continuous Only GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the organizational process definition process. Elaboration: This policy establishes organizational expectations for establishing and maintaining a set of standard processes for use by the organization and making organizational process assets available across the organization. GP 2.2 Plan the Process Establish and maintain the plan for performing the organizational process definition process. Elaboration: This plan for performing the organi zational process definition process can be part of (or referenced by) the organization’s process improvement plan. GP 2.3 Provide Resources Provide adequate resources for performing the organizational process definition process, developing the work products, and providing the services of the process. Elaboration: A process group typically manages the organizational process definition activities. This group typically is staffed by a core of professionals whose primary responsibility is c oordinating organizational process",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 236 improvement. This group is supported by process owners and people with expertise in various disci plines such as the following: • Project management • The appropriate engineering disciplines • Configuration management • Quality assurance Examples of other resources provided include the following tools: • Database management systems • Process modeling tools • Web page builders and browsers GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the organizational process definition process. GP 2.5 Train People Train the people performing or supporting the organizational process definition process as needed. Elaboration: Examples of training topics include the following: • CMMI and other process and process improvement reference models • Planning, managing, and monitoring processes • Process modeling and definition • Developing a tailorable standard process • Developing work environment standards • Ergonomics GP 2.6 Manage Configurations Place designated work products of the organizational process definition process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 237Elaboration: Examples of work products placed under control include the following: • Organization’s set of standard processes • Descriptions of the lifecycle models • Tailoring guidelines for the organization’s set of standard processes • Definitions of the common set of product and process measures • Organization’s measurement data IPPD Addition Examples of work products placed under control include the following: • Empowerment rules and guidelines for people and integrated teams • Organizational process documentation for issue resolution GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the organizational process definition process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Reviewing the organization’s set of standard processes • Reviewing the organization’s lifecycle models • Resolving issues on the tailoring guidelines • Assessing the definitions of the common set of process and product measures • Reviewing the work environment standards IPPD Addition Examples of activities for stakeholder involvement also include the following: • Establishing and maintaining IPPD empowerment mechanisms • Establishing and maintaining organizational rules and guidelines for the structuring and forming of integrated teams GP 2.8 Monitor and Control the Process Monitor and control the organizational process definition process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 238 Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Percentage of projects using the process architectures and process elements of the organization’s set of standard processes • Defect density of each process element of the organization’s set of standard processes • Number of worker's compensation claims due to ergonomic problems • Schedule for development of a process or process change GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the organizational process definition process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Establishing organizational process assets IPPD Addition Examples of activities reviewed also include the following: • Determining rules and guidelines for the degree of empowerment provided to people and integrated teams • Establishing and maintaining an issue resolution process Examples of work products reviewed include the following: • Organization’s set of standard processes • Descriptions of the lifecycle models • Tailoring guidelines for the organization’s set of standard processes • Organization’s measurement data IPPD Addition Examples of work products reviewed also include the following: • Empowerment rules and guidelines for people and integrated teams • Organizational process documentation",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 239GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the organizational process definition process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined organizational process definition process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the organizational process definition process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Submission of lessons learned to the organization's process asset library • Submission of measurement data to the organization's measurement repository • Status of the change requests submitted to modify the organization's standard process • Record of non-standard tailoring requests IPPD Addition Examples of work products, measures, measurement results, and improvement information also include the following: • Status of performance review input from integrated teams",
        "CMMI for Development Version 1.2 Organizational Process Definition +IPPD (OPD+IPPD) 240 Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the organizational process definition process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the organizational process definition process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the organizational process definition process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the organizational process definition process.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 241ORGANIZATIONAL PROCESS FOCUS A Process Management Process Area at Maturity Level 3 Purpose The purpose of Organizational Process Focus (OPF) is to plan, implement, and deploy organizational process improvements based on a thorough understanding of the cu rrent strengths and weaknesses of the organization’s processes and process assets. Introductory Notes The organization's processes include all the processes used by the organization and its projects. Candidate improvements to the organization's processes and process assets are obtained from various sources, including measurement of the processes, lessons learned in implementing the processes, results of process appraisals, results of product evaluation activities, re sults of benchmarking against other organizations’ processes, and recommendations from other improvement initiatives in the organization. Process improvement occurs within the context of the organization’s needs and is used to address the organization’s objectives. The organization encourages participation in process improvement activities by those who will perform the proce ss. The responsibility for facilitating and managing the organization’s pr ocess improvement activities, including coordinating the participation of others, is typically assigned to a process group. The organization pr ovides the long-term commitment and resources required to sponsor this group and to ensure the effective and timely deployment of the improvements. Careful planning is required to ensur e that process improvement efforts across the organization are adeq uately managed and implemented. The organization’s planning for process improvement results in a process improvement plan. The organization’s process impr ovement plan will address appraisal planning, process action planning, pilot planning, and deployment planning. Appraisal plans describe the appraisal timeline and schedule, the scope of the appraisal, the re sources required to perform the appraisal, the reference model against which the appraisal will be performed, and the logistics for the appraisal. Process action plans usually result from appraisals and document how specific improvements targeti ng the weaknesses uncovered by an",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 242 appraisal will be implemented. In cases in which it is determined that the improvement described in the pr ocess action plan should be tested on a small group before deploying it across the organization, a pilot plan is generated. Finally, when the improvement is to be deployed, a deployment plan is used. This plan describes when and how the improvement will be deployed across the organization. Organizational process assets are used to describe, implement, and improve the organization's proc esses (see the definition of “organizational process assets” in the glossary). Related Process Areas Refer to the Organizational Process Definition process area for more information about the organizational process assets. Specific Goal and Practice Summary SG 1 Determine Process Improvement Opportunities SP 1.1 Establish Organizational Process Needs SP 1.2 Appraise the Organization’s Processes SP 1.3 Identify the Organization's Process Improvements SG 2 Plan and Implement Process Improvements SP 2.1 Establish Process Action Plans SP 2.2 Implement Process Action Plans SG 3 Deploy Organizational Process Assets and Incorporate Lessons Learned SP 3.1 Deploy Organizational Process Assets SP 3.2 Deploy Standard Processes SP 3.3 Monitor Implementation SP 3.4 Incorporate Process-Related Experiences into the Organizational Process Assets Specific Practices by Goal SG 1 Determine Process Improvement Opportunities Strengths, weaknesses, and improvement opportunities for the organization's processes are identified periodically and as needed. Strengths, weaknesses, and impr ovement opportunities may be determined relative to a process standard or model such as a CMMI model or International Organizati on for Standardization (ISO) standard. The process improvements should be selected specifically to address the organization’s needs. SP 1.1 Establish Organizational Process Needs Establish and maintain the description of the process needs and objectives for the organization.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 243IPPD Addition Integrated processes that emphasize parallel rather than serial development are a cornerstone of IPPD implementation. The processes for developing the product and for developing product-related lifecycle processes, such as the manufacturing process and the support process processes, are integrated and conducted concurrently. Such integrated processes need to accommodate the in formation provided by stakeholders representing all phases of the product lifecycle from both business and technical functions. Processes for effective teamwork will also be needed. IPPD Addition Examples of processes for effective teamwork include the following: • Communications • Collaborative decision making • Issue resolution • Team building The organization’s processes operate in a business context that must be understood. The organization’ s business objectives, needs, and constraints determine the needs and objectives for the organization’s processes. Typically, the issues re lated to finance, technology, quality, human resources, and marketing are im portant process considerations. The organization’s process needs and objectives cover aspects that include the following: • Characteristics of the processes • Process-performance objectives , such as time-to-market and delivered quality • Process effectiveness Typical Work Products 1. Organization’s proc ess needs and objectives Subpractices 1. Identify the policie s, standards, and business objectives that are applicable to the organization's processes. 2. Examine relevant process standar ds and models for best practices. 3. Determine the organization’s process-performance objectives. Process-performance objectives may be expressed in quantitative or qualitative terms.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 244 Refer to the Measurement and Analysis process area for more information about establishi ng measurement objectives. Examples of process-performance objectives include the following: • Cycle time • Defect removal rates • Productivity 4. Define the essential characterist ics of the organization’s processes. The essential characteristics of the organization’s processes are determined based on the following: • Processes currently being used in the organization • Standards imposed by the organization • Standards commonly imposed by customers of the organization Examples of process characteristics include the following: • Level of detail used to describe the processes • Process notation used • Granularity of the processes 5. Document the organization’ s process needs and objectives. 6. Revise the organization’s pr ocess needs and objectives as needed. SP 1.2 Appraise the Organization’s Processes Appraise the organization's processes periodically and as needed to maintain an understanding of their strengths and weaknesses. Process appraisals may be performed for the following reasons: • To identify processes that should be improved • To confirm progress and make t he benefits of process improvement visible • To satisfy the needs of a customer-supplier relationship • To motivate and facilitate buy-in The buy-in gained during a pr ocess appraisal can be eroded significantly if it is not followed by an appraisal-based action plan. Typical Work Products 1. Plans for the organization's process appraisals",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 2452. Appraisal findings that addre ss strengths and weaknesses of the organization's processes 3. Improvement recommendations for the organization's processes Subpractices 1. Obtain sponsorship of the process appraisal from senior management. Senior management sponsorship includes the commitment to have the organization's managers and staff participate in the process appraisal and to provide the resources and funding to analyze and communicate the findings of the appraisal. 2. Define the scope of the process appraisal. Process appraisals may be performed on the entire organization or may be performed on a smaller part of an organization such as a single project or business area. The scope of the process appraisal addresses the following: • Definition of the organization (e.g., sites or business areas) that will be covered by the appraisal • Identification of the project and suppor t functions that will represent the organization in the appraisal • Processes that will be appraised 3. Determine the method and criteria for process appraisal. Process appraisals can occur in many forms. Process appraisals should address the needs and objectives of the organization, which may change over time. For example, the appraisal may be based on a process model, such as a CMMI model, or on a national or international standard, such as ISO 9001 [ISO 2000]. The appraisals may also be based on a benchmark comparison with other organizations. The appraisal method may assume a variety of characteristics in terms of time and effort expended, makeup of the appraisal team, and the method and depth of investigation. 4. Plan, schedule, and prepare for the process appraisal. 5. Conduct the process appraisal. 6. Document and deliver the appr aisal’s activities and findings. SP 1.3 Identify the Organization's Process Improvements Identify improvements to the organization's processes and process assets. Typical Work Products 1. Analysis of candidat e process improvements",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 246 2. Identification of improvements for the organization's processes Subpractices 1. Determine candidate process improvements. Candidate process improvements are typically determined by doing the following: • Measure the processes and analyze the measurement results • Review the processes for effectiveness and suitability • Review the lessons learned from tailo ring the organization’s set of standard processes • Review the lessons learned from implementing the processes • Review process improvement proposals submitted by the organization’s managers, staff, and other relevant stakeholders • Solicit inputs on process improvements from senior management and leaders in the organization • Examine the results of process appraisals and other process-related reviews • Review results of other organizational improvement initiatives 2. Prioritize the candidat e process improvements. Criteria for prioritization are as follows: • Consider the estimated cost and effort to implement the process improvements • Appraise the expected improvement against the organization’s improvement objectives and priorities • Determine the potential barriers to the process improvements and develop strategies for overcoming these barriers Examples of techniques to help determine and prioritize the possible improvements to be implemented include the following: • A gap analysis that compares current conditions in the organization with optimal conditions • Force-field analysis of potential improvements to identify potential barriers and strategies for overcoming those barriers • Cause-and-effect analyses to provide information on the potential effects of different improvements that can then be compared 3. Identify and document the process improvements that will be implemented. 4. Revise the list of planned proce ss improvements to keep it current.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 247SG 2 Plan and Implement Process Improvements Process actions that address improvements to the organization’s processes and process assets are planned and implemented. Successful implementation of impr ovements requires participation in process action planning and implementa tion by process owners, those performing the process, and support organizations. SP 2.1 Establish Process Action Plans Establish and maintain process action plans to address improvements to the organization's processes and process assets. Establishing and maintaining process ac tion plans typically involves the following roles: • Management steering committees to set strategies and oversee process improvement activities • Process group staff to facilit ate and manage process improvement activities • Process action teams to defi ne and implement process actions • Process owners to manage deployment • Practitioners to perform the process This involvement helps to obtain buy-in on the process improvements and increases the likelihood of effective deployment. Process action plans are detailed implementation pl ans. These plans differ from the organization’s process improvement plan in that they are plans targeting specific improv ements that have been defined to address weaknesses usually uncovered by appraisals. Typical Work Products 1. Organization's approved process action plans Subpractices 1. Identify strategies, approac hes, and actions to address the identified proces s improvements. New, unproven, and major changes are piloted before they are incorporated into normal use. 2. Establish proces s action teams to im plement the actions. The teams and people performing the process improvement actions are called “process action teams.” Process action teams typically include process owners and those who perform the process. 3. Document process action plans.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 248 Process action plans typically cover the following: • Process improvement infrastructure • Process improvement objectives • Process improvements that will be addressed • Procedures for planning and tr acking process actions • Strategies for piloting and implementing the process actions • Responsibility and authority for implementing the process actions • Resources, schedules, and assignments for implementing the process actions • Methods for determining the effectiveness of the process actions • Risks associated with process action plans 4. Review and negotiate process action plans with relevant stakeholders. 5. Review process action plans as necessary. SP 2.2 Implement Process Action Plans Implement process action plans. Typical Work Products 1. Commitments among the various process action teams 2. Status and results of impl ementing process action plans 3. Plans for pilots Subpractices 1. Make process action plans readily available to relevant stakeholders. 2. Negotiate and document commi tments among the process action teams and revise their proces s action plans as necessary. 3. Track progress and commitment s against process action plans. 4. Conduct joint reviews with t he process action teams and relevant stakeholders to monitor the progress and results of the process actions. 5. Plan pilots needed to test selected process improvements. 6. Review the activities and work products of proc ess action teams. 7. Identify, document, and track to closure issues in implementing process action plans. 8. Ensure that the results of implementing process action plans satisfy the organization’s proc ess improvement objectives.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 249SG 3 Deploy Organizational Process Assets and Incorporate Lessons Learned The organizational process assets are deployed across the organization and process-related experiences are incorporated into the organizational process assets. The specific practices within th is specific goal describe ongoing activities. New opportunities to benef it from the organizational process assets and changes to them may arise throughout the life of each project. Deployment of the standard processes and other organizational process assets must be continually supported within the organization, particularly for new projects at startup. SP 3.1 Deploy Organizational Process Assets Deploy organizational process assets across the organization. Deploying organizational process as sets or changes to organizational process assets should be perform ed in an orderly manner. Some organizational process assets or changes to organizational process assets may not be appropriate for use in some parts of the organization (because of customer requirements or the current lifecycle phase being implemented, for example). It is ther efore important that those that are or will be executing the process, as well as other organization functions (such as training and quality assur ance), be involved in the deployment as necessary. Refer to the Organizational Process Definition process area for more information about how the deployment of organizational process assets is supported and enabled by the organization’s process asset library. Typical Work Products 1. Plans for deploying organizational process assets and changes to them across the organization 2. Training materials for deploying organizational process assets and changes to them 3. Documentation of changes to organizational process assets 4. Support materials for deploying organizational process assets and changes to them Subpractices 1. Deploy organizational proce ss assets across the organization. Typical activities performed as a part of this deployment include the following: • Identifying the organizational process assets that should be adopted by those who perform the process • Determining how the organizational proce ss assets are made available (e.g., via Web site)",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 250 • Identifying how changes to the organizati onal process assets are communicated • Identifying the resources (e.g., methods and tools) needed to support the use of the organizational process assets • Planning the deployment • Assisting those who use the organizational process assets • Ensuring that training is available for those who use the organizational process assets Refer to the Organizational Training process area for more information about coor dination of training. 2. Document the changes to t he organizational process assets. Documenting changes to the organizational process assets serves two main purposes: • To enable communication of the changes • To understand the relationship of changes in the organizational process assets to changes in process performance and results 3. Deploy the changes that were made to the organizational process assets across the organization. Typical activities performed as a part of deploying changes include the following: • Determining which changes are appropriate for those who perform the process • Planning the deployment • Arranging for the associated support needed to successfully transition the changes 4. Provide guidance and consultation on the use of the organizational process assets. SP 3.2 Deploy Standard Processes Deploy the organization’s set of standard processes to projects at their startup and deploy changes to them as appropriate throughout the life of each project. It is important that new projects us e proven and effective processes to perform critical early activities (e.g., project planning, receiving requirements, and obtaining resources). Projects should also periodically update their defined processes to incorporate the latest changes made to the organization’s set of standard processes when it will benefit them. This periodic updating helps to ensure that all project acti vities derive the fu ll benefit of what other projects have learned.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 251Refer to the Organizational Process Definition process area for more information about the organizati on’s set of standard processes and tailoring guidelines. Typical Work Products 1. Organization's list of projects and status of process deployment on each project (i.e., existing and planned projects) 2. Guidelines for deploying the organization’s set of standard processes on new projects 3. Records of tailoring the organization’s set of standard processes and implementing them on identified projects Subpractices 1. Identify projects within the organization that are starting up. 2. Identify active projects that would benefit from implementing the organization’s current set of standard processes. 3. Establish plans to implement the organization’s current set of standard processes on the identified projects. 4. Assist projects in tailori ng the organization’s set of standard processes to meet project needs. Refer to the Integrated Project Management process area for more information about tailoring the organization’s set of standard processes to meet the unique needs and objectives of the project. 5. Maintain records of tailoring and implementing processes on the identified projects. 6. Ensure that the defined processe s resulting from process tailoring are incorporated into the plans for process-compliance audits. Process-compliance audits address objective evaluations of project activities against the project’s defined processes. 7. As the organization’s set of standard processes are updated, identify which projects sh ould implement the changes. SP 3.3 Monitor Implementation Monitor the implementation of the organization’s set of standard processes and use of process assets on all projects. By monitoring implementation, t he organization ensures that the organization’s set of standard proc esses and other process assets are appropriately deployed to all projects . Monitoring implementation also helps the organization develop an understanding of the organizational process assets being used and wher e they are used within the",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 252 organization. Monitoring also helps to establish a broader context for interpreting and using process and pr oduct measures, lessons learned, and improvement information obtained from projects. Typical Work Products 1. Results of monitoring proc ess implementati on on projects 2. Status and results of pr ocess-compliance evaluations 3. Results of reviewing selected pr ocess artifacts created as part of process tailoring and implementation Subpractices 1. Monitor projects for their use of the organization’s process assets and changes to them. 2. Review selected process artifa cts created during the life of each project. Reviewing selected process artifacts created during the life of a project ensures that all projects are making appropriate use of the organization’s set of standard processes. 3. Review the results of process- compliance evaluations to determine how well the organization’s set of standard processes has been deployed. Refer to the Process and Product Quality Assurance process area for more information about objec tively evaluating processes against applicable process descriptions, standards, and procedures. 4. Identify, document, and track to closure issues related to implementing the organization’s set of standard processes. SP 3.4 Incorporate Process-Related Experiences into the Organizational Process Assets Incorporate process-related work products, measures, and improvement information derived from planning and performing the process into the organizational process assets. Typical Work Products 1. Process improvement proposals 2. Process lessons learned 3. Measurements on the or ganizational process assets 4. Improvement recommendations for the organizational process assets",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 2535. Records of the organization's process improvement activities 6. Information on the organizational process assets and improvements to them Subpractices 1. Conduct periodic reviews of t he effectiveness and suitability of the organization’s set of standard processes and related organizational process assets relative to the organization’s business objectives. 2. Obtain feedback about the use of the organizational process assets. 3. Derive lessons learned from defin ing, piloting, implementing, and deploying the organizational process assets. 4. Make available lessons learned to the people in the organization as appropriate. Actions may have to be taken to ensure that lessons learned are used appropriately. Examples of inappropriate use of lessons learned include the following: • Evaluating the performance of people • Judging process performance or results Examples of ways to prevent inappropriate use of lessons learned include the following: • Controlling access to the lessons learned • Educating people about the appropriate use of lessons learned 5. Analyze the organization' s common set of measures. Refer to the Measurement and Analysis process area for more information about analyzing measures. Refer to the Organizational Process Definition process area for more information about est ablishing an organizational measurement repository, including common measures. 6. Appraise the processes, me thods, and tools in use in the organization and develop recommendations for improving the organizational process assets.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 254 This appraisal typically includes the following: • Determining which of the processes, methods, and tools are of potential use to other parts of the organization • Appraising the quality and effectiveness of the organizational process assets • Identifying candidate improvements to the organizational process assets • Determining compliance with the organization’s set of standard processes and tailoring guidelines 7. Make the best of the organizati on's processes, methods, and tools available to the people in t he organization as appropriate. 8. Manage process improvement proposals. Process improvement proposals can address both process and technology improvements. The activities for managing process improvement proposals typically include the following: • Soliciting process im provement proposals • Collecting process improvement proposals • Reviewing process improvement proposals • Selecting the process improvement proposals that will be implemented • Tracking the implementation of process improvement proposals Process improvement proposals are documented as process change requests or problem reports, as appropriate. Some process improvement proposals may be incorporated into the organization’s process action plans. 9. Establish and maintain record s of the organization's process improvement activities. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 255Continuous Only GP 1.1 Perform Specific Practices Perform the specific practices of the organizational process focus process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the organizational process focus process. Elaboration: This policy establishes organizati onal expectations for determining process improvement opportunities for the processes being used and for planning, implementing, and depl oying process improvements across the organization. GP 2.2 Plan the Process Establish and maintain the plan for performing the organizational process focus process. Elaboration: This plan for performing the organizational process focus process, which is often called “the process im provement plan,” differs from the process action plans described in spec ific practices in this process area. The plan called for in this generic practice addresses the comprehensive planning for all of the s pecific practices in this process area, from the establishment of organizational process needs all the way through to the incorporation of pr ocess-related experiences into the organizational process assets.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 256 GP 2.3 Provide Resources Provide adequate resources for performing the organizational process focus process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Database management systems • Process improvement tools • Web page builders and browsers • Groupware • Quality-improvement tools (e.g., cause-and-effect diagrams, affinity diagrams, and Pareto charts) GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the organizational process focus process. Elaboration: Two groups are typically estab lished and assigned responsibility for process improvement: (1) a management steering committee for process improvement to provide senior management sponsorship, and (2) a process group to facilitate and manage the proces s improvement activities. GP 2.5 Train People Train the people performing or supporting the organizational process focus process as needed. Elaboration: Examples of training topics include the following: • CMMI and other process improvement reference models • Planning and managing process improvement • Tools, methods, and analysis techniques • Process modeling • Facilitation techniques • Change management",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 257GP 2.6 Manage Configurations Place designated work products of the organizational process focus process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Process improvement proposals • Organization’s approved process action plans • Training materials for deploying organizational process assets • Guidelines for deploying the organization’s set of standard processes on new projects • Plans for the organization’s process appraisals GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the organizational process focus process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Coordinating and collaborating on process improvement activities with process owners, those who are or will be performing the process, and support organizations (e.g., training staff and quality assurance representatives) • Establishing the organizational process needs and objectives • Appraising the organization’s processes • Implementing process action plans • Coordinating and collaborating on the execution of pilots to test selected improvements • Deploying organizational process assets and changes to organizational process assets • Communicating the plans, status, activities, and results related to planning, implementing, and deploying process improvements GP 2.8 Monitor and Control the Process Monitor and control the organizational process focus process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 258 Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of process improvement proposals submitted, accepted, or implemented • CMMI maturity level or capability level • Schedule for deployment of an organizational process asset • Percentage of projects using the current organization’s set of standard processes (or tailored version of same) • Issue trends associated with implementing the organization’s set of standard processes (i.e., number of issues identified and number closed) GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the organizational process focus process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Determining process improvement opportunities • Planning and coordinating process improvement activities • Deploying the organization’s set of standard processes on projects at their startup Examples of work products reviewed include the following: • Process improvement plans • Process action plans • Process deployment plans • Plans for the organization’s process appraisals GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the organizational process focus process with higher level management and resolve issues. Elaboration: These reviews are typically in the form of a briefing presented to the management steering committee by the process group and the process action teams.",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 259Examples of presentation topics include the following: • Status of improvements being developed by process action teams • Results of pilots • Results of deployments • Schedule status for achieving significant milestones (e.g., readiness for an appraisal, or progress toward achieving a targeted organizational maturity level or capability level profile) Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined organizational process focus process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the organizational process focus process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Criteria used for prioritizing candidate process improvements • Appraisal findings that address strengths and weaknesses of the organization's processes • Status of improvement activities against the schedule • Records of tailoring the organization’s set of standard processes and implementing them on identified projects",
        "CMMI for Development Version 1.2 Organizational Process Focus (OPF) 260 Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the organizational process focus process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the organizational process focus process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the organizational process focus process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the organizational process focus process.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 261ORGANIZATIONAL PROCESS PERFORMANCE A Process Management Process Area at Maturity Level 4 Purpose The purpose of Organizational Process Performance (OPP) is to establish and maintain a quantitativ e understanding of the performance of the organization’s set of standard processes in support of quality and process-performance objectives , and to provide the process- performance data, baselines, and m odels to quantitatively manage the organization’s projects. Introductory Notes Process performance is a measure of the actual results achieved by following a process. Process perform ance is characterized by process measures (e.g., effort, cycle time , and defect removal effectiveness) and product measures (e.g., reliab ility, defect densit y, capacity, response time, and cost). The common measures for the organi zation are composed of process and product measures t hat can be used to summarize the actual performance of processes in individua l projects in the organization. The organizational data for these meas ures are analyzed to establish a distribution and range of results, which characteri ze the expected performance of the process when us ed on any individual project in the organization. In this process area, the phras e “quality and process-performance objectives” covers objectives and requirements for product quality, service quality, and process performanc e. As indicated above, the term “process performance” includes qua lity; however, to emphasize the importance of quality, the phras e “quality and process-performance objectives” is used rather than just “process-performance objectives.” The expected process performance can be used in establishing the project’s quality and process-perfo rmance objectives and can be used as a baseline against which act ual project performance can be compared. This information is used to quantitatively manage the project. Each quantitatively managed pr oject, in turn, provides actual performance results that become a part of the baseline data for the organizational process assets. The associated process-performanc e models are used to represent past and current process performance and to predict future results of",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 262 the process. For example, the latent defects in the delivered product can be predicted using measurement s of defects identified during product verification activities. When the organization has measures , data, and analytical techniques for critical process, product, and servic e characteristics, it is able to do the following: • Determine whether processes are behaving consistently or have stable trends (i.e., are predictable) • Identify processes where the perfo rmance is within natural bounds that are consistent across process implementation teams • Establish criteria for identifyi ng whether a process or subprocess should be statistically managed, and determine pertinent measures and analytical techniques to be used in such management • Identify processes that show unusual (e.g., sporadic or unpredictable) behavior • Identify any aspects of the proc esses that can be improved in the organization’s set of standard processes • Identify the implem entation of a process which performs best Related Process Areas Refer to the Quantitative Project Management process area for more information about the use of pr ocess-performance baselines and models. Refer to the Measurement and Analysis process area for more information about specifying me asures and collecting and analyzing data. Specific Goal and Practice Summary SG 1 Establish Performance Baselines and Models SP 1.1 Select Processes SP 1.2 Establish Process-Performance Measures SP 1.3 Establish Quality and Process-Performance Objectives SP 1.4 Establish Process-Performance Baselines SP 1.5 Establish Process-Performance Models",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 263Specific Practices by Goal SG 1 Establish Performance Baselines and Models Baselines and models, which characterize the expected process performance of the organization's set of standard processes, are established and maintained. Prior to establishing process-perfo rmance baselines and models, it is necessary to determine which proc esses are suitable to be measured (the Select Processes specific practi ce), which measures are useful for determining process performance (the Establish Process-Performance Measures specific pr actice), and the qualit y and process-performance objectives for those processes (the Establish Quality and Process- Performance Objectives s pecific practice). These specific practices are often interrelated and may need to be per formed concurrently to select the appropriate processes, m easures, and quality and process- performance objectives. Often, the selection of one process, measure, or objective will constrain the selectio n of the others. For example, if a certain process is selected, the measures and object ives for that process may be constrained by the process itself. SP 1.1 Select Processes Select the processes or subprocesses in the organization's set of standard processes that are to be included in the organization's process-performance analyses. Refer to the Organizational Process Definition process area for more information about the structure of the organizational process assets. The organization’s set of standard pr ocesses consists of a set of standard processes that, in turn, are composed of subprocesses. Typically, it will not be possible, usef ul, or economically justifiable to apply statistical management tec hniques to all processes or subprocesses of the organization’s se t of standard processes. Selection of the processes and/or subproc esses is based on the needs and objectives of both the or ganization and projects. Examples of criteria which may be used for the selection of a process or subprocess for organizational analysis include the following: • The relationship of the subprocess to key business objectives • Current availability of valid historical data relevant to the subprocess • The current degree of variability of this data • Subprocess stability (e.g. stable performance in comparable instances) • The availability of corporate or commercial information that can be used to build predictive models",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 264 The existence of projec t data that indicates the process or subprocess has been or can be stabiliz ed is a useful criterion for selection of a process or subprocess. Typical Work Products 1. List of processes or subpr ocesses identified for process- performance analyses SP 1.2 Establish Process-Performance Measures Establish and maintain definitions of the measures that are to be included in the organization’s process-performance analyses. Refer to the Measurement and Analysis process area for more information about selecting measures. Typical Work Products 1. Definitions for the selected measures of process performance Subpractices 1. Determine which of the organi zation’s business objectives for quality and process performance need to be addressed by the measures. 2. Select measures that provi de appropriate insight into the organization’s quality and process performance. The Goal Question Metric paradigm is an approach that can be used to select measures that provide insight into the organization’s business objectives. Examples of criteria used to select measures include the following: • Relationship of the measures to the organization’s business objectives • Coverage that the measures provide over the entire life of the product or service • Visibility that the measures provide into the process performance • Availability of the measures • Extent to which the measures are objective • Frequency at which the observations of the measure can be collected • Extent to which the measures are cont rollable by changes to the process or subprocess • Extent to which the measures represent the users’ view of effective process performance 3. Incorporate the selected measur es into the organization’s set of common measures.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 265Refer to the Organizational Process Definition process area for more information about establishi ng organizational process assets. 4. Revise the set of measures as necessary. SP 1.3 Establish Quality and Process-Performance Objectives Establish and maintain quantitative objectives for quality and process performance for the organization. The organization’s quality and proce ss-performance objectives should have the following attributes: • Based on the organization’s business objectives • Based on the past perfo rmance of projects • Defined to gauge process performance in areas such as product quality, productivity, cycle time, or response time • Constrained by the inherent vari ability or natural bounds of the selected process or subprocess Typical Work Products 1. Organization's quality and pr ocess-performance objectives Subpractices 1. Review the organization’s busi ness objectives related to quality and process performance. Examples of business objectives include the following: • Achieve a development cycle of a specified duration for a specified release of a product • Achieve an average response time less than a specified duration for a specified version of a service • Deliver functionality of the product to a target percentage of estimated cost • Decrease the cost of maintenance of the products by a specified percent 2. Define the organization’s quantit ative objectives for quality and process performance. Objectives may be established for process or subprocess measurements (e.g., effort, cycle time, and defect removal effectiveness) as well as for product measurements (e.g., reliability and defect density) and service measurements (e.g., capacity and response times) where appropriate.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 266 Examples of quality and process-performance objectives include the following: • Achieve a specified productivity • Deliver work products with no more t han a specified number of latent defects • Shorten time to delivery to a specifi ed percentage of the process-performance baseline • Reduce the total lifecycle cost of new and existing products by a percentage • Deliver a percentage of the specified product functionality 3. Define the priorities of the or ganization’s objectives for quality and process performance. 4. Review, negotiate, and obtain co mmitment for the organization’s quality and process-performance object ives and their priorities from the relevant stakeholders. 5. Revise the organization’s quantit ative objectives for quality and process performance as necessary. Examples of when the organization’s quantitative objectives for quality and process performance may need to be revised include the following: • When the organization’s business objectives change • When the organization’s processes change • When actual quality and process perform ance differs significantly from the objectives SP 1.4 Establish Process-Performance Baselines Establish and maintain the organization's process-performance baselines. The organization’s process-performan ce baselines are a measurement of performance for the organization’s set of standard processes at various levels of detail, as appr opriate. The processes include the following: • Sequence of connected processes • Processes that cover the entire life of the project • Processes for developing individual work products There may be several process-perfo rmance baselines to characterize performance for subgroups of the organization.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 267Examples of criteria used to categorize subgroups include the following: • Product line • Line of business • Application domain • Complexity • Team size • Work product size • Process elements from the organization’s set of standard processes Allowable tailoring of the organization’s set of standard processes may significantly affect the comparability of the data for inclusion in process- performance baselines. The effects of tailoring should be considered in establishing baselines. Depending on the tailoring allowed, separate performance baselines may exis t for each type of tailoring. Refer to the Quantitative Project Management process area for more information about the use of process-performance baselines. Typical Work Products 1. Baseline data on the organi zation’s process performance Subpractices 1. Collect measurements from the organization’s projects. The process or subprocess in use when the measurement was taken is recorded to enable appropriate use later. Refer to the Measurement and Analysis process area for information about collecting and analyzing data. 2. Establish and maintain the or ganization’s process-performance baselines from the collect ed measurements and analyses. Refer to the Measurement and Analysis process area for information about establishing objectives for measurement and analysis, specifying the measur es and analyses to be performed, obtaining and analyzing measures , and reporting results. Process-performance baselines are derived by analyzing the collected measures to establish a distribution and range of results that characterize the expected performance for selected processes or subprocesses when used on any individual project in the organization. The measurements from stable subprocesses from projects should be used; other data may not be reliable.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 268 3. Review and get agreement with relevant stakeholders about the organization's process-performance baselines. 4. Make the organization's process- performance information available across the organization in the organization's measurement repository. The organization’s process-performance baselines are used by the projects to estimate the natural bounds for process performance. Refer to the Organizational Process Definition process area for more information about establishing the organization’s measurement repository. 5. Compare the organization’s proc ess-performance baselines to the associated objectives. 6. Revise the organization’s pr ocess-performance baselines as necessary. Examples of when the organization’s process-performance baselines may need to be revised include the following: • When the processes change • When the organization’s results change • When the organization’s needs change SP 1.5 Establish Process-Performance Models Establish and maintain the process-performance models for the organization’s set of standard processes. Process-performance models are used to estimate or predict the value of a process-performance measure fr om the values of other process, product, and service measurements. These process-performance models typically use process and product measurements collected throughout the life of the project to estimate progress toward achieving objectives that cannot be measured until later in the project’s life. The process-performance models are used as follows: • The organization uses them for estimating, analyzing, and predicting the process performance associated with the processes in the organization’s set of standard processes. • The organization uses them to assess the (potential) return on investment for process improvement activities. • Projects use them for estima ting, analyzing, and predicting the process performance for t heir defined processes. • Projects use them for selecting processes or subprocesses for use.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 269These measures and models are defined to provide insight into, and to provide the ability to predict, critic al process and product characteristics that are relevant to business value. Examples of areas of concern to projects in which models may be useful include the following: • Schedule and cost • Reliability • Defect identification and removal rates • Defect removal effectiveness • Latent defect estimation • Response time • Project progress • Combinations of these areas Examples of process-performance models include the following: • System dynamics models • Reliability growth models • Complexity models Refer to the Quantitative Project Management process area for more information about the use of process-performance models. Typical Work Products 1. Process-performance models Subpractices 1. Establish the process- performance models based on the organization’s set of standard processes and the organization’s process-performance baselines. 2. Calibrate the process-performance models based on the organization’s past results and current needs. 3. Review the process-performanc e models and get agreement with relevant stakeholders. 4. Support the projects’ use of the process-performance models. 5. Revise the process-perfo rmance models as necessary.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 270 Examples of when the process-performance models may need to be revised include the following: • When the processes change • When the organization’s results change • When the organization’s needs change Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the organizational process performance process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the organizational process performance process. Elaboration: This policy establishes organizational expectations for establishing and maintaining process-performance baseli nes for the organization’s set of standard processes.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 271GP 2.2 Plan the Process Establish and maintain the plan for performing the organizational process performance process. Elaboration: This plan for performing the organizational process performance process can be included in (or referenced by) the organization’s process improvement plan, which is described in the Organizational Process Focus process area, or it may be documented in a separate plan that describes only the plan for the organizational process performance process. GP 2.3 Provide Resources Provide adequate resources for performing the organizational process performance process, developing the work products, and providing the services of the process. Elaboration: Special expertise in st atistics and statistical process control may be needed to establish the process- performance baselines for the organization’s set of standard processes. Examples of other resources provided include the following tools: • Database management systems • System dynamics model • Process modeling tools • Statistical analysis packages • Problem-tracking packages GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the organizational process performance process. GP 2.5 Train People Train the people performing or supporting the organizational process performance process as needed.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 272 Elaboration: Examples of training topics include the following: • Process and process improvement modeling • Quantitative and statistical methods (e.g., estimating models, Pareto analysis, and control charts) GP 2.6 Manage Configurations Place designated work products of the organizational process performance process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Organization’s quality and process-performance objectives • Definitions of the selected measures of process performance • Baseline data on the organization’s process performance GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the organizational process performance process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing the organization’s quality and process-performance objectives and their priorities • Reviewing and resolving issues on the organization’s process-performance baselines • Reviewing and resolving issues on the organization’s process-performance models GP 2.8 Monitor and Control the Process Monitor and control the organizational process performance process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 273Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Trends in the organization’s process performance with respect to changes in work products and task attributes (e.g., size growth, effort, schedule, and quality) • Schedule for collecting and reviewing measures to be used for establishing a process-performance baseline GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the organizational process performance process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Establishing process-performance baselines and models Examples of work products reviewed include the following: • Process-performance plans • Organization’s quality and process-performance objectives • Definitions of the selected measures of process performance GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the organizational process performance process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined organizational process performance process.",
        "CMMI for Development Version 1.2 Organizational Process Performance (OPP) 274 GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the organizational process performance process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Process-performance baselines • Percent of measurement data that is rejected because of inconsistencies with the process-performance measurement definitions Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the organizational process performance process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the organizational process performance process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the organizational process performance process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the organizational process performance process.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 275ORGANIZATIONAL TRAINING A Process Management Process Area at Maturity Level 3 Purpose The purpose of Organizational Training (OT) is to develop the skills and knowledge of people so they can perform their roles effectively and efficiently. Introductory Notes Organizational Training includes training to support the organization’s strategic business objectives and to meet the tactical training needs that are common across projects and s upport groups. Specific training needs identified by individual proj ects and support groups are handled at the project and support group le vel and are outside the scope of Organizational Training. Project and support groups are responsible for identifying and addressing thei r specific training needs. Refer to the Project Planning proc ess area for more information about the specific training need s identified by projects. An organizational training program involves the following: • Identifying the training needed by the organization • Obtaining and providing training to address those needs • Establishing and mainta ining training capability • Establishing and maintaining training records • Assessing training effectiveness Effective training requires assessm ent of needs, planning, instructional design, and appropriate training m edia (e.g., workbooks and computer software), as well as a repository of training process data. As an organizational process, the main components of training include a managed training development pr ogram, documented plans, personnel with appropriate mastery of specif ic disciplines and other areas of knowledge, and mechanisms for meas uring the effectiveness of the training program. The identification of process trai ning needs is primarily based on the skills that are required to perform the organization’s set of standard processes.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 276 Refer to the Organizational Process Definition process area for more information about the organizati on’s set of standard processes. Certain skills may be effectively and efficiently imparted through vehicles other than in-class trai ning experiences (e.g., informal mentoring). Other skills require more formalized training vehicles, such as in a classroom, by Web-based tr aining, through guided self-study, or via a formalized on-the-job training pr ogram. The formal or informal training vehicles employed for eac h situation should be based on an assessment of the need for trai ning and the performance gap to be addressed. The term “training” us ed throughout this process area is used broadly to include all of these learning options. Success in training can be measured in terms of the availability of opportunities to acquire the sk ills and knowledge needed to perform new and ongoing enterprise activities. Skills and knowledge may be technica l, organizational, or contextual. Technical skills pertain to the ability to use the equipment, tools, materials, data, and processes requi red by a project or a process. Organizational skills pertain to behavior within and according to the employee’s organization structure, role and responsibilities, and general operating principles and methods. C ontextual skills are the self- management, communication, and interpersonal abilities needed to successfully perform in the organi zational and social context of the project and support groups. The phrase “project and support groups” is used frequently in the text of the process area description to indicate an organization-level perspective. Related Process Areas Refer to the Organizational Process Definition process area for more information about the organization’s process assets. Refer to the Project Planning proc ess area for more information about the specific training need s identified by projects. Refer to the Decision Analysis and Resolution process area for how to apply decision-making criteria when determining training approaches.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 277Specific Goal and Practice Summary SG 1 Establish an Organizational Training Capability SP 1.1 Establish the Strategic Training Needs SP 1.2 Determine Which Training Needs Are the Responsibility of the Organization SP 1.3 Establish an Organizational Training Tactical Plan SP 1.4 Establish Training Capability SG 2 Provide Necessary Training SP 2.1 Deliver Training SP 2.2 Establish Training Records SP 2.3 Assess Training Effectiveness Specific Practices by Goal SG 1 Establish an Organizational Training Capability A training capability, which supports the organization's management and technical roles, is established and maintained. The organization identifies the traini ng required to deve lop the skills and the knowledge necessary to perform enterprise activities. Once the needs are identified, a training program addressing those needs is developed. IPPD Addition Cross-functional training, leadership tr aining, interpersonal skills training, and training in the skills needed to integrate appropriate business and technical functions is needed by int egrated team members. The potentially wider range of requirements and participant backgrounds may require relevant stakeholders who were not involved in requirements development to take cross training in the disciplines involved in product design in order to commit to requirements with a full understanding of the range of requirements and their interrelationships. SP 1.1 Establish the Strategic Training Needs Establish and maintain the strategic training needs of the organization. Strategic training needs address long-term objectives to build a capability by filling significant knowledge gaps, introducing new technologies, or implementing majo r changes in behavior. Strategic planning typically looks two to five years into the future.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 278 Examples of sources of strategic training needs include the following: • Organization’s standard processes • Organization’s strategic business plan • Organization’s process improvement plan • Enterprise-level initiatives • Skill assessments • Risk analyses IPPD Addition IPPD requires leadership and interper sonal skills beyond those typically found in traditional development environments. Specific skills emphasized in an IPPD environment include the following: • The ability to integrate all appropriate business and technical functions and their processes • The ability to coordinate and collaborate with others Typical Work Products 1. Training needs 2. Assessment analysis Subpractices 1. Analyze the organization’s strategic business objectives and process improvement plan to i dentify potential future training needs. 2. Document the strategic trai ning needs of the organization. Examples of categories of training needs include (but are not limited to) the following: • Process analysis and documentation • Engineering (e.g., requirements analysis, design, testing, configuration management, and quality assurance) • Service delivery • Selection and management of suppliers • Management (e.g., estimating, tracking, and risk management) • Disaster recovery and continuity of operations 3. Determine the roles and skills needed to perform the organization’s set of standard processes.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 2794. Document the training needed to perform the roles in the organization’s set of standard processes. 5. Document the training needed to maintain the safe, secure and continued operation of the business. 6. Revise the organization’s strat egic needs and required training as necessary. SP 1.2 Determine Which Training Needs Are the Responsibility of the Organization Determine which training needs are the responsibility of the organization and which will be left to the individual project or support group. Refer to the Project Planning proc ess area for more information about project- and support-group-spec ific plans for training. In addition to strategic training needs, organizational training addresses training requirements that are common across projects and support groups. Projects and support groups have the primary responsibility for identifying and addressing their specific training needs. The organization’s training staff is onl y responsible for addressing common cross-project and support group training needs (e.g., training in work environments common to multiple proj ects). In some cases, however, the organization’s training staff may address additional training needs of projects and support groups, as negotiat ed with them, within the context of the training resources availa ble and the organization’s training priorities. Typical Work Products 1. Common project and support group training needs 2. Training commitments Subpractices 1. Analyze the training needs ident ified by the various projects and support groups. Analysis of project and support group needs is intended to identify common training needs that can be most efficiently addressed organization-wide. These needs-analysis activities are used to anticipate future training needs that are first visible at the project and support group level. 2. Negotiate with the various projects and support groups on how their specific training needs will be satisfied. The support provided by the organization’s training staff depends on the training resources available and the organization’s training priorities.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 280 Examples of training appropriately performed by the project or support group include the following: • Training in the application or service domain of the project • Training in the unique tools and methods used by the project or support group • Training in safety, security, and human factors 3. Document the commitments for providing training support to the projects and support groups. SP 1.3 Establish an Organizational Training Tactical Plan Establish and maintain an organizational training tactical plan. The organizational training tactical pl an is the plan to deliver the training that is the responsibility of t he organization and is necessary for individuals to perform their roles effectively. This plan addresses the near-term execution of training and is adjusted periodically in response to changes (e.g., in needs or res ources) and to evaluations of effectiveness. Typical Work Products 1. Organizational training tactical plan Subpractices 1. Establish plan content. Organizational training tactical plans typically contain the following: • Training needs • Training topics • Schedules based on training activities and their dependencies • Methods used for training • Requirements and quality standards for training materials • Training tasks, roles, and responsibilities • Required resources including tools, facilit ies, environments, staffing, and skills and knowledge 2. Establish commitments to the plan. Documented commitments by those responsible for implementing and supporting the plan are essential for the plan to be effective. 3. Revise plan and commitments as necessary.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 281SP 1.4 Establish Training Capability Establish and maintain training capability to address organizational training needs. Refer to the Decision Analysis and Resolution process area for how to apply decision-making criteria when selecting training approaches and developing training materials. Typical Work Products 1. Training materials and supporting artifacts Subpractices 1. Select the appropriate approaches to satisfy specific organizational training needs. Many factors may affect the selection of training approaches, including audience- specific knowledge, costs and schedule, work environment, and so on. Selection of an approach requires consideration of the means to provide skills and knowledge in the most effective way possible given the constraints. Examples of training approaches include the following: • Classroom training • Computer-aided instruction • Guided self-study • Formal apprenticeship and mentoring programs • Facilitated videos • Chalk talks • Brown-bag lunch seminars • Structured on-the-job training 2. Determine whether to develop training materials internally or acquire them externally. Determine the costs and benefits of internal training development or of obtaining training externally.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 282 Example criteria that can be used to determine the most effective mode of knowledge or skill acquisition include the following: • Performance objectives • Time available to prepare for project execution • Business objectives • Availability of in-house expertise • Availability of training from external sources Examples of external sources of training include the following: • Customer-provided training • Commercially available training courses • Academic programs • Professional conferences • Seminars 3. Develop or obtain training materials. Training may be provided by the project, by support groups, by the organization, or by an external organization. The organization’s training staff coordinates the acquisition and delivery of training regardless of its source. Examples of training materials include the following: • Courses • Computer-aided instruction • Videos 4. Develop or obtain qualified instructors. To ensure that internally provided training instructors have the necessary knowledge and training skills, criteria can be defined to identify, develop, and qualify them. In the case of externally provided training, the organization’s training staff can investigate how the training provider determines which instructors will deliver the training. This can also be a factor in selecting or continuing to use a specific training provider. 5. Describe the training in the organization's training curriculum.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 283Examples of the information provided in the training descriptions for each course include the following: • Topics covered in the training • Intended audience • Prerequisites and preparation for participating • Training objectives • Length of the training • Lesson plans • Completion criteria for the course • Criteria for granting training waivers 6. Revise the training materials and supporting artifacts as necessary. Examples of situations in which the training materials and supporting artifacts may need to be revised include the following: • Training needs change (e.g., when new tec hnology associated with the training topic is available) • An evaluation of the training identifies the need for change (e.g., evaluations of training effectiveness surveys, traini ng program performance assessments, or instructor evaluation forms) SG 2 Provide Necessary Training Training necessary for individuals to perform their roles effectively is provided. In selecting people to be trained, the following should be taken into consideration: • Background of the target population of training participants • Prerequisite background to receive training • Skills and abilities needed by people to perform their roles • Need for cross-discipline technical management training for all disciplines, including project management • Need for managers to have training in appropriate organizational processes • Need for training in the basic princi ples of all appropriate disciplines to support personnel in qualit y management, configuration management, and other relat ed support functions • Need to provide competency devel opment for critical functional areas • Need to maintain the competencie s and qualifications of personnel to operate and maintain work env ironments common to multiple projects",
        "CMMI for Development Version 1.2 Organizational Training (OT) 284 SP 2.1 Deliver Training Deliver the training following the organizational training tactical plan. Typical Work Products 1. Delivered training course Subpractices 1. Select the people who will receive the training necessary to perform their roles effectively. Training is intended to impart knowledge and skills to people performing various roles within the organization. Some people already possess the knowledge and skills required to perform well in their designated roles. Training can be waived for these people, but care should be taken that training waivers are not abused. 2. Schedule the training, including an y resources, as necessary (e.g., facilities and instructors). Training should be planned and scheduled. Training is provided that has a direct bearing on the expectations of work performance. Therefore, optimal training occurs in a timely manner with regard to imminent job-performance expectations. These expectations often include the following: • Training in the use of specialized tools • Training in procedures that are new to the individual who will perform them 3. Conduct the training. Experienced instructors should perform training. When possible, training is conducted in settings that closely resemble actual performance conditions and includes activities to simulate actual work situations. This approach includes integration of tools, methods, and procedures for competency development. Training is tied to work responsibilities so that on-the-job activities or other outside experiences will reinforce the training within a reasonable time after the training. 4. Track the delivery of training against the plan. SP 2.2 Establish Training Records Establish and maintain records of the organizational training. Refer to the Project Monitoring and Control process area for information about how project or support group training records are maintained. The scope of this practice is for the training performed at the organizational level. Establishment and maintenance of training records for project- or support- group-sponsored training is the responsibility of each individual project or support group.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 285Typical Work Products 1. Training records 2. Training updates to the organizational repository Subpractices 1. Keep records of all students w ho successfully complete each training course or other approved tr aining activity as well as those who are unsuccessful. 2. Keep records of all staff who have been waived from specific training. The rationale for granting a waiver should be documented, and both the manager responsible and the manager of the excepted individual should approve the waiver for organizational training. 3. Keep records of all students w ho successfully complete their designated required training. 4. Make training records available to the appropriate people for consideration in assignments. Training records may be part of a skills matrix developed by the training organization to provide a summary of the experience and education of people, as well as training sponsored by the organization. SP 2.3 Assess Training Effectiveness Assess the effectiveness of the organization’s training program. A process should exist to determine the effectiveness of training (i.e., how well the training is meet ing the organization’s needs). Examples of methods used to assess training effectiveness include the following: • Testing in the training context • Post-training surveys of training participants • Surveys of managers’ satisfaction with post-training effects • Assessment mechanisms embedded in courseware Measures may be taken to assess the benefit of the training against both the project’s and organization’s objectives. Particular attention should be paid to the need for various training methods, such as training teams as integral work units. When used, performance objectives should be shared with course participants, and should be unambiguous, observable, and verifiab le. The results of the training- effectiveness assessment should be used to revise training materials as described in the Establish Traini ng Capability specific practice.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 286 Typical Work Products 1. Training-effectiveness surveys 2. Training program performance assessments 3. Instructor evaluation forms 4. Training examinations Subpractices 1. Assess in-progress or comple ted projects to determine whether staff knowledge is adequate for performing project tasks. 2. Provide a mechanism for assessing the effectiveness of each training course with respect to established organizational, project, or individual learning (o r performance) objectives. 3. Obtain student evaluations of how well training activities met their needs. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the organizational training process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 287 Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the organizational training process. Elaboration: This policy establishes organizational expectations for identifying the strategic training needs of the organi zation, and providing that training. GP 2.2 Plan the Process Establish and maintain the plan for performing the organizational training process. Elaboration: This plan for performing the organiza tional training process differs from the tactical plan for organizational training described in a specific practice in this process area. The pl an called for in this generic practice would address the comprehensive pl anning for all of the specific practices in this process area, fr om the establishment of strategic training needs all the way through to the assessment of the effectiveness of the organizational tr aining effort. In contrast, the organizational training tactical plan ca lled for in the specific practice would address the periodic planning for the delivery of individual training offerings. GP 2.3 Provide Resources Provide adequate resources for performing the organizational training process, developing the work products, and providing the services of the process.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 288 Elaboration: Examples of people (full or part time, internal or external), and skills needed include the following: • Subject-matter experts • Curriculum designers • Instructional designers • Instructors • Training administrators Special facilities may be required for training. When necessary, the facilities required for the activities in the Organizational Training process area are developed or purchased. Examples of other resources provided include the following tools: • Instruments for analyzing training needs • Workstations to be used for training • Instructional design tools • Packages for developing presentation materials GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the organizational training process. GP 2.5 Train People Train the people performing or supporting the organizational training process as needed. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.5 and the Organizational Training process area.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 289Examples of training topics include the following: • Knowledge and skills needs analysis • Instructional design • Instructional techniques (e.g., train the trainer) • Refresher training on subject matter GP 2.6 Manage Configurations Place designated work products of the organizational training process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Organizational training tactical plan • Training records • Training materials and supporting artifacts • Instructor evaluation forms GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the organizational training process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing a collaborative environment for discussion of training needs and training effectiveness to ensure that the organization’s training needs are met • Identifying training needs • Reviewing the organizational training tactical plan • Assessing training effectiveness GP 2.8 Monitor and Control the Process Monitor and control the organizational training process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 290 Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of training courses delivered (e.g., planned versus actual) • Post-training evaluation ratings • Training program quality survey ratings • Schedule for delivery of training • Schedule for development of a course GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the organizational training process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Identifying training needs and making training available • Providing necessary training Examples of work products reviewed include the following: • Organizational training tactical plan • Training materials and supporting artifacts • Instructor evaluation forms GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the organizational training process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 291GP 3.1 Establish a Defined Process Establish and maintain the description of a defined organizational training process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the organizational training process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Results of training effectiveness surveys • Training program performance assessment results • Course evaluations • Training requirements from an advisory group Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the organizational training process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the organizational training process to achieve the established quantitative quality and process- performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process.",
        "CMMI for Development Version 1.2 Organizational Training (OT) 292 Continuous Only GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the organizational training process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the organizational training process.",
        "CMMI for Development Version 1.2 Product Integration (PI) 293PRODUCT INTEGRATION An Engineering Process Area at Maturity Level 3 Purpose The purpose of Product Integration (PI) is to assemble the product from the product components, ensure that the pro duct, as integrated, functions properly, and deliver the product. Introductory Notes This process area addresses the inte gration of product components into more complex product components or into complete products. The scope of this process area is to achieve complete product integration through progressive assembly of product components, in one stage or in incremental stages, ac cording to a defined integration sequence and procedures. Throughout the process areas, where we use the terms product and product component, their intended meanings also encompass services and their components. A critical aspect of product integr ation is the management of internal and external interfaces of t he products and product components to ensure compatibility among the interfaces. Attent ion should be paid to interface management throughout the project. Product integration is more than ju st a one-time assembly of the product components at the conclu sion of design and fabrication. Product integration can be conducted incrementa lly, using an iterative process of assembling product co mponents, evaluating them, and then assembling more product components. This process may begin with analysis and simulations (e.g., thr eads, rapid protot ypes, virtual prototypes, and physical prototypes ) and steadily progress through increasingly more realistic increm ental functionality until the final product is achieved. In each successive build, prototypes (virtual, rapid, or physical) are constructed, eval uated, improved, and reconstructed based on knowledge gained in the eval uation process. The degree of virtual versus physical prototyping required depends on the functionality of the design tools, the complexity of the product, and its associated risk. There is a high probability t hat the product, integrated in this manner, will pass product verification and validation. For some products and services, the last integrati on phase will occur when they are deployed at the intended operational site.",
        "CMMI for Development Version 1.2 Product Integration (PI) 294 Related Process Areas Refer to the Requirements Development process area for more information about identifyi ng interface requirements. Refer to the Technical Solution proc ess area for more information about defining the interfaces and the in tegration environment (when the integration environment needs to be developed). Refer to the Verification process area for more information about verifying the interfaces, the integration environment, and the progressively assembled product components. Refer to the Validation process area for more information about performing validation of the pro duct components and the integrated product. Refer to the Risk Management process area for more information about identifying risks and the use of protot ypes in risk mitigation for both interface compatibility and prod uct component integration. Refer to the Decision Analysis and Resolution process area for more information about using a formal ev aluation process for selecting the appropriate integration sequence and procedures and for deciding whether the integration environmen t should be acquired or developed. Refer to the Configuration Management process area for more information about managing changes to interface definitions and about the distribution of information. Refer to the Supplier Agreement Management process area for more information about acquiring prod uct components or parts of the integration environment. Specific Goal and Practice Summary SG 1 Prepare for Product Integration SP 1.1 Determine Integration Sequence SP 1.2 Establish the Product Integration Environment SP 1.3 Establish Product Integration Procedures and Criteria SG 2 Ensure Interface Compatibility SP 2.1 Review Interface Descriptions for Completeness SP 2.2 Manage Interfaces SG 3 Assemble Product Components and Deliver the Product SP 3.1 Confirm Readiness of Product Components for Integration SP 3.2 Assemble Product Components SP 3.3 Evaluate Assembled Product Components SP 3.4 Package and Deliver the Product or Product Component",
        "CMMI for Development Version 1.2 Product Integration (PI) 295Specific Practices by Goal SG 1 Prepare for Product Integration Preparation for product integration is conducted. Preparing for integration of produc t components involves establishing and maintaining an integration sequence, the environment for performing the integration, and int egration procedures . The specific practices of the Prepare for Product Integration specific goal build on each other in the following way. The first specific practice determines the sequence for product and produc t component integration. The second determines the environment t hat will be used to carry out the product and product component in tegration. The third develops procedures and criteria for product and product component integration. Preparation for integration starts early in the project and the integration sequence is developed concu rrently with the practices in the Technical Solution process area. SP 1.1 Determine Integration Sequence Determine the product component integration sequence. The product components that are inte grated may include those that are a part of the product to be delivered along with test equipment, test software, or other integration item s such as fixtures. Once you have analyzed alternative test and assemb ly integration sequences, select the best integration sequence. The product integration sequence can provide for incremental assembly and evaluation of product components that provide a problem-free foundation for incorporation of other product components as they become available, or for prototy pes of high-risk product components. The integration sequence should be harmonized with the selection of solutions and the design of produc t and product components in the Technical Solution process area. Refer to the Decision Analysis and Resolution process area for more information about using a formal evaluation process to select the appropriate product in tegration sequence. Refer to the Risk Management process area for more information about identifying and handling risks associat ed with the integration sequence. Refer to the Supplier Agreement Management process area for more information about transitioning ac quired product components and the need for handling those product comp onents in the product integration sequence.",
        "CMMI for Development Version 1.2 Product Integration (PI) 296 Typical Work Products 1. Product integration sequence 2. Rationale for selecting or re jecting integration sequences Subpractices 1. Identify the product co mponents to be integrated. 2. Identify the verifications to be performed during the integration of the product components. 3. Identify alternative product component integration sequences. This can include defining the specific tools and test equipment to support the product integration. 4. Select the best integration sequence. 5. Periodically review the product integration sequence and revise as needed. Assess the product integration sequence to ensure that variations in production and delivery schedules have not had an adverse impact on the sequence or compromised the factors on which earlier decisions were made. 6. Record the rationale for decisions made and deferred. SP 1.2 Establish the Product Integration Environment Establish and maintain the environment needed to support the integration of the product components. Refer to the Technical Solution proc ess area for more information about make-or-buy decisions. The environment for product integr ation can either be acquired or developed. To establish an environm ent, requirements for the purchase or development of equipment, software, or other resources will need to be developed. These requirements are gathered when implementing the processes associated with the Requirements Development process area. The product integration envir onment may include the reuse of existing organizational resources. T he decision to acquire or develop the product integration environment is addressed in the processes associated with the Technical Solution process area. The environment required at each st ep of the product integration process may include test equipment, si mulators (taking the place of unavailable product components), pieces of real equipment, and recording devices. Typical Work Products 1. Verified environment for product integration",
        "CMMI for Development Version 1.2 Product Integration (PI) 2972. Support documentation for the pr oduct integration environment Subpractices 1. Identify the requirements for t he product integration environment. 2. Identify verification criteria and procedures for the product integration environment. 3. Decide whether to make or buy the needed product integration environment. Refer to the Supplier Agreement Management process area for more information about acquiri ng parts of the integration environment. 4. Develop an integration envir onment if a suitable environment cannot be acquired. For unprecedented, complex projects, the product integration environment can be a major development. As such, it would involve project planning, requirements development, technical solutions, verification, validation, and risk management. 5. Maintain the product integr ation environment throughout the project. 6. Dispose of those portions of t he environment that are no longer useful. SP 1.3 Establish Product Integration Procedures and Criteria Establish and maintain procedures and criteria for integration of the product components. Procedures for the integration of the product components can include such things as the number of incr emental iterations to be performed and details of the expected tests and other evaluations to be carried out at each stage. Criteria can indicate the readiness of a product component for integration or it s acceptability. Procedures and criteria for product integration address the following: • Level of testing for build components • Verification of interfaces • Thresholds of performance deviation • Derived requirements for the asse mbly and its external interfaces • Allowable substitutions of components • Testing environment parameters • Limits on cost of testing",
        "CMMI for Development Version 1.2 Product Integration (PI) 298 • Quality/cost tradeoffs for integration operations • Probability of proper functioning • Delivery rate and its variation • Lead time from order to delivery • Personnel availability • Availability of the integr ation facility/line/environment Criteria can be defined for how the product components are to be verified and the functions they are expected to have. Criteria can be defined for how the assembled pro duct components and final integrated product are to be validated and delivered. Criteria may also constrain the degree of simulation permitted for a product component to pass a test, or may constrain the environment to be used for the integration test. Pertinent parts of the schedule an d criteria for assembly should be shared with suppliers of work prod ucts to reduce the occurrence of delays and component failure Refer to the Supplier Agreement Management process area for more information about commun icating with suppliers Typical Work Products 1. Product integration procedures 2. Product integration criteria Subpractices 1. Establish and maintain produc t integration procedures for the product components. 2. Establish and maintain criteria for product component integration and evaluation. 3. Establish and maintain criteria for validation and delivery of the integrated product. SG 2 Ensure Interface Compatibility The product component interfaces, both internal and external, are compatible. Many product integration problems arise from unknown or uncontrolled aspects of both internal and exter nal interfaces. Effective management of product component interface r equirements, spec ifications, and designs helps ensure t hat implemented interfaces will be complete and compatible.",
        "CMMI for Development Version 1.2 Product Integration (PI) 299SP 2.1 Review Interface Descriptions for Completeness Review interface descriptions for coverage and completeness. The interfaces should include, in addition to product component interfaces, all the interfaces with the product integration environment. Typical Work Products 1. Categories of interfaces 2. List of interfaces per category 3. Mapping of the interfaces to the product components and the product integration environment Subpractices 1. Review interface data for completeness and ensure complete coverage of all interfaces. Consider all the product components and prepare a relationship table. Interfaces are usually classified in three main classes: environmental, physical, and functional. Typical categories for these classes include the following: mechanical, fluid, sound, electrical, climatic, electromagnetic, thermal, message, and the human-machine or human interface. Examples of interfaces (e.g., for mechanical or electronic components) that may be classified within these three classes include the following: • Mechanical interfaces (e.g., weight and size, center of gravity, clearance of parts in operation, space required for maintenance, fixed links, mobile links, and shocks and vibrations received from the bearing structure) • Noise interfaces (e.g., noise transmitted by the structure, noise transmitted in the air, and acoustics) • Climatic interfaces (e.g., temperature, humidity, pressure, and salinity) • Thermal interfaces (e.g., heat dissipation, transmission of heat to the bearing structure, and air conditioning characteristics) • Fluid interfaces (e.g., fresh water inlet/outlet, seawater inlet/outlet for a naval/coastal product, air conditioning, comp ressed air, nitrogen, fuel, lubricating oil, and exhaust gas outlet) • Electrical interfaces (e.g., power supply consumption by network with transients and peak values; nonsensitive control signal for power supply and communications; sensitive signal [e.g., analog links]; disturbing signal [e.g., microwave]; and grounding signal to comply with the TEMPEST standard) • Electromagnetic interfaces (e.g., magnetic field, radio and radar links, optical band link wave guides, and coaxial and optical fibers) • Human-machine interface (e.g., audio or voice synthesis, audio or voice recognition, display [analog dial, televisi on screen, or liquid-crystal display, indicators' light-emitting diodes], and manual controls [pedal, joystick, ball, keys, push buttons, or touch screen]) • Message interfaces (e.g., origination, destination, stimulus, protocols, and data characteristics)",
        "CMMI for Development Version 1.2 Product Integration (PI) 300 2. Ensure that product compone nts and interfaces are marked to ensure easy and correct connecti on to the joining product component. 3. Periodically review the ad equacy of interface descriptions. Once established, the interface descriptions must be periodically reviewed to ensure there is no deviation between the existing descriptions and the products being developed, processed, produced, or bought. The interface descriptions for product components should be reviewed with relevant stakeholders to avoid misinterpretations, reduce delays, and prevent the development of interfaces that do not work properly. SP 2.2 Manage Interfaces Manage internal and external interface definitions, designs, and changes for products and product components. Interface requirements drive the development of t he interfaces necessary to integrate produc t components. Managing product and product component interfaces starts very early in the development of the product. The definitions and designs for interfaces affect not only the product components and external sy stems, but can also affect the verification and validation environments. Refer to the Requirements Development process area for more information about requirements for interfaces. Refer to the Technical Solution proc ess area for more information about design of interfaces bet ween product components. Refer to the Requirements Management process area for more information about managing the changes to the interface requirements. Refer to the Configuration Management process area for more information about dist ributing changes to the interface descriptions (specifications) so that everyone can know the current state of the interfaces. Management of the interfaces includes maintenance of the consistency of the interfaces throughout the lif e of the product, and resolution of conflict, noncompliance, and c hange issues. The management of interfaces between produ cts acquired from supp liers and other products or product components is critical for success of the project. Refer to the Supplier Agreement Management process area for more information about managing suppliers.",
        "CMMI for Development Version 1.2 Product Integration (PI) 301The interfaces should include, in addition to product component interfaces, all the interfaces with the environment as well as other environments for verification, va lidation, operations, and support. The interface changes are docum ented, maintained, and readily accessible. Typical Work Products 1. Table of relationships am ong the product components and the external environment (e.g., main power supply, fastening product, and computer bus system) 2. Table of relationships among the different product components 3. List of agreed-to interfaces defined for each pair of product components, when applicable 4. Reports from the interface control working group meetings 5. Action items for updating interfaces 6. Application progr am interface (API) 7. Updated interface description or agreement Subpractices 1. Ensure the compatibility of the in terfaces throughout the life of the product. 2. Resolve conflict, noncom pliance, and change issues. 3. Maintain a repository for inte rface data accessible to project participants. A common accessible repository for interface data provides a mechanism to ensure that everyone knows where the current interface data resides and can access it for use. SG 3 Assemble Product Components and Deliver the Product Verified product components are assembled and the integrated, verified, and validated product is delivered. Integration of product components pr oceeds according to the product integration sequence and available procedures. Before integration, each product component should be conf irmed to be compliant with its interface requirements. Product com ponents are assembled into larger, more complex product components. These assembled product components are checked for correct interoperation. This process continues until product int egration is complete. If, during this process, problems are identified, the problem should be documented and a corrective action process initiated.",
        "CMMI for Development Version 1.2 Product Integration (PI) 302 Ensure that the assembly of the product components into larger and more complex product components is conducted according to the product integration sequence and av ailable procedures. The timely receipt of needed product components and the involvement of the right people contribute to the successfu l integration of the product components that compose the product. SP 3.1 Confirm Readiness of Product Components for Integration Confirm, prior to assembly, that each product component required to assemble the product has been properly identified, functions according to its description, and that the product component interfaces comply with the interface descriptions. Refer to the Verification process area for more information about verifying product components. Refer to the Technical Solution proc ess area for more information about unit test of product components. The purpose of this specific practi ce is to ensure that the properly identified product component that m eets its description can actually be assembled according to the product integration sequence and available procedures. The product components are checked for quantity, obvious damage, and consist ency between the product component and interface descriptions. Those conducting product integration are ultimately responsible for checking to make sure everything is proper with the product components before assembly. Typical Work Products 1. Acceptance documents for t he received product components 2. Delivery receipts 3. Checked packing lists 4. Exception reports 5. Waivers Subpractices 1. Track the status of all pr oduct components as soon as they become available for integration. 2. Ensure that product components are delivered to the product integration environment in accor dance with the product integration sequence and available procedures. 3. Confirm the receipt of each pr operly identified product component.",
        "CMMI for Development Version 1.2 Product Integration (PI) 3034. Ensure that each received product component meets its description. 5. Check the configuration status against the expected configuration. 6. Perform a pre-check (e.g., by a visual inspecti on and using basic measures) of all the physical in terfaces before connecting product components together. SP 3.2 Assemble Product Components Assemble product components according to the product integration sequence and available procedures. The assembly activities of this specific practice and the evaluation activities of the next specific practi ce are conducted iteratively, from the initial product components, through th e interim assemblies of product components, to the product as a whole. Typical Work Products 1. Assembled product or product components Subpractices 1. Ensure the readiness of the product integration environment. 2. Ensure that the assembly sequence is properly performed. Record all appropriate information (e.g., configuration status, serial numbers of the product components, types, and calibration date of the meters). 3. Revise the product integration sequence and available procedures as appropriate. SP 3.3 Evaluate Assembled Product Components Evaluate assembled product components for interface compatibility. Refer to the Verification process area for more information about verifying assembled product components. Refer to the Validation process area for more information about validating assembled product components. This evaluation involves examini ng and testing assembled product components for performance, suitab ility, or readiness using the available procedures and environment. It is performed as appropriate for different stages of assembly of product components as identified in the product integration sequence and available procedures. The product integration sequence and av ailable procedures may define a more refined integration and evaluation sequence than might be envisioned just by examining the pro duct architecture. For example, if",
        "CMMI for Development Version 1.2 Product Integration (PI) 304 an assembly of product components is composed of four less complex product components, the integration sequence w ill not necessarily call for the simultaneous integration and ev aluation of the f our units as one. Rather, the four less complex units may be integrated progressively, one at a time, with an evaluation after each assembly operation prior to realizing the more complex pr oduct component that matched the specification in the product archit ecture. Alternatively, the product integration sequence and availabl e procedures coul d have determined that only a final evaluation was the best one to perform. Typical Work Products 1. Exception reports 2. Interface evaluation reports 3. Product integration summary reports Subpractices 1. Conduct the evaluation of assembled product components following the product integr ation sequence and available procedures. 2. Record the evaluation results. Example results include the following: • Any adaptation required to the integration procedure • Any change to the product configurat ion (spare parts, new release) • Evaluation procedure deviations SP 3.4 Package and Deliver the Product or Product Component Package the assembled product or product component and deliver it to the appropriate customer. Refer to the Verification process area for more information about verifying the product or an assemb ly of product components before packaging. Refer to the Validation process area for more information about validating the product or an asse mbly of product components before packaging.",
        "CMMI for Development Version 1.2 Product Integration (PI) 305The packaging requirements for some products can be addressed in their specifications and verification cr iteria. This is es pecially important when items are stored and transported by the customer. In such cases, there may be a spectrum of env ironmental and stress conditions specified for the package. In other circumstances, factors such as the following may become important: • Economy and ease of transporta tion (e.g., containerization) • Accountability (e.g., shrink wrapping) • Ease and safety of unpacking (e .g., sharp edges, strength of binding methods, childproofing, environmental friendliness of packing material, and weight) The adjustment required to fit product components together in the factory could be different from the one required to fit product components together when installed on the operational site. In that case, the product’s logbook for the customer should be used to record such specific parameters. Typical Work Products 1. Packaged product or product components 2. Delivery documentation Subpractices 1. Review the requirements, design, product, verification results, and documentation to ensure that iss ues affecting the packaging and delivery of the product are identified and resolved. 2. Use effective methods to pa ckage and deliver the assembled product. For Software Engineering Examples of software packaging and delivery methods include the following: • Magnetic tape • Diskettes • Hardcopy documents • Compact disks • Other electronic distribution such as the Internet 3. Satisfy the applicable requirem ents and standards for packaging and delivering the product. Examples of requirements and standards include those for safety, the environment, security, transportability, and disposal.",
        "CMMI for Development Version 1.2 Product Integration (PI) 306 For Software Engineering Examples of requirements and standards for packaging and delivering software include the following: • Type of storage and delivery media • Custodians of the master and backup copies • Required documentation • Copyrights • License provisions • Security of the software 4. Prepare the operational site for installation of the product. Preparing the operational site may be the responsibility of the customer or end users. 5. Deliver the product and related documentation and confirm receipt. 6. Install the product at the oper ational site and confirm correct operation. Installing the product may be the responsibility of the customer or the end users. In some circumstances, very little may need to be done to confirm correct operation. In other circumstances, final verification of the integrated product occurs at the operational site. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the product integration process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process.",
        "CMMI for Development Version 1.2 Product Integration (PI) 307 Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the product integration process. Elaboration: This policy establishes organiza tional expectations for developing product integration sequences , procedures, and an environment; ensuring interface compatibilit y among product components; assembling the product component s; and delivering the product and product components. GP 2.2 Plan the Process Establish and maintain the plan for performing the product integration process. Elaboration: This plan for performing the produc t integration process addresses the comprehensive planning for all of the s pecific practices in this process area, from the preparation for produc t integration all the way through to the delivery of the final product. GP 2.3 Provide Resources Provide adequate resources for performing the product integration process, developing the work products, and providing the services of the process. Elaboration: Product component interface coor dination may be accomplished with an Interface Control Working Group consisting of peopl e who represent external and internal interfaces. Su ch groups can be used to elicit needs for interface requirements development.",
        "CMMI for Development Version 1.2 Product Integration (PI) 308 Special facilities may be require d for assembling and delivering the product. When necessary, the facilities required for the activities in the Product Integration process ar ea are developed or purchased. Examples of other resources provided include the following tools: • Prototyping tools • Analysis tools • Simulation tools • Interface management tools • Assembly tools (e.g., compilers, make files, joining tools, jigs, and fixtures) GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the product integration process. GP 2.5 Train People Train the people performing or supporting the product integration process as needed. Elaboration: Examples of training topics include the following: • Application domain • Product integration procedures and criteria • Organization’s facilities for integration and assembly • Assembly methods • Packaging standards GP 2.6 Manage Configurations Place designated work products of the product integration process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Product Integration (PI) 309Elaboration: Examples of work products placed under control include the following: • Acceptance documents for the received product components • Evaluated assembled product and product components • Product integration sequence • Product integration procedures and criteria • Updated interface description or agreement GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the product integration process as planned. Elaboration: Select relevant stakeholders from customers, end users, developers, producers, testers, suppliers, ma rketers, maintainers, disposal personnel, and others who may be affected by, or may affect, the product as well as the process. Examples of activities for stakeholder involvement include the following: • Reviewing interface descriptions for completeness • Establishing the product integration sequence • Establishing the product integration procedures and criteria • Assembling and delivering the product and product components • Communicating the results after evaluation • Communicating new, effective product integration processes to give affected people the opportunity to improve their performance GP 2.8 Monitor and Control the Process Monitor and control the product integration process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Product Integration (PI) 310 Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Product component integration profile (e.g., product component assemblies planned and performed, and number of exceptions found) • Integration evaluation problem report trends (e.g., number written and number closed) • Integration evaluation problem report aging (i.e., how long each problem report has been open) • Schedule for conduct of specific integration activities GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the product integration process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Establishing and maintaining a product integration sequence • Ensuring interface compatibility • Assembling product components and delivering the product Examples of work products reviewed include the following: • Product integration sequence • Product integration procedures and criteria • Acceptance documents for the received product components • Assembled product and product components GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the product integration process with higher level management and resolve issues.",
        "CMMI for Development Version 1.2 Product Integration (PI) 311Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined product integration process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the product integration process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Records of the receipt of product components, exception reports, confirmation of configuration status, and results of readiness checking • Percent of total development effort spent in product integration (actual to date plus estimate to complete) • Defects found in the product and test environment during product integration • Problem reports resulting from product integration Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the product integration process, which address quality and process performance, based on customer needs and business objectives.",
        "CMMI for Development Version 1.2 Product Integration (PI) 312 Continuous Only GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the product integration process to achieve the established quantitative quality and process- performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the product integration process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the product integration process.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 313PROJECT MONITORING AND CONTROL A Project Management Process Area at Maturity Level 2 Purpose The purpose of Project Monitoring an d Control (PMC) is to provide an understanding of the project’s progr ess so that appropriate corrective actions can be taken when the project’s performance deviates significantly from the plan. Introductory Notes A project’s documented plan is the basis for monitoring activities, communicating status, and taking co rrective action. Progress is primarily determined by comparing actual work product and task attributes, effort, cost, and sc hedule to the plan at prescribed milestones or control levels with in the project schedule or work breakdown structure (WBS). Appropr iate visibility enables timely corrective action to be taken when performance deviates significantly from the plan. A deviation is signifi cant if, when left unresolved, it precludes the project from meeting its objectives. The term “project plan” is used throug hout these practices to refer to the overall plan for controlling the project. When actual status deviates signifi cantly from the expected values, corrective actions are taken as appr opriate. These actions may require replanning, which may include revising the original plan, establishing new agreements, or including additional mitigation activities within the current plan. Related Process Areas Refer to the Project Planning proc ess area for more information about the project plan, including how it specifies the appropriate level of project monitoring, the measures used to monitor progress, and known risks. Refer to the Measurement and Anal ysis process area for information about the process of measuring, anal yzing, and recording information.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 314 Specific Goal and Practice Summary SG 1 Monitor Project Against Plan SP 1.1 Monitor Project Planning Parameters SP 1.2 Monitor Commitments SP 1.3 Monitor Project Risks SP 1.4 Monitor Data Management SP 1.5 Monitor Stakeholder Involvement SP 1.6 Conduct Progress Reviews SP 1.7 Conduct Milestone Reviews SG 2 Manage Corrective Action to Closure SP 2.1 Analyze Issues SP 2.2 Take Corrective Action SP 2.3 Manage Corrective Action Specific Practices by Goal SG 1 Monitor Project Against Plan Actual performance and progress of the project are monitored against the project plan. SP 1.1 Monitor Project Planning Parameters Monitor the actual values of the project planning parameters against the project plan. Project planning parameters constitute typical indicators of project progress and performance and include a ttributes of work products and tasks, cost, effort, and schedule. A ttributes of the work products and tasks include such items as size, complexity, we ight, form, fit, or function. Monitoring typically involves measur ing the actual va lues of project planning parameters, comparing actual values to the estimates in the plan, and identifying significant devia tions. Recording actual values of the project planning parameters includes recording associated contextual information to help under stand the measures. An analysis of the impact that significant devia tions have on determining what corrective actions to take is handled in the second specific goal and its specific practices in this process area. Typical Work Products 1. Records of project performance 2. Records of significant deviations Subpractices 1. Monitor progress against the schedule.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 315Progress monitoring typically includes the following: • Periodically measuring the actual co mpletion of activities and milestones • Comparing actual completion of acti vities and milestones against the schedule documented in the project plan • Identifying significant deviations from the schedule estimates in the project plan 2. Monitor the project's cost and expended effort. Effort and cost monitoring typically includes the following: • Periodically measuring the actual effort and cost expended and staff assigned • Comparing actual effort, costs, staffing, and training to the estimates and budget documented in the project plan • Identifying significant deviations from the budget in the project plan 3. Monitor the attributes of the work products and tasks. Refer to the Project Planning process area for information about the attributes of work products and tasks. Monitoring the attributes of the work products and tasks typically includes the following: • Periodically measuring the actual attributes of the work products and tasks, such as size or complexity (and the changes to the attributes) • Comparing the actual attributes of t he work products and tasks (and the changes to the attributes) to the estimates documented in the project plan • Identifying significant deviations from the estimates in the project plan 4. Monitor resources provided and used. Refer to the Project Planning process area for information about planned resources. Examples of resources include the following: • Physical facilities • Computers, peripherals, and software used in design, manufacturing, testing, and operation • Networks • Security environment • Project staff • Processes 5. Monitor the knowledge and skills of project personnel. Refer to the Project Planning process area for information about planning for knowledge and skills needed to perform the project.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 316 Monitoring the knowledge and skills of the project personnel typically includes the following: • Periodically measuring the acquisition of knowledge and skills by project personnel • Comparing actual training obtained to that documented in the project plan • Identifying significant deviations from estimates in the project plan 6. Document the significant dev iations in the project planning parameters. SP 1.2 Monitor Commitments Monitor commitments against those identified in the project plan. Typical Work Products 1. Records of commitment reviews Subpractices 1. Regularly review commitments (both external and internal). 2. Identify commitments that have not been satisf ied or that are at significant risk of not being satisfied. 3. Document the results of the commitment reviews. SP 1.3 Monitor Project Risks Monitor risks against those identified in the project plan. Refer to the Project Planning proc ess area for more information about identifying project risks. Refer to the Risk Management process area for more information about risk management activities. Typical Work Products 1. Records of project risk monitoring Subpractices 1. Periodically review the documentati on of the risks in the context of the project’s current st atus and circumstances. 2. Revise the documentation of t he risks, as additional information becomes available, to incorporate changes. 3. Communicate risk status to relevant stakeholders. Examples of risk status include the following: • A change in the probability that the risk occurs • A change in risk priority",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 317SP 1.4 Monitor Data Management Monitor the management of project data against the project plan. Refer to the Plan for Data Management specific practice in the Project Planning process area for more info rmation about ident ifying the types of data that should be managed and ho w to plan for their management. Once the plans for the management of project data are made, the management of that data must be moni tored to ensure that those plans are accomplished. Typical Work Products 1. Records of data management Subpractices 1. Periodically review data management activities against their description in the project plan. 2. Identify and document significant issues and their impacts. 3. Document the results of data management activity reviews. SP 1.5 Monitor Stakeholder Involvement Monitor stakeholder involvement against the project plan. Refer to the Plan Stakeholder Involv ement specific practice in the Project Planning process area for more information about identifying relevant stakeholders and planning the appropriate involvement with them. Once the stakeholders are identified and the extent of their involvement within the project is specified in pr oject planning, that involvement must be monitored to ensure that the approp riate interactions are occurring. Typical Work Products 1. Records of stakeholder involvement Subpractices 1. Periodically review the stat us of stakeholder involvement. 2. Identify and document significant issues and their impacts. 3. Document the results of t he stakeholder involvement status reviews.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 318 SP 1.6 Conduct Progress Reviews Periodically review the project's progress, performance, and issues. Progress reviews are reviews on the project to keep stakeholders informed. These project reviews c an be informal reviews and may not be specified explicitly in the project plans. Typical Work Products 1. Documented project review results Subpractices 1. Regularly communicate status on assigned activities and work products to relevant stakeholders. Managers, staff members, customers, end users, suppliers, and other relevant stakeholders within the organization are included in the reviews as appropriate. 2. Review the results of coll ecting and analyzing measures for controlling the project. Refer to the Measurement and Analysis process area for more information about the process fo r measuring and analyzing project performance data. 3. Identify and document significant issues and deviations from the plan. 4. Document change requests and pr oblems identified in any of the work products and processes. Refer to the Configuration Management process area for more information about how changes are managed. 5. Document the results of the reviews. 6. Track change requests and problem reports to closure. SP 1.7 Conduct Milestone Reviews Review the accomplishments and results of the project at selected project milestones. Refer to the Project Planning proc ess area for more information about milestone planning. Milestone reviews are planned during project planning and are typically formal reviews.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 319Typical Work Products 1. Documented milestone review results Subpractices 1. Conduct reviews at meaningful points in the project’s schedule, such as the completion of se lected stages, with relevant stakeholders. Managers, staff members, customers, end users, suppliers, and other relevant stakeholders within the organization are included in the milestone reviews as appropriate. 2. Review the commitments, plan, status, and risks of the project. 3. Identify and document significant issues and their impacts. 4. Document the results of the review, action items, and decisions. 5. Track action items to closure. SG 2 Manage Corrective Action to Closure Corrective actions are managed to closure when the project's performance or results deviate significantly from the plan. SP 2.1 Analyze Issues Collect and analyze the issues and determine the corrective actions necessary to address the issues. Typical Work Products 1. List of issues needi ng corrective actions Subpractices 1. Gather issues for analysis. Issues are collected from reviews and the execution of other processes. Examples of issues to be gathered include the following: • Issues discovered through performing ve rification and validation activities • Significant deviations in the project planni ng parameters from the estimates in the project plan • Commitments (either internal or external) that have not been satisfied • Significant changes in risk status • Data access, collection, privacy, or security issues • Stakeholder representation or involvement issues",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 320 2. Analyze issues to determi ne need for corrective action. Refer to the Project Planning process area for information about corrective action criteria. Corrective action is required when the issue, if left unresolved, may prevent the project from meeting its objectives. SP 2.2 Take Corrective Action Take corrective action on identified issues. Typical Work Products 1. Corrective action plan Subpractices 1. Determine and document the appropriate actions needed to address the identified issues. Refer to the Project Planning process area for more information about the project plan w hen replanning is needed. Examples of potential actions include the following: • Modifying the statement of work • Modifying requirements • Revising estimates and plans • Renegotiating commitments • Adding resources • Changing processes • Revising project risks 2. Review and get agreement with relevant stakeholders on the actions to be taken. 3. Negotiate changes to internal and external commitments. SP 2.3 Manage Corrective Action Manage corrective actions to closure. Typical Work Products 1. Corrective action results",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 321Subpractices 1. Monitor corrective actions for completion. 2. Analyze results of corrective ac tions to determine the effectiveness of the corrective actions. 3. Determine and document appropriat e actions to correct deviations from planned results for corrective actions. Lessons learned as a result of taking corrective action can be inputs to planning and risk management processes. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the project monitoring and control process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the project monitoring and control process. Elaboration: This policy establishes organizati onal expectations for monitoring performance against the project plan and managing corrective action to closure when actual performance or results deviate significantly from the plan. GP 2.2 Plan the Process Establish and maintain the plan for performing the project monitoring and control process.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 322 Elaboration: This plan for performing the project monitoring and control process can be part of (or referenced by) the project plan, as described in the Project Planning process area. GP 2.3 Provide Resources Provide adequate resources for performing the project monitoring and control process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Cost tracking systems • Effort reporting systems • Action item tracking systems • Project management and scheduling programs GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the project monitoring and control process. GP 2.5 Train People Train the people performing or supporting the project monitoring and control process as needed. Elaboration: Examples of training topics include the following: • Monitoring and control of projects • Risk management • Data management GP 2.6 Manage Configurations Place designated work products of the project monitoring and control process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 323Elaboration: Examples of work products placed under control include the following: • Project schedules with status • Project measurement data and analysis • Earned value reports GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the project monitoring and control process as planned. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.7 and the Monitor Stakeholder Involvem ent practice in the Project Monitoring and Control process area. Examples of activities for stakeholder involvement include the following: • Assessing the project against the plan • Reviewing commitments and resolving issues • Reviewing project risks • Reviewing data management activities • Reviewing project progress • Managing corrective actions to closure GP 2.8 Monitor and Control the Process Monitor and control the project monitoring and control process against the plan for performing the process and take appropriate corrective action. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.8 and the Project Monitoring and Control process area.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 324 Examples of measures and work products used in monitoring and controlling include the following: • Number of open and closed corrective actions • Schedule with status for monthly financial data collection, analysis, and reporting • Number and types of reviews performed • Review schedule (planned versus actual and slipped target dates) • Schedule for collection and analysis of monitoring data GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the project monitoring and control process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Monitoring project performance against the project plan • Managing corrective actions to closure Examples of work products reviewed include the following: • Records of project performance • Project review results GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the project monitoring and control process with higher level management and resolve issues. Staged Only GG3 and its practices do not apply for a maturity level 2 rating, but do apply for a maturity level 3 rating and above.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 325Continuous/Maturity Levels 3 - 5 Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined project monitoring and control process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the project monitoring and control process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Records of significant deviations • Criteria for what constitutes a deviation • Corrective action results Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the project monitoring and control process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the project monitoring and control process to achieve the established quantitative quality and process-performance objectives.",
        "CMMI for Development Version 1.2 Project Monitoring and Control (PMC) 326 Continuous Only GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the project monitoring and control process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the project monitoring and control process.",
        "CMMI for Development Version 1.2 Project Planning (PP) 327PROJECT PLANNING A Project Management Process Area at Maturity Level 2 Purpose The purpose of Project Planning (PP) is to establish and maintain plans that define project activities. Introductory Notes The Project Planning process ar ea involves the following: • Developing the project plan • Interacting with stakeholders appropriately • Getting commitment to the plan • Maintaining the plan Planning begins with requirements that define the product and project. Planning includes estimating the attr ibutes of the work products and tasks, determining the resour ces needed, negotiating commitments, producing a schedule, and identifying and analyz ing project risks. Iterating through these activities ma y be necessary to establish the project plan. The project plan prov ides the basis for performing and controlling the project’s activities that address the commitments with the project’s customer. The project plan will usually need to be revised as the project progresses to address changes in requirements and commitments, inaccurate estimates, corrective actions, and process changes. Specific practices describing both planning and replanning are contained in this process area. The term “project plan” is used throughout the gener ic and specific practices in this process area to re fer to the overall plan for controlling the project.",
        "CMMI for Development Version 1.2 Project Planning (PP) 328 Related Process Areas Refer to the Requirements Development process area for more information about developing require ments that define the product and product components. Product and product component requirements and changes to those requirements serve as a basis for planning and replanning. Refer to the Requirements Management process area for more information about managing requirements needed for planning and replanning. Refer to the Risk Management process area for more information about identifying and managing risks. Refer to the Technical Solution proc ess area for more information about transforming requirements into product and product component solutions. Specific Goal and Practice Summary SG 1 Establish Estimates SP 1.1 Estimate the Scope of the Project SP 1.2 Establish Estimates of Work Product and Task Attributes SP 1.3 Define Project Lifecycle SP 1.4 Determine Estimates of Effort and Cost SG 2 Develop a Project Plan SP 2.1 Establish the Budget and Schedule SP 2.2 Identify Project Risks SP 2.3 Plan for Data Management SP 2.4 Plan for Project Resources SP 2.5 Plan for Needed Knowledge and Skills SP 2.6 Plan Stakeholder Involvement SP 2.7 Establish the Project Plan SG 3 Obtain Commitment to the Plan SP 3.1 Review Plans That Affect the Project SP 3.2 Reconcile Work and Resource Levels SP 3.3 Obtain Plan Commitment Specific Practices by Goal SG 1 Establish Estimates Estimates of project planning parameters are established and maintained. Project planning parameters include all information needed by the project to perform the necessary planning, organizing, staffing, directing, coordinating, reporting, and budgeting.",
        "CMMI for Development Version 1.2 Project Planning (PP) 329Estimates of planning parameters should have a sound basis to instill confidence that any pl ans based on these estimates are capable of supporting project objectives. Factors that are typically consi dered when estimating these parameters include the following: • Project requirements, including the product requirements, the requirements imposed by the or ganization, the requirements imposed by the customer, and other requirements that impact the project • Scope of the project • Identified tasks and work products • Technical approach • Selected project lifecycle model (e.g., waterfall, incremental, or spiral) • Attributes of the work products and tasks (e.g., size or complexity) • Schedule • Models or historical data for conver ting the attributes of the work products and tasks into labor hours and cost • Methodology (e.g., models, data, algorithms) used to determine needed material, skills, labor hours, and cost Documentation of the estimating rationale and supporting data is needed for stakeholders’ review a nd commitment to the plan and for maintenance of the plan as the project progresses. SP 1.1 Estimate the Scope of the Project Establish a top-level work breakdown structure (WBS) to estimate the scope of the project. The WBS evolves with the project. Init ially a top-level WBS can serve to structure the initial estimating. T he development of a WBS divides the overall project into an interconnected set of manageable components. Typically, the WBS is a product ori ented structure that provides a scheme for identifying and organizing the logical units of work to be managed, which are called “work packages.” The WBS provides a reference and organizational mechani sm for assigning effort, schedule, and responsibility and is used as t he underlying framework to plan, organize, and control the work done on the project. Some projects use the term “contract WBS” to refer to the portion of the WBS placed under contract (possibly the entire WBS). Not all projects have a contract WBS (e.g., internally funded development). Typical Work Products 1. Task descriptions",
        "CMMI for Development Version 1.2 Project Planning (PP) 330 2. Work package descriptions 3. WBS Subpractices 1. Develop a WBS based on the product architecture. The WBS provides a scheme for organizing the project’s work around the product and product components that the work supports. The WBS should permit the identification of the following items: • Identified risks and their mitigation tasks • Tasks for deliverables and supporting activities • Tasks for skill and knowledge acquisition • Tasks for development of needed support plans, such as configuration management, quality assurance, and verification plans • Tasks for integration and management of nondevelopmental items 2. Identify the work pack ages in sufficient detail to specify estimates of project tasks, res ponsibilities, and schedule. The top-level WBS is intended to help in gauging the project work effort in terms of tasks and organizational roles and responsibilities. The amount of detail in the WBS at this more detailed level helps in developing realistic schedules, thereby minimizing the need for management reserve. 3. Identify product or product co mponents that will be externally acquired. Refer to the Supplier Agreement Management process area for more information about acquiring products from sources external to the project. 4. Identify work products that will be reused. SP 1.2 Establish Estimates of Work Product and Task Attributes Establish and maintain estimates of the attributes of the work products and tasks. Size is the primary input to many mo dels used to estimate effort, cost, and schedule. The models can also be based on inputs such as connectivity, complexity, and structure.",
        "CMMI for Development Version 1.2 Project Planning (PP) 331Examples of types of work products for which size estimates are made include the following: • Deliverable and nondeliverable work products • Documents and files • Operational and support hardware, firmware, and software Examples of size measures include the following: • Number of functions • Function points • Source lines of code • Number of classes and objects • Number of requirements • Number and complexity of interfaces • Number of pages • Number of inputs and outputs • Number of technical risk items • Volume of data • Number of logic gates for integrated circuits • Number of parts (e.g., printed circuit boards, components, and mechanical parts) • Physical constraints (e.g., weight and volume) The estimates should be consistent with proj ect requirements to determine the project’s effort, cost, and schedule. A relative level of difficulty or complexity should be assigned for each size attribute. Typical Work Products 1. Technical approach 2. Size and complexity of tasks and work products 3. Estimating models 4. Attribute estimates Subpractices 1. Determine the technical approach for the project. The technical approach defines a top-level strategy for development of the product. It includes decisions on architectural features, such as distributed or client/server; state-of-the-art or established technologies to be applied, such as robotics, composite materials, or artificial intelligence; and breadth of the",
        "CMMI for Development Version 1.2 Project Planning (PP) 332 functionality expected in the final products, such as safety, security, and ergonomics. 2. Use appropriate methods to deter mine the attributes of the work products and tasks that will be us ed to estimate the resource requirements. Methods for determining size and complexity should be based on validated models or historical data. The methods for determining attributes evolve as our understanding of the relationship of product characteristics to attributes increases. Examples of current methods include the following: • Number of logic gates for integrated circuit design • Lines of code or function points for software • Number/complexity of requirements for systems engineering • Number of square feet for standard-specified residential homes 3. Estimate the attributes of the work products and tasks. SP 1.3 Define Project Lifecycle Define the project lifecycle phases on which to scope the planning effort. The determination of a project’s li fecycle phases provides for planned periods of evaluation and decision making. These are normally defined to support logical decision points at which significant commitments are made concerning resources and te chnical approach. Such points provide planned events at which project course corrections and determinations of future scope and cost can be made. The project lifecycle phases need to be defined depending on the scope of requirements, the estimates for project resources, and the nature of the project. Larger projects may c ontain multiple phases, such as concept exploration, development, production, operations, and disposal. Within these phases, subphases may be needed. A development phase may include subphases such as requirements analysis, design, fabrication, integration, and verification. The determination of project phases typically includes selecti on and refinement of one or more development models to address interdependencies and appropriate sequencing of the activi ties in the phases. Depending on the strategy for devel opment, there may be intermediate phases for the creation of prototypes, increments of capability, or spiral model cycles.",
        "CMMI for Development Version 1.2 Project Planning (PP) 333Understanding the project lifecycle is crucial in determining the scope of the planning effort and the timing of th e initial planning, as well as the timing and criteria (critical milestones) for replanning. Typical Work Products 1. Project lifecycle phases SP 1.4 Determine Estimates of Effort and Cost Estimate the project effort and cost for the work products and tasks based on estimation rationale. Estimates of effort and cost are generally based on the results of analysis using models or historical data applied to size, activities, and other planning parameters. Confidenc e in these estimates is based on the rationale for the selected model and the nature of the data. There may be occasions when the availabl e historical data does not apply, such as where efforts are unprecedented or where the type of task does not fit available models. An effort is unprecedented (to some degree) if a similar product or component has never been built. An effort may also be unprecedented if the development group has never built such a product or component. Unprecedented efforts are more risky, r equire more research to develop reasonable bases of estimate, and require more management reserve. The uniqueness of the project must be documented when using these models to ensure a common underst anding of any assumptions made in the initial planning stages. Typical Work Products 1. Estimation rationale 2. Project effort estimates 3. Project cost estimates Subpractices 1. Collect the models or historical data that will be used to transform the attributes of the work produc ts and tasks into estimates of the labor hours and cost. Many parametric models have been developed to aid in estimating cost and schedule. The use of these models as the sole source of estimation is not recommended because these models are based on historical project data that may or may not be pertinent to your project. Multiple models and/or methods can be used to ensure a high level of confidence in the estimate. Historical data include the cost, effort, and schedule data from previously executed projects, plus appropriate scaling data to account for differing sizes and complexity.",
        "CMMI for Development Version 1.2 Project Planning (PP) 334 2. Include supporting infrastructure needs when estimating effort and cost. The supporting infrastructure includes resources needed from a development and sustainment perspective for the product. Consider the infrastructure resource needs in the development environment, the test environment, the production environment, the target environment, or any appropriate combination of these when estimating effort and cost. Examples of infrastructure resources include the following: • Critical computer resources (e.g., memory, disk and network capacity, peripherals, communication channels, and the capacities of these) • Engineering environments and tools (e.g., tools for prototyping, assembly, computer-aided design [CAD], and simulation) • Facilities, machinery, and equipment (e.g., test benches and recording devices) 3. Estimate effort and cost usi ng models and/or historical data. Effort and cost inputs used for estimating typically include the following: • Judgmental estimates provided by an exper t or group of experts (e.g., Delphi Method) • Risks, including the extent to which the effort is unprecedented • Critical competencies and roles needed to perform the work • Product and product component requirements • Technical approach • WBS • Size estimates of work products and anticipated changes • Cost of externally acquired products • Selected project lifecycle model and processes • Lifecycle cost estimates • Capability of tools provided in engineering environment • Skill levels of managers and staff needed to perform the work • Knowledge, skill, and training needs • Facilities needed (e.g., office and meeting space and workstations) • Engineering facilities needed • Capability of manufacturing process(es) • Travel • Level of security required for tasks, wo rk products, hardware, software, personnel, and work environment • Service level agreements for call centers and warranty work • Direct labor and overhead",
        "CMMI for Development Version 1.2 Project Planning (PP) 335SG 2 Develop a Project Plan A project plan is established and maintained as the basis for managing the project. A project plan is a formal, approved document used to manage and control the execution of the project. It is based on the project requirements and the es tablished estimates. The project plan should consider all phases of the project lifecycle. Project planning should ensure that a ll plans affecting the project are consistent with the overall project plan. SP 2.1 Establish the Budget and Schedule Establish and maintain the project’s budget and schedule. The project’s budget and schedul e are based on the developed estimates and ensure that budget alloca tion, task complexity, and task dependencies are appropriately addressed. Event-driven, resource-limited schedul es have proven to be effective in dealing with project risk. Identifying accomplishments to be demonstrated before initiation of the event provides some flexibility in the timing of the event, a common und erstanding of what is expected, a better vision of the state of the proj ect, and a more accurate status of the project’s tasks. Typical Work Products 1. Project schedules 2. Schedule dependencies 3. Project budget Subpractices 1. Identify ma jor milestones. Milestones are often imposed to ensure completion of certain deliverables by the milestone. Milestones can be event based or calendar based. If calendar based, once milestone dates have been agreed on, it is often very difficult to change them. 2. Identify schedule assumptions. When schedules are initially developed, it is common to make assumptions about the duration of certain activities. These assumptions are frequently made on items for which little if any estimation data is available. Identifying these assumptions provides insight into the level of confidence (uncertainties) in the overall schedule. 3. Identify constraints.",
        "CMMI for Development Version 1.2 Project Planning (PP) 336 Factors that limit the flexibility of management options need to be identified as early as possible. The examination of the attributes of the work products and tasks often will bring these issues to the surface. Such attributes can include task duration, resources, inputs, and outputs. 4. Identify task dependencies. Typically, the tasks for a project can be accomplished in some ordered sequence that will minimize the duration of the project. This involves the identification of predecessor and successor tasks to determine the optimal ordering. Examples of tools that can help determine an optimal ordering of task activities include the following: • Critical Path Method (CPM) • Program Evaluation and Review Technique (PERT) • Resource-limited scheduling 5. Define the budget and schedule. Establishing and maintaining the project’s budget and schedule typically includes the following: • Defining the committed or expected availability of resources and facilities • Determining time phasing of activities • Determining a breakout of subordinate schedules • Defining the dependencies between the activities (predecessor or successor relationships) • Defining the schedule activities and milestones to support accuracy in progress measurement • Identifying milestones for delivery of products to the customer • Defining activities of appropriate duration • Defining milestones of appropriate time separation • Defining a management reserve based on the confidence level in meeting the schedule and budget • Using appropriate historical data to verify the schedule • Defining incremental funding requirements • Documenting project assumptions and rationale 6. Establish corrective action criteria. Criteria are established for determining what constitutes a significant deviation from the project plan. A basis for gauging issues and problems is necessary to determine when a corrective action should be taken. The corrective actions may require replanning, which may include revising the original plan, establishing new agreements, or including mitigation activities within the current plan.",
        "CMMI for Development Version 1.2 Project Planning (PP) 337SP 2.2 Identify Project Risks Identify and analyze project risks. Refer to the Risk Management process area for more information about risk management activities. Refer to the Monitor Project Risks specific practice in the Project Monitoring and Control process area for more information about risk monitoring activities. Risks are identified or discover ed and analyzed to support project planning. This specific practice shoul d be extended to all the plans that affect the project to ensure that the appropriate interfacing is taking place between all relevant stakehol ders on identified risks. Project planning risk identification and analys is typically include the following: • Identifying risks • Analyzing the risks to det ermine the impact, probability of occurrence, and time frame in which problems are likely to occur • Prioritizing risks Typical Work Products 1. Identified risks 2. Risk impacts and probability of occurrence 3. Risk priorities Subpractices 1. Identify risks. The identification of risks involves the identification of potential issues, hazards, threats, vulnerabilities, and so on that could negatively affect work efforts and plans. Risks must be identified and described in an understandable way before they can be analyzed. When identifying risks, it is a good idea to use a standard method for defining risks. Risk identification and analysis tools can be used to help identify possible problems.",
        "CMMI for Development Version 1.2 Project Planning (PP) 338 Examples of risk identification and analysis tools include the following: • Risk taxonomies • Risk assessments • Checklists • Structured interviews • Brainstorming • Performance models • Cost models • Network analysis • Quality factor analysis 2. Document the risks. 3. Review and obtain agreement wi th relevant stakeholders on the completeness and correctness of the documented risks. 4. Revise the risks as appropriate. Examples of when identified risks may need to be revised include the following: • When new risks are identified • When risks become problems • When risks are retired • When project circumstances change significantly SP 2.3 Plan for Data Management Plan for the management of project data. IPPD Addition When integrated teams are formed, pr oject data includes data developed and used solely within a particular team as well as data applicable across integrated team boundaries, if ther e are multiple integrated teams. Data are the various forms of documentation required to support a program in all of its areas (e.g., administration, engineering, configuration management, finance, logistics, quality, safety, manufacturing, and procurement). Th e data can take any form (e.g., reports, manuals, notebooks, charts, dr awings, specifications, files, or correspondence). The data may exist in any medium (e.g., printed or drawn on various materials, photograp hs, electronic, or multimedia). Data may be deliverable (e.g., items identified by a program’s contract data requirements) or data may be no ndeliverable (e.g., informal data, trade studies and analyses, internal meeting minutes, internal design",
        "CMMI for Development Version 1.2 Project Planning (PP) 339review documentation, lessons lear ned, and action item s). Distribution can take many forms, including electronic transmission. The data requirements for the projec t should be established for both the data items to be created and t heir content and form, based on a common or standard set of data r equirements. Uniform content and format requirements for data item s facilitate understanding of data content and help with c onsistent management of the data resources. The reason for collecting each doc ument should be clear. This task includes the analysis and verificati on of project deliverables and nondeliverables, contract and no ncontract data requirements, and customer-supplied data. Often, data is collected with no clear understanding of how it will be used. Data is costly and should be collected only when needed. Typical Work Products 1. Data management plan 2. Master list of managed data 3. Data content and format description 4. Data requirements lists fo r acquirers and for suppliers 5. Privacy requirements 6. Security requirements 7. Security procedures 8. Mechanism for data retrieval, reproduction, and distribution 9. Schedule for collection of project data 10. Listing of project data to be collected Subpractices 1. Establish requirements and procedures to ensure privacy and security of the data. Not everyone will have the need or clearance necessary to access the project data. Procedures must be established to identify who has access to what data as well as when they have access to the data. 2. Establish a mechanism to ar chive data and to access archived data. Accessed information should be in an understandable form (e.g., electronic or computer output from a database) or represented as originally generated. 3. Determine the project data to be identified, collected, and distributed.",
        "CMMI for Development Version 1.2 Project Planning (PP) 340 SP 2.4 Plan for Project Resources Plan for necessary resources to perform the project. IPPD Addition When integrated teams are formed, planning for project resources should consider staffing of the integrated teams. Defining project resources (labor, machinery/equipment, materials, and methods) and quantities needed to perf orm project activities builds on the initial estimates and provides additional information that can be applied to expand the WBS used to manage the project. The top-level WBS developed earlie r as an estimation mechanism is typically expanded by decomposing t hese top levels into work packages that represent singular work units that can be separately assigned, performed, and tracked. This s ubdivision is done to distribute management responsibility and provid e better management control. Each work package or work product in the WBS should be assigned a unique identifier (e.g., number) to permit tracking. A WBS can be based on requirements, activities, work pr oducts, or a combination of these items. A dictionary that describes t he work for each work package in the WBS should accompany the work breakdown structure. Typical Work Products 1. WBS work packages 2. WBS task dictionary 3. Staffing requirements based on project size and scope 4. Critical facilities/equipment list 5. Process/workflow def initions and diagrams 6. Program administration requirements list Subpractices 1. Determine process requirements. The processes used to manage a project must be identified, defined, and coordinated with all the relevant stakeholders to ensure efficient operations during project execution. 2. Determine staffing requirements. The staffing of a project depends on the decomposition of the project requirements into tasks, roles, and responsibilities for accomplishing the project requirements as laid out within the work packages of the WBS.",
        "CMMI for Development Version 1.2 Project Planning (PP) 341Staffing requirements must consider the knowledge and skills required for each of the identified positions, as defined in the Plan for Needed Knowledge and Skills specific practice. 3. Determine facilities, equipm ent, and component requirements. Most projects are unique in some sense and require some set of unique assets to accomplish the objectives of the project. The determination and acquisition of these assets in a timely manner are crucial to project success. Lead-time items need to be identified early to determine how they will be addressed. Even when the required assets are not unique, compiling a list of all of the facilities, equipment, and parts (e.g., number of computers for the personnel working on the project, software applications, and office space) provides insight into aspects of the scope of an effort that are often overlooked. SP 2.5 Plan for Needed Knowledge and Skills Plan for knowledge and skills needed to perform the project. Refer to the Organizational Training process area for more information about knowledge and skills informati on to be incorporated into the project plan. Knowledge delivery to projects in volves both training of project personnel and acquisition of k nowledge from outside sources. Staffing requirements are depende nt on the knowledge and skills available to support the execution of the project. Typical Work Products 1. Inventory of skill needs 2. Staffing and new hire plans 3. Databases (e.g., skills and training) Subpractices 1. Identify the knowledge and skill s needed to perform the project. 2. Assess the knowledge and skills available. 3. Select mechanisms for prov iding needed knowledge and skills. Example mechanisms include the following: • In-house training (both or ganizational and project) • External training • Staffing and new hires • External skill acquisition",
        "CMMI for Development Version 1.2 Project Planning (PP) 342 The choice of in-house training or outsourced training for the needed knowledge and skills is determined by the availability of training expertise, the project’s schedule, and the business objectives. 4. Incorporate selected mechanisms into the project plan. SP 2.6 Plan Stakeholder Involvement Plan the involvement of identified stakeholders. IPPD Addition When integrated teams are formed, stakeholder involvement should be planned down to the integrated team level. Stakeholders are identified from all phas es of the projec t lifecycle by identifying the type of people and f unctions needing representation in the project and describing their rele vance and the degree of interaction for specific project activities. A two-dimensional matrix with stakeholders along one axis and project ac tivities along the other axis is a convenient format for accomplishing this identificati on. Relevance of the stakeholder to the activity in a particular project phase and the amount of interaction expected woul d be shown at the intersection of the project phase activity ax is and the stakeholder axis. For the inputs of stakeholders to be usef ul, careful selection of relevant stakeholders is necessary. For eac h major activity, identify the stakeholders who are affected by t he activity and those who have expertise that is needed to conduct the activity. This list of relevant stakeholders will probably change as t he project moves through the phases of the project lifecycle. It is important, however, to ensure that relevant stakeholders in the latter phases of the lifecycle have early input to requirements and design decisions that affect them. Examples of the type of material that should be included in a plan for stakeholder interaction include the following: • List of all relevant stakeholders • Rationale for stakeholder involvement • Roles and responsibilities of the relevant stakeholders with respect to the project, by project lifecycle phase • Relationships between stakeholders • Relative importance of the stakeholder to success of the project, by project lifecycle phase • Resources (e.g., training, materials, time, and funding) needed to ensure stakeholder interaction • Schedule for phasing of stakeholder interaction",
        "CMMI for Development Version 1.2 Project Planning (PP) 343Conduct of this specific prac tice relies on shared or exchanged information with the previous Plan for Needed Knowledge and Skills specific practice. Typical Work Products 1. Stakeholder involvement plan SP 2.7 Establish the Project Plan Establish and maintain the overall project plan content. A documented plan that addresses al l relevant planning items is necessary to achieve the mu tual understanding, commitment, and performance of individuals, groups , and organizations that must execute or support the plans. The plan generated fo r the project defines all aspects of the effort, tying t ogether in a logical manner: project lifecycle considerations; techni cal and management tasks; budgets and schedules; milestones; data management, risk identification, resource and skill requirements; and stakeholder identification and interaction. Infrastructure descriptions in clude responsibility and authority relationships for project staff, management, and support organizations. For Software Engineering For software, the planning document is often referred to as one of the following: • Software development plan • Software project plan • Software plan For Hardware Engineering For hardware, the planning document is often referred to as a hardware development plan. Development activities in preparation for production may be included in the hardware development plan or defined in a separate production plan.",
        "CMMI for Development Version 1.2 Project Planning (PP) 344 Examples of plans that have been used in the U.S. Department of Defense community include the following: • Integrated Master Plan—an event-driven plan that documents significant accomplishments with pass/fail criteria for both business and technical elements of the project and that ties each accomplishment to a key program event. • Integrated Master Schedule—an integrated and networked multi-layered schedule of program tasks required to complete the work effort documented in a related Integrated Master Plan. • Systems Engineering Management Plan—a plan that details the integrated technical effort across the project. • Systems Engineering Master Schedule—an event-based schedule that contains a compilation of key technical accomplishments, each with measurable criteria, requiring successful completion to pass identified events. • Systems Engineering Detailed Schedule—a detailed, time-dependent, task- oriented schedule that associates specific dates and milestones with the Systems Engineering Master Schedule. Typical Work Products 1. Overall project plan SG 3 Obtain Commitment to the Plan Commitments to the project plan are established and maintained. To be effective, plans require co mmitment by those responsible for implementing and suppo rting the plan. SP 3.1 Review Plans That Affect the Project Review all plans that affect the project to understand project commitments. IPPD Addition When integrated teams are formed, t heir integrated work plans are among the plans to review. Plans developed within other proc ess areas will typically contain information similar to that called fo r in the overall project plan. These plans may provide additional detailed guidance and should be compatible with and support the overal l project plan to indicate who has the authority, responsibilit y, accountability, and cont rol. All plans that affect the project should be reviewed to ensure a common understanding of the scope, objectives, roles, and relationships that are required for the project to be succe ssful. Many of these plans are described by the Plan the Process generic practice in each of the process areas.",
        "CMMI for Development Version 1.2 Project Planning (PP) 345Typical Work Products 1. Record of the reviews of plans that affect the project SP 3.2 Reconcile Work and Resource Levels Reconcile the project plan to reflect available and estimated resources. IPPD Addition When integrated teams are formed, sp ecial attention should be paid to resource commitments in circumstance s of distributed integrated teams and when people are on multiple integrat ed teams in one or more projects. To establish a project t hat is feasible, obtain commitment from relevant stakeholders and reconcile any differe nces between the estimates and the available resources. Reconcilia tion is typically accomplished by lowering or deferring technical performance requirements, negotiating more resources, finding ways to increase productivity, outsourcing, adjusting the staff skill mix, or revising all plans that affect the project or schedules. Typical Work Products 1. Revised methods and correspondi ng estimating parameters (e.g., better tools and use of off-the-shelf components) 2. Renegotiated budgets 3. Revised schedules 4. Revised requirements list 5. Renegotiated stakeholder agreements SP 3.3 Obtain Plan Commitment Obtain commitment from relevant stakeholders responsible for performing and supporting plan execution. IPPD Addition When integrated teams are formed, t he integrated team plans should have buy-in from the team members, the in terfacing teams, the project, and the process owners of the standard proces ses that the team has selected for tailored application. Obtaining commitment involves interaction among all relevant stakeholders both internal and external to the project. The individual or group making a commitment should have confidence that the work can be performed within cost, schedule, and performance constraints. Often, a provisional commitment is adequate to allow the effort to begin",
        "CMMI for Development Version 1.2 Project Planning (PP) 346 and to permit research to be perform ed to increase confidence to the appropriate level needed to obtain a full commitment. Typical Work Products 1. Documented requests for commitments 2. Documented commitments Subpractices 1. Identify needed support and negotia te commitments with relevant stakeholders. The WBS can be used as a checklist for ensuring that commitments are obtained for all tasks. The plan for stakeholder interaction should identify all parties from whom commitment should be obtained. 2. Document all organization al commitments, both full and provisional, ensuring appropriate level of signatories. Commitments must be documented to ensure a consistent mutual understanding as well as for tracking and maintenance. Provisional commitments should be accompanied by a description of the risks associated with the relationship. 3. Review internal commitments with senior management as appropriate. 4. Review external commitments with senior management as appropriate. Management may have the necessary insight and authority to reduce risks associated with external commitments. 5. Identify commitment s on interfaces betw een elements in the project, and with other projects and organizational units so that they can be monitored. Well-defined interface specifications form the basis for commitments.",
        "CMMI for Development Version 1.2 Project Planning (PP) 347Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the project planning process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the project planning process. Elaboration: This policy establishes organizational expectations for estimating the planning parameters, making internal and external commitments, and developing the plan for managing the project. GP 2.2 Plan the Process Establish and maintain the plan for performing the project planning process. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.2 and the Project Planning process area. GP 2.3 Provide Resources Provide adequate resources for performing the project planning process, developing the work products, and providing the services of the process.",
        "CMMI for Development Version 1.2 Project Planning (PP) 348 Elaboration: Special expertise, equipment, and faci lities in project planning may be required. Special expertise in project planning may include the following: • Experienced estimators • Schedulers • Technical experts in applicabl e areas (e.g., product domain and technology) Examples of other resources provided include the following tools: • Spreadsheet programs • Estimating models • Project planning and scheduling packages GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the project planning process. GP 2.5 Train People Train the people performing or supporting the project planning process as needed. Elaboration: Examples of training topics include the following: • Estimating • Budgeting • Negotiating • Risk identification and analysis • Data management • Planning • Scheduling GP 2.6 Manage Configurations Place designated work products of the project planning process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Project Planning (PP) 349Elaboration: Examples of work products placed under control include the following: • Work breakdown structure • Project plan • Data management plan • Stakeholder involvement plan GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the project planning process as planned. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.7 and the Plan Stakeholder Involvement practice in the Project Planning process area. Examples of activities for stakeholder involvement include the following: • Establishing estimates • Reviewing and resolving issues on the completeness and correctness of the project risks • Reviewing data management plans • Establishing project plans • Reviewing project plans and resolving issues on work and resource issues GP 2.8 Monitor and Control the Process Monitor and control the project planning process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of revisions to the plan • Cost, schedule, and effort variance per plan revision • Schedule for development and maintenance of program plans",
        "CMMI for Development Version 1.2 Project Planning (PP) 350 GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the project planning process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Establishing estimates • Developing the project plan • Obtaining commitments to the project plan Examples of work products reviewed include the following: • WBS • Project plan • Data management plan • Stakeholder involvement plan GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the project planning process with higher level management and resolve issues. Staged Only GG3 and its practices do not apply for a maturity level 2 rating, but do apply for a maturity level 3 rating and above. Continuous/Maturity Levels 3 - 5 Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined project planning process.",
        "CMMI for Development Version 1.2 Project Planning (PP) 351Continuous/Maturity Levels 3 - 5 Only GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the project planning process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, m easures, measurement results, and improvement information include the following: • Project data library structure • Project attribute estimates • Risk impacts and probability of occurrence Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the project planning process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the project planning process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the project planning process in fulfilling the relevant business objectives of the organization.",
        "CMMI for Development Version 1.2 Project Planning (PP) 352 Continuous Only GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the project planning process.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 353PROCESS AND PRODUCT QUALITY ASSURANCE A Support Process Area at Maturity Level 2 Purpose The purpose of Process and Product Quality Assurance (PPQA) is to provide staff and management with obj ective insight into processes and associated work products. Introductory Notes The Process and Product Quality Assu rance process area involves the following: • Objectively evaluating perform ed processes, work products, and services against the applicable pr ocess descriptions, standards, and procedures • Identifying and documenti ng noncompliance issues • Providing feedback to project st aff and managers on the results of quality assurance activities • Ensuring that noncompliance issues are addressed The Process and Product Quality Assu rance process area supports the delivery of high-quality products and se rvices by providing the project staff and managers at all levels wi th appropriate visibility into, and feedback on, processes and associat ed work products throughout the life of the project. The practices in the Process and Product Quality Assurance process area ensure that planned processe s are implemented, while the practices in the Verification proc ess area ensure that the specified requirements are satisfied. These two process areas may on occasion address the same work product but from different perspectives. Projects should take advantage of the overlap in order to minimize duplication of effort while taking care to main tain the separate perspectives. Objectivity in process and product quality assurance evaluations is critical to the success of the projec t. (See the definition of “objectively evaluate” in the glossary.) Object ivity is achieved by both independence and the use of criteria. A combinati on of methods prov iding evaluations against criteria by those not produci ng the work product is often used. Less formal methods can be used to provide broad day-to-day coverage. More formal methods c an be used periodically to assure objectivity.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 354 Examples of ways to perform objective evaluations include the following: • Formal audits by organizationally separate quality assurance organizations • Peer reviews which may be performed at various levels of formality • In-depth review of work at the place it is performed (i.e., desk audits) • Distributed review and comment of work products Traditionally, a quality assurance gr oup that is independent of the project provides this objectivi ty. It may be appropriate in some organizations, however, to implem ent the process and product quality assurance role without that kind of independence. For example, in an organization with an open, quality-or iented culture, the process and product quality assurance role may be performed, partially or completely, by peers; and the qua lity assurance function may be embedded in the process. For sma ll organizations, this might be the most feasible approach. If quality assurance is embedded in the process, several issues must be addressed to ensure objectivity. Ev eryone performing quality assurance activities should be trained in qua lity assurance. Those performing quality assurance activities for a wo rk product should be separate from those directly involved in developing or maintaining the work product. An independent reporting channel to the appropriate level of organizational management must be av ailable so that noncompliance issues can be escalated as necessary. For example, in implementing peer reviews as an objective evaluation method: • Members are trained and roles are assigned for people attending the peer reviews. • A member of the peer review who did not produce this work product is assigned to perform the role of QA. • Checklists are available to support the QA activity. • Defects are recorded as part of the peer review report and are tracked and escalated outside the project when necessary. Quality assurance should begin in t he early phases of a project to establish plans, processes, standar ds, and procedures that will add value to the project and satisfy the requirements of the project and the organizational policies. Those perfo rming quality assurance participate in establishing the plans, proce sses, standards, and procedures to ensure that they fit the project’s needs and that they will be useable for performing quality assurance evaluat ions. In addition, the specific processes and associated work produc ts that will be evaluated during the project are designated. This designation may be based on sampling or on objective criteria that are c onsistent with organizational policies and project requirements and needs.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 355When noncompliance issues are ident ified, they are first addressed within the project and resolved ther e if possible. Any noncompliance issues that cannot be resolved within the project are escalated to an appropriate level of management for resolution. This process area applies primarily to evaluations of the activities and work products of a project, but it also applies to evaluations of nonproject activities and work products such as training activities. For these activities and work produc ts, the term “project” should be appropriately interpreted. Related Process Areas Refer to the Project Planning proc ess area for more information about identifying processes and associated work products that will be objectively evaluated. Refer to the Verification process area for more information about satisfying specified requirements. Specific Goal and Practice Summary SG 1 Objectively Evaluate Processes and Work Products SP 1.1 Objectively Evaluate Processes SP 1.2 Objectively Evaluate Work Products and Services SG 2 Provide Objective Insight SP 2.1 Communicate and Ensure Resolution of Noncompliance Issues SP 2.2 Establish Records Specific Practices by Goal SG 1 Objectively Evaluate Processes and Work Products Adherence of the performed process and associated work products and services to applicable process descriptions, standards, and procedures is objectively evaluated. SP 1.1 Objectively Evaluate Processes Objectively evaluate the designated performed processes against the applicable process descriptions, standards, and procedures. Objectivity in quality assurance evaluati ons is critical to the success of the project. A description of the qua lity assurance reporting chain and how it ensures objecti vity should be defined. Typical Work Products 1. Evaluation reports 2. Noncompliance reports",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 356 3. Corrective actions Subpractices 1. Promote an environment (created as part of project management) that encourages employee partici pation in identifying and reporting quality issues. 2. Establish and maintain clearly st ated criteria for the evaluations. The intent of this subpractice is to provide criteria, based on business needs, such as the following: • What will be evaluated • When or how often a process will be evaluated • How the evaluation will be conducted • Who must be involved in the evaluation 3. Use the stated criteria to evaluate performed processes for adherence to process descripti ons, standards, and procedures. 4. Identify each nonc ompliance found duri ng the evaluation. 5. Identify lessons learned that c ould improve processes for future products and services. SP 1.2 Objectively Evaluate Work Products and Services Objectively evaluate the designated work products and services against the applicable process descriptions, standards, and procedures. Typical Work Products 1. Evaluation reports 2. Noncompliance reports 3. Corrective actions Subpractices 1. Select work products to be evaluated, based on documented sampling criteria if sampling is used. 2. Establish and maintain clearly st ated criteria for the evaluation of work products.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 357The intent of this subpractice is to provide criteria, based on business needs, such as the following: • What will be evaluated during the evaluation of a work product • When or how often a work product will be evaluated • How the evaluation will be conducted • Who must be involved in the evaluation 3. Use the stated criteria during the evaluations of work products. 4. Evaluate work products before t hey are delivered to the customer. 5. Evaluate work products at selected milestones in their development. 6. Perform in-progress or increment al evaluations of work products and services against proce ss descriptions, standards, and procedures. 7. Identify each case of noncomp liance found during the evaluations. 8. Identify lessons learned that c ould improve processes for future products and services. SG 2 Provide Objective Insight Noncompliance issues are objectively tracked and communicated, and resolution is ensured. SP 2.1 Communicate and Ensure Resolution of Noncompliance Issues Communicate quality issues and ensure resolution of noncompliance issues with the staff and managers. Noncompliance issues are problems ident ified in evaluations that reflect a lack of adherence to applicable standards, process descriptions, or procedures. The status of noncomplia nce issues provides an indication of quality trends. Quality issues include noncompliance issues and results of trend analysis. When local resolution of noncomplia nce issues cannot be obtained, use established escalation mechanisms to ensure that the appropriate level of management can resolve the issue. Track noncompliance issues to resolution. Typical Work Products 1. Corrective action reports 2. Evaluation reports 3. Quality trends",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 358 Subpractices 1. Resolve each noncompliance with the appropriate members of the staff where possible. 2. Document noncompliance issues when they cannot be resolved within the project. Examples of ways to resolve noncompliance within the project include the following: • Fixing the noncompliance • Changing the process descriptions, standards , or procedures that were violated • Obtaining a waiver to cover the noncompliance issue 3. Escalate noncompliance issues that cannot be resolved within the project to the appropriate leve l of management designated to receive and act on noncompliance issues. 4. Analyze the noncomplia nce issues to see if there are any quality trends that can be ident ified and addressed. 5. Ensure that relevant stakeholde rs are aware of the results of evaluations and the quality trends in a timely manner. 6. Periodically review open noncomp liance issues and trends with the manager designated to receive and act on noncompliance issues. 7. Track noncompliance issues to resolution. SP 2.2 Establish Records Establish and maintain records of the quality assurance activities. Typical Work Products 1. Evaluation logs 2. Quality assurance reports 3. Status reports of corrective actions 4. Reports of quality trends Subpractices 1. Record process and product qua lity assurance activities in sufficient detail such that st atus and results are known. 2. Revise the status and history of the quality assurance activities as necessary.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 359Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the process and product quality assurance process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the process and product quality assurance process. Elaboration: This policy establishes organizati onal expectations for objectively evaluating whether processes and as sociated work products adhere to the applicable process descriptions , standards, and procedures; and ensuring that noncomp liance is addressed. This policy also establishes organiza tional expectations for process and product quality assurance being in pl ace for all projects. Process and product quality assurance must po ssess sufficient independence from project management to provide object ivity in identifying and reporting noncompliance issues. GP 2.2 Plan the Process Establish and maintain the plan for performing the process and product quality assurance process. Elaboration: This plan for performing the proc ess and product quality assurance process can be included in (or referenc ed by) the project plan, which is described in the Project Planning process area.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 360 GP 2.3 Provide Resources Provide adequate resources for performing the process and product quality assurance process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Evaluation tools • Noncompliance tracking tool GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the process and product quality assurance process. Elaboration: To guard against subjectivity or bias, ensure that those people assigned responsibility and authority for proc ess and product quality assurance can perform their evaluations with sufficient independence and objectivity. GP 2.5 Train People Train the people performing or supporting the process and product quality assurance process as needed. Elaboration: Examples of training topics include the following: • Application domain • Customer relations • Process descriptions, standards, procedures, and methods for the project • Quality assurance objectives, process descriptions, standards, procedures, methods, and tools GP 2.6 Manage Configurations Place designated work products of the process and product quality assurance process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 361Elaboration: Examples of work products placed under control include the following: • Noncompliance reports • Evaluation logs and reports GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the process and product quality assurance process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing criteria for the objective evaluations of processes and work products • Evaluating processes and work products • Resolving noncompliance issues • Tracking noncompliance issues to closure GP 2.8 Monitor and Control the Process Monitor and control the process and product quality assurance process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Variance of objective process evaluations planned and performed • Variance of objective work product evaluations planned and performed • Schedule for objective evaluations GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the process and product quality assurance process against its process description, standards, and procedures, and address noncompliance. Elaboration: Refer to Table 6.2 on page 95 in Generic Goals and Generic Practices for more information about the relati onship between generic practice 2.9 and the Process and Product Qualit y Assurance process area.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 362 Examples of activities reviewed include the following: • Objectively evaluating processes and work products • Tracking and communicating noncompliance issues Examples of work products reviewed include the following: • Noncompliance reports • Evaluation logs and reports GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the process and product quality assurance process with higher level management and resolve issues. Staged Only GG3 and its practices do not apply for a maturity level 2 rating, but do apply for a maturity level 3 rating and above. Continuous/Maturity Levels 3 - 5 Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined process and product quality assurance process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the process and product quality assurance process to support the future use and improvement of the organization’s processes and process assets.",
        "CMMI for Development Version 1.2 Process and Product Quality Assurance (PPQA) 363Continuous/Maturity Levels 3 - 5 Only Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Evaluation logs • Quality trends • Noncompliance report • Status reports of corrective action • Cost of quality reports for the project Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the process and product quality assurance process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the process and product quality assurance process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the process and product quality assurance process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the process and product quality assurance process.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 364 QUANTITATIVE PROJECT MANAGEMENT A Project Management Process Area at Maturity Level 4 Purpose The purpose of Quantitative Pr oject Management (QPM) is to quantitatively manage the project’s defined process to achieve the project’s established quality and pr ocess-performance objectives. Introductory Notes The Quantitative Project Managemen t process area involves the following: • Establishing and maintaining t he project’s quality and process- performance objectives • Identifying suitable subprocesse s that compose the project’s defined process based on historic al stability and capability data found in process-performance baselines or models • Selecting the subprocesses of the project’s defined process to be statistically managed • Monitoring the project to determi ne whether the project’s objectives for quality and process perform ance are being satisfied, and identifying appropriate corrective action • Selecting the measures and anal ytic techniques to be used in statistically managing the selected subprocesses • Establishing and maintaining an und erstanding of the variation of the selected subprocesses usi ng the selected measures and analytic techniques • Monitoring the performance of the selected subprocesses to determine whether they are capable of satisfying their quality and process-performance objectives, and identifying corrective action • Recording statistical and qua lity management data in the organization’s measurement repository The quality and process-perform ance objectives, measures, and baselines identified here are dev eloped as described in the Organizational Process Performanc e process area. Subsequently, the results of performing the processe s associated with the Quantitative Project Management process area (e .g., measurement definitions and measurement data) become part of th e organizational process assets referred to in the Organizational Pr ocess Performance process area.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 365To effectively address the specific pr actices in this process area, the organization should have alread y established a set of standard processes and related organizational process assets, such as the organization’s measurement reposit ory and the organization’s process asset library for use by each projec t in establishing its defined process. The project’s defined process is a set of subprocesses that form an integrated and coherent lifecycle for th e project. It is established, in part, through selecting and tailoring processes from the organization’s set of standard processes. (See the definiti on of “defined process” in the glossary.) The project should also ensure t hat the measurements and progress of the supplier’s efforts are made availa ble. Establishment of effective relationships with suppliers is necessary for the successful implementation of this proces s area’s specific practices. Process performance is a measure of the actual process results achieved. Process performance is characterized by both process measures (e.g., effort, cycle time , and defect removal efficiency) and product measures (e.g., reliability, defect density, and response time). Subprocesses are defined components of a larger defined process. For example, a typical organization’ s development process may be defined in terms of subprocesses such as requirements development, design, build, test, and peer review. The subprocesses themselves may be further decomposed as necessary into other subprocesses and process elements. One essential element of quant itative management is having confidence in estimates (i.e., being abl e to predict the extent to which the project can fulfill its quality and process-performance objectives). The subprocesses that will be statis tically managed are chosen based on identified needs for predictable perfo rmance. (See the definitions of “statistically managed process, ” “quality and process-performance objective,” and “quantitatively man aged process” in the glossary.) Another essential element of quant itative management is understanding the nature and extent of the va riation experienced in process performance, and recognizing when th e project’s actual performance may not be adequate to achieve the project’s quality and process- performance objectives. Statistical management involves stat istical thinking and the correct use of a variety of statistical techniques, such as run charts, control charts, confidence intervals, prediction in tervals, and tests of hypotheses. Quantitative management uses data from statistical management to help the project predict whether it will be able to achieve its quality and process-performance objectives and identify what corrective action should be taken.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 366 This process area applies to managi ng a project, but the concepts found here also apply to managing other groups and functions. Applying these concepts to m anaging other groups and functions may not necessarily contribute to achiev ing the organization’s business objectives, but may help these grou ps and functions c ontrol their own processes. Examples of other groups and functions include the following: • Quality assurance • Process definition and improvement • Effort reporting • Customer complaint handling • Problem tracking and reporting Related Process Areas Refer to the Project Monitoring and Control process area for more information about monitoring and controlling the project and taking corrective action. Refer to the Measurement and Analysis process area for more information about establishing meas urable objectives, specifying the measures and analyses to be performed, obtaining and analyzing measures, and providing results. Refer to the Organizational Proce ss Performance process area for more information about the organization’s quality and process- performance objectives, proce ss-performance analyses, process- performance baselines, and pr ocess-performance models. Refer to the Organizational Process Definition process area for more information about the organizati onal process assets, including the organization’s meas urement repository. Refer to the Integrated Project Management process area for more information about establishing and maintaining the project’s defined process. Refer to the Causal Analysis and Resolution process area for more information about how to identify the causes of defects and other problems, and taking action to prev ent them from occurring in the future. Refer to the Organizational Innov ation and Deployment process area for more information about selecti ng and deploying improvements that support the organization’s quality and process-performance objectives.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 367Specific Goal and Practice Summary SG 1 Quantitatively Manage the Project SP 1.1 Establish the Project’s Objectives SP 1.2 Compose the Defined Process SP 1.3 Select the Subprocesses that Will Be Statistically Managed SP 1.4 Manage Project Performance SG 2 Statistically Manage Subprocess Performance SP 2.1 Select Measures and Analytic Techniques SP 2.2 Apply Statistical Methods to Understand Variation SP 2.3 Monitor Performance of the Selected Subprocesses SP 2.4 Record Statistical Management Data Specific Practices by Goal SG 1 Quantitatively Manage the Project The project is quantitatively managed using quality and process- performance objectives. SP 1.1 Establish the Project’s Objectives Establish and maintain the project’s quality and process- performance objectives. When establishing the project’s quality and process-performance objectives, it is often useful to think ahead about which processes from the organization’s set of standard processes will be included in the project’s defined process, and what the historical data indicates regarding their process performance. These considerations will help in establishing realistic objectives for the project. Later, as the project’s actual performance becomes k nown and more predictable, the objectives may need to be revised. Typical Work Products 1. The project’s quality and pr ocess-performance objectives Subpractices 1. Review the organization's obj ectives for quality and process performance. The intent of this review is to ensure that the project understands the broader business context in which the project will need to operate. The project’s objectives for quality and process performance are developed in the context of these overarching organizational objectives. Refer to the Organizational Proce ss Performance process area for more information about the organi zation’s quality and process- performance objectives.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 368 2. Identify the quality and process performance needs and priorities of the customer, suppliers, end users, and other relevant stakeholders. Examples of quality and process-performance attributes for which needs and priorities might be identified include the following: • Functionality • Reliability • Maintainability • Usability • Duration • Predictability • Timeliness • Accuracy 3. Identify how process perfo rmance is to be measured. Consider whether the measures established by the organization are adequate for assessing progress in fulfilling customer, end-user, and other stakeholder needs and priorities. It may be necessary to supplement these with additional measures. Refer to the Measurement and Analysis process area for more information about defining measures. 4. Define and document meas urable quality and process- performance objectives for the project. Defining and documenting objectives for the project involve the following: • Incorporating the organization’s qualit y and process-performance objectives • Writing objectives that reflect t he quality and process-performance needs and priorities of the customer, end users, and other stakeholders, and the way these objectives should be measured Examples of quality attributes for which objectives might be written include the following: • Mean time between failures • Critical resource utilization • Number and severity of defects in the released product • Number and severity of customer comp laints concerning the provided service",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 369Examples of process-performance attributes for which objectives might be written include the following: • Percentage of defects removed by product ve rification activities (perhaps by type of verification, such as peer reviews and testing) • Defect escape rates • Number and density of defects (by severity) found during the first year following product delivery (or start of service) • Cycle time • Percentage of rework time 5. Derive interim objectives for eac h lifecycle phase, as appropriate, to monitor progress toward achieving the project’s objectives. An example of a method to predict future results of a process is the use of process-performance models to predict the latent defects in the delivered product using interim measures of defects identified during product verification activities (e.g., peer reviews and testing). 6. Resolve conflicts among t he project’s quality and process- performance objectives (e.g., if one objective cannot be achieved without compromising another objective). Resolving conflicts involves the following: • Setting relative priorities for the objectives • Considering alternative objectives in light of long-term business strategies as well as short-term needs • Involving the customer, end users, senior management, project management, and other relevant stakeholders in the tradeoff decisions • Revising the objectives as necessary to reflect the results of the conflict resolution 7. Establish traceability to th e project’s quality and process- performance objectives from their sources. Examples of sources for objectives include the following: • Requirements • Organization’s quality and process-performance objectives • Customer’s quality and process-performance objectives • Business objectives • Discussions with customers and potential customers • Market surveys",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 370 An example of a method to identify and trace these needs and priorities is Quality Function Deployment (QFD). 8. Define and negotiate quality and process-performance objectives for suppliers. Refer to the Supplier Agreement Management process area for more information about establis hing and maintaining agreements with suppliers. 9. Revise the project’s quality and process-performance objectives as necessary. SP 1.2 Compose the Defined Process Select the subprocesses that compose the project’s defined process based on historical stability and capability data. Refer to the Integrated Project Management process area for more information about establishing and maintaining the project’s defined process. Refer to the Organizational Process Definition process area for more information about the organization’s process asset library, which might include a process element of known and nee ded capability. Refer to the Organizational Proce ss Performance process area for more information about the org anization’s process-performance baselines and process- performance models. Subprocesses are identified from the process elements in the organization’s set of standard proce sses and the process artifacts in the organization’s process asset library. Typical Work Products 1. Criteria used in identifyi ng which subprocesses are valid candidates for inclusion in the project’s defined process 2. Candidate subprocesses for incl usion in the project’s defined process 3. Subprocesses to be included in the project’s defined process 4. Identified risks when select ed subprocesses lack a process- performance history Subpractices 1. Establish the criteria to use in identifying which subprocesses are valid candidates for use.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 371Identification may be based on the following: • Quality and process-performance objectives • Existence of process-performance data • Product line standards • Project lifecycle models • Customer requirements • Laws and regulations 2. Determine whether the subprocesse s that are to be statistically managed, and that were obtained from the organizational process assets, are suitable fo r statistical management. A subprocess may be more suitable for statistical management if it has a history of the following: • Stable performance in previous comparable instances • Process-performance data that satisf ies the project’s quality and process- performance objectives Historical data are primarily obtained from the organization’s process-performance baselines. However, these data may not be available for all subprocesses. 3. Analyze the interaction of subprocesses to understand the relationships among t he subprocesses and the measured attributes of the subprocesses. Examples of analysis techniques include system dynamics models and simulations. 4. Identify the risk when no subprocess is available that is known to be capable of satisfying the qua lity and process-performance objectives (i.e., no c apable subprocess is ava ilable or the capability of the subprocess is not known). Even when a subprocess has not been selected to be statistically managed, historical data and process-performance models may indicate that the subprocess is not capable of satisfying the quality and process-performance objectives. Refer to the Risk Management process area for more information about risk identification and analysis. SP 1.3 Select the Subprocesses that Will Be Statistically Managed Select the subprocesses of the project's defined process that will be statistically managed. Selecting the subprocesses to be statistically managed is often a concurrent and iterative process of identifying applicable project and organization quality and process-perfo rmance objectives, selecting the",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 372 subprocesses, and identifying the pr ocess and product attributes to measure and control. Often the se lection of a process, quality and process-performance objective, or measurable attribute will constrain the selection of the other two. For example, if a particular process is selected, the measurable attr ibutes and quality and process- performance objectives may be c onstrained by that process. Typical Work Products 1. Quality and process-performance objectives that will be addressed by statistical management 2. Criteria used in selecting whic h subprocesses will be statistically managed 3. Subprocesses that will be statistically managed 4. Identified process and produc t attributes of the selected subprocesses that should be measured and controlled Subpractices 1. Identify which of the quality and process-per formance objectives of the project will be st atistically managed. 2. Identify the criteria to be used in selecting the subprocesses that are the main contributors to ac hieving the identified quality and process-performance objectives and for which predictable performance is important. Examples of sources for criteria used in selecting subprocesses include the following: • Customer requirements related to quality and process performance • Quality and process-performance objecti ves established by the customer • Quality and process-performance objectives established by the organization • Organization’s performance baselines and models • Stable performance of the subprocess on other projects • Laws and regulations 3. Select the subprocesses that w ill be statistically managed using the selection criteria. It may not be possible to statistically manage some subprocesses (e.g., where new subprocesses and technologies are being piloted). In other cases, it may not be economically justifiable to apply statistical techniques to certain subprocesses. 4. Identify the produc t and process attributes of the selected subprocesses that will be measured and controlled.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 373Examples of product and process attributes include the following: • Defect density • Cycle time • Test coverage SP 1.4 Manage Project Performance Monitor the project to determine whether the project’s objectives for quality and process performance will be satisfied, and identify corrective action as appropriate. Refer to the Measurement and Analysis process area for more information about analyzi ng and using measures. A prerequisite for such a comparis on is that the selected subprocesses of the project’s defined process are being statistically managed and their process capability is understood. The s pecific practices of specific goal 2 provide detail on statistically managing the selected subprocesses. Typical Work Products 1. Estimates (predictions) of the achievement of the project’s quality and process-performance objectives 2. Documentation of the risks in achieving the project’s quality and process-performance objectives 3. Documentation of actions neede d to address the deficiencies in achieving the project’s objectives Subpractices 1. Periodically review the perfo rmance of each s ubprocess and the capability of each subpr ocess selected to be statistically managed to appraise progress toward achieving the project’s quality and process-performance objectives. The process capability of each selected subprocess is determined with respect to that subprocess’ established quality and process-performance objectives. These objectives are derived from the project’s quality and process-performance objectives, which are for the project as a whole. 2. Periodically review the actual results achieved against established interim objectives for each phase of the project lifecycle to appraise progress toward achieving the project’s quality and process- performance objectives. 3. Track suppliers’ results for achieving their quality and process- performance objectives.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 374 4. Use process-performance m odels calibrated with obtained measures of critical attributes to estimate progress toward achieving the project’s quality and process-performance objectives. Process-performance models are used to estimate progress toward achieving objectives that cannot be measured until a future phase in the project lifecycle. An example is the use of process-performance models to predict the latent defects in the delivered product using interim measures of defects identified during peer reviews. Refer to the Organizational Proce ss Performance process area for more information about process-performance models. The calibration is based on the results obtained from performing the previous subpractices. 5. Identify and manage the risks a ssociated with achieving the project’s quality and proce ss-performance objectives. Refer to the Risk Management process area for more information about identifying and managing risks. Example sources of the risks include the following: • Inadequate stability and capability data in the organization’s measurement repository • Subprocesses having inadequate performance or capability • Suppliers not achieving their qualit y and process-performance objectives • Lack of visibility into supplier capability • Inaccuracies in the organization’s pr ocess-performance models for predicting future performance • Deficiencies in predicted process performance (estimated progress) • Other identified risks associated with identified deficiencies 6. Determine and document actions needed to address the deficiencies in achieving t he project’s quality and process- performance objectives. The intent of these actions is to plan and deploy the right set of activities, resources, and schedule to place the project back on track as much as possible to meet its objectives.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 375Examples of actions that can be taken to address deficiencies in achieving the project’s objectives include the following: • Changing quality or process-performance objec tives so that they are within the expected range of the project’s defined process • Improving the implementation of the projec t’s defined process so as to reduce its normal variability (reducing variability may bring the project’s performance within the objectives without having to move the mean) • Adopting new subprocesses and technologies that have the potential for satisfying the objectives and managing the associated risks • Identifying the risk and risk mitigation strategies for the deficiencies • Terminating the project Refer to the Project Monitoring and Control process area for more information about taki ng corrective action. SG 2 Statistically Manage Subprocess Performance The performance of selected subprocesses within the project's defined process is statistically managed. This specific goal describes an acti vity critical to achieving the Quantitatively Manage the Project specific goal of this process area. The specific practices under this specific goal describe how to statistically manage the subprocesse s whose selection was described in the specific practices under the first specific goal. When the selected subprocesses are statistically managed , their capability to achieve their objectives can be determined. By t hese means, it will be possible to predict whether the project will be abl e to achieve its objectives, which is key to quantitativel y managing the project. SP 2.1 Select Measures and Analytic Techniques Select the measures and analytic techniques to be used in statistically managing the selected subprocesses. Refer to the Measurement and Analysis process area for more information about establishing m easurable objectives; on defining, collecting, and analyzing measur es; and on revising measures and statistical analysis techniques. Typical Work Products 1. Definitions of the measures and analytic techniques to be used in (or proposed for) statistica lly managing the subprocesses 2. Operational definitions of the measures, their collection points in the subprocesses, and how the int egrity of the measures will be determined",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 376 3. Traceability of measures back to the project’s quality and process- performance objectives 4. Instrumented organizational support environment to support automatic data collection Subpractices 1. Identify common meas ures from the organizational process assets that support statistical management. Refer to the Organizational Process Definition process area for more information about common measures. Product lines or other stratification criteria may categorize common measures. 2. Identify additional measures t hat may be needed for this instance to cover critical product and proc ess attributes of the selected subprocesses. In some cases, measures may be research oriented. Such measures should be explicitly identified. 3. Identify the measur es that are appropriate for statistical management. Critical criteria for selecting statistical management measures include the following: • Controllable (e.g., can a measure’s values be changed by changing how the subprocess is implemented?) • Adequate performance indicator (e.g., is the measure a good indicator of how well the subprocess is performing relative to the objectives of interest?) Examples of subprocess measures include the following: • Requirements volatility • Ratios of estimated to measured values of the planning parameters (e.g., size, cost, and schedule) • Coverage and efficiency of peer reviews • Test coverage and efficiency • Effectiveness of training (e.g., percent of planned training completed and test scores) • Reliability • Percentage of the total defects inserted or found in the different phases of the project lifecycle • Percentage of the total effort expended in the different phases of the project lifecycle",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 3774. Specify the operational definitions of the measures, their collection points in the subprocesses, and how the integrity of the measures will be determined. Operational definitions are stated in precise and unambiguous terms. They address two important criteria as follows: • Communication: What has been measured, how it was measured, what the units of measure are, and what has been included or excluded • Repeatability: Whether the measurement can be repeated, given the same definition, to get the same results 5. Analyze the relationship of the identified measures to the organization’s and project’s objecti ves, and derive objectives that state specific target measures or ranges to be met for each measured attribute of eac h selected subprocess. 6. Instrument the organizational support environment to support collection, derivation, and analys is of statistical measures. The instrumentation is based on the following: • Description of the organization’s set of standard processes • Description of the project’s defined process • Capabilities of the organizational support environment 7. Identify the appropriat e statistical analysis techniques that are expected to be useful in stat istically managing the selected subprocesses. The concept of “one size does not fit all” applies to statistical analysis techniques. What makes a particular technique appropriate is not just the type of measures, but more important, how the measures will be used and whether the situation warrants applying that technique. The appropriateness of the selection may need to be investigated from time to time. Examples of statistical analysis techniques are given in the next specific practice. 8. Revise the measures and stat istical analysis techniques as necessary. SP 2.2 Apply Statistical Met hods to Understand Variation Establish and maintain an understanding of the variation of the selected subprocesses using the selected measures and analytic techniques. Refer to the Measurement and Analysis process area for more information about collecting, analyzi ng, and using measurement results.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 378 Understanding variation is achieved, in part, by collecting and analyzing process and product measures so that special causes of variation can be identified and addressed to ac hieve predictable performance. A special cause of process variati on is characterized by an unexpected change in process performance. Spec ial causes are also known as “assignable causes” becaus e they can be ident ified, analyzed, and addressed to prevent recurrence. The identification of s pecial causes of variat ion is based on departures from the system of common causes of variation. These departures can be identified by the presence of ex treme values, or other identifiable patterns in the data collected from t he subprocess or associated work products. Knowledge of variation an d insight about potential sources of anomalous patterns are typically needed to detect special causes of variation. Sources of anomalous patterns of variation may include the following: • Lack of process compliance • Undistinguished influences of multiple underlying subprocesses on the data • Ordering or timing of activities within the subprocess • Uncontrolled inputs to the subprocess • Environmental changes during subprocess execution • Schedule pressure • Inappropriate sampling or grouping of data Typical Work Products 1. Collected measures 2. Natural bounds of proces s performance for each measured attribute of each selected subprocess 3. Process performance compared to the natural bounds of process performance for each measured attribute of each selected subprocess Subpractices 1. Establish trial natural bounds for subprocesses having suitable historical performance data. Refer to the Organizational Proce ss Performance process area for more information about organi zational process-performance baselines. Natural bounds of an attribute are the range within which variation normally occurs. All processes will show some variation in process and product measures",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 379each time they are executed. The issue is whether this variation is due to common causes of variation in the normal performance of the process or to some special cause that can and should be identified and removed. When a subprocess is initially executed, suitable data for establishing trial natural bounds are sometimes available from prior instances of the subprocess or comparable subprocesses, process-performance baselines, or process- performance models. These data are typically contained in the organization’s measurement repository. As the subprocess is executed, data specific to that instance are collected and used to update and replace the trial natural bounds. However, if the subprocess in question has been materially tailored, or if the conditions are materially different from those in previous instantiations, the data in the repository may not be relevant and should not be used. In some cases, there may be no historical comparable data (e.g., when introducing a new subprocess, when entering a new application domain, or when significant changes have been made to the subprocess). In such cases, trial natural bounds will have to be made from early process data of this subprocess. These trial natural bounds must then be refined and updated as subprocess execution continues. Examples of criteria for determining whether data are comparable include the following: • Product lines • Application domain • Work product and task attributes (e.g., size of product) • Size of project 2. Collect data, as defined by the selected measures, on the subprocesses as they execute. 3. Calculate the natural bounds of process performance for each measured attribute. Examples of where the natural bounds are calculated include the following: • Control charts • Confidence intervals (for parameters of distributions) • Prediction intervals (for future outcomes) 4. Identify special c auses of variation. An example of a criterion for detecting a special cause of process variation in a control chart is a data point that falls outside of the 3-sigma control limits. The criteria for detecting special causes of variation are based on statistical theory and experience and depend on economic justification. As criteria are added,",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 380 special causes are more likely to be identified if present, but the likelihood of false alarms also increases. 5. Analyze the special cause of process variation to determine the reasons the anomaly occurred. Examples of techniques for analyzing the reasons for special causes of variation include the following: • Cause-and-effect (fishbone) diagrams • Designed experiments • Control charts (applied to subprocess inputs or to lower level subprocesses) • Subgrouping (analyzing the same data segregated into smaller groups based on an understanding of how the subprocess was implemented facilitates isolation of special causes) Some anomalies may simply be extremes of the underlying distribution rather than problems. The people implementing a subprocess are usually the ones best able to analyze and understand special causes of variation. 6. Determine what corrective ac tion should be taken when special causes of variati on are identified. Removing a special cause of process variation does not change the underlying subprocess. It addresses an error in the way the subprocess is being executed. Refer to the Project Monitoring and Control process area for more information about taki ng corrective action. 7. Recalculate the natural bounds for each measured attribute of the selected subprocesses as necessary. Recalculating the (statistically estimated) natural bounds is based on measured values that signify that the subprocess has changed, not on expectations or arbitrary decisions. Examples of when the natural bounds may need to be recalculated include the following: • There are incremental improvements to the subprocess • New tools are deployed for the subprocess • A new subprocess is deployed • The collected measures suggest that the subprocess mean has permanently shifted or the subprocess variation has permanently changed",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 381SP 2.3 Monitor Performance of the Selected Subprocesses Monitor the performance of the selected subprocesses to determine their capability to satisfy their quality and process- performance objectives, and identify corrective action as necessary. The intent of this specific practice is to do the following: • Determine statistically the pr ocess behavior expected from the subprocess • Appraise the probability that t he process will meet its quality and process-performance objectives • Identify the corrective action to be taken, based on a statistical analysis of the process-performance data Corrective action may include re negotiating the affected project objectives, identifying and implement ing alternative subprocesses, or identifying and measuring lower leve l subprocesses to achieve greater detail in the performance data. Any or all of these ac tions are intended to help the project use a more capabl e process. (See the definition of “capable process” in the glossary.) A prerequisite for comparing the capability of a selected subprocess against its quality and process-perfo rmance objectives is that the performance of the subprocess is st able and predictable with respect to its measured attributes. Process capability is analyzed fo r those subprocesses and those measured attributes for which (derived) objectives have been established. Not all subprocesses or measured attributes that are statistically managed are analyzed r egarding proces s capability. The historical data may be inadequate for initially determining whether the subprocess is capable. It also is possible that the estimated natural bounds for subprocess performance may shift away from the quality and process-performance objectives. In either case, statistical control implies monitoring capabilit y as well as stability. Typical Work Products 1. Natural bounds of proces s performance for each selected subprocess compared to its est ablished (derived) objectives 2. For each subprocess, its process capability 3. For each subprocess, the ac tions needed to address deficiencies in its process capability",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 382 Subpractices 1. Compare the quality and process- performance objectives to the natural bounds of the measured attribute. This comparison provides an appraisal of the process capability for each measured attribute of a subprocess. These comparisons can be displayed graphically, in ways that relate the estimated natural bounds to the objectives or as process capability indices, which summarize the relationship of the objectives to the natural bounds. 2. Monitor changes in quality and process-performance objectives and selected subprocess’ process capability. 3. Identify and docum ent subprocess capability deficiencies. 4. Determine and document ac tions needed to address subprocess capability deficiencies. Examples of actions that can be taken when a selected subprocess’s performance does not satisfy its objectives include the following: • Changing quality and process-performance objectives so that they are within the subprocess’ process capability • Improving the implementation of the existing subprocess so as to reduce its normal variability (reducing variability may bring the natural bounds within the objectives without having to move the mean) • Adopting new process elements and subprocesses and technologies that have the potential for satisfying the objectives and managing the associated risks • Identifying risks and risk mitigation stra tegies for each subprocess’s process capability deficiency Refer to the Project Monitoring and Control process area for more information about taki ng corrective action. SP 2.4 Record Statistical Management Data Record statistical and quality management data in the organization’s measurement repository. Refer to the Measurement and Analysis process area for more information about managing and storin g data, measurement definitions, and results. Refer to the Organizational Process Definition process area for more information about the organizati on’s measurement repository. Typical Work Products 1. Statistical and quality management data recorded in the organization’s measurement repository",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 383Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the quantitative project management process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the quantitative project management process. Elaboration: This policy establishes organizati onal expectations for quantitatively managing the project using quality an d process-performance objectives, and statistically managing selected su bprocesses within the project’s defined process. GP 2.2 Plan the Process Establish and maintain the plan for performing the quantitative project management process.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 384 Elaboration: This plan for performing the quantit ative project management process can be included in (or referenced by ) the project plan, which is described in the Project Planning process area. GP 2.3 Provide Resources Provide adequate resources for performing the quantitative project management process, developing the work products, and providing the services of the process. Elaboration: Special expertise in st atistics and statistical process control may be needed to define the techniques for statistical management of selected subprocesses, but staff will use the tools and techniques to perform the statistical management. Special expe rtise in statistics may also be needed for analyzing and interpreti ng the measures resulting from statistical management. Examples of other resources provided include the following tools: • System dynamics models • Automated test-coverage analyzers • Statistical process and quality control packages • Statistical analysis packages GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the quantitative project management process. GP 2.5 Train People Train the people performing or supporting the quantitative project management process as needed. Elaboration: Examples of training topics include the following: • Process modeling and analysis • Process measurement data selection, definition, and collection",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 385GP 2.6 Manage Configurations Place designated work products of the quantitative project management process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Subprocesses to be included in the project’s defined process • Operational definitions of the measures, their collection points in the subprocesses, and how the integrity of the measures will be determined • Collected measures GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the quantitative project management process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing project objectives • Resolving issues among the project’s quality and process-performance objectives • Appraising performance of the selected subprocesses • Identifying and managing the risks in achieving the project’s quality and process- performance objectives • Identifying what corrective action should be taken GP 2.8 Monitor and Control the Process Monitor and control the quantitative project management process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Profile of subprocesses under statistical management (e.g., number planned to be under statistical management, number currently being statistically managed, and number that are statistically stable) • Number of special causes of variation identified • Schedule of data collection, analysis, and reporting activities in a measurement and analysis cycle as it relates to quantitative management activities",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 386 GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the quantitative project management process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Quantitatively managing the project using quality and process-performance objectives • Statistically managing selected subprocesses within the project’s defined process Examples of work products reviewed include the following: • Subprocesses to be included in the project’s defined process • Operational definitions of the measures • Collected measures GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the quantitative project management process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined quantitative project management process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the quantitative project management process to support the future use and improvement of the organization’s processes and process assets.",
        "CMMI for Development Version 1.2 Quantitative Project Management (QPM) 387Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Records of statistical and quality management data from the project, including results from the periodic review of the actual performance of the statistically managed subprocesses against established interim objectives of the project • Process and product quality assurance report that identifies inconsistent but compliant implementations of subprocesses being considered for statistical management Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the quantitative project management process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the quantitative project management process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the quantitative project management process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the quantitative project management process.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 388 REQUIREMENTS DEVELOPMENT An Engineering Process Area at Maturity Level 3 Purpose The purpose of Requirements Development (RD) is to produce and analyze customer, product, and product component requirements. Introductory Notes This process area describes three types of requirements: customer requirements, product requirem ents, and product component requirements. Taken together, thes e requirements address the needs of relevant stakeholders, including t hose pertinent to various product lifecycle phases (e.g., acceptance testing criter ia) and product attributes (e.g., safety, reliability, and mainta inability). Requirements also address constraints caused by the selection of design solutions (e.g., integration of commercial off-the-shelf products). All development projects have requirem ents. In the case of a project that is focused on maintenance activi ties, the changes to the product or product components are based on changes to the existing requirements, design, or implement ation. The requirements changes, if any, might be documented in change r equests from the customer or users, or they might take the form of new requirements received from the requirements development process. Regardless of their source or form, the maintenance activities t hat are driven by changes to requirements are managed accordingly. Requirements are the basis fo r design. The development of requirements includes the following activities: • Elicitation, analysis, validati on, and communication of customer needs, expectations, and constr aints to obtain customer requirements that constitute an understanding of what will satisfy stakeholders • Collection and coordinat ion of stakeholder needs • Development of the lifecycle requirements of the product • Establishment of the customer requirements • Establishment of initia l product and product component requirements consistent wi th customer requirements",
        "CMMI for Development Version 1.2 Requirements Development (RD) 389This process area addresses all cust omer requirements rather than only product-level requirements because the customer may also provide specific design requirements. Customer requirements are further refined into product and product component requirements. In addition to customer requirements, product and product component requirements are derived from the selected design solutions. Throughout the pr ocess areas, where we use the terms product and product component, their intended meanings also encompass services and their components. Requirements are identified and refi ned throughout the phases of the product lifecycle. Design decisions , subsequent corrective actions, and feedback during each phase of the pr oduct’s lifecycle are analyzed for impact on derived and allocated requirements. The Requirements Development proc ess area includes three specific goals. The Develop Customer Requi rements specific goal addresses defining a set of customer requirements to use in the development of product requirements. The Develop Product Requirements specific goal addresses defining a set of product or product component requirements to use in the design of products and product components. The Analyze and Validate Requirements specific goal addresses the necessary analysis of customer, product, and pr oduct component requirements to define, derive, and underst and the requirements. T he specific practices of the third specific goal are intended to assist the specific practices in the first two specific goals. T he processes associated with the Requirements Development process area and those associated with the Technical Solution process area may interact recursively with one another. Analyses are used to understand, def ine, and select the requirements at all levels from competing alte rnatives. These analyses include the following: • Analysis of needs and requir ements for each product lifecycle phase, including needs of relev ant stakeholders, the operational environment, and factors that reflec t overall customer and end-user expectations and satisfaction, su ch as safety, security, and affordability • Development of an operational concept • Definition of the required functionality The definition of functionality, also re ferred to as “functional analysis,” is not the same as structured analysis in software development and does not presume a functionally oriented software design. In object-oriented software design, it relates to defin ing what are called “services” or “methods.” The definition of functions, their logi cal groupings, and their",
        "CMMI for Development Version 1.2 Requirements Development (RD) 390 association with requirements is referred to as a “functional architecture.” Analyses occur recursively at succes sively more detailed layers of a product’s architecture until suffic ient detail is available to enable detailed design, acquisition, and testin g of the product to proceed. As a result of the analysis of requi rements and the operational concept (including functionality, support, maintenance, and disposal), the manufacturing or production concept produces more derived requirements, including consi deration of the following: • Constraints of various types • Technological limitations • Cost and cost drivers • Time constraints and schedule drivers • Risks • Consideration of iss ues implied but not exp licitly stated by the customer or end user • Factors introduced by the developer’s unique business considerations, r egulations, and laws A hierarchy of logical entities (functions and subf unctions, object classes and subclasses) is establ ished through iteration with the evolving operational concept. Requi rements are refined, derived, and allocated to these logical entities. Requirements and logical entities are allocated to products, product co mponents, people, or associated processes. Involvement of relevant stakeholde rs in both requirements development and analysis gives them visibility into the evolution of requirements. This activity continually assures them that the r equirements are being properly defined. Related Process Areas Refer to the Requirements Management process area for more information about managing cust omer and product requirements, obtaining agreement with the r equirements provider, obtaining commitments with those implem enting the requirements, and maintaining traceability. Refer to the Technical Solution proc ess area for more information about how the outputs of the requirement s development processes are used, and the development of alternativ e solutions and designs used in refining and deriving requirements.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 391Refer to the Product Integration process area for more information about interface requirements and interface management. Refer to the Verification process area for more information about verifying that the resulting product meets the requirements. Refer to the Validation process area for more information about how the product built will be validat ed against the customer needs. Refer to the Risk Management process area for more information about identifying and managing risks that are related to requirements. Refer to the Configuration Manageme nt process area for information about ensuring that key work prod ucts are controlled and managed. Specific Goal and Practice Summary SG 1 Develop Customer Requirements SP 1.1 Elicit Needs SP 1.2 Develop the Customer Requirements SG 2 Develop Product Requirements SP 2.1 Establish Product and Product Component Requirements SP 2.2 Allocate Product Component Requirements SP 2.3 Identify Interface Requirements SG 3 Analyze and Validate Requirements SP 3.1 Establish Operational Concepts and Scenarios SP 3.2 Establish a Definition of Required Functionality SP 3.3 Analyze Requirements SP 3.4 Analyze Requirements to Achieve Balance SP 3.5 Validate Requirements Specific Practices by Goal SG 1 Develop Customer Requirements Stakeholder needs, expectations, constraints, and interfaces are collected and translated into customer requirements. The needs of stakeholders (e.g., cu stomers, end users, suppliers, builders, testers, manufacturers, and logistics support personnel) are the basis for determining customer requirements. The stakeholder needs, expectations, constraints, in terfaces, operati onal concepts, and product concepts are analyzed, harmo nized, refined, and elaborated for translation into a set of customer requirements. Frequently, stakeholder needs, expectati ons, constraints, and interfaces are poorly identified or conf licting. Since stakeholder needs, expectations, constraints, and limitat ions should be clearly identified and understood, an iterative process is used throughout the life of the project to accomplish this objec tive. To facilitate the required interaction, a surrogate for the end user or customer is frequently involved to represent their needs and help resolve conflicts. The",
        "CMMI for Development Version 1.2 Requirements Development (RD) 392 customer relations or marketing part of the organization as well as members of the development team from disciplines such as human engineering or support can be used as surrogates. Environmental, legal, and other constraints shoul d be considered when creating and resolving the set of customer requirements. SP 1.1 Elicit Needs Elicit stakeholder needs, expectations, constraints, and interfaces for all phases of the product lifecycle. Eliciting goes beyond collecting require ments by proactively identifying additional requirements not explicitly provided by customers. Additional requirements should address the various product lifecycle activities and their impact on the product. Examples of techniques to elicit needs include the following: • Technology demonstrations • Interface control working groups • Technical control working groups • Interim project reviews • Questionnaires, interviews, and operational scenarios obtained from end users • Operational walkthroughs and end-user task analysis • Prototypes and models • Brainstorming • Quality Function Deployment • Market surveys • Beta testing • Extraction from sources such as documents, standards, or specifications • Observation of existing products, environments, and workflow patterns • Use cases • Business case analysis • Reverse engineering (for legacy products) • Customer satisfaction surveys",
        "CMMI for Development Version 1.2 Requirements Development (RD) 393Examples of sources of requirements that might not be identified by the customer include the following: • Business policies • Standards • Business environmental requirements (e.g., laboratories, testing and other facilities, and information technology infrastructure) • Technology • Legacy products or product components (reuse product components) Subpractices 1. Engage relevant stakeholders usi ng methods for eliciting needs, expectations, constraints, and external interfaces. SP 1.2 Develop the Customer Requirements Transform stakeholder needs, expectations, constraints, and interfaces into customer requirements. The various inputs from the re levant stakeholders must be consolidated, missing information mu st be obtained, and conflicts must be resolved in documenting the recognized set of customer requirements. The customer requirements may include needs, expectations, and constraints with r egard to verification and validation. In some situations, the customer pr ovides a set of requirements to the project, or the requirements exist as an output of a previous project's activities. In these sit uations, the customer r equirements could conflict with the relevant stakeholders' nee ds, expectations, constraints, and interfaces and will need to be transfo rmed into the recognized set of customer requirements after appropr iate resolution of conflicts. Relevant stakeholders representing all phases of the product's lifecycle should include business as well as te chnical functions. In this way, concepts for all product-related lif ecycle processes are considered concurrently with the concepts for the products. Customer requirements result from informed decisions on the business as well as technical effects of their requirements. Typical Work Products 1. Customer requirements 2. Customer constraints on the conduct of verification 3. Customer constraints on the conduct of validation",
        "CMMI for Development Version 1.2 Requirements Development (RD) 394 Subpractices 1. Translate the stakeholder needs, expectations, constraints, and interfaces into documented customer requirements. 2. Define constraints for verification and validation. SG 2 Develop Product Requirements Customer requirements are refined and elaborated to develop product and product component requirements. Customer requirements are analyz ed in conjunction with the development of the operational conc ept to derive more detailed and precise sets of requirements called “product and product component requirements.” Product and product component requirements address the needs associated with each pr oduct lifecycle phase. Derived requirements arise from constraints, consideration of is sues implied but not explicitly stated in the custom er requirements baseline, and factors introduced by the selected architec ture, the design, and the developer’s unique business considerations. The requirements are reexamined with each successive, lower level set of requirements and functional architecture, and the preferred product concept is refined. The requirements are allocated to product functions and product components including objects, people, and processes. The traceability of requirements to functions, objects, tests, issues, or other entities is documented. The allocated requirement s and functions are the basis for the synthesis of the technical solu tion. As internal components are developed, additional interfaces ar e defined and interface requirements are established. Refer to the Maintain Bidirectional Traceability of Requirements specific practice of the Requirements Management process area for more information about maintaini ng bidirectional traceability. SP 2.1 Establish Product and Product Component Requirements Establish and maintain product and product component requirements, which are based on the customer requirements. The customer requirements may be ex pressed in the customer’s terms and may be nontechnical descriptions. The product requirements are the expression of these requirement s in technical terms that can be used for design decis ions. An example of this translation is found in the first House of Quality Function D eployment, which maps customer desires into technical parameters. For instance, “solid sounding door” might be mapped to size, weight, fit, dampening, and resonant frequencies.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 395Product and product component requi rements address the satisfaction of customer, business, and project objectives and associated attributes, such as effectivene ss and affordability. Derived requirements also address the cost and performance of other lifecycle phases (e.g., production, operations, and disposal) to the extent compatible with business objectives. The modification of requirements due to approved requirement changes is covered by the “maintain” function of this specific practice; whereas, the administration of requirement changes is covered by the Requirements Management process area. Refer to the Requirements Management process area for more information about managing changes to requirements. Typical Work Products 1. Derived requirements 2. Product requirements 3. Product component requirements Subpractices 1. Develop requirements in techni cal terms necessary for product and product component design. Develop architecture requirements addressing critical product qualities and performance necessary for product architecture design. 2. Derive requirements that result from design decisions. Refer to the Technical Solution pr ocess area for more information about developing the solutions t hat generate additional derived requirements. Selection of a technology brings with it additional requirements. For instance, use of electronics requires additional technology-specific requirements such as electromagnetic interference limits. 3. Establish and maintain rela tionships between requirements for consideration during change management and requirements allocation. Refer to the Requirements Management process area for more information about maintaini ng requirements traceability. Relationships between requirements can aid in evaluating the impact of changes.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 396 SP 2.2 Allocate Product Component Requirements Allocate the requirements for each product component. Refer to the Technical Solution proc ess area for more information about allocation of requirements to pr oducts and product components. This specific practice provides inform ation for defining the allocation of requirements but must interact with the specific practices in the Technical Solution process area to establish solutions to which the requirements are allocated. The requirements for product co mponents of the defined solution include allocation of product perform ance; design constraints; and fit, form, and function to meet requireme nts and facilitate production. In cases where a higher level requirement specifies performance that will be the responsibility of two or more product components, the performance must be partitioned for unique allocation to each product component as a derived requirement. Typical Work Products 1. Requirement allocation sheets 2. Provisional requirement allocations 3. Design constraints 4. Derived requirements 5. Relationships among derived requirements Subpractices 1. Allocate requirements to functions. 2. Allocate requirements to product components. 3. Allocate design constr aints to product components. 4. Document relationships among allocated requirements. Relationships include dependencies in which a change in one requirement may affect other requirements. SP 2.3 Identify Interface Requirements Identify interface requirements. Interfaces between func tions (or between objects) are identified. Functional interfaces may drive the de velopment of alternative solutions described in the Technical Solution process area. Refer to the Product Integration process area for more information about the management of interfaces and the integration of products and product components.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 397Interface requirements between products or product components identified in the product architecture are defined. They are controlled as part of product and product componen t integration and are an integral part of the architecture definition. Typical Work Products 1. Interface requirements Subpractices 1. Identify interfaces bot h external to the product and internal to the product (i.e., between functi onal partitions or objects). As the design progresses, the product architecture will be altered by technical solution processes, creating new interfaces between product components and components external to the product. Interfaces with product-related lifecycle processes should also be identified. Examples of these interfaces include interfaces with test equipment, transportation systems, support systems, and manufacturing facilities. 2. Develop the requirements fo r the identified interfaces. Refer to the Technical Solution pr ocess area for more information about generating new interfaces during the design process. Requirements for interfaces are defined in terms such as origination, destination, stimulus, data characteristics for software, and electrical and mechanical characteristics for hardware. SG 3 Analyze and Validate Requirements The requirements are analyzed and validated, and a definition of required functionality is developed. The specific practices of the Analyze and Validate Requirements specific goal support the development of the requirements in both the Develop Customer Requirements s pecific goal and the Develop Product Requirements specific goal . The specific practices associated with this specific goal cover analyzing and validating the requirements with respect to the user’s intended environment. Analyses are performed to det ermine what impact the intended operational environment will have on the ability to satisfy the stakeholders’ needs, expectations, constraints, and interfaces. Considerations, such as feasibilit y, mission needs, cost constraints, potential market size, and acquisition strategy, must all be taken into account, depending on the product co ntext. A definition of required functionality is also established. All specified us age modes for the product are considered, and a time line analysis is generated for time- critical sequencing of functions.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 398 The objectives of the analyses are to determi ne candidate requirements for product concepts that will sati sfy stakeholder ne eds, expectations, and constraints; and then to translate these concepts into requirements. In parallel with this activity, the par ameters that will be used to evaluate the effectiveness of the product ar e determined based on customer input and the preliminary product concept. Requirements are validated to increase the probability that the resulting product will perform as int ended in the use environment. SP 3.1 Establish Operational Concepts and Scenarios Establish and maintain operational concepts and associated scenarios. A scenario is typically a sequence of events that might occur in the use of the product, which is used to make explicit some of the needs of the stakeholders. In contrast, an operati onal concept for a product usually depends on both the design solution and the scenario. For example, the operational concept for a satellit e-based communications product is quite different from one based on l andlines. Since the alternative solutions have not usually been defin ed when preparing the initial operational concepts, conceptual so lutions are developed for use when analyzing the requirements. The oper ational concepts are refined as solution decisions are made and lowe r level detailed requirements are developed. Just as a design decision for a pr oduct may become a requirement for product components, the operat ional concept may become the scenarios (requirements) for produc t components. Operational concepts and scenarios are evolved to facili tate the selection of product component solutions that, when impl emented, will satisfy the intended use of the product. Operational c oncepts and scenarios document the interaction of the product component s with the environment, users, and other product components, regardle ss of engineering discipline. They should be documented for all modes and states within operations, product deployment, delivery, s upport (including maintenance and sustainment), training, and disposal. The scenarios may include operati onal sequences, provided those sequences are an expression of cust omer requirements rather than operational concepts. Typical Work Products 1. Operational concept 2. Product or product component installation, operational, maintenance, and support concepts 3. Disposal concepts",
        "CMMI for Development Version 1.2 Requirements Development (RD) 3994. Use cases 5. Timeline scenarios 6. New requirements Subpractices 1. Develop operational conc epts and scenarios that include functionality, performance, main tenance, support, and disposal as appropriate. Identify and develop scenarios, consistent with the level of detail in the stakeholder needs, expectations, and constraints in which the proposed product or product component is expected to operate. 2. Define the environment in whic h the product or product component will operate, including boundar ies and constraints. 3. Review operational concepts and scenarios to refine and discover requirements. Operational concept and scenario development is an iterative process. The reviews should be held periodically to ensure that they agree with the requirements. The review may be in the form of a walkthrough. 4. Develop a detailed operational concept, as products and product components are selected, that def ines the interaction of the product, the end user, and the env ironment, and that satisfies the operational, maintenance, support, and disposal needs. SP 3.2 Establish a Definition of Required Functionality Establish and maintain a definition of required functionality. The definition of functionality, also re ferred to as “functional analysis,” is the description of what the product is intended to do. The definition of functionality can include actions, s equence, inputs, out puts, or other information that communicates the manner in which the product will be used. Functional analysis is not the same as structured analysis in software development and does not presume a functionally oriented software design. In object-oriented software desi gn, it relates to defining what are called “services” or “methods.” The def inition of functions, their logical groupings, and their association with requirements is referred to as a functional architecture. (See the definit ion of “functional architecture” in the glossary.) Typical Work Products 1. Functional architecture 2. Activity diagrams and use cases",
        "CMMI for Development Version 1.2 Requirements Development (RD) 400 3. Object-oriented analysis with services or methods identified Subpractices 1. Analyze and quantify functionality required by end users. 2. Analyze requirements to identif y logical or functional partitions (e.g., subfunctions). 3. Partition requirements into gr oups, based on established criteria (e.g., similar functionality, performan ce, or coupling), to facilitate and focus the requirements analysis. 4. Consider the sequencing of time-c ritical functions both initially and subsequently during product component development. 5. Allocate customer requirement s to functional partitions, objects, people, or support elements to s upport the synthesis of solutions. 6. Allocate functional and perform ance requirements to functions and subfunctions. SP 3.3 Analyze Requirements Analyze requirements to ensure that they are necessary and sufficient. In light of the operational concept and scenarios, the requirements for one level of the product hierarch y are analyzed to determine whether they are necessary and sufficient to m eet the objectives of higher levels of the product hierarchy. The anal yzed requirements then provide the basis for more detailed and precise re quirements for lower levels of the product hierarchy. As requirements are defined, thei r relationship to higher level requirements and the higher leve l defined functionality must be understood. One of the ot her actions is the deter mination of which key requirements will be used to track prog ress. For instance, the weight of a product or size of a software product may be monitored through development based on its risk. Refer to the Verification process area for more information about verification methods that could be used to support this analysis. Typical Work Products 1. Requirements defects reports 2. Proposed requirements changes to resolve defects 3. Key requirements 4. Technical performance measures",
        "CMMI for Development Version 1.2 Requirements Development (RD) 401Subpractices 1. Analyze stakeholder needs, expecta tions, constraints, and external interfaces to remove conflicts and to organize into related subjects. 2. Analyze requirements to deter mine whether they satisfy the objectives of higher level requirements. 3. Analyze requirements to ensure t hat they are complete, feasible, realizable, and verifiable. While design determines the feasibility of a particular solution, this subpractice addresses knowing which requirements affect feasibility. 4. Identify key requirements that have a strong influence on cost, schedule, functionality, risk, or performance. 5. Identify technical per formance measures that will be tracked during the development effort. Refer to the Measurement and Analysis process area for more information about the use of measurements. 6. Analyze operational concepts and scenarios to refine the customer needs, constraints, and interf aces and to discover new requirements. This analysis may result in more detailed operational concepts and scenarios as well as supporting the derivation of new requirements. SP 3.4 Analyze Requirements to Achieve Balance Analyze requirements to balance stakeholder needs and constraints. Stakeholder needs and constraint s can address cost, schedule, performance, functionality, reusabl e components, maintainability, or risk. Typical Work Products 1. Assessment of risks related to requirements Subpractices 1. Use proven models, simulations , and prototyping to analyze the balance of stakeholder ne eds and constraints. Results of the analyses can be used to reduce the cost of the product and the risk in developing the product. 2. Perform a risk assessment on the requirements and functional architecture.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 402 Refer to the Risk Management proc ess area for information about performing a risk assessment on customer and product requirements and the functional architecture. 3. Examine product lif ecycle concepts for impac ts of requirements on risks. SP 3.5 Validate Requirements Validate requirements to ensure the resulting product will perform as intended in the user's environment. Requirements validation is performed early in the development effort with end users to gain confidence t hat the requirements are capable of guiding a development that results in successful final validation. This activity should be integrated with risk management activities. Mature organizations will typically perform requirements validation in a more sophisticated way using multiple techniques and will broaden the basis of the validation to include other stakeholder needs and expectations. Examples of techniques used for requirements validation include the following: • Analysis • Simulations • Prototyping • Demonstrations Typical Work Products 1. Record of analysis methods and results Subpractices 1. Analyze the requirements to det ermine the risk that the resulting product will not perform approp riately in its intended-use environment. 2. Explore the adequacy and completeness of requirements by developing product representations (e.g., prototypes, simulations, models, scenarios, and storyboards) and by obtaining feedback about them from rele vant stakeholders. Refer to the Validation process area for information about preparing for and performing validation on products and product components. 3. Assess the design as it matures in the context of the requirements validation environment to identif y validation i ssues and expose unstated needs and customer requirements.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 403Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the requirements development process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the requirements development process. Elaboration: This policy establishes organizati onal expectations for collecting stakeholder needs, formulating product and product component requirements, and analyzing and va lidating those requirements. GP 2.2 Plan the Process Establish and maintain the plan for performing the requirements development process. Elaboration: This plan for performing the requir ements development process can be part of (or referenced by) the projec t plan as described in the Project Planning process area.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 404 GP 2.3 Provide Resources Provide adequate resources for performing the requirements development process, developing the work products, and providing the services of the process. Elaboration: Special expertise in the applicat ion domain, methods for eliciting stakeholder needs, and methods and t ools for specifying and analyzing customer, product, and product component requirements may be required. Examples of other resources provided include the following tools: • Requirements specification tools • Simulators and modeling tools • Prototyping tools • Scenario definition and management tools • Requirements tracking tools GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the requirements development process. GP 2.5 Train People Train the people performing or supporting the requirements development process as needed. Elaboration: Examples of training topics include the following: • Application domain • Requirements definition and analysis • Requirements elicitation • Requirements specification and modeling • Requirements tracking GP 2.6 Manage Configurations Place designated work products of the requirements development process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 405Elaboration: Examples of work products placed under control include the following: • Customer requirements • Functional architecture • Product and product component requirements • Interface requirements GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the requirements development process as planned. Elaboration: Select relevant stakeholders from customers, end users, developers, producers, testers, suppliers, ma rketers, maintainers, disposal personnel, and others who may be affected by, or may affect, the product as well as the process. Examples of activities for stakeholder involvement include the following: • Reviewing the adequacy of requirements in meeting needs, expectations, constraints, and interfaces • Establishing operational concepts and scenarios • Assessing the adequacy of requirements • Establishing product and product component requirements • Assessing product cost, schedule, and risk GP 2.8 Monitor and Control the Process Monitor and control the requirements development process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Cost, schedule, and effort expended for rework • Defect density of requirements specifications • Schedule for activities to develop a set of requirements.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 406 GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the requirements development process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Collecting stakeholder needs • Formulating product and product component requirements • Analyzing and validating product and product component requirements Examples of work products reviewed include the following: • Product requirements • Product component requirements • Interface requirements • Functional architecture GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the requirements development process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined requirements development process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the requirements development process to support the future use and improvement of the organization’s processes and process assets.",
        "CMMI for Development Version 1.2 Requirements Development (RD) 407Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • List of the requirements for a product that are found to be ambiguous • Number of requirements introduced at each phase of the project lifecycle • Lessons learned from the requirements allocation process Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the requirements development process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the requirements development process to achieve the established quantitative quality and process- performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the requirements development process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the requirements development process.",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 408 REQUIREMENTS MANAGEMENT An Engineering Process Area at Maturity Level 2 Purpose The purpose of Requirements Management (REQM) is to manage the requirements of the project’s products and product components and to identify inconsistencie s between those requirem ents and the project’s plans and work products. Introductory Notes Requirements management processes manage all requirements received or generated by the project, including both technical and nontechnical requirements as well as those requirements levied on the project by the organization. In particular, if the Requirements Development process area is implem ented, its processes will generate product and product com ponent requirements that will also be managed by the requirements management proc esses. Throughout the process areas, where we use the terms product and product component, their intended meanings also encompass services and their components. When the Requirements Management, Requirements Development, and Technical Solution process ar eas are all implemented, their associated processes may be closely tied and be performed concurrently. The project takes appropriate steps to ensure that the agreed-on set of requirements is managed to support the planning and execution needs of the project. When a project rece ives requirements from an approved requirements provider, the requirements are reviewed with the requirements provider to resolv e issues and prevent misunderstanding before the requirements are incorporat ed into the project’s plans. Once the requirements provider and the requirements receiver reach an agreement, commitment to the r equirements is obtained from the project participants. The project m anages changes to the requirements as they evolve and identifies any in consistencies that occur among the plans, work products, and requirements. Part of the management of require ments is to document requirements changes and rationale and to maintain bidirectional traceability between source requirements and all product and product component requirements (See the definition of “bidirectional traceability” in the glossary.)",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 409All development projects have requirem ents. In the case of a project that is focused on maintenance activi ties, the changes to the product or product components are based on changes to the existing requirements, design, or implement ation. The requirements changes, if any, might be documented in change r equests from the customer or users, or they might take the form of new requirements received from the requirements development process. Regardless of their source or form, the maintenance activities t hat are driven by changes to requirements are managed accordingly. Related Process Areas Refer to the Requirements Development process area for more information about transforming stakeholder needs into product requirements and deciding how to allo cate or distribute requirements among the product components. Refer to the Technical Solution proc ess area for more information about transforming requirements into technical solutions. Refer to the Project Planning proc ess area for more information about how project plans reflect requi rements and need to be revised as requirements change. Refer to the Configuration Management process area for more information about base lines and controlling chang es to configuration documentation for requirements. Refer to the Project Monitoring and Control process area for more information about tracking and cont rolling the activities and work products that are based on the requirements and taking appropriate corrective action. Refer to the Risk Management process area for more information about identifying and handling risks associated with requirements. Specific Goal and Practice Summary SG 1 Manage Requirements SP 1.1 Obtain an Understanding of Requirements SP 1.2 Obtain Commitment to Requirements SP 1.3 Manage Requirements Changes SP 1.4 Maintain Bidirectional Traceability of Requirements SP 1.5 Identify Inconsistencies Between Project Work and Requirements",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 410 Specific Practices by Goal SG 1 Manage Requirements Requirements are managed and inconsistencies with project plans and work products are identified. The project maintains a current and approved set of requirements over the life of the project by doing the following: • Managing all changes to the requirements • Maintaining the relationships am ong the requirements, the project plans, and the work products • Identifying inconsist encies among the require ments, the project plans, and the work products • Taking corrective action Refer to the Technical Solution proc ess area for more information about determining the feasibility of the requirements. Refer to the Requirements Development process area for more information about ensuring that t he requirements reflect the needs and expectations of the customer. Refer to the Project Monitoring and Control process area for more information about taki ng corrective action. SP 1.1 Obtain an Understanding of Requirements Develop an understanding with the requirements providers on the meaning of the requirements. As the project matures and requirement s are derived, all activities or disciplines will receive requirement s. To avoid requirements creep, criteria are established to designat e appropriate channels, or official sources, from which to receive requ irements. The receiving activities conduct analyses of the r equirements with the requirements provider to ensure that a compatible, shar ed understanding is reached on the meaning of the requirements. The resu lt of this analysis and dialog is an agreed-to set of requirements. Typical Work Products 1. Lists of criteria for dist inguishing appropriate requirements providers 2. Criteria for evaluation and acceptance of requirements 3. Results of analyses against criteria 4. An agreed-to set of requirements",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 411Subpractices 1. Establish criteria for disti nguishing appropriate requirements providers. 2. Establish objective criteria for the evaluation and acceptance of requirements. Lack of evaluation and acceptance criteria often results in inadequate verification, costly rework, or customer rejection. Examples of evaluation and acceptance criteria include the following: • Clearly and properly stated • Complete • Consistent with each other • Uniquely identified • Appropriate to implement • Verifiable (testable) • Traceable 3. Analyze requirements to ensure that the established criteria are met. 4. Reach an understanding of the r equirements with the requirements provider so that the project participants can commit to them. SP 1.2 Obtain Commitment to Requirements Obtain commitment to the requirements from the project participants. Refer to the Project Monitoring and Control process area for more information about monitoring the commitments made. IPPD Addition When integrated teams are formed, the project participants are the integrated teams and their members. Commitment to the requirement for interacting with other integrated team s is as important for each integrated team as its commitments to product and other project requirements. Whereas the previous specific practice dealt with reaching an understanding with the requirements pr oviders, this specific practice deals with agreements and commitments among those who have to carry out the activities necessary to implement the requirements. Requirements evolve throughout the project, especially as described by the specific practices of the R equirements Development process area and the Technical Solution process area. As the requirements evolve,",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 412 this specific practice ensures that project parti cipants commit to the current, approved requirements and the resulting changes in project plans, activities, and work products. Typical Work Products 1. Requirements impact assessments 2. Documented commitments to requirements and requirements changes Subpractices 1. Assess the impact of require ments on existing commitments. The impact on the project participants should be evaluated when the requirements change or at the start of a new requirement. 2. Negotiate and record commitments. Changes to existing commitments should be negotiated before project participants commit to the requirement or requirement change. SP 1.3 Manage Requirements Changes Manage changes to the requirements as they evolve during the project. Refer to the Configuration Management process area for more information about maintaining and co ntrolling the requi rements baseline and on making the requirements and change data available to the project. During the project, requirements c hange for a variety of reasons. As needs change and as work proceeds , additional requirements are derived and changes may have to be made to the existing requirements. It is essential to manage these additions and changes efficiently and effectively. To e ffectively analyze the impact of the changes, it is necessary that the s ource of each requirement is known and the rationale for any change is documented. The project manager may, however, want to track appropr iate measures of requirements volatility to judge whether new or revised controls are necessary. Typical Work Products 1. Requirements status 2. Requirements database 3. Requirements decision database",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 413Subpractices 1. Document all requirements and requirements changes that are given to or generated by the project. 2. Maintain the requirements change history with the rationale for the changes. Maintaining the change history helps track requirements volatility. 3. Evaluate the impact of requirem ent changes from the standpoint of relevant stakeholders. 4. Make the requirements and c hange data available to the project. SP 1.4 Maintain Bidirectional Traceability of Requirements Maintain bidirectional traceability among the requirements and work products. The intent of this spec ific practice is to maintain the bidirectional traceability of requirements for eac h level of product decomposition. (See the definition of “bidirectional traceability” in the glossary.) When the requirements are managed well, traceability can be established from the source requirement to its lower level requirements and from the lower level requirements back to their source. Such bidirectional traceability helps determine that all source requirements have been completely addressed and that all lower level requirements can be traced to a valid source. Requirements traceability can also co ver the relationships to other entities such as intermediate and final work products, changes in design documentation, and test plans. The traceability can cover horizontal relationships, such as across in terfaces, as well as vertical relationships. Traceability is par ticularly needed in conducting the impact assessment of requirements changes on the project's activities and work products. Typical Work Products 1. Requirements traceability matrix 2. Requirements tracking system Subpractices 1. Maintain requirements traceabilit y to ensure that the source of lower level (derived) requirements is documented. 2. Maintain requirements traceability from a requirement to its derived requirements and allocation to f unctions, interfaces, objects, people, processes, and work products. 3. Generate the requireme nts traceability matrix.",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 414 SP 1.5 Identify Inconsistencies Between Project Work and Requirements Identify inconsistencies between the project plans and work products and the requirements. Refer to the Project Monitoring and Control process area for more information about moni toring and controlling the project plans and work products for consistency with requirements and taking corrective actions when necessary. This specific practice finds the inconsistenc ies between the requirements and the project plans and work products and initiates the corrective action to fix them. Typical Work Products 1. Documentation of inconsistencies includi ng sources, conditions, and rationale 2. Corrective actions Subpractices 1. Review the project’s plans, activities, and work products for consistency with the requirements and the changes made to them. 2. Identify the source of the inconsistency and the rationale. 3. Identify changes t hat need to be made to the plans and work products resulting from changes to the requirements baseline. 4. Initiate corrective actions. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the requirements management process to develop work products and provide services to achieve the specific goals of the process area.",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 415 GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the requirements management process. Elaboration: This policy establishes organiza tional expectations for managing requirements and identifying inconsis tencies between the requirements and the project plans and work products. GP 2.2 Plan the Process Establish and maintain the plan for performing the requirements management process. Elaboration: This plan for performing the requ irements management process can be part of (or referenced by) the projec t plan as described in the Project Planning process area. GP 2.3 Provide Resources Provide adequate resources for performing the requirements management process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Requirements tracking tools • Traceability tools GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the requirements management process. GP 2.5 Train People Train the people performing or supporting the requirements management process as needed.",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 416 Elaboration: Examples of training topics include the following: • Application domain • Requirements definition, analysis, review, and management • Requirements management tools • Configuration management • Negotiation and conflict resolution GP 2.6 Manage Configurations Place designated work products of the requirements management process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Requirements • Requirements traceability matrix GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the requirements management process as planned. Elaboration: Select relevant stakeholders from customers, end users, developers, producers, testers, suppliers, ma rketers, maintainers, disposal personnel, and others who may be affected by, or may affect, the product as well as the process. Examples of activities for stakeholder involvement include the following: • Resolving issues on the understanding of the requirements • Assessing the impact of requirements changes • Communicating the bidirectional traceability • Identifying inconsistencies among project plans, work products, and requirements GP 2.8 Monitor and Control the Process Monitor and control the requirements management process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 417Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Requirements volatility (percentage of requirements changed) • Schedule for coordination of requirements • Schedule for analysis of a proposed requirements change GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the requirements management process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Managing requirements • Identifying inconsistencies among project plans, work products, and requirements Examples of work products reviewed include the following: • Requirements • Requirements traceability matrix GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the requirements management process with higher level management and resolve issues. Elaboration: Proposed changes to commitments to be made external to the organization are reviewed with higher level management to ensure that all commitments can be accomplished. Staged Only GG3 and its practices do not apply for a maturity level 2 rating, but do apply for a maturity level 3 rating and above.",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 418 Continuous/Maturity Levels 3 - 5 Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined requirements management process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the requirements management process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Requirements traceability matrix • Number of unfunded requirements changes after baselining • Lessons learned in resolving ambiguous requirements Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the requirements management process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the requirements management process to achieve the established quantitative quality and process- performance objectives.",
        "CMMI for Development Version 1.2 Requirements Management (REQM) 419Continuous Only GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the requirements management process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the requirements management process.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 420 RISK MANAGEMENT A Project Management Process Area at Maturity Level 3 Purpose The purpose of Risk Management (RSKM) is to identify potential problems before they occur so that risk-handling activities can be planned and invoked as needed across the life of the product or project to mitigate adverse impact s on achieving objectives. Introductory Notes Risk management is a continuous, forw ard-looking process that is an important part of management. Ri sk management should address issues that could endang er achievement of cr itical objectives. A continuous risk management approach is applied to effectively anticipate and mitigate the risks that may have a critical impact on the project. Effective risk management includes early and aggressive risk identification through the collaborat ion and involvement of relevant stakeholders, as described in t he stakeholder involvement plan addressed in the Project Planning pr ocess area. Strong leadership across all relevant stakeholders is needed to establish an environment for the free and open disclosure and discussion of risk. Risk management must consider both internal and external sources for cost, schedule, and performance risk as well as other risks. Early and aggressive detection of risk is import ant because it is typically easier, less costly, and less disruptive to make changes and correct work efforts during the earlier, rather t han the later, phases of the project. Risk management can be divided into three parts: defining a risk management strategy; identifying and analyzing risks; and handling identified risks, including the implem entation of risk mitigation plans when needed. As represented in the Project Pl anning and Project Monitoring and Control process areas, organizations may initially focu s simply on risk identification for awareness, and react to the realization of these risks as they occur. The Risk Management process area describes an evolution of these specif ic practices to systemat ically plan, anticipate, and mitigate risks to proactively mini mize their impact on the project.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 421Although the primary emphasis of the Risk Management process area is on the project, the concept s can also be applied to manage organizational risks. Related Process Areas Refer to the Project Planning proc ess area for more information about identification of project risks and pl anning for involvement of relevant stakeholders. Refer to the Project Monitoring and Control process area for more information about moni toring project risks. Refer to the Decision Analysis and Resolution process area for more information about using a formal evaluation process to evaluate alternatives for selection and mitigation of identified risks. Specific Goal and Practice Summary SG 1 Prepare for Risk Management SP 1.1 Determine Risk Sources and Categories SP 1.2 Define Risk Parameters SP 1.3 Establish a Risk Management Strategy SG 2 Identify and Analyze Risks SP 2.1 Identify Risks SP 2.2 Evaluate, Categorize, and Prioritize Risks SG 3 Mitigate Risks SP 3.1 Develop Risk Mitigation Plans SP 3.2 Implement Risk Mitigation Plans Specific Practices by Goal SG 1 Prepare for Risk Management Preparation for risk management is conducted. Preparation is conducted by establishing and maintaining a strategy for identifying, analyzing, an d mitigating risks. This is typically documented in a risk management plan. The risk management strategy addresses the specific actions and management approach used to apply and control the risk management program . This includes identifying the sources of risk; the scheme us ed to categorize risks; and the parameters used to evaluate, bound, and control risks for effective handling. SP 1.1 Determine Risk Sources and Categories Determine risk sources and categories. Identification of risk sources provides a basis for systematically examining changing situations over ti me to uncover circumstances that",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 422 impact the ability of the project to m eet its objectives. Risk sources are both internal and external to the pr oject. As the project progresses, additional sources of risk may be ident ified. Establishing categories for risks provides a mechanism for co llecting and organizing risks as well as ensuring appropriate scrutiny and management attention for those risks that can have more serious consequences on meeting project objectives. Typical Work Products 1. Risk source lists (external and internal) 2. Risk categories list Subpractices 1. Determine risk sources. Risk sources are the fundamental drivers that cause risks within a project or organization. There are many sources of risks, both internal and external, to a project. Risk sources identify common areas where risks may originate. Typical internal and external risk sources include the following: • Uncertain requirements • Unprecedented efforts—estimates unavailable • Infeasible design • Unavailable technology • Unrealistic schedule estimates or allocation • Inadequate staffing and skills • Cost or funding issues • Uncertain or inadequate subcontractor capability • Uncertain or inadequate vendor capability • Inadequate communication with actual or potential customers or with their representatives • Disruptions to continuity of operations Many of these sources of risk are often accepted without adequate planning. Early identification of both internal and external sources of risk can lead to early identification of risks. Risk mitigation plans can then be implemented early in the project to preclude occurrence of the risks or reduce the consequences of their occurrence. 2. Determine risk categories. Risk categories reflect the “bins” for collecting and organizing risks. A reason for identifying risk categories is to help in the future consolidation of the activities in the risk mitigation plans.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 423The following factors may be considered when determining risk categories: • The phases of the project’s lifecycle model (e.g., requirements, design, manufacturing, test and evaluation, delivery, and disposal) • The types of processes used • The types of products used • Program management risks (e.g., contract risks, budget/cost risks, schedule risks, resources risks, performance risks, and supportability risks) A risk taxonomy can be used to provide a framework for determining risk sources and categories. SP 1.2 Define Risk Parameters Define the parameters used to analyze and categorize risks, and the parameters used to control the risk management effort. Parameters for evaluating, categor izing, and prioritizing risks include the following: • Risk likelihood (i.e., probability of risk occurrence) • Risk consequence (i.e., impact and se verity of risk occurrence) • Thresholds to trigger management activities Risk parameters are used to provide common and consistent criteria for comparing the various risks to be managed. Without these parameters, it would be very difficult to gauge the severity of the unwanted change caused by the risk and to prioritize the necessary actions required for risk mitigation planning. Typical Work Products 1. Risk evaluation, categorizati on, and prioritization criteria 2. Risk management requirements (e.g ., control and approval levels, and reassessment intervals) Subpractices 1. Define consistent criteria for evaluating and quantifying risk likelihood and severity levels. Consistently used criteria (e.g., the bounds on the likelihood and severity levels) allow the impacts of different risks to be commonly understood, to receive the appropriate level of scrutiny, and to obtain the management attention warranted. In managing dissimilar risks (e.g., personnel safety versus environmental pollution), it is important to ensure consistency in end result (e.g., a high risk of environmental pollution is as important as a high risk to personnel safety). 2. Define thresholds for each risk category.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 424 For each risk category, thresholds can be established to determine acceptability or unacceptability of risks, prioritization of risks, or triggers for management action. Examples of thresholds include the following: • Project-wide thresholds could be established to involve senior management when product costs exceed 10 percent of the ta rget cost or when Cost Performance Indexes (CPIs) fall below 0.95. • Schedule thresholds could be established to involve senior management when Schedule Performance Indexes (SPIs) fall below 0.95. • Performance thresholds could be set to involve senior management when specified key items (e.g., processor utilization or average response times) exceed 125 percent of the intended design. These may be refined later, for each identified risk, to establish points at which more aggressive risk monitoring is employed or to signal the implementation of risk mitigation plans. 3. Define bounds on the extent to which thresholds are applied against or within a category. There are few limits to which risks can be assessed in either a quantitative or qualitative fashion. Definition of bounds (or boundary conditions) can be used to help scope the extent of the risk management effort and avoid excessive resource expenditures. Bounds may include exclusion of a risk source from a category. These bounds can also exclude any condition that occurs less than a given frequency. SP 1.3 Establish a Risk Management Strategy Establish and maintain the strategy to be used for risk management. A comprehensive risk management strategy addresses items such as the following: • The scope of the risk management effort • Methods and tools to be used for ri sk identification, risk analysis, risk mitigation, risk monitoring, and communication • Project-specific sources of risks • How these risks are to be organi zed, categorized, compared, and consolidated • Parameters, including likelihood, consequence, and thresholds, for taking action on identified risks • Risk mitigation techniques to be used, such as prototyping, piloting, simulation, alternative designs , or evolutionary development • Definition of risk measures to monitor the status of the risks • Time intervals for risk monitoring or reassessment",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 425The risk management strategy shoul d be guided by a common vision of success that describes the desired fu ture project outcomes in terms of the product that is delivered, its co st, and its fitness for the task. The risk management strategy is often doc umented in an organizational or a project risk management plan. The risk management strategy is reviewed with relevant stakeholde rs to promote commitment and understanding. Typical Work Products 1. Project risk management strategy SG 2 Identify and Analyze Risks Risks are identified and analyzed to determine their relative importance. The degree of risk impacts the resources assigned to handle an identified risk and the determinati on of when appropriate management attention is required. Analyzing risks entails identifying risks from the internal and external sources identified and then evaluati ng each identified risk to determine its likelihood and consequenc es. Categorization of the risk, based on an evaluation against the establis hed risk categories and criteria developed for the risk management stra tegy, provides the information needed for risk handling. Related risks may be grouped for efficient handling and effective use of risk management resources. SP 2.1 Identify Risks Identify and document the risks. IPPD Addition The particular risks associated with conducting the project using integrated teams should be considered, such as risks associated with loss of inter- team or intra-team coordination. The identification of potential issues, hazards, threats, and vulnerabilities that could negatively affect work ef forts or plans is the basis for sound and successful ri sk management. Risks must be identified and described in an unders tandable way before they can be analyzed and managed properly. Risk s are documented in a concise statement that includes the cont ext, conditions, and consequences of risk occurrence. Risk identification should be an org anized, thorough approach to seek out probable or realistic risks in ac hieving objectives. To be effective, risk identification should not be an attempt to address every possible event regardless of how highly impr obable it may be. Use of the categories and parameters developed in the risk management strategy,",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 426 along with the identified sources of risk, can provide the discipline and streamlining appropriate to risk identific ation. The identified risks form a baseline to initiate risk management activities. The list of risks should be reviewed periodically to reexamine possible sources of risk and changing conditions to uncover source s and risks previously overlooked or nonexistent when the risk manage ment strategy was last updated. Risk identification activities focus on the identification of risks, not placement of blame. The results of ri sk identification ac tivities are not used by management to evaluate t he performance of individuals. There are many methods for identif ying risks. Typical identification methods include the following: • Examine each element of the proj ect work breakdown structure to uncover risks. • Conduct a risk assessment using a risk taxonomy. • Interview subject matter experts. • Review risk management efforts from similar products. • Examine lessons-learn ed documents or databases. • Examine design specificati ons and agreement requirements. Typical Work Products 1. List of identified risks, in cluding the context, conditions, and consequences of ri sk occurrence Subpractices 1. Identify the risks associated wi th cost, schedule, and performance. Cost, schedule, and performance risks should be examined to the extent that they impact project objectives. There may be potential risks discovered that are outside the scope of the project’s objectives but vital to customer interests. For example, the risks in development costs, product acquisition costs, cost of spare (or replacement) products, and product disposition (or disposal) costs have design implications. The customer may not have considered the full cost of supporting a fielded product or using a delivered service. The customer should be informed of such risks, but actively managing those risks may not be necessary. The mechanisms for making such decisions should be examined at project and organization levels and put in place if deemed appropriate, especially for risks that impact the ability to verify and validate the product. In addition to the cost risks identified above, other cost risks may include those associated with funding levels, funding estimates, and distributed budgets. Schedule risks may include risks associated with planned activities, key events, and milestones.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 427Performance risks may include risks associated with the following: • Requirements • Analysis and design • Application of new technology • Physical size • Shape • Weight • Manufacturing and fabrication • Functional performance and operation • Verification • Validation • Performance maintenance attributes Performance maintenance attributes are those characteristics that enable an in- use product or service to provide originally required performance, such as maintaining safety and security performance. There are other risks that do not fall into cost, schedule, or performance categories. Examples of these other risks include the following: • Risks associated with strikes • Diminishing sources of supply • Technology cycle time • Competition 2. Review environmental element s that may impact the project. Risks to a project that frequently are missed include those supposedly outside the scope of the project (i.e., the project does not control whether they occur but can mitigate their impact), such as weather, natural or manmade disasters that affect continuity of operations, political changes, and telecommunications failures. 3. Review all elements of the wo rk breakdown structure as part of identifying risks to help ensure that all aspects of the work effort have been considered. 4. Review all elements of the projec t plan as part of identifying risks to help ensure that all aspects of the project have been considered. Refer to the Project Planning process area for more information about identifying project risks.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 428 5. Document the context, condi tions, and potential consequences of the risk. Risks statements are typically documented in a standard format that contains the risk context, conditions, and consequences of occurrence. The risk context provides additional information such that the intent of the risk can be easily understood. In documenting the context of the risk, consider the relative time frame of the risk, the circumstances or conditions surrounding the risk that has brought about the concern, and any doubt or uncertainty. 6. Identify the releva nt stakeholders associ ated with each risk. SP 2.2 Evaluate, Categorize, and Prioritize Risks Evaluate and categorize each identified risk using the defined risk categories and parameters, and determine its relative priority. The evaluation of risks is needed to assign relative importance to each identified risk, and is used in determining when appropriate management attention is required. Oft en it is useful to aggregate risks based on their interrelationships, and develop options at an aggregate level. When an aggregate risk is formed by a roll up of lower level risks, care must be taken to ensure that important lower level risks are not ignored. Collectively, the activities of ri sk evaluation, categorization, and prioritization are sometimes called “ri sk assessment” or “risk analysis.” Typical Work Products 1. List of risks, with a priority assigned to each risk Subpractices 1. Evaluate the identified risks using the defined risk parameters. Each risk is evaluated and assigned values in accordance with the defined risk parameters, which may include likelihood, consequence (severity, or impact), and thresholds. The assigned risk parameter values can be integrated to produce additional measures, such as risk exposure, which can be used to prioritize risks for handling. Often, a scale with three to five values is used to evaluate both likelihood and consequence. Likelihood, for example, can be categorized as remote, unlikely, likely, highly likely, or a near certainty.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 429Examples for consequences include the following: • Low • Medium • High • Negligible • Marginal • Significant • Critical • Catastrophic Probability values are frequently used to quantify likelihood. Consequences are generally related to cost, schedule, environmental impact, or human measures (e.g., labor hours lost and severity of injury). This evaluation is often a difficult and time-consuming task. Specific expertise or group techniques may be needed to assess the risks and gain confidence in the prioritization. In addition, priorities may require reevaluation as time progresses. 2. Categorize and group risks ac cording to the defined risk categories. Risks are categorized into the defined risk categories, providing a means to look at risks according to their source, taxonomy, or project component. Related or equivalent risks may be grouped for efficient handling. The cause-and-effect relationships between related risks are documented. 3. Prioritize risks for mitigation. A relative priority is determined for each risk based on the assigned risk parameters. Clear criteria should be used to determine the risk priority. The intent of prioritization is to determine the most effective areas to which resources for mitigation of risks can be applied with the greatest positive impact to the project. SG 3 Mitigate Risks Risks are handled and mitigated, where appropriate, to reduce adverse impacts on achieving objectives. The steps in handling risks incl ude developing risk-handling options, monitoring risks, and performing ri sk-handling activities when defined thresholds are exceeded. Risk mi tigation plans are developed and implemented for selected risks to proactively reduce the potential impact of risk occurrence. This c an also include contingency plans to deal with the impact of selected risks that may occur despite attempts to mitigate them. The risk paramete rs used to trigger risk-handling activities are defined by t he risk management strategy.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 430 SP 3.1 Develop Risk Mitigation Plans Develop a risk mitigation plan for the most important risks to the project as defined by the risk management strategy. A critical component of a risk mitigat ion plan is to develop alternative courses of action, workarounds, and fallback positions, with a recommended course of action for each critical risk. The risk mitigation plan for a given risk includes tec hniques and methods used to avoid, reduce, and control the pr obability of occurrence of the risk, the extent of damage incurred should the ri sk occur (sometimes called a “contingency plan”), or both. Risks are monitored and when they exceed the established th resholds, the risk mitigation plans are deployed to return the impacted effort to an acceptable risk level. If the risk cannot be mitigated, a contingency plan can be invoked. Both risk mitigation and contingenc y plans are often generated only for selected risks where the consequences of t he risks are determined to be high or unacceptable; other risks may be accepted and simply monitored. Options for handling risks typically in clude alternatives such as the following: • Risk avoidance: Changing or lowe ring requirements while still meeting the user’s needs • Risk control: Taking active steps to minimize risks • Risk transfer: Reallocating r equirements to lower the risks • Risk monitoring: Watching and periodi cally reevaluating the risk for changes to the assigned risk parameters • Risk acceptance: Acknowledgment of risk but not taking any action Often, especially for high risks, more than one approach to handling a risk should be generated. For example, in the case of an event that disrupts continuity of operations, approaches to risk management can include the following: • Resource reserves to respond to disruptive events • Lists of appropriate back-up equipment to be available • Back-up personnel for key personnel • Plans and results of/for testing emergency response systems • Posted procedures for emergencies • Disseminated lists of key contacts and information resources for emergencies In many cases, risks will be acc epted or watched. Ri sk acceptance is usually done when the risk is judged t oo low for formal mitigation, or when there appears to be no viable wa y to reduce the risk. If a risk is accepted, the rationale for this decision should be documented. Risks",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 431are watched when there is an objecti vely defined, verifiable, and documented threshold of performan ce, time, or risk exposure (the combination of likelihood and cons equence) that will trigger risk mitigation planning or invoke a contingency plan if it is needed. Adequate consideration should be given early to technology demonstrations, models, simulations, pilots, and prototypes as part of risk mitigation planning. Typical Work Products 1. Documented handling options for each identified risk 2. Risk mitigation plans 3. Contingency plans 4. List of those responsible for tracking and addressing each risk Subpractices 1. Determine the levels and thre sholds that define when a risk becomes unacceptable and triggers the execution of a risk mitigation plan or a contingency plan. Risk level (derived using a risk model) is a measure combining the uncertainty of reaching an objective with the consequences of failing to reach the objective. Risk levels and thresholds that bound planned or acceptable performance must be clearly understood and defined to provide a means with which risk can be understood. Proper categorization of risk is essential for ensuring appropriate priority based on severity and the associated management response. There may be multiple thresholds employed to initiate varying levels of management response. Typically, thresholds for the execution of risk mitigation plans are set to engage before the execution of contingency plans. 2. Identify the person or group re sponsible for addressing each risk. 3. Determine the cost-to-benefit ratio of implementing the risk mitigation plan for each risk. Risk mitigation activities should be examined for the benefits they provide versus the resources they will expend. Just like any other design activity, alternative plans may need to be developed and the costs and benefits of each alternative assessed. The most appropriate plan is then selected for implementation. At times the risk may be significant and the benefits small, but the risk must be mitigated to reduce the probability of incurring unacceptable consequences. 4. Develop an overall risk mitigation plan for the project to orchestrate the implementation of the individual risk mi tigation and contingency plans.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 432 The complete set of risk mitigation plans may not be affordable. A tradeoff analysis should be performed to prioritize the risk mitigation plans for implementation. 5. Develop contingency plans for sele cted critical risks in the event their impacts are realized. Risk mitigation plans are developed and implemented as needed to proactively reduce risks before they become problems. Despite best efforts, some risks may be unavoidable and will become problems that impact the project. Contingency plans can be developed for critical risks to describe the actions a project may take to deal with the occurrence of this impact. The intent is to define a proactive plan for handling the risk, either to reduce the risk (mitigation) or respond to the risk (contingency), but in either event to manage the risk. Some risk management literature may consider contingency plans a synonym or subset of risk mitigation plans. These plans also may be addressed together as risk-handling or risk action plans. SP 3.2 Implement Risk Mitigation Plans Monitor the status of each risk periodically and implement the risk mitigation plan as appropriate. To effectively control and manage ri sks during the work effort, follow a proactive program to regularly monitor risks and the status and results of risk-handling actions. The risk management strategy defines the intervals at which the risk status s hould be revisited. This activity may result in the discovery of new risks or new risk-handling options that can require replanning and re assessment. In either event, the acceptability thresholds associated with the risk should be compared against the status to determine the need for im plementing a risk mitigation plan. Typical Work Products 1. Updated lists of risk status 2. Updated assessments of risk likelihood, consequence, and thresholds 3. Updated lists of risk-handling options 4. Updated list of actions taken to handle risks 5. Risk mitigation plans Subpractices 1. Monitor risk status. After a risk mitigation plan is initiated, the risk is still monitored. Thresholds are assessed to check for the potential execution of a contingency plan.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 433A periodic mechanism for monitoring should be employed. 2. Provide a method for tracki ng open risk-handling action items to closure. Refer to the Project Monitoring and Control process area for more information about tracking action items. 3. Invoke selected risk-handling opt ions when monitored risks exceed the defined thresholds. Quite often, risk handling is only performed for those risks judged to be “high” and “medium.” The risk-handling strategy for a given risk may include techniques and methods to avoid, reduce, and control the likelihood of the risk or the extent of damage incurred should the risk (anticipated event or situation) occur or both. In this context, risk handling includes both risk mitigation plans and contingency plans. Risk-handling techniques are developed to avoid, reduce, and control adverse impact to project objectives and to bring about acceptable outcomes in light of probable impacts. Actions generated to handle a risk require proper resource loading and scheduling within plans and baseline schedules. This replanning effort needs to closely consider the effects on adjacent or dependent work initiatives or activities. Refer to the Project Monitoring and Control process area for more information about revising the project plan. 4. Establish a schedule or peri od of performance for each risk- handling activity that includes the start date and anticipated completion date. 5. Provide continued commitment of resources for each plan to allow successful execution of t he risk-handling activities. 6. Collect performance measures on the risk-handling activities. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 434 Continuous Only GP 1.1 Perform Specific Practices Perform the specific practices of the risk management process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the risk management process. Elaboration: This policy establishes organizational expectations for defining a risk management strategy and identifying, analyzing, and mitigating risks. GP 2.2 Plan the Process Establish and maintain the plan for performing the risk management process. Elaboration: This plan for performing the risk management process can be included in (or referenced by) the project plan, which is described in the Project Planning process area. The plan called fo r in this generic practice would address the comprehensive planning for a ll of the specific practices in this process area. In particular, th is plan provides the overall approach for risk mitigation, but is distinct from mitigation plans (including contingency plans) fo r specific risks. In c ontrast, the risk mitigation plans called for in the specific practices would address more focused items such as the levels that trigger risk-handling activities.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 435GP 2.3 Provide Resources Provide adequate resources for performing the risk management process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Risk management databases • Risk mitigation tools • Prototyping tools • Modeling and simulation GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the risk management process. GP 2.5 Train People Train the people performing or supporting the risk management process as needed. Elaboration: Examples of training topics include the following: • Risk management concepts and activities (e.g., risk identification, evaluation, monitoring, and mitigation) • Measure selection for risk mitigation GP 2.6 Manage Configurations Place designated work products of the risk management process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Risk management strategy • Identified risk items • Risk mitigation plans",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 436 GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the risk management process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing a collaborative environment for free and open discussion of risk • Reviewing the risk management strategy and risk mitigation plans • Participating in risk identification, analysis, and mitigation activities • Communicating and reporting risk management status GP 2.8 Monitor and Control the Process Monitor and control the risk management process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of risks identified, managed, tracked, and controlled • Risk exposure and changes to the risk exposure for each assessed risk, and as a summary percentage of management reserve • Change activity for the risk mitigation plans (e.g., processes, schedule, and funding) • Occurrence of unanticipated risks • Risk categorization volatility • Comparison of estimated versus actual risk mitigation effort and impact • Schedule for risk analysis activities • Schedule of actions for a specific mitigation GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the risk management process against its process description, standards, and procedures, and address noncompliance.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 437Elaboration: Examples of activities reviewed include the following: • Establishing and maintaining a risk management strategy • Identifying and analyzing risks • Mitigating risks Examples of work products reviewed include the following: • Risk management strategy • Risk mitigation plans GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the risk management process with higher level management and resolve issues. Elaboration: Reviews of the project risk stat us are held on a periodic and event- driven basis, with appropriate levels of management, to provide visibility into the potential for project risk exposure and appropriate corrective action. Typically, these reviews include a summ ary of the most critical risks, key risk parameters (such as like lihood and consequence of the risks), and the status of risk mitigation efforts. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined risk management process.",
        "CMMI for Development Version 1.2 Risk Management (RSKM) 438 GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the risk management process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Risk parameters • Risk categories • Risk status reports Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the risk management process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the risk management process to achieve the established quantitative quality and process- performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the risk management process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the risk management process.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 439SUPPLIER AGREEMENT MANAGEMENT A Project Management Process Area at Maturity Level 2 Purpose The purpose of Supplier Agreem ent Management (SAM) is to manage the acquisition of products from suppliers. Introductory Notes The Supplier Agreement Managemen t process area involves the following: • Determining the type of acquisition that will be used for the products to be acquired • Selecting suppliers • Establishing and maintaining agreements with suppliers • Executing the supplier agreement • Monitoring selected supplier processes • Evaluating selected s upplier work products • Accepting delivery of acquired products • Transitioning acquired pr oducts to the project This process area primarily addresse s the acquisition of products and product components that are deliv ered to the project’s customer. Throughout the process areas, w here we use the terms product and product component, their intended me anings also encompass services and their components. Examples of products and product components that may be acquired by the project include the following: • Subsystems (e.g., navigational system on an airplane) • Software • Hardware • Documentation (e.g., installation, operator's, and user's manuals) • Parts and materials (e.g., gauges, switches, wheels, steel, and raw materials) To minimize risks to the project, th is process area can also address the acquisition of significant produc ts and product components not",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 440 delivered to the project’s customer but used to develop and maintain the product or service (for exam ple, development tools and test environments). Typically, the products to be acqui red by the project are determined during the early stages of the planni ng and development of the product. The Technical Solution process area provides practices for determining the products and product components that may be acquired from suppliers. This process area does not directly address arrangements in which the supplier is integrated into the project team and uses the same processes and reports to the same management as the product developers (for example, integrated teams). Typically, these situations are handled by other processes or functions, possibly external to the project, though some of the specific practices of this process area may be useful in managing the formal agreement with such a supplier. Suppliers may take many forms depending on business needs, including in-house vendors (i.e., vendors that are in the same organization but are external to t he project), fabrication capabilities and laboratories, and commercial vendors. (See the definition of “supplier” in the glossary.) A formal agreement is established to manage the relationship between the organization and the supplier. A formal agreement is any legal agreement between the organization (r epresenting the project) and the supplier. This agreement may be a c ontract, license, service level agreement, or memorandum of agreement. The acquired product is delivered to the project from the supplier according to this formal agreement (also known as the “supplier agreement”). Related Process Areas Refer to the Project Monitoring and Control process area for more information about monitoring proj ects and taking corrective action. Refer to the Requirements Development process area for more information about defining requirements. Refer to the Requirements Management process area for more information about managing requirement s, including the traceability of requirements for products acquired from suppliers. Refer to the Technical Solution proc ess area for more information about determining the products and pr oduct components that may be acquired from suppliers.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 441Specific Goal and Practice Summary SG 1 Establish Supplier Agreements SP 1.1 Determine Acquisition Type SP 1.2 Select Suppliers SP 1.3 Establish Supplier Agreements SG 2 Satisfy Supplier Agreements SP 2.1 Execute the Supplier Agreement SP 2.2 Monitor Selected Supplier Processes SP 2.3 Evaluate Selected Supplier Work Products SP 2.4 Accept the Acquired Product SP 2.5 Transition Products Specific Practices by Goal SG 1 Establish Supplier Agreements Agreements with the suppliers are established and maintained. SP 1.1 Determine Acquisition Type Determine the type of acquisition for each product or product component to be acquired. Refer to the Technical Solution proc ess area for more information about identifying the products and pro duct components to be acquired. There are many different types of acquisition that can be used to acquire products and product com ponents that will be used by the project. Examples of types of acquisition include the following: • Purchasing commercial off-the-shelf (COTS) products • Obtaining products through a contractual agreement • Obtaining products from an in-house vendor • Obtaining products from the customer • Combining some of the above (e.g., contracting for a modification to a COTS product or having another part of the business enterprise codevelop products with an external supplier) In the event that COTS products are desired, care in evaluating and selecting these products and the vendor may be critical to the project. Things to consider in the selecti on decision include pr oprietary issues and the availability of the products. Typical Work Products 1. List of the acquisition types t hat will be used for all products and product components to be acquired",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 442 SP 1.2 Select Suppliers Select suppliers based on an evaluation of their ability to meet the specified requirements and established criteria. Refer to the Decision Analysis and Resolution process area for more information about formal evaluati on approaches that can be used to select suppliers. Refer to the Requirements Management process area for more information about spec ified requirements. Criteria should be established to addr ess factors that are important to the project. Examples of factors include the following: • Geographical location of the supplier • Supplier’s performance records on similar work • Engineering capabilities • Staff and facilities available to perform the work • Prior experience in similar applications Typical Work Products 1. Market studies 2. List of candidate suppliers 3. Preferred supplier list 4. Trade study or other record of evaluation criteria, advantages and disadvantages of candidate supplie rs, and rationale for selection of suppliers 5. Solicitation materials and requirements Subpractices 1. Establish and document criteria for evaluating potential suppliers. 2. Identify potential supp liers and distribute solicitation material and requirements to them. A proactive manner of performing this activity is to conduct market research to identify potential sources of candidate products to be acquired, including candidates from suppliers of custom-made products and vendors of COTS products. Refer to the Organizational I nnovation and Deployment process area for examples of source s of process and technology improvements and how to pilot and evaluate such improvements.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 4433. Evaluate proposals according to evaluation criteria. 4. Evaluate risks associated with each proposed supplier. Refer to the Risk Management process area for more information about evaluating project risks. 5. Evaluate proposed suppliers' ability to perform the work. Examples of methods to evaluate the proposed supplier’s ability to perform the work include the following: • Evaluation of prior experience in similar applications • Evaluation of prior performance on similar work • Evaluation of management capabilities • Capability evaluations • Evaluation of staff available to perform the work • Evaluation of available facilities and resources • Evaluation of the project’s ability to work with the proposed supplier • Evaluation of the impact of candidate COTS products on the project's plan and commitments When COTS products are being evaluated consider the following: • Cost of the COTS products • Cost and effort to incorporate the COTS products into the project • Security requirements • Benefits and impacts that may result from future product releases Future releases of the COTS product may provide additional features that support planned or anticipated enhancements for the project, but may result in the supplier discontinuing support of its current release. 6. Select the supplier. SP 1.3 Establish Supplier Agreements Establish and maintain formal agreements with the supplier. IPPD Addition When integrated teams are formed, team membership should be negotiated with suppliers and incorporated into the agreement. The agreement should identify any integrated decision making, reporting requirements (business and technical), and trade studies requiring supplier involvement. The supplier efforts should be orchestrated to support the IPPD efforts undertaken by the acquirer.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 444 A formal agreement is any legal agreement between the organization (representing the project) and the supplier. This agreement may be a contract, license, service leve l agreement, or memorandum of agreement. The content of the agree ment should spec ify the reviews, monitoring, evaluations, and acceptance tests to be performed, if such activities are appropriate to the acquisition or product being acquired. Typical Work Products 1. Statements of work 2. Contracts 3. Memoranda of agreement 4. Licensing agreement Subpractices 1. Revise the requirements (e.g., product requirements and service level requirements) to be fulfilled by the supplier to reflect negotiations with the supplier when necessary. Refer to the Requirements Development process area for more information about revising requirements. Refer to the Requirements Management process area for more information about managing changes to requirements. 2. Document what the project will provide to the supplier. Include the following: • Project-furnished facilities • Documentation • Services 3. Document the supplier agreement. The supplier agreement should include a statement of work, a specification, terms and conditions, a list of deliverables, a schedule, a budget, and a defined acceptance process. This subpractice typically includes the following: • Establishing the statement of work, spec ification, terms and conditions, list of deliverables, schedule, budget, and acceptance process • Identifying who from the project and supplier are responsible and authorized to make changes to the supplier agreement • Identifying how requirements changes and changes to the supplier agreement are to be determined, communicated, and addressed",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 445• Identifying standards and procedures that will be followed • Identifying critical dependencies between the project and the supplier • Identifying the type and depth of project ov ersight of the supplier, procedures, and evaluation criteria to be used in monitoring supplier performance including selection of processes to be monitored and work products to be evaluated • Identifying the types of reviews that will be conducted with the supplier • Identifying the supplier’s responsibilities for ongoing maintenance and support of the acquired products • Identifying warranty, ownership, and usage rights for the acquired products • Identifying acceptance criteria In some cases, selection of COTS products may require a supplier agreement in addition to the agreements in the product's license. Examples of what could be covered in an agreement with a COTS supplier include the following: • Discounts for large quantity purchases • Coverage of relevant stakeholders under the licensing agreement, including project suppliers, team members, and the project's customer • Plans for future enhancements • On-site support, such as responses to queries and problem reports • Additional capabilities that are not in the product • Maintenance support, including support after the product is withdrawn from general availability 4. Periodically review the supplier agreement to ensure it accurately reflects the project's relationship with the supplier and current risks and market conditions. 5. Ensure that all parties to the agreement understand and agree to all requirements before implem enting the agreement or any changes. 6. Revise the supplier agreement as necessary to reflect changes to the supplier's processes or work products. 7. Revise the project's plans and commitments, including changes to the project's processes or work products, as necessary to reflect the supplier agreement. Refer to the Project Monitoring and Control process area for more information about revising the project plan.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 446 SG 2 Satisfy Supplier Agreements Agreements with the suppliers are satisfied by both the project and the supplier. SP 2.1 Execute the Supplier Agreement Perform activities with the supplier as specified in the supplier agreement. Refer to the Project Monitoring and Control process area for more information about monitoring proj ects and taking corrective action. Typical Work Products 1. Supplier progress reports and performance measures 2. Supplier review materials and reports 3. Action items tracked to closure 4. Documentation of pr oduct and document deliveries Subpractices 1. Monitor supplier progress and perf ormance (schedule, effort, cost, and technical performance) as defined in the supplier agreement. 2. Conduct reviews with the supp lier as specified in the supplier agreement. Refer to the Project Monitoring and Control process area for more information about conducting reviews. Reviews cover both formal and informal reviews and include the following steps: • Preparing for the review • Ensuring that relevant stakeholders participate • Conducting the review • Identifying, documenting, and tracking all action items to closure • Preparing and distributing to the relevant stakeholders a summary report of the review 3. Conduct technical reviews with the supplier as defined in the supplier agreement. Technical reviews typically include the following: • Providing the supplier with visibility into the needs and desires of the project’s customers and end users, as appropriate • Reviewing the supplier’s technical activities and verifying that the supplier’s interpretation and implementation of the requirements are consistent with the project’s interpretation",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 447• Ensuring that technical commitments are being met and that technical issues are communicated and resolved in a timely manner • Obtaining technical information about the supplier’s products • Providing appropriate technical information and support to the supplier 4. Conduct management reviews with the supplier as defined in the supplier agreement. Management reviews typically include the following: • Reviewing critical dependencies • Reviewing project risks involving the supplier • Reviewing schedule and budget Technical and management reviews may be coordinated and held jointly. 5. Use the results of reviews to improve the supplier’s performance and to establish and nurture long-term relationships with preferred suppliers. 6. Monitor risks involving the supp lier and take corrective action as necessary. Refer to the Project Monitoring and Control process area for more information about moni toring project risks. SP 2.2 Monitor Selected Supplier Processes Select, monitor, and analyze processes used by the supplier. In situations where there must be tight alignment between some of the processes implemented by the s upplier and those of the project, monitoring these processes will hel p prevent interface problems. The selection must consider the im pact of the supplier's processes on the project. On larger projects with significant subcontracts for development of critical components, monitoring of key processes is expected. For most vendor agreem ents where a product is not being developed or for smaller, less critical components, the selection process may determine that monitoring is not appropriate. Between these extremes, the overall risk should be considered in selecting processes to be monitored. The processes selected for monito ring should include engineering, project management (inc luding contracting), and support processes critical to successful project performance. Monitoring, if not performed with adequate care, can at one extreme be invasive and burdensome, or at t he other extreme be uninformative and ineffective. There should be sufficient monitoring to detect issues, as",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 448 early as possible, that may affect the supplier's ability to satisfy the requirements of the supplier agreement. Analyzing selected processes involv es taking the data obtained from monitoring selected supplier proce sses and analyzing it to determine whether there are serious issues. Typical Work Products 1. List of processes selected for monitoring or rationale for non- selection 2. Activity reports 3. Performance reports 4. Performance curves 5. Discrepancy reports Subpractices 1. Identify the supplier pr ocesses that are critical to the success of the project. 2. Monitor the selected supplier' s processes for compliance with requirements of the agreement. 3. Analyze the results of monitori ng the selected processes to detect issues as early as possible that may affect the supplier's ability to satisfy the requirements of the agreement. Trend analysis can rely on internal and external data. Refer to the Verification process area for more information about recording the results of verification and analyses. Refer to the Project Monitoring and Control process area for more information about taki ng corrective action. SP 2.3 Evaluate Selected Supplier Work Products Select and evaluate work products from the supplier of custom- made products. The scope of this specific practice is limited to suppliers providing the project with custom-made products, particularly those that present some risk to the program due to comple xity or criticality. The intent of this specific practice is to eval uate selected work products produced by the supplier to help detect issues as early as po ssible that may affect the supplier's ability to satisfy t he requirements of the agreement. The work products selected for evaluati on should include critical products, product components, and work products that provide insight into quality issues as early as possible.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 449Typical Work Products 1. List of work products select ed for monitoring or rationale for non- selection 2. Activity reports 3. Discrepancy reports Subpractices 1. Identify those wo rk products that are critical to the success of the project and that should be evaluat ed to help detect issues early. Examples of work products that may be critical to the success of the project include the following: • Requirements • Analyses • Architecture • Documentation 2. Evaluate the selected work products. Work products are evaluated to ensure the following: • Derived requirements are traceable to higher level requirements • The architecture is feasible and will sa tisfy future product growth and reuse needs. • Documentation that will be used to operate and to support the product is adequate. • Work products are consistent with one another. • Products and product components (e.g., custom-made, off-the-shelf, and customer-supplied products) can be integrated. 3. Determine and document acti ons needed to address deficiencies identified in the evaluations. Refer to the Project Monitoring and Control process area for more information about taki ng corrective action. SP 2.4 Accept the Acquired Product Ensure that the supplier agreement is satisfied before accepting the acquired product. Acceptance reviews and tests and co nfiguration audits should be completed before accepting the pr oduct as defined in the supplier agreement. Typical Work Products 1. Acceptance te st procedures",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 450 2. Acceptance test results 3. Discrepancy reports or corrective action plans Subpractices 1. Define the acc eptance procedures. 2. Review and obtain agreement wi th relevant stakeholders on the acceptance procedures before the acceptance review or test. 3. Verify that the acquired produc ts satisfy their requirements. Refer to the Verification process area for more information about verifying products. 4. Confirm that the nontechnical commitments associated with the acquired work product are satisfied. This may include confirming that the appropriate license, warranty, ownership, usage, and support or maintenance agreements are in place and that all supporting materials are received. 5. Document the results of t he acceptance review or test. 6. Establish and obtain supplier agreement on an action plan for any acquired work products that do not pass their acceptance review or test. 7. Identify, document, and tra ck action items to closure. Refer to the Project Monitoring and Control process area for more information about tracking action items. SP 2.5 Transition Products Transition the acquired products from the supplier to the project. Before the acquired product is transfe rred to the project for integration, appropriate planning and evaluation should occur to ensure a smooth transition. Refer to the Product Integration process area for more information about integrating the acquired products. Typical Work Products 1. Transition plans 2. Training reports 3. Support and maintenance reports",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 451Subpractices 1. Ensure that there are appropriate facilities to receive, store, use, and maintain the acquired products. 2. Ensure that appropriate training is provided for those involved in receiving, storing, using, and ma intaining the acquired products. 3. Ensure that storing, distribut ing, and using the acquired products are performed according to the te rms and conditions specified in the supplier agreement or license. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the supplier agreement management process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the supplier agreement management process. Elaboration: This policy establishes organizational expectations for establishing, maintaining, and satisfying supplier agreements. GP 2.2 Plan the Process Establish and maintain the plan for performing the supplier agreement management process. Elaboration: Portions of this plan for per forming the supplier agreement management process can be part of (o r referenced by) the project plan as described in the Project Planni ng process area. Often, however,",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 452 some portions of the plan reside outside of the project with an independent group, such as contract management. GP 2.3 Provide Resources Provide adequate resources for performing the supplier agreement management process, developing the work products, and providing the services of the process. Elaboration: Examples of resources provided include the following tools: • Preferred supplier lists • Requirements tracking programs • Project management and scheduling programs GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the supplier agreement management process. GP 2.5 Train People Train the people performing or supporting the supplier agreement management process as needed. Elaboration: Examples of training topics include the following: • Regulations and business practices related to negotiating and working with suppliers • Acquisition planning and preparation • COTS products acquisition • Supplier evaluation and selection • Negotiation and conflict resolution • Supplier management • Testing and transitioning of acquired products • Receiving, storing, using, and maintaining acquired products GP 2.6 Manage Configurations Place designated work products of the supplier agreement management process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 453Elaboration: Examples of work products placed under control include the following: • Statements of work • Supplier agreements • Memoranda of agreement • Subcontracts • Preferred supplier lists GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the supplier agreement management process as planned. Elaboration: Examples of activities for stakeholder involvement include the following: • Establishing criteria for evaluation of potential suppliers • Reviewing potential suppliers • Establishing supplier agreements • Resolving issues with suppliers • Reviewing supplier performance GP 2.8 Monitor and Control the Process Monitor and control the supplier agreement management process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of changes made to the requirements for the supplier • Cost and schedule variance per supplier agreement • Number of supplier work product evaluations completed (planned versus actuals) • Number of supplier process evaluations completed (planned versus actuals) • Schedule for selecting a supplier and establishing an agreement",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 454 GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the supplier agreement management process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Establishing and maintaining supplier agreements • Satisfying supplier agreements Examples of work products reviewed include the following: • Plan for Supplier Agreement Management • Supplier agreements GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the supplier agreement management process with higher level management and resolve issues. Staged Only GG3 and its practices do not apply for a maturity level 2 rating, but do apply for a maturity level 3 rating and above. Continuous/Maturity Levels 3 - 5 Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined supplier agreement management process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the supplier agreement management process to support the future use and improvement of the organization’s processes and process assets.",
        "CMMI for Development Version 1.2 Supplier Agreement Management (SAM) 455Continuous/Maturity Levels 3 - 5 Only Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Results of supplier reviews • Trade studies used to select suppliers • Revision history of supplier agreements • Supplier performance reports • Results of supplier work product and process evaluations Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the supplier agreement management process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the supplier agreement management process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the supplier agreement management process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the supplier agreement management process.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 456 TECHNICAL SOLUTION An Engineering Process Area at Maturity Level 3 Purpose The purpose of Technical Solution (TS) is to design, develop, and implement solutions to require ments. Solutions, designs, and implementations encompass pr oducts, product components, and product-related lifecycle processes ei ther singly or in combination as appropriate. Introductory Notes The Technical Solution process area is applicable at any level of the product architecture and to every product, product component, and product-related lifecycle process. Throughout the process areas, where we use the terms product and prod uct component, their intended meanings also encompass servic es and their components. The process area focuses on the following: • Evaluating and selecting soluti ons (sometimes referred to as “design approaches,” “design conc epts,” or “preliminary designs”) that potentially satisfy an appropriate set of allocated requirements • Developing detailed designs for t he selected solutions (detailed in the context of containi ng all the information needed to manufacture, code, or otherwise implement t he design as a product or product component) • Implementing the designs as a product or product component Typically, these activities interactiv ely support each other. Some level of design, at times fairly detailed, may be needed to select solutions. Prototypes or pilots may be used as a means of gaining sufficient knowledge to develop a technical data package or a complete set of requirements. Technical Solution specific practice s apply not only to the product and product components but also to pr oduct-related lifecycle processes. The product-related lifec ycle processes are developed in concert with the product or product component. Such development may include selecting and adapting existing processes (including standard processes) for use as well as developing new processes. Processes associated with the Techni cal Solution process area receive the product and product compone nt requirements from the",
        "CMMI for Development Version 1.2 Technical Solution (TS) 457requirements management processe s. The requirements management processes place the requirements, which originate in requirements development processes, under approp riate configurat ion management and maintain their traceability to previous requirements. For a maintenance or sustainment pr oject, the requirements in need of maintenance actions or redesign may be driven by user needs or latent defects in the product components. New requirements may arise from changes in the operating environm ent. Such requirements can be uncovered during verification of the product(s) where actual performance can be compared agains t the specified performance and unacceptable degradation can be identified. Processes associated with the Technical Solution process ar ea should be used to perform the maintenance or sustainment design efforts. Related Process Areas Refer to the Requirements Development process area for more information about requirements allo cations, establishing an operational concept, and interface r equirements definition. Refer to the Verification process area for more information about conducting peer reviews and verifyi ng that the product and product components meet requirements. Refer to the Decision Analysis and Resolution process area for more information about fo rmal evaluation. Refer to the Requirements Management process area for more information about managing requirements. The specific practices in the Requirements Management process area are performed interactively with those in the Techni cal Solution process area. Refer to the Organizational Innov ation and Deployment process area for more information about improving the organization’s technology. Specific Goal and Practice Summary SG 1 Select Product Component Solutions SP 1.1 Develop Alternative Solutions and Selection Criteria SP 1.2 Select Product Component Solutions SG 2 Develop the Design SP 2.1 Design the Product or Product Component SP 2.2 Establish a Technical Data Package SP 2.3 Design Interfaces Using Criteria SP 2.4 Perform Make, Buy, or Reuse Analyses SG 3 Implement the Product Design SP 3.1 Implement the Design SP 3.2 Develop Product Support Documentation",
        "CMMI for Development Version 1.2 Technical Solution (TS) 458 Specific Practices by Goal SG 1 Select Product Component Solutions Product or product component solutions are selected from alternative solutions. Alternative solutions and their relative merits are considered in advance of selecting a solution. Key requirements, design issues, and constraints are estab lished for use in alternative solution analysis. Architectural features that provi de a foundation for product improvement and evolution are consider ed. Use of commercial off-the-shelf (COTS) product components are consider ed relative to cost, schedule, performance, and risk. COTS alternat ives may be used with or without modification. Sometimes such item s may require modifications to aspects such as interfaces or a custom ization of some of the features to better achieve product requirements. One indicator of a good design proce ss is that the design was chosen after comparing and evaluating it against alternative solutions. Decisions on architecture, custom development versus off the shelf, and product component modularization ar e typical of the design choices that are addressed. Some of these decisions may require the use of a formal evaluation process. Refer to the Decision Analysis and Resolution process area for more information about the use of a formal evaluation process. Sometimes the search for solutions examines alternative instances of the same requirements with no allocations needed for lower level product components. Such is the case at the bottom of the product architecture. There are also cases w here one or more of the solutions are fixed (e.g., a specific solution is directed or available product components, such as COTS, are investigated for use). In the general case, solutions are defined as a set. That is, when defining the next layer of product co mponents, the solution for each of the product components in the set is established. The alternative solutions are not only different ways of addressing the same requirements, but they also reflect a different allocation of requirements among the product components comprising the solution set. The objective is to optimize the set as a whole and not the individual pieces. There will be significant interaction with processes associated with the Requirements Development process area to support the provisional allocations to product components unt il a solution set is selected and final allocations are established. Product-related lifecycle proce sses are among the product component solutions that are selected from alte rnative solutions. Examples of these product-related lifecycle processes are the manufacturing, delivery, and support processes.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 459SP 1.1 Develop Alternative Solutions and Selection Criteria Develop alternative solutions and selection criteria. Refer to the Allocate Product Component Requirements specific practice in the Requirements De velopment process area for more information about obtaining allocati ons of requirements to solution alternatives for the product components. Refer to the Decision Analysis and Resolution process area for more information about establishing cr iteria used in making decisions. IPPD Addition The activity of selecting alternative solutions and issues to be subject to decision analyses and trade studies is accomplished by the involvement of relevant stakeholders. These stakeholders represent both business and technical functions and the concurrent development of the product and the product-related lifecycle processes (e .g., manufacturing, support, training, verification, and disposal). In this wa y, important issues surface earlier in product development than with traditi onal serial development and can be addressed before they bec ome costly mistakes. Alternative solutions need to be identified and analyzed to enable the selection of a balanced solution across t he life of the product in terms of cost, schedule, and performance. These solutions are based on proposed product architectures that address critical product qualities and span a design space of feasible solutions. Specific practices associated with the Develop the De sign specific goal provide more information on developing potential product architectures that can be incorporated into alternative solutions for the product. Alternative solutions frequently enc ompass alternative requirement allocations to different product com ponents. These alternative solutions can also include the use of COTS solu tions in the product architecture. Processes associated with the R equirements Development process area would then be employed to provide a more complete and robust provisional allocation of requireme nts to the alternative solutions. Alternative solutions span the acc eptable range of cost, schedule, and performance. The product component requirements are received and used along with design iss ues, constraints, and cr iteria to develop the alternative solutions. Selection crit eria would typically address costs (e.g., time, people, and money), benefit s (e.g., performance, capability, and effectiveness), and risks (e.g., technical, cost, and schedule). Considerations for alternative soluti ons and selection criteria include the following: • Cost of development, manufactu ring, procurement, maintenance, and support, etc. • Performance",
        "CMMI for Development Version 1.2 Technical Solution (TS) 460 • Complexity of the product component and product-related lifecycle processes • Robustness to product operati ng and use conditions, operating modes, environments, and variations in product-related lifecycle processes • Product expansion and growth • Technology limitations • Sensitivity to construc tion methods and materials • Risk • Evolution of requirements and technology • Disposal • Capabilities and limitations of end users and operators • Characteristics of COTS products The considerations lis ted here are a basic set; organizations should develop screening criteria to narrow down the list of alternatives that are consistent with their business objectives. Produc t lifecycle cost, while being a desirable parameter to minimi ze, may be outside the control of development organizations. A custom er may not be willing to pay for features that cost more in the short term but ultimately decrease cost over the life of the product. In such cases, customers should at least be advised of any potential for reducing life cycle costs. The criteria used in selections of final solutions s hould provide a balanced approach to costs, benefits, and risks. Typical Work Products 1. Alternative solution screening criteria 2. Evaluation reports of new technologies 3. Alternative solutions 4. Selection criteria for final selection 5. Evaluation reports of COTS products Subpractices 1. Identify screening criteria to select a set of alternative solutions for consideration. 2. Identify technologies currently in use and new product technologies for competitive advantage. Refer to the Organizational I nnovation and Deployment process area for more information about improving the organization’s technology.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 461The project should identify technologies applied to current products and processes and monitor the progress of currently used technologies throughout the life of the project. The project should identify, select, evaluate, and invest in new technologies to achieve competitive advantage. Alternative solutions could include newly developed technologies, but could also include applying mature technologies in different applications or to maintain current methods. 3. Identify candidate COTS products that sati sfy the requirements. Refer to the Supplier Agreement Management process area for more information about evaluating suppliers. These requirements include the following: • Functionality, performance, quality, and reliability • Terms and conditions of warranties for the products • Risk • Suppliers' responsibilities for ongoing maintenance and support of the products 4. Generate alternative solutions. 5. Obtain a complete requirement s allocation for each alternative. 6. Develop the criteria for select ing the best alternative solution. Criteria should be included that address design issues for the life of the product, such as provisions for more easily inserting new technologies or the ability to better exploit commercial products. Examples include criteria related to open design or open architecture concepts for the alternatives being evaluated. SP 1.2 Select Product Component Solutions Select the product component solutions that best satisfy the criteria established. Refer to the Allocate Product Component Requirements and Identify Interface Requirements specific practices of the Requirements Development process area for inform ation on establishing the allocated requirements for product compone nts and interface requirements among product components. Selecting product components that best satisfy the criteria establishes the requirement allocations to product components. Lower level requirements are generated from the selected alternative and used to develop the product component desi gn. Interface requirements among product components are described, primarily functionally. Physical interface descriptions are included in the documentation for interfaces to items and activities external to the product. The description of the solutions and the rationale for selection are documented. The document ation evolves throughout development as",
        "CMMI for Development Version 1.2 Technical Solution (TS) 462 solutions and detailed designs are developed and those designs are implemented. Maintaining a record of rationale is critical to downstream decision making. Such records k eep downstream stakeholders from redoing work and provide insights to apply technology as it becomes available in applicable circumstances. Typical Work Products 1. Product component select ion decisions and rationale 2. Documented relationships between requirements and product components 3. Documented solutions , evaluations, and rationale Subpractices 1. Evaluate each alternative solu tion/set of solutions against the selection criteria established in the context of the operating concepts and scenarios. Develop timeline scenarios for product operation and user interaction for each alternative solution. 2. Based on the evaluation of alte rnatives, assess the adequacy of the selection criteria and update these criteria as necessary. 3. Identify and resolve issues wi th the alternative solutions and requirements. 4. Select the best set of alter native solutions that satisfy the established selection criteria. 5. Establish the requirements asso ciated with the selected set of alternatives as the set of allo cated requirements to those product components. 6. Identify the product component so lutions that will be reused or acquired. Refer to the Supplier Agreement Management process area for more information about acquiring products and product components. 7. Establish and maintain the documentation of the solutions, evaluations, and rationale. SG 2 Develop the Design Product or product component designs are developed. Product or product component designs must provide the appropriate content not only for impl ementation, but also fo r other phases of the product lifecycle such as modifi cation, reprocurement, maintenance,",
        "CMMI for Development Version 1.2 Technical Solution (TS) 463sustainment, and installation. The design documentation provides a reference to support mutual understan ding of the design by relevant stakeholders and supports future c hanges to the design both during development and in subsequent phases of the product lifecycle. A complete design description is doc umented in a technical data package that includes a full range of features and parameters including form, fit, function, interface, m anufacturing process characteristics, and other parameters. Established organizati onal or project design standards (e.g., checklists, templates, and objec t frameworks) form the basis for achieving a high degree of defin ition and completeness in design documentation. IPPD Addition The integrated teams develop the de signs of the appropriate product- related lifecycle processes concurrently with the design of the product. These processes may be selected without modification from the organization’s set of standard processes, if appropriate. SP 2.1 Design the Product or Product Component Develop a design for the product or product component. Product design consists of two br oad phases that may overlap in execution: preliminary and deta iled design. Preliminary design establishes product capabilities and th e product architecture, including product partitions, product component i dentifications, s ystem states and modes, major intercomponent inte rfaces, and external product interfaces. Detailed design fully defin es the structure and capabilities of the product components. Refer to the Requirements Development process area for more information about developing architecture requirements. Architecture definition is driven from a set of architectural requirements developed during the requirements development processes. These requirements express the qualities and performance points that are critical to the success of the produc t. The architecture defines structural elements and coordination mechanisms that either directly satisfy requirements or support the achievem ent of the requirements as the details of the product design are es tablished. Architectures may include standards and design rules gover ning development of product components and their interfaces as well as guidance to aid product developers. Specific practices in the Select Product Component Solutions specific goal contain more information about using product architectures as a basis for alternative solutions. Architects postulate and develop a model of the product, making judgments about allocation of r equirements to product components including hardware and software. Mult iple architectures, supporting",
        "CMMI for Development Version 1.2 Technical Solution (TS) 464 alternative solutions, may be deve loped and analyzed to determine the advantages and disadvantages in t he context of the architectural requirements. Operational concepts and scenarios are used to generate use cases and quality scenarios that are used to refine the architecture. They are also used as a means to evaluate t he suitability of the architecture for its intended purpose during architecture evaluations, which are conducted periodically thr oughout product design. Refer to the Establish Operati onal Concepts and Scenarios specific practice of the Requirements Devel opment process area for information about developing operational concepts and scenarios used in architecture evaluation. Examples of architecture definition tasks include the following: • Establishing the structural relations of partitions and rules regarding interfaces between elements within partitions, and between partitions • Identifying major internal interfaces and all external interfaces • Identifying product components and interfaces between them • Defining coordination mechanisms (e.g., for software and hardware) • Establishing infrastructure capabilities and services • Developing product component templates or classes and frameworks • Establishing design rules and authority for making decisions • Defining a process/thread model • Defining physical deployment of software to hardware • Identifying major reuse approaches and sources During detailed design, the product ar chitecture details are finalized, product components are completely defined, and interfaces are fully characterized. Product component designs may be optimized for certain qualities or performance characteri stics. Designers may evaluate the use of legacy or COTS products fo r the product components. As the design matures, the requirements assigned to lower level product components are tracked to ensure that those requirements are satisfied. Refer to the Requirements Management process area for more information about tracking requi rements for product components.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 465For Software Engineering Detailed design is focused on software product component development. The internal structure of product co mponents is defined, data schemas are generated, algorithms are developed, and heuristics are established to provide product component capabilities that satisfy allocated requirements. For Hardware Engineering Detailed design is focused on product development of electronic, mechanical, electro-optical, and other hardware products and their components. Electrical schemati cs and interconnection diagrams are developed, mechanical and optical assembly models are generated, and fabrication and assembly processes are developed. Typical Work Products 1. Product architecture 2. Product component designs Subpractices 1. Establish and maintain criter ia against which the design can be evaluated. Examples of attributes, in addition to expected performance, for which design criteria can be established, include the following: • Modular • Clear • Simple • Maintainable • Verifiable • Portable • Reliable • Accurate • Secure • Scalable • Usable 2. Identify, develop, or acquire the design met hods appropriate for the product. Effective design methods can embody a wide range of activities, tools, and descriptive techniques. Whether a given method is effective or not depends on the situation. Two companies may have very effective design methods for products in which they specialize, but these methods may not be effective in cooperative",
        "CMMI for Development Version 1.2 Technical Solution (TS) 466 ventures. Highly sophisticated methods are not necessarily effective in the hands of designers who have not been trained in the use of the methods. Whether a method is effective also depends on how much assistance it provides the designer, and the cost effectiveness of that assistance. For example, a multiyear prototyping effort may not be appropriate for a simple product component but might be the right thing to do for an unprecedented, expensive, and complex product development. Rapid prototyping techniques, however, can be highly effective for many product components. Methods that use tools to ensure that a design will encompass all the necessary attributes needed to implement the product component design can be very effective. For example, a design tool that “knows” the capabilities of the manufacturing processes can allow the variability of the manufacturing process to be accounted for in the design tolerances. Examples of techniques and methods that facilitate effective design include the following: • Prototypes • Structural models • Object-oriented design • Essential systems analysis • Entity relationship models • Design reuse • Design patterns 3. Ensure that the design adher es to applicable design standards and criteria. Examples of design standards include the following (some or all of these standards may be design criteria, particularly in circumstances where the standards have not been established): • Operator interface standards • Test Scenarios • Safety standards • Design constraints (e.g., electromagnetic compatibility, signal integrity, and environmental) • Production constraints • Design tolerances • Parts standards (e.g., production scrap and waste) 4. Ensure that the design adher es to allocated requirements.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 467Identified COTS product components must be taken into account. For example, putting existing product components into the product architecture might modify the requirements and the requirements allocation. 5. Document the design. SP 2.2 Establish a Technical Data Package Establish and maintain a technical data package. A technical data package provides the developer with a comprehensive description of the product or produc t component as it is developed. Such a package also provides procur ement flexibility in a variety of circumstances such as performance- based contracting or build to print. The design is recorded in a tec hnical data package that is created during preliminary design to document the architecture definition. This technical data package is maintained throughout the life of the product to record essential details of t he product design. The technical data package provides the description of a product or product component (including product-related lifecycle processes if not handled as separate product components) that supports an acquisition strategy, or the implementation, production, enginee ring, and logistics support phases of the product lifecycle. The descrip tion includes the definition of the required design configuration and pr ocedures to ensure adequacy of product or product component perform ance. It includes all applicable technical data such as drawings, a ssociated lists, spec ifications, design descriptions, design dat abases, standards, perform ance requirements, quality assurance provisions, and pa ckaging details. The technical data package includes a description of the selected alternative solution that was chosen for implementation. A technical data package should include the following if such information is appropriate for the type of product and product component (for example, material and manufacturing requirements may not be useful for product components associated with software services or processes): • Product architecture description • Allocated requirements • Product component descriptions • Product-related lifecycle process descriptions, if not described as separate product components • Key product characteristics • Required physical charac teristics and constraints • Interface requirements • Materials requirements (bills of material and material characteristics)",
        "CMMI for Development Version 1.2 Technical Solution (TS) 468 • Fabrication and manufacturing requi rements (for both the original equipment manufacturer and field support) • The verification criteria used to ensure that requirements have been achieved • Conditions of use (environment s) and operating/usage scenarios, modes and states for operations, support, training, manufacturing, disposal, and verifications th roughout the life of the product • Rationale for decisions and c haracteristics (requirements, requirement allocations, and design choices) Because design descriptions can invo lve a very large amount of data and can be crucial to successful product component development, it is advisable to establish criteria for organizing the data and for selecting the data content. It is particularly us eful to use the product architecture as a means of organizing this data and abstracting views that are clear and relevant to an issue or feature of interest. These views include the following: • Customers • Requirements • The environment • Functional • Logical • Security • Data • States/modes • Construction • Management These views are documented in the technical data package. Typical Work Products 1. Technical data package Subpractices 1. Determine the number of levels of design and the appropriate level of documentation for each design level. Determining the number of levels of product components (e.g., subsystem, hardware configuration item, circuit board, computer software configuration item [CSCI], computer software product component, and computer software unit) that require documentation and requirements traceability is important to manage documentation costs and to support integration and verification plans.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 4692. Base detailed design descrip tions on the allocated product component requirements, architec ture, and higher level designs. 3. Document the design in the technical data package. 4. Document the rationale for key (i.e., significant effect on cost, schedule, or technical performance) decisions made or defined. 5. Revise the technical data package as necessary. SP 2.3 Design Interfaces Using Criteria Design product component interfaces using established criteria. Interface designs include the following: • Origination • Destination • Stimulus and data characteristics for software • Electrical, mechanical, and functi onal characteristics for hardware • Services lines of communication The criteria for interfaces frequently reflect critical parameters that must be defined, or at least investigated, to ascertain their applicability. These parameters are often peculiar to a given type of product (e.g., software, mechanical, electrical, and service) and are often associated with safety, security, durability, and mission-critical characteristics. Refer to the Identify Interface Requi rements specific practice in the Requirements Development process area for more information about identifying product and product compo nent interface requirements. Typical Work Products 1. Interface desi gn specifications 2. Interface c ontrol documents 3. Interface specif ication criteria 4. Rationale for selected interface design Subpractices 1. Define interface criteria. These criteria can be a part of the organizational process assets. Refer to the Organizational Process Definition process area for more information about establishi ng and maintaining organizational process assets.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 470 2. Identify interfaces associat ed with other product components. 3. Identify interfaces associ ated with external items. 4. Identify interfaces between product components and the product- related lifecycle processes. For example, such interfaces could include those between a product component to be fabricated and the jigs and fixtures used to enable that fabrication during the manufacturing process. 5. Apply the criteria to the interface design alternatives. Refer to the Decision Analysis and Resolution process area for more information about identif ying criteria and selecting alternatives based on those criteria. 6. Document the selected interfac e designs and the rationale for the selection. SP 2.4 Perform Make, Buy, or Reuse Analyses Evaluate whether the product components should be developed, purchased, or reused based on established criteria. The determination of what produc ts or product components will be acquired is frequently referred to as a “make-or-buy analysis.” It is based on an analysis of the needs of the project. This make-or-buy analysis begins early in the project dur ing the first iteration of design; continues during the desi gn process; and is comple ted with the decision to develop, acquire, or reuse the product. Refer to the Requirements Development process area for more information about determining t he product and product component requirements. Refer to the Requirements Management process area for more information about managing requirements. Factors affecting the make-or-bu y decision include the following: • Functions the products will provi de and how these f unctions will fit into the project • Available project resources and skills • Costs of acquiring versus developing internally • Critical delivery and integration dates • Strategic business alliances, including high-level business requirements • Market research of available products, including COTS products • Functionality and quality of available products",
        "CMMI for Development Version 1.2 Technical Solution (TS) 471• Skills and capabilities of potential suppliers • Impact on core competencies • Licenses, warranties, responsibilit ies, and limitations associated with products being acquired • Product availability • Proprietary issues • Risk reduction The make-or-buy decision can be c onducted using a formal evaluation approach. Refer to the Decision Analysis and Resolution process area for more information about defining criter ia and alternatives and performing formal evaluations. As technology evolves, so does the rationale for choosing to develop or purchase a product component. While complex development efforts may favor purchasing an off-the-shel f product component, advances in productivity and tools may provide an opposing rationale. Off-the-shelf products may have incomplete or in accurate documentation and may or may not be supported in the future. Once the decision is made to purchase an off-the-shelf product component, the requirements are used to establish a supplier agreement. There are times when “off the shelf” refers to an existing item that may not be readily availabl e in the marketplac e. For example, some types of aircraft and engines are not truly “off the shelf” but can be readily procured. In some cases the use of such nondeveloped items is because the specifics of t he performance and other product characteristics expected need to be with in the limits spec ified. In these cases, the requirements and acceptance criteria may need to be included in the supplier agreement an d managed. In other cases, the off-the-shelf product is literally o ff the shelf (word processing software, for example) and there is no agreement with the supplier that needs to be managed. Refer to the Supplier Agreement Management process area for more information about how to addre ss the acquisition of the product components that will be purchased. Typical Work Products 1. Criteria for design and product component reuse 2. Make-or-buy analyses 3. Guidelines for choos ing COTS product components",
        "CMMI for Development Version 1.2 Technical Solution (TS) 472 Subpractices 1. Develop criteria for the r euse of product component designs. 2. Analyze designs to determine if product components should be developed, reused, or purchased. 3. Analyze implications for main tenance when consi dering purchased or nondevelopmental (e.g., COTS, government off the shelf, and reuse) items. Examples of implications for maintenance include the following: • Compatibility with future releases of COTS products • Configuration management of vendor changes • Defects in the nondevelopment item and their resolution • Unplanned obsolescence SG 3 Implement the Product Design Product components, and associated support documentation, are implemented from their designs. Product components are implemented fr om the designs established by the specific practices in the De velop the Design specific goal. The implementation usually includes unit testing of the product components before sending them to product in tegration and development of end- user documentation. SP 3.1 Implement the Design Implement the designs of the product components. Once the design has been completed, it is implemented as a product component. The characteristics of that implementation depend on the type of product component. Design implementation at the top level of the pr oduct hierarchy involves the specification of each of the product com ponents at the next level of the product hierarchy. This activity includes the allocation, refinement, and verification of each product component. It also involves the coordination between the variou s product component development efforts. Refer to the Requirements Development process area for more information about the allocation and refinement of requirements. Refer to the Product Integration process area for more information about the management of interfaces and the integration of products and product components.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 473Example characteristics of this implementation are as follows: • Software is coded. • Data is documented. • Services are documented. • Electrical and mechanical parts are fabricated. • Product-unique manufacturing processes are put into operation. • Processes are documented. • Facilities are constructed. • Materials are produced (e.g., a product-unique material could be petroleum, oil, a lubricant, or a new alloy). Typical Work Products 1. Implemented design Subpractices 1. Use effective methods to im plement the product components. For Software Engineering Examples of software coding methods include the following: • Structured programming • Object-oriented programming • Automatic code generation • Software code reuse • Use of applicable design patterns For Hardware Engineering Examples of hardware implementation methods include the following: • Gate level synthesis • Circuit board layout (place and route) • Computer Aided Design drawing • Post layout simulation • Fabrication methods 2. Adhere to applicable standards and criteria.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 474 Examples of implementation standards include the following: • Language standards (e.g., standards for software programming languages and hardware description languages) • Drawing requirements • Standard parts lists • Manufactured parts • Structure and hierarchy of software product components • Process and quality standards Examples of criteria include the following: • Modularity • Clarity • Simplicity • Reliability • Safety • Maintainability 3. Conduct peer reviews of t he selected product components. Refer to the Verification process area for more information about conducting peer reviews. 4. Perform unit testing of the product component as appropriate. Note that unit testing is not limited to software. Unit testing involves the testing of individual hardware or software units or groups of related items prior to integration of those items. Refer to the Verification process area for more information about verification methods and procedur es and about verifying work products against their s pecified requirements. For Software Engineering Examples of unit testing methods include the following: • Statement coverage testing • Branch coverage testing • Predicate coverage testing • Path coverage testing • Boundary value testing • Special value testing",
        "CMMI for Development Version 1.2 Technical Solution (TS) 475For Hardware Engineering Examples of unit testing methods include the following: • Functional testing • Radiation inspection testing • Environmental testing 5. Revise the product component as necessary. An example of when the product component may need to be revised is when problems surface during implementation that could not be foreseen during design. SP 3.2 Develop Product Support Documentation Develop and maintain the end-use documentation. This specific practice develops and maintains the documentation that will be used to install, operat e, and maintain the product. Typical Work Products 1. End-user training materials 2. User's manual 3. Operator's manual 4. Maintenance manual 5. Online help Subpractices 1. Review the requirements, desi gn, product, and test results to ensure that issues affecting the installation, operation, and maintenance documentation ar e identified and resolved. 2. Use effective methods to devel op the installation, operation, and maintenance documentation. 3. Adhere to the applicabl e documentation standards.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 476 Examples of documentation standards include the following: • Compatibility with designated word processors • Acceptable fonts • Numbering of pages, sections, and paragraphs • Consistency with a designated style manual • Use of abbreviations • Security classification markings • Internationalization requirements 4. Develop preliminary versions of the installation, operation, and maintenance documentation in early p hases of the project lifecycle for review by the relevant stakeholders. 5. Conduct peer reviews of t he installation, operation, and maintenance documentation. Refer to the Verification process area for more information about conducting peer reviews. 6. Revise the installation, oper ation, and maint enance documentation as necessary. Examples of when documentation may need to be revised include when the following events occur: • Requirements change • Design changes are made • Product changes are made • Documentation errors are identified • Workaround fixes are identified Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 477Continuous Only GP 1.1 Perform Specific Practices Perform the specific practices of the technical solution process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the technical solution process. Elaboration: This policy establishes organizati onal expectations for addressing the iterative cycle in which product component solutions are selected, product and product component designs are developed, and the product component desi gns are implemented. GP 2.2 Plan the Process Establish and maintain the plan for performing the technical solution process. Elaboration: This plan for performing the technica l solution process can be part of (or referenced by) the project plan as described in the Project Planning process area.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 478 GP 2.3 Provide Resources Provide adequate resources for performing the technical solution process, developing the work products, and providing the services of the process. Elaboration: Special facilities may be require d for developing, designing, and implementing solutions to requireme nts. When necessary, the facilities required for the activities in the Technical Solution process area are developed or purchased. Examples of other resources provided include the following tools: • Design specification tools • Simulators and modeling tools • Prototyping tools • Scenario definition and management tools • Requirements tracking tools • Interactive documentation tools GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the technical solution process. GP 2.5 Train People Train the people performing or supporting the technical solution process as needed. Elaboration: Examples of training topics include the following: • Application domain of the product and product components • Design methods • Interface design • Unit testing techniques • Standards (e.g., product, safety, human factors, and environmental) GP 2.6 Manage Configurations Place designated work products of the technical solution process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 479Elaboration: Examples of work products placed under control include the following: • Product, product component and interface designs • Technical data packages • Interface design documents • Criteria for design and product component reuse • Implemented designs (e.g., software code and fabricated product components) • User, installation, operation, and maintenance documentation GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the technical solution process as planned. Elaboration: Select relevant stakeholders from customers, end users, developers, producers, testers, suppliers, ma rketers, maintainers, disposal personnel, and others who may be affected by, or may affect, the product as well as the process. Examples of activities for stakeholder involvement include the following: • Developing alternative solutions and selection criteria • Obtaining approval on external interface specifications and design descriptions • Developing the technical data package • Assessing the make, buy, or reuse alternatives for product components • Implementing the design GP 2.8 Monitor and Control the Process Monitor and control the technical solution process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 480 Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Cost, schedule, and effort expended for rework • Percentage of requirements addressed in the product or product component design • Size and complexity of the product, product components, interfaces, and documentation • Defect density of technical solutions work products • Schedule for design activities GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the technical solution process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Selecting product component solutions • Developing product and product component designs • Implementing product component designs Examples of work products reviewed include the following: • Technical data packages • Product, product component, and interface designs • Implemented designs (e.g., software code and fabricated product components) • User, installation, operation, and maintenance documentation GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the technical solution process with higher level management and resolve issues.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 481Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined technical solution process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the technical solution process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Results of the make, buy, or reuse analysis • Design defect density • Results of applying new methods and tools Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the technical solution process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the technical solution process to achieve the established quantitative quality and process- performance objectives.",
        "CMMI for Development Version 1.2 Technical Solution (TS) 482 Continuous Only GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the technical solution process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the technical solution process.",
        "CMMI for Development Version 1.2 Validation (VAL) 483VALIDATION An Engineering Process Area at Maturity Level 3 Purpose The purpose of Validation (VAL) is to demonstrate that a product or product component fulfills its int ended use when placed in its intended environment. Introductory Notes Validation activities can be applied to all aspects of the product in any of its intended environments, such as oper ation, training, manufacturing, maintenance, and support services . The methods employed to accomplish validation can be applied to work products as well as to the product and product components. (Throughout the process areas, where we use the terms product and product component, their intended meanings also encompass services and their components.) The work products (e.g., requirements, designs, and protot ypes) should be selected on the basis of which are the best predictors of how well the product and product co mponent will satisfy user needs and thus validation is performed early and in crementally throughout the product lifecycle. The validation environment should re present the intended environment for the product and product componen ts as well as represent the intended environment suitable for va lidation activities with work products. Validation demonstrates that the product, as pr ovided, will fulfill its intended use; whereas, verification addresses whether the work product properly reflects the specified require ments. In other words, verification ensures that “you built it right”; whereas, validation ensures that “you built the right thing.” Validation ac tivities use approaches similar to verification (e.g., test, analysis, inspection, demonstration, or simulation). Often, the end users and other relevant stakeholders are involved in the validation activiti es. Both validation and verification activities often run c oncurrently and may use portions of the same environment. Refer to the Verification process area for more information about verification activities. Whenever possible, validation should be accomplished using the product or product component operat ing in its intended environment.",
        "CMMI for Development Version 1.2 Validation (VAL) 484 The entire environment can be used or only part of it. However, validation issues can be discovered earl y in the life of the project using work products by involving relevant stakeholders. Validation activities for services can be applied to work products such as proposals, service catalogs, statements of wo rk, and service records. When validation issues are identified, they are referred to the processes associated with the Requirements De velopment, Technical Solution, or Project Monitoring and Control process areas for resolution. The specific practices of this proc ess area build on each other in the following way: • The Select Products for Validati on specific practice enables the identification of the product or product component to be validated and the methods to be used to perform the validation. • The Establish the Validation Envi ronment specific practice enables the determination of the environment that will be used to carry out the validation. • The Establish Validation Procedures and Criteria specific practice enables the development of validatio n procedures and criteria that are aligned with the characterist ics of selected products, customer constraints on validation, met hods, and the validation environment. • The Perform Validation specific pr actice enables the performance of validation according to the me thods, procedures, and criteria. Related Process Areas Refer to the Requirements Development process area for more information about requirements validation. Refer to the Technical Solution proc ess area for more information about transforming requirements into produc t specifications and for corrective action when validation issues are ident ified that affect the product or product component design. Refer to the Verification process area for more information about verifying that the product or produc t component meets its requirements. Specific Goal and Practice Summary SG 1 Prepare for Validation SP 1.1 Select Products for Validation SP 1.2 Establish the Validation Environment SP 1.3 Establish Validation Procedures and Criteria SG 2 Validate Product or Product Components SP 2.1 Perform Validation SP 2.2 Analyze Validation Results",
        "CMMI for Development Version 1.2 Validation (VAL) 485Specific Practices by Goal SG 1 Prepare for Validation Preparation for validation is conducted. Preparation activities include se lecting products and product components for validation and est ablishing and maintaining the validation environment, procedures, and criteria. The items selected for validation may include only the product or it may include appropriate levels of the product components that are used to build the product. Any product or product component may be subject to validation, including replacement, maintenance, and traini ng products, to name a few. The environment required to vali date the product or product component is prepared. The environment may be purchased or may be specified, designed, and built. The environment s used for product integration and verification may be considered in collaboration with the validation environment to reduce cost and impr ove efficiency or productivity. SP 1.1 Select Products for Validation Select products and product components to be validated and the validation methods that will be used for each. Products and product components are selected for validation on the basis of their relationship to us er needs. For each product component, the scope of the validation (e.g., oper ational behavior, maintenance, training, and user interf ace) should be determined. Examples of products and product components that can be validated include the following: • Product and product component requirements and designs • Product and product components (e.g., system, hardware units, software, and service documentation) • User interfaces • User manuals • Training materials • Process documentation The requirements and constraints for performing validation are collected. Then, validation methods are selected based on their ability to demonstrate that user needs are satisfied. The validation methods not only define the approach to product validation, but also drive the needs for the facilities, equipment, and environments. This may result in the generation of lower level produc t component requirements that are handled by the requirements devel opment processes. Derived requirements, such as interface r equirements to test sets and test",
        "CMMI for Development Version 1.2 Validation (VAL) 486 equipment, can be generated. These r equirements are also passed to the requirements development proce sses to ensure that the product or product components can be validated in an environment that supports the methods. Validation methods should be selected ear ly in the life of the project so that they are clearly understood and agreed to by the relevant stakeholders. The validation methods address the development, maintenance, support, and training for the produc t or product component as appropriate. Examples of validation methods include the following: • Discussions with the users, perhaps in the context of a formal review • Prototype demonstrations • Functional demonstrations (e.g., system, hardware units, software, service documentation, and user interfaces) • Pilots of training materials • Test of products and product components by end users and other relevant stakeholders • Analyses of product and product components (e.g., simulations, modeling, and user analyses) For Hardware Engineering Hardware validation activities include modeling to validate form, fit, and function of mechanical designs; ther mal modeling; maintainability and reliability analysis; timeline demonstrations; and electrical design simulations of electronic or mechanical product components. Typical Work Products 1. Lists of products and product co mponents selected for validation 2. Validation methods for each product or product component 3. Requirements for performing vali dation for each product or product component 4. Validation constraints for eac h product or product component Subpractices 1. Identify the key principles, f eatures, and phases for product or product component validation thr oughout the life of the project. 2. Determine which categorie s of user needs (operational, maintenance, training, or support) are to be validated.",
        "CMMI for Development Version 1.2 Validation (VAL) 487The product or product component must be maintainable and supportable in its intended operational environment. This specific practice also addresses the actual maintenance, training, and support services that may be delivered along with the product. An example of evaluation of maintenance concepts in the operational environment is a demonstration that maintenance tools are operating with the actual product. 3. Select the product and produc t components to be validated. 4. Select the evaluation methods for product or product component validation. 5. Review the validation selecti on, constraints, and methods with relevant stakeholders. SP 1.2 Establish the Validation Environment Establish and maintain the environment needed to support validation. The requirements for the validati on environment are driven by the product or product components select ed, by the type of the work products (e.g., design, prototype, and final version), and by the methods of validation. These may yield requirements for the purchase or development of equipment, software, or other resources. These requirements are provided to the requirements development processes for development. The validation envi ronment may include the reuse of existing resources. In this case, arrangements for the use of these resources must be made. Examples of the type of elements in a validation environment include the following: • Test tools interfaced with the product being validated (e.g., scope, electronic devices, and probes) • Temporary embedded test software • Recording tools for dump or further analysis and replay • Simulated subsystems or component s (by software, electronics, or mechanics) • Simulated interfaced systems (e.g ., a dummy warship for testing a naval radar) • Real interfaced systems (e.g., ai rcraft for testing a radar with trajectory tracking facilities) • Facilities and custom er-supplied products • The skilled people to operate or use all the preceding elements • Dedicated computing or network test environment (e.g., pseudo- operational telecommunications-net work testbed or facility with actual trunks, switches, and sy stems established for realistic integration and validation trials)",
        "CMMI for Development Version 1.2 Validation (VAL) 488 Early selection of the products or product components to be validated, the work products to be used in the validation, and the validation methods is needed to ensure that the validation environment will be available when necessary. The validation environment should be ca refully controlled to provide for replication, analysis of results, and revalidation of problem areas. Typical Work Products 1. Validation environment Subpractices 1. Identify validation env ironment requirements. 2. Identify custome r-supplied products. 3. Identify reuse items. 4. Identify test equipment and tools. 5. Identify validation resources that are available for reuse and modification. 6. Plan the availability of resources in detail. SP 1.3 Establish Validation Procedures and Criteria Establish and maintain procedures and criteria for validation. Validation procedures and criteria are defined to ensure that the product or product component will fulfill it s intended use when placed in its intended environment. Acceptance test cases and procedures may meet the need for va lidation procedures. The validation procedures and criteria include test and evaluation of maintenance, training, and support services. Examples of sources for validation criteria include the following: • Product and product component requirements • Standards • Customer acceptance criteria • Environmental performance • Thresholds of performance deviation Typical Work Products 1. Validation procedures 2. Validation criteria",
        "CMMI for Development Version 1.2 Validation (VAL) 4893. Test and evaluation procedures for maintenance, training, and support Subpractices 1. Review the product requirements to ensure that issues affecting validation of the product or pr oduct component are identified and resolved. 2. Document the environment, oper ational scenario, procedures, inputs, outputs, and criteria for the validation of the selected product or product component. 3. Assess the design as it matures in the context of the validation environment to identif y validation issues. SG 2 Validate Product or Product Components The product or product components are validated to ensure that they are suitable for use in their intended operating environment. The validation methods, procedures, and criteria are used to validate the selected products and produc t components and any associated maintenance, training, and support se rvices using the appropriate validation environment. Validation ac tivities are performed throughout the product lifecycle. SP 2.1 Perform Validation Perform validation on the selected products and product components. To be acceptable to users, a product or product component must perform as expected in its in tended operational environment. Validation activities are performed and the resulting data are collected according to the established me thods, procedures, and criteria. The as-run validation procedur es should be documented and the deviations occurring during the ex ecution should be noted, as appropriate. Typical Work Products 1. Validation reports 2. Validation results 3. Validation cross-reference matrix 4. As-run procedures log 5. Operational demonstrations",
        "CMMI for Development Version 1.2 Validation (VAL) 490 SP 2.2 Analyze Validation Results Analyze the results of the validation activities. The data resulting from validation te sts, inspections, demonstrations, or evaluations are analyzed against the defined validation criteria. Analysis reports indicate whether the nee ds were met; in the case of deficiencies, these repor ts document the degree of success or failure and categorize probable cause of failur e. The collected test, inspection, or review results are compared with established evaluation criteria to determine whether to proceed or to address requirements or design issues in the requirements devel opment or technical solution processes. Analysis reports or as-run validati on documentation may also indicate that bad test results are due to a validation procedure problem or a validation environment problem. Typical Work Products 1. Validation deficiency reports 2. Validation issues 3. Procedure change request Subpractices 1. Compare actual result s to expected results. 2. Based on the establis hed validation criteria, identify products and product components that do not per form suitably in their intended operating environments, or ident ify problems with the methods, criteria, and/or environment. 3. Analyze the validat ion data for defects. 4. Record the results of t he analysis and identify issues. 5. Use validation results to compare actual measurements and performance to intended use or operational need. Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products.",
        "CMMI for Development Version 1.2 Validation (VAL) 491Continuous Only GP 1.1 Perform Specific Practices Perform the specific practices of the validation process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the validation process. Elaboration: This policy establishes organizati onal expectations for selecting products and product components for va lidation; for selecting validation methods; and for establishing and main taining validation procedures, criteria, and environments that ensure the products and product components satisfy user needs in t heir intended operating environment. GP 2.2 Plan the Process Establish and maintain the plan for performing the validation process. Elaboration: This plan for performing the validat ion process can be included in (or referenced by) the project plan, which is described in the Project Planning process area.",
        "CMMI for Development Version 1.2 Validation (VAL) 492 GP 2.3 Provide Resources Provide adequate resources for performing the validation process, developing the work products, and providing the services of the process. Elaboration: Special facilities may be required fo r validating the product or product components. When necessary, the faci lities required for validation are developed or purchased. Examples of other resources provided include the following tools: • Test-management tools • Test-case generators • Test-coverage analyzers • Simulators • Load, stress, and performance tools GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the validation process. GP 2.5 Train People Train the people performing or supporting the validation process as needed. Elaboration: Examples of training topics include the following: • Application domain • Validation principles, standards, and methods • Intended-use environment GP 2.6 Manage Configurations Place designated work products of the validation process under appropriate levels of control.",
        "CMMI for Development Version 1.2 Validation (VAL) 493Elaboration: Examples of work products placed under control include the following: • Lists of products and product components selected for validation • Validation methods, procedures, and criteria • Validation reports GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the validation process as planned. Elaboration: Select relevant stakeholders from customers, end users, developers, producers, testers, suppliers, ma rketers, maintainers, disposal personnel, and others who may be affected by, or may affect, the product as well as the process. Examples of activities for stakeholder involvement include the following: • Selecting the products and product components to be validated • Establishing the validation methods, procedures, and criteria • Reviewing results of product and product component validation and resolving issues • Resolving issues with the customers or end users Issues with the customers or end us ers are resolved particularly when there are significant deviations from their baseline needs for the following: • Waivers on the contract or agreement (what, when, and for which products) • Additional in-depth studies, tr ials, tests, or evaluations • Possible changes in the contracts or agreements GP 2.8 Monitor and Control the Process Monitor and control the validation process against the plan for performing the process and take appropriate corrective action.",
        "CMMI for Development Version 1.2 Validation (VAL) 494 Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Number of validation activities completed (planned versus actual) • Validation problem report trends (e.g., number written and number closed) • Validation problem report aging (i.e., how long each problem report has been open) • Schedule for a specific validation activity GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the validation process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Selecting the products and product components to be validated • Establishing and maintaining validation methods, procedures, and criteria • Validating products or product components Examples of work products reviewed include the following: • Validation methods, procedures, and criteria GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the validation process with higher level management and resolve issues. Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined validation process.",
        "CMMI for Development Version 1.2 Validation (VAL) 495GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the validation process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Product component prototype • Percent of time the validation environment is available • Number of product defects found through validation per development phase • Validation analysis report Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the validation process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the validation process to achieve the established quantitative quality and process-performance objectives. GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the validation process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the validation process.",
        "CMMI for Development Version 1.2 Verification (VER) 496 VERIFICATION An Engineering Process Area at Maturity Level 3 Purpose The purpose of Verification (VER) is to ensure that selected work products meet their s pecified requirements. Introductory Notes The Verification process area invo lves the following: verification preparation, verification performanc e, and identification of corrective action. Verification includes verification of the product and intermediate work products against all selected requirem ents, including customer, product, and product component requirements. Throughout the process areas, where we use the terms product and product component, their intended meanings also encompass servic es and their components. Verification is inherently an increm ental process because it occurs throughout the development of the product and work products, beginning with verification of the r equirements, progressing through the verification of the evolving work products, and culminating in the verification of the completed product. The specific practices of this proc ess area build on each other in the following way: • The Select Work Products for Veri fication specific practice enables the identification of t he work products to be verified, the methods to be used to perform the verificati on, and the requirements to be satisfied by each selected work product. • The Establish the Verification Envi ronment specific practice enables the determination of the environment that will be used to carry out the verification. • The Establish Verification Procedures and Criteria specific practice then enables the development of ve rification procedures and criteria that are aligned with the sele cted work products, requirements, methods, and characteristics of the verification environment. • The Perform Verification specific practice conducts the verification according to the available methods, procedures, and criteria.",
        "CMMI for Development Version 1.2 Verification (VER) 497Verification of work products subst antially increases the likelihood that the product will meet the cust omer, product, and product component requirements. The Verification and Validation proce ss areas are similar, but they address different issues. Validation de monstrates that the product, as provided (or as it will be provided) , will fulfill its intended use, whereas verification addresses whether the wo rk product properly reflects the specified requirements. In other wo rds, verification ensures that “you built it right”; whereas, validation ensures that “y ou built the right thing.” Peer reviews are an important part of verification and are a proven mechanism for effective defect remova l. An important corollary is to develop a better understanding of the wo rk products and the processes that produced them so that def ects can be prevented and process improvement opportunities can be identified. Peer reviews involve a methodical exam ination of work products by the producers’ peers to identify defects and other changes that are needed. Examples of peer review methods include the following: • Inspections • Structured walkthroughs Related Process Areas Refer to the Validation process area for more information about confirming that a product or product component fu lfills its intended use when placed in its intended environment. Refer to the Requirements Development process area for more information about the generation and development of customer, product, and product component requirements. Refer to the Requirements Management process area for more information about managing requirements.",
        "CMMI for Development Version 1.2 Verification (VER) 498 Specific Goal and Practice Summary SG 1 Prepare for Verification SP 1.1 Select Work Products for Verification SP 1.2 Establish the Verification Environment SP 1.3 Establish Verification Procedures and Criteria SG 2 Perform Peer Reviews SP 2.1 Prepare for Peer Reviews SP 2.2 Conduct Peer Reviews SP 2.3 Analyze Peer Review Data SG 3 Verify Selected Work Products SP 3.1 Perform Verification SP 3.2 Analyze Verification Results Specific Practices by Goal SG 1 Prepare for Verification Preparation for verification is conducted. Up-front preparation is necessary to ensure that verification provisions are embedded in product and pr oduct component requirements, designs, developmental plans, and sc hedules. Verification includes selection, inspection, testing, analysis, and demonstration of work products. Methods of verification include, but are not limited to, inspections, peer reviews, audits, walkthroughs, analys es, simulations, testing, and demonstrations. Practices related to peer reviews as a specific verification method are included in specific goal 2. Preparation also entails the definiti on of support tools, test equipment and software, simulations, pr ototypes, and facilities. SP 1.1 Select Work Products for Verification Select the work products to be verified and the verification methods that will be used for each. Work products are selected based on their contribution to meeting project objectives and requirements, and to addressing project risks. The work products to be verified may include those associated with maintenance, training, and suppor t services. The work product requirements for verification are incl uded with the verification methods. The verification methods address the approach to work product verification and the specific approaches that will be used to verify that specific work products m eet their requirements.",
        "CMMI for Development Version 1.2 Verification (VER) 499For Software Engineering Examples of verification methods include the following: • Path coverage testing • Load, stress, and performance testing • Decision-table-based testing • Functional decomposition-based testing • Test-case reuse • Acceptance tests For Systems Engineering Verification for systems engineeri ng typically includes prototyping, modeling, and simulation to verify adequacy of system design (and allocation). For Hardware Engineering Verification for hardware engineering typically requires a parametric approach that considers various environmental conditions (e.g., pressure, temperature, vibration, and humidit y), various input ranges (e.g., input power could be rated at 20V to 32V for a planned nominal of 28V), variations induced from part to part tolerance issues, and many other variables. Hardware verification normally tests most variables separately except when problematic inte ractions are suspected. Selection of the verification methods typically begins with involvement in the definition of product and product component requirements to ensure that these requirements are veri fiable. Reverification should be addressed by the verification methods to ensure that rework performed on work products does not cause uni ntended defects. Suppliers should be involved in this selection to ens ure that the project's methods are appropriate for the supplier's environment. IPPD Addition The verification methods should be dev eloped concurrently and iteratively with the product and product component designs. Typical Work Products 1. Lists of work products selected for verification 2. Verification methods for each selected work product",
        "CMMI for Development Version 1.2 Verification (VER) 500 Subpractices 1. Identify work produc ts for verification. 2. Identify the requirements to be satisfied by each selected work product. Refer to the Maintain Bidirect ional Traceability of Requirements specific practice in the Require ments Management process area to help identify the requirem ents for each work product. 3. Identify the verifi cation methods that are available for use. 4. Define the verification methods to be used for each selected work product. 5. Submit for integration with the pr oject plan the identification of work products to be verified, the requi rements to be satisfied, and the methods to be used. Refer to the Project Planning process area for information about coordinating with project planning. SP 1.2 Establish the Verification Environment Establish and maintain the environment needed to support verification. An environment must be established to enable verification to take place. The verification environment can be acquired, dev eloped, reused, modified, or a combination of t hese, depending on the needs of the project. The type of environment require d will depend on the work products selected for verification and the ve rification methods used. A peer review may require little more than a package of materials, reviewers, and a room. A product test may require simulators, emulators, scenario generators, data reduction tools, env ironmental controls , and interfaces with other systems. Typical Work Products 1. Verification environment Subpractices 1. Identify verification env ironment requirements. 2. Identify verification resources that are available for reuse and modification. 3. Identify verification equipment and tools. 4. Acquire verification support equipment and an environment, such as test equipment and software.",
        "CMMI for Development Version 1.2 Verification (VER) 501SP 1.3 Establish Verification Procedures and Criteria Establish and maintain verification procedures and criteria for the selected work products. IPPD Addition The verification procedures and criteria should be developed concurrently and iteratively with the product and product component designs. Verification criteria are defined to ensure that the wo rk products meet their requirements. Examples of sources for verification criteria include the following: • Product and product component requirements • Standards • Organizational policies • Test type • Test parameters • Parameters for tradeoff between quality and cost of testing • Type of work products • Suppliers • Proposals and agreements Typical Work Products 1. Verification procedures 2. Verification criteria Subpractices 1. Generate the set of comprehensive, integrated verification procedures for work products and any commercial off-the-shelf products, as necessary. 2. Develop and refine the verification criteria when necessary. 3. Identify the expected results, any tolerances allowed in observation, and other criteria fo r satisfying the requirements. 4. Identify any equipment and env ironmental components needed to support verification.",
        "CMMI for Development Version 1.2 Verification (VER) 502 SG 2 Perform Peer Reviews Peer reviews are performed on selected work products. Peer reviews involve a methodical exam ination of work products by the producers’ peers to identify defects for removal and to recommend other changes that are needed. The peer review is an important and effective verification method implemented via inspections, struct ured walkthroughs, or a number of other collegial review methods. Peer reviews are primarily applied to work products developed by the projects, but they can also be applie d to other work products such as documentation and training work produ cts that are typically developed by support groups. SP 2.1 Prepare for Peer Reviews Prepare for peer reviews of selected work products. Preparation activities for peer revi ews typically include identifying the staff who will be invited to participat e in the peer review of each work product; identifying the key reviewer s who must participate in the peer review; preparing and updating any materials that will be used during the peer reviews, such as checklists and review criteria, and scheduling peer reviews. Typical Work Products 1. Peer review schedule 2. Peer review checklist 3. Entry and exit criteria for work products 4. Criteria for requiring another peer review 5. Peer review training material 6. Selected work products to be reviewed Subpractices 1. Determine what type of peer review will be conducted. Examples of types of peer reviews include the following: • Inspections • Structured walkthroughs • Active reviews",
        "CMMI for Development Version 1.2 Verification (VER) 5032. Define requirements for collect ing data during the peer review. Refer to the Measurement and Analysis process area for information about identif ying and collecting data. 3. Establish and maintain entry and exit criteria for the peer review. 4. Establish and maintain criteria for requiring another peer review. 5. Establish and maintain checklists to ensure that the work products are reviewed consistently. Examples of items addressed by the checklists include the following: • Rules of construction • Design guidelines • Completeness • Correctness • Maintainability • Common defect types The checklists are modified as necessary to address the specific type of work product and peer review. The peers of the checklist developers and potential users review the checklists. 6. Develop a detailed peer review schedule, including the dates for peer review training and for when materials for peer reviews will be available. 7. Ensure that the work product sati sfies the peer review entry criteria prior to distribution. 8. Distribute the work product to be reviewed and its related information to the participants early enough to enable participants to adequately prepare for the peer review. 9. Assign roles for the peer review as appropriate. Examples of roles include the following: • Leader • Reader • Recorder • Author 10. Prepare for the peer review by re viewing the work product prior to conducting the peer review.",
        "CMMI for Development Version 1.2 Verification (VER) 504 SP 2.2 Conduct Peer Reviews Conduct peer reviews on selected work products and identify issues resulting from the peer review. One of the purposes of conducting a peer review is to find and remove defects early. Peer reviews are performed incrementally as work products are being developed. Thes e reviews are structured and are not management reviews. Peer reviews may be performed on key wo rk products of specification, design, test, and implementation acti vities and specif ic planning work products. The focus of the peer review should be on the work product in review, not on the person who produced it. When issues arise during the peer review, they should be communicated to the primary deve loper of the work product for correction. Refer to the Project Monitoring and Control process area for information about tracking issues that arise during a peer review. Peer reviews should address the following guidelines: there must be sufficient preparation, the conduc t must be managed and controlled, consistent and sufficient data mu st be recorded (an example is conducting a formal inspection), and action items must be recorded. Typical Work Products 1. Peer review results 2. Peer review issues 3. Peer review data Subpractices 1. Perform the assigned roles in the peer review. 2. Identify and document defects and other issues in the work product. 3. Record the results of the peer review, including the action items. 4. Collect peer review data. Refer to the Measurement and Analysis process area for more information about data collection. 5. Identify action items and communi cate the issues to relevant stakeholders. 6. Conduct an additional peer review if the defined criteria indicate the need. 7. Ensure that the exit criteria for the peer review are satisfied.",
        "CMMI for Development Version 1.2 Verification (VER) 505SP 2.3 Analyze Peer Review Data Analyze data about preparation, conduct, and results of the peer reviews. Refer to the Measurement and Analysis process area for more information about obtai ning and analyzing data. Typical Work Products 1. Peer review data 2. Peer review action items Subpractices 1. Record data related to the prepar ation, conduct, and results of the peer reviews. Typical data are product name, product size, composition of the peer review team, type of peer review, preparation time per reviewer, length of the review meeting, number of defects found, type and origin of defect, and so on. Additional information on the work product being peer reviewed may be collected, such as size, development stage, operating modes examined, and requirements being evaluated. 2. Store the data for futu re reference and analysis. 3. Protect the data to ensure t hat peer review data are not used inappropriately. Examples of inappropriate use of peer review data include using data to evaluate the performance of people and using data for attribution. 4. Analyze the peer review data. Examples of peer review data that can be analyzed include the following: • Phase defect was injected • Preparation time or rate versus expected time or rate • Number of defects versus number expected • Types of defects detected • Causes of defects • Defect resolution impact",
        "CMMI for Development Version 1.2 Verification (VER) 506 SG 3 Verify Selected Work Products Selected work products are verified against their specified requirements. The verification methods, procedures, and criteria are used to verify the selected work products and any a ssociated maintenance, training, and support services using the appropriate verification environment. Verification activities should be performed throughout the product lifecycle. Practices related to peer reviews as a specific verification method are included in specific goal 2. SP 3.1 Perform Verification Perform verification on the selected work products. Verifying products and work products incrementally promotes early detection of problems and can result in the early removal of defects. The results of verification save cons iderable cost of fault isolation and rework associated with troubleshooting problems. Typical Work Products 1. Verification results 2. Verification reports 3. Demonstrations 4. As-run procedures log Subpractices 1. Perform verification of sele cted work products against their requirements. 2. Record the results of verification activities. 3. Identify action items resulting from verification of work products. 4. Document the “as-run” verifica tion method and the deviations from the available methods and proc edures discovered during its performance. SP 3.2 Analyze Verification Results Analyze the results of all verification activities. Actual results must be compared to established verification criteria to determine acceptability. The results of the analysis are reco rded as evidence that verification was conducted.",
        "CMMI for Development Version 1.2 Verification (VER) 507For each work product, all available verification results are incrementally analyzed to ensure that the requirements have been met. Since a peer review is one of seve ral verification methods, peer review data should be included in this analysis activity to ensure that the verification results are analyzed sufficient ly. Analysis reports or “as-run” method documentation may also indica te that bad verification results are due to method problems, criteria problems, or a verification environment problem. Typical Work Products 1. Analysis report (e.g., statistics on performances, caus al analysis of nonconformances, comparison of the behavior between the real product and models, and trends) 2. Trouble reports 3. Change requests for the ve rification methods, criteria, and environment Subpractices 1. Compare actual result s to expected results. 2. Based on the established verificati on criteria, identify products that have not met their requirements or identify problems with the methods, procedures, criteria, and verification environment 3. Analyze the verifi cation data on defects. 4. Record all results of the analysis in a report. 5. Use verification results to compare actual measurements and performance to technical performance parameters. 6. Provide information on how def ects can be resolved (including verification methods, criteria, and verification environment) and initiate corrective action. Refer to the corrective action prac tices of Project Monitoring and Control process area for more information about implementing corrective action.",
        "CMMI for Development Version 1.2 Verification (VER) 508 Generic Practices by Goal Continuous Only GG 1 Achieve Specific Goals The process supports and enables achievement of the specific goals of the process area by transforming identifiable input work products to produce identifiable output work products. GP 1.1 Perform Specific Practices Perform the specific practices of the verification process to develop work products and provide services to achieve the specific goals of the process area. GG 2 Institutionalize a Managed Process The process is institutionalized as a managed process. Staged Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the staged representation. GP 2.1 Establish an Organizational Policy Establish and maintain an organizational policy for planning and performing the verification process. Elaboration: This policy establishes organizational expectations for establishing and maintaining verification methods, procedures, criteria, and the verification environment, as well as for performing peer reviews and verifying selected work products. GP 2.2 Plan the Process Establish and maintain the plan for performing the verification process.",
        "CMMI for Development Version 1.2 Verification (VER) 509Elaboration: This plan for performing the verification process can be included in (or referenced by) the project plan, which is described in the Project Planning process area. GP 2.3 Provide Resources Provide adequate resources for performing the verification process, developing the work products, and providing the services of the process. Elaboration: Special facilities may be required fo r verifying selected work products. When necessary, the facilities requi red for the activities in the Verification process area are developed or purchased. Certain verification methods may require special tools, equipment, facilities, and training (e.g., peer re views may require meeting rooms and trained moderators; and certain verification tests may require special test equipment and people sk illed in the use of the equipment). Examples of other resources provided include the following tools: • Test management tools • Test-case generators • Test-coverage analyzers • Simulators GP 2.4 Assign Responsibility Assign responsibility and authority for performing the process, developing the work products, and providing the services of the verification process. GP 2.5 Train People Train the people performing or supporting the verification process as needed.",
        "CMMI for Development Version 1.2 Verification (VER) 510 Elaboration: Examples of training topics include the following: • Application or service domain • Verification principles, standards, and methods (e.g., analysis, demonstration, inspection, and test) • Verification tools and facilities • Peer review preparation and procedures • Meeting facilitation GP 2.6 Manage Configurations Place designated work products of the verification process under appropriate levels of control. Elaboration: Examples of work products placed under control include the following: • Verification procedures and criteria • Peer review training material • Peer review data • Verification reports GP 2.7 Identify and Involve Relevant Stakeholders Identify and involve the relevant stakeholders of the verification process as planned. Elaboration: Select relevant stakeholders from customers, end users, developers, producers, testers, suppliers, ma rketers, maintainers, disposal personnel, and others who may be affected by, or may affect, the product as well as the process. Examples of activities for stakeholder involvement include the following: • Selecting work products and methods for verification • Establishing verification procedures and criteria • Conducting peer reviews • Assessing verification results and identifying corrective action",
        "CMMI for Development Version 1.2 Verification (VER) 511GP 2.8 Monitor and Control the Process Monitor and control the verification process against the plan for performing the process and take appropriate corrective action. Elaboration: Examples of measures and work products used in monitoring and controlling include the following: • Verification profile (e.g., the number of verifications planned and performed, and the defects found; or perhaps categorized by verification method or type) • Number of defects detected by defect category • Verification problem report trends (e.g., number written and number closed) • Verification problem report status (i.e., how long each problem report has been open) • Schedule for a specific verification activity GP 2.9 Objectively Evaluate Adherence Objectively evaluate adherence of the verification process against its process description, standards, and procedures, and address noncompliance. Elaboration: Examples of activities reviewed include the following: • Selecting work products for verification • Establishing and maintaining verification procedures and criteria • Performing peer reviews • Verifying selected work products Examples of work products reviewed include the following: • Verification procedures and criteria • Peer review checklists • Verification reports GP 2.10 Review Status with Higher Level Management Review the activities, status, and results of the verification process with higher level management and resolve issues.",
        "CMMI for Development Version 1.2 Verification (VER) 512 Continuous Only GG 3 Institutionalize a Defined Process The process is institutionalized as a defined process. This generic goal's appearance here reflects its location in the continuous representation. GP 3.1 Establish a Defined Process Establish and maintain the description of a defined verification process. GP 3.2 Collect Improvement Information Collect work products, measures, measurement results, and improvement information derived from planning and performing the verification process to support the future use and improvement of the organization’s processes and process assets. Elaboration: Examples of work products, measures, measurement results, and improvement information include the following: • Peer review records that include conduct time and average preparation time • Number of product defects found through verification per development phase • Verification and analysis report Continuous Only GG 4 Institutionalize a Quantitatively Managed Process The process is institutionalized as a quantitatively managed process. GP 4.1 Establish Quantitative Objectives for the Process Establish and maintain quantitative objectives for the verification process, which address quality and process performance, based on customer needs and business objectives. GP 4.2 Stabilize Subpr ocess Performance Stabilize the performance of one or more subprocesses to determine the ability of the verification process to achieve the established quantitative quality and process-performance objectives.",
        "CMMI for Development Version 1.2 Verification (VER) 513Continuous Only GG 5 Institutionalize an Optimizing Process The process is institutionalized as an optimizing process. GP 5.1 Ensure Continuous Process Improvement Ensure continuous improvement of the verification process in fulfilling the relevant business objectives of the organization. GP 5.2 Correct Root Causes of Problems Identify and correct the root causes of defects and other problems in the verification process.",
        "CMMI for Development Version 1.2 Verification (VER) 514",
        "CMMI for Development Version 1.2 The Appendices and Glossary 515PART THREE The Appendices and Glossary",
        "CMMI for Development Version 1.2 The Appendices and Glossary 516",
        "CMMI for Development Version 1.2 References 517A. References Publicly Available Sources Ahern 2003 Ahern, Dennis M.; Clouse, Aaron; & Turner, Richard. CMMI Distilled: A Practical Introduction to Integrated Process Improvement, Second Edition. Boston: Addison-Wesley, 2003. Ahern 2005 Ahern, Dennis M.; Armstrong, Jim; Clouse, Aaron; Ferguson, Jack R.; Hayes, Will; & Nidiffer, Kenneth E. CMMI SCAMPI Distilled: Appraisals for Process Improvement. Boston: Addison-Wesley, 2005. Chrissis 2003 Chrissis, Mary Beth; Konr ad, Mike; & Shrum, Sandy. CMMI: Guidelines for Process Integration and Product Improvement. Boston: Addison-Wesley, 2003. Crosby 1979 Crosby, Philip B. Quality Is Free The Art of Making Quality Certain. New York: McGraw-Hill, 1979. Curtis 2002 Curtis, Bill; Hefley, Willia m E.; & Miller, Sally A. The People Capability Maturity Model Gu idelines for Improving the Workforce. Boston: Addison-Wesley, 2002. Deming 1986 Deming, W. Edwards. Out of the Crisis. Cambridge, MA: MIT Center for Advanced Engineering, 1986. DoD 1996 Department of Defense. DoD Guide to Integrated Product and Process Development (Version 1.0). Washington, DC: Office of the Under Secretary of Defense (Acquisition and Technology), February 5, 1996. http://www.abm.rda.hq.navy.m il/navyaos/content/download/ 1000/4448/file/ippdhdbk.pdf. Dymond 2004 Dymond, Kenneth M. A Guide to the CMMI: Interpreting the Capability Maturity Model Integration. Annapolis, MD: Process Transition International Inc., 2004. EIA 1994 Electronic Industries Alliance. EIA Interim Standard: Systems Engineering (EIA/IS-632). Washington, DC, 1994.",
        "CMMI for Development Version 1.2 References 518 EIA 1998 Electronic Industries Alliance. Systems Engineering Capability Model (EIA/IS-731). Washington, DC, 1998; (Note: This model has been retired by EIA.) GEIA 2004 Government Electronic Industries Alliance. Data Management (GEIA-859). Washington, DC, 2004. http://webstore.ansi.org/ansi docstore/product.asp?sku=GEI A-859-2004. Gibson 2006 Gibson, Diane L.; Goldenson, Dennis R. & Kost, Keith. Performance Results of CMMI-Based Process Improvement. (CMU/SEI-2006-TR-004, ESC-TR-2006-004). Pittsburgh, PA: Software E ngineering Institute, Carnegie Mellon University, August 2006. http://www.sei.cmu.edu/publicat ions/documents/06.reports/0 6tr004.html. Humphrey 1989 Humphrey, Watts S. Managing the Software Process. Reading, MA: Addison-Wesley, 1989. IEEE 1990 Institute of Electrical and Electronics Engineers. IEEE Standard Computer Dictionary: A Compilation of IEEE Standard Computer Glossaries. New York: IEEE, 1990. ISO 1987 International Organization for Standardization. ISO 9000: International Standard. 1987. http://www.iso.ch/. ISO 1995 International Organization for Standardization and International Electrotechnical Commission. ISO/IEC TR 12207 Information Technology—Software Life Cycle Processes, 1995. http://www.jtc1-sc7.org. ISO 1998 International Organization for Standardization and International Electrotechnical Commission. ISO/IEC TR 15504 Information Technology—Software Process Assessment, 1998. http://www.iso.ch/. ISO 2000 International Organization for Standardization. ISO 9001, Quality Management Syst ems—Requirements, 2000. http://www.iso.ch/. ISO 2002a International Organization for Standardization and International Electrotechnical Commission. ISO/IEC 15939 Software Engineering—Software Measurement Process, 2002. http://www.iso.ch/.",
        "CMMI for Development Version 1.2 References 519ISO 2002b International Organization for Standardization and International Electrotechnical Commission. ISO/IEC 15288 Systems Engineering—System Life Cycle Processes, 2002. http://www.jtc1-sc7.org/. ISO 2006 International Organization for Standardization and International Electrotechnical Commission. ISO/IEC TR 15504 Information Technology—Software Process Assessment Part 1: Concept s and Vocabulary, Part 2: Performing an Assessment, Part 3: Guidance on Performing an Assessment, Part 4: Guidance on Use for Process Improvement and Process Capabilit y Determination, Part 5: An Exemplar Process A ssessment Model, 2003-2006. http://www.jtc1-sc7.org/. Juran 1988 Juran, Joseph M. Juran on Planning for Quality. New York: Macmillan, 1988. McGarry 2000 McGarry, John; Card, David; Jones, Cheryl; Layman, Beth; Clark, Elizabeth; Dean, Joseph; & Hall, Fred. Practical Software Measurement: Objective Information for Decision Makers. Boston: Addison-Wesley, 2002. SEI 1995 Software Engineering Institute. The Capability Maturity Model: Guidelines for Improv ing the Software Process. Reading, MA: Addison-Wesley, 1995. SEI 1997a Integrated Product De velopment Capability Maturity Model, Draft Version 0.98. Pittsburgh, PA: Enterprise Process Improvement Collaboration and Software Engineering Institute, Carnegie Mellon University, July 1997. (Note: This model was never officially released and is no longer publicly available.) SEI 1997b Software Engineering Institute. Software CMM, Version 2.0 (Draft C), October 22, 1997. (Note: This model was never officially released and is no longer publicly available.) SEI 2001 Paulk, Mark C. & Chrissis, Mary Beth. The 2001 High Maturity Workshop (CMU/SEI-2001-SR-014). Pittsburgh, PA: Software Engineering Institute, Carnegie Mellon University, January 2002. http://www.sei.cmu.edu/publicat ions/documents/01.reports/0 1sr014.html.",
        "CMMI for Development Version 1.2 References 520 SEI 2002a CMMI Product Development Team. CMMI for Systems Engineering/Software Engineering/Integrated Product and Process Development/Supplier Sourcing, Version 1.1 Staged Representation (CMU /SEI-2002-TR-012, ESC-TR- 2002-012). Pittsburgh, PA: Software Engineering Institute, Carnegie Mellon University, March 2002. http://www.sei.cmu.edu/publicat ions/documents/02.reports/0 2tr012.html. SEI 2002b CMMI Product Development Team. CMMI for Systems Engineering/Software Engineering/Integrated Product and Process Development/Supplier Sourcing, Version 1.1 Continuous Representation (CMU/SEI-2002-TR-011, ESC- TR-2002-011). Pittsburgh, PA: Software Engineering Institute, Carnegie Mellon University, March 2002. http://www.sei.cmu.edu/publicat ions/documents/02.reports/0 2tr011.html. SEI 2002c Software Engineering Institute. Software Acquisition Capability Maturity Model (SA-CMM) Version 1.03 (CMU/SEI-2002-TR-010, ESC-TR-2002-010). Pittsburgh, PA: Software Engineering Institute, Carnegie Mellon University, March 2002. http://www.sei.cmu.edu/publicat ions/documents/02.reports/0 2tr010.html. SEI 2004 Software Engineering Institute. CMMI A-Specification, Version 1.6. http://www.sei.cmu.edu/cmmi /background/aspec.html (February 2004). SEI 2005 Software Engineering Institute. CMMI Acquisition Module (CMMI-AM) Version 1.1 (CMU/SEI-2005-TR-011). Pittsburgh, PA: Software E ngineering Institute, Carnegie Mellon University, May 2005. http://www.sei.cmu.edu/publicat ions/documents/05.reports/0 5tr011/05tr011.html. SEI 2006a CMMI Product Development Team. ARC v1.2, Appraisal Requirements for CMMI, Version 1.2 (CMU/SEI-2006-TR- 011). Pittsburgh, PA: Software Engineering Institute, Carnegie Mellon University, July 2006. http://www.sei.cmu.edu/publicat ions/documents/01.reports/0 6tr011.html.",
        "CMMI for Development Version 1.2 References 521SEI 2006b CMMI Product Development Team. SCAMPI v1.2, Standard CMMI Appraisal Method for Pr ocess Improvement, Version 1.2: Method Definition Docum ent (CMU/SEI-2006-HB-002). Pittsburgh, PA: Software E ngineering Institute, Carnegie Mellon University, July 2006. http://www.sei.cmu.edu/publicat ions/documents/06.reports/0 6hb002.html. Shewhart 1931 Shewhart, Walter A. Economic Control of Quality of Manufactured Product. New York: Van Nostrand, 1931. Regularly Updated Sources SEI 1 Software Engineering Institute. The IDEAL Model. http://www.sei.cmu.edu/ideal/ideal.html. SEI 2 Software Engineering Institute. CMMI Frequently Asked Questions (FAQs). http://www.sei.cmu.edu/cmmi/ adoption/cmmi-faq.html. SEI 3 Software Engineering Institute. CMMI Performance Results. http://www.sei.cmu.edu/c mmi/results.html.",
        "CMMI for Development Version 1.2 Acronyms 522 B. Acronyms API application program interface ARC Appraisal Requirements for CMMI CAD computer-aided design CAR Causal Analysis and Resolution (process area) CCB configuration control board CL capability level CM Configuration Management (process area) CMM Capability Maturity Model CMMI Capability Maturity Model Integration CMMI-DEV CMMI for Development CMMI-DEV+IPPD CMMI for Development +IPPD COTS commercial off the shelf CPI cost performance index CPM critical path method CSCI computer software configuration item DAR Decision Analysis and Resolution (process area) DoD Department of Defense EIA Electronic Industries Alliance EIA/IS Electronic Industries Alliance/Interim Standard EPG engineering process group FCA functional configuration audit",
        "CMMI for Development Version 1.2 Acronyms 523GG generic goal GP generic practice IBM International Business Machines IDEAL Initiating, Diagnosing, Est ablishing, Acting, Learning IEEE Institute of Electrical and Electronics Engineers INCOSE International Council on Systems Engineering IPD-CMM Integrated Product De velopment Capability Maturity Model IPM Integrated Project Management (process area) IPM+IPPD Integrated Project Management +IPPD (process area) IPPD integrated product and process development ISO International Organization for Standardization ISO/IEC International Organization for Standardization and International Electrotechnical Commission MA Measurement and Analysis (process area) MDD Method Definition Document ML maturity level NDI nondevelopmental item NDIA National Defense Industrial Association OID Organizational Innovation and Deployment (process area) OPD Organizational Process Definition (process area) OPD+IPPD Organizational Process Definition +IPPD (process area) OPF Organizational Process Focus (process area) OPP Organizational Process Performance (process area) OT Organizational Training (process area) OUSD (AT&L) Office of the Under Secretar y of Defense (Acquisition, Technology, and Logistics)",
        "CMMI for Development Version 1.2 Acronyms 524 P-CMM People Capability Maturity Model PA process area PCA physical configuration audit PERT Program Evaluation and Review Technique PI Product Integration (process area) PMC Project Monitoring and Control (process area) PP Project Planning (process area) PPQA Process and Product Quality Assurance (process area) QA quality assurance QFD Quality Function Deployment QPM Quantitative Project Management (process area) RD Requirements Development (process area) REQM Requirements Management (process area) ROI return on investment RSKM Risk Management (process area) SA-CMM Software Acquisition Capability Maturity Model SAM Supplier Agreement Management (process area) SCAMPI Standard CMMI Appraisal Method for Process Improvement SECM Systems Engineering Capability Model SEI Software Engineering Institute SG specific goal SP specific practice SPI schedule performance index SW-CMM Capability Maturity Model for Software or Software Capability Maturity Model",
        "CMMI for Development Version 1.2 Acronyms 525TS Technical Solution (process area) URL uniform resource locator VAL Validation (process area) VER Verification (process area) WBS work breakdown structure",
        "CMMI for Development Version 1.2 Project Participants 526 C. CMMI for Development Project Participants Many talented people have been part of the product team that has created and maintained the CMMI Produc t Suite since its inception. This appendix recognizes the people invo lved in the update of CMMI for the version 1.2 release. The four primary groups involved in this development were the Product Team , Sponsors, Steering Group, and Configuration Control Board. Curr ent members of these groups are listed. If you wish to see a more complete listing of participants from previous years, see Appendix C of the version 1.1 models. Product Team The Product Team reviewed change reques ts submitted by CMMI users to change the CMMI Product Suite, including the framework, models, training, and appraisal materials. De velopment activities were based on change requests, version 1.2 guidelines provided by the Steering Group, and input from Configur ation Control Board members. The program manager for the version 1.2 release was Mike Phillips. He coordinated the efforts of the following teams. Model Team Members • Armstrong, Jim (Systems and Software Consortium) • Bate, Roger (Software Engineering Institute) • Cepeda, Sandra (RD&E Command, Software Engineering Directorate) • Chrissis, Mary Beth (Sof tware Engineering Institute) • Clouse, Aaron (Raytheon) • D'Ambrosa, Mike (BAE Systems) • Hollenbach, Craig (Northrop Grumman) • Konrad, Mike (Software Engineering Institute)15 • Norimatsu, So (Norimatsu Proc ess Engineering Laboratory, Inc.) • Richter, Karen (Institu te for Defense Analyses) • Shrum, Sandy (Software Engineering Institute) 15 Team Leader",
        "CMMI for Development Version 1.2 Project Participants 527SCAMPI Upgrade Team Members • Busby, Mary (Lockheed Martin)16 • Cepeda, Sandra (RD&E Command, Software Engineering Directorate) • Ferguson, Jack (Software Engineering Institute)16 • Hayes, Will (Software Engineering Institute) • Heil, James (U.S. Army) in memoriam • Kirkham, Denise (Boeing) • Masters, Steve (Software Engineering Institute) • Ming, Lisa (BAE Systems) • Ryan, Charlie (Software Engineering Institute) • Sumpter, Beth (National Security Agency) • Ulrich, Ron (Northrop Grumman) • Wickless, Joe (Software Engineering Institute) Training Team Members • Chrissis, Mary Beth (Sof tware Engineering Institute) • Gibson, Diane (Softwar e Engineering Institute) • Knorr, Georgeann (Softwar e Engineering Institute) • Kost, Keith (Software Engineering Institute) • Matthews, Jeanne (Software Engineering Institute) • Shrum, Sandy (Software Engineering Institute) • Svolou, Agapi (Software Engineering Institute) • Tyson, Barbara (Softw are Engineering Institute)17 • Wickless, Joe (Software Engineering Institute) • Wolf, Gary (Raytheon) Architecture Team Members • Bate, Roger (Software Engineering Institute) • Chrissis, Mary Beth (Sof tware Engineering Institute) • Hoffman, Hubert (General Motors) • Hollenbach, Craig (Northrop Grumman) • Ming, Lisa (BAE Systems) 16 Co-Team Leaders 17 Team Leader",
        "CMMI for Development Version 1.2 Project Participants 528 • Phillips, Mike (Software Engineering Institute)18 • Scibilia, John (U.S. Army) • Wilson, Hal (Northrop Grumman) • Wolf, Gary (Raytheon) Hardware Team Members • Armstrong, Jim (Systems and Software Consortium) • Bishop, Jamie (Lockheed Martin) • Cattan, Denise (Spirula) • Clouse, Aaron (Raytheon) • Connell, Clifford (Raytheon) • Fisher, Jerry (Aerospace Corporation) • Hertneck, Christian (Siemens) • Nussbaum, Winfried (Siemens) • Phillips, Mike (Software Engineering Institute)19 • Zion, Christian (THALES) Piloting Team Members • Brown, Rhonda (Software Engineering Institute)20 • Chrissis, Mary Beth (Sof tware Engineering Institute) • Ferguson, Jack (Software Engineering Institute) • Konrad, Mike (Software Engineering Institute) • Phillips, Marilyn (Q-Labs, Inc.) • Phillips, Mike (Software Engineering Institute)20 • Tyson, Barbara (Softwar e Engineering Institute) Quality Team Members • Brown, Rhonda (Software Engineering Institute)21 • Kost, Keith (Software Engineering Institute) • McSteen, Bill (Softwar e Engineering Institute) • Shrum, Sandy (Software Engineering Institute) 18 Team Leader 19 Team Leader 20 Co-Team Leaders 21 Team Leader",
        "CMMI for Development Version 1.2 Project Participants 529Sponsors The CMMI version 1.2 project wa s sponsored by both government and industry. Government sponsorship was provided by the U.S. Department of Defense (DoD), specifically t he Office of the Under Secretary of Defense (Acquisition, Technology, and Logistics) (OUSD [AT&L]). Industry sponsorship was provided by the Systems Engineering Committee of the National Defense Industrial Association (NDIA). • Rassa, Bob (NDIA Systems Engineering Division) • Schaeffer, Mark (OUSD [AT&L]) Steering Group The Steering Group has guided and ap proved the plans of the version 1.2 Product Team, provided consulta tion on significant CMMI project issues, and ensured involvement fr om a variety of interested communities. Steering Group Members • Baldwin, Kristen (OUSD [AT&L] DS/SE) • Chittister, Clyde (Software Engineering Institute) • D'Agosto, Tony (U.S. Army RDECOM-ARDEC) • Gill, Jim (Boeing Integrated Defense Systems) • Kelly, John (NASA HQ) • Lundeen, Kathy (Defense C ontract Management Agency) • McCarthy, Larry (Motorola, Inc.) • Nicol, Mike (U.S. Air Force ASC/EN)22 • Peterson, Bill (Software Engineering Institute) • Rassa, Bob (Raytheon Space & Airborne Systems)23 • Weszka, Joan (Lockheed Martin) • Wilson, Hal (Northrop Grumman Mission Systems) • Zettervall, Brenda (U.S. Navy, ASN/RDA CHENG) 22 Government Co-Chair 23 Industry Co-Chair",
        "CMMI for Development Version 1.2 Project Participants 530 Ex-Officio Steering Group Members • Anderson, Lloyd (Departm ent of Homeland Security) • Bate, Roger; chief architect (S oftware Engineering Institute) • Drake, Thomas (Nati onal Security Agency) • Phillips, Mike; CMMI program manager (Software Engineering Institute) • Sumpter, Beth (National Security Agency) • Yedlin, Debbie (General Motors) Steering Group Support: Acquisition • Gallagher, Brian (Softwar e Engineering Institute) Steering Group Support: CCB • Konrad, Mike (Software Engineering Institute) Configuration Control Board The Configuration Control Board has been the official mechanism for controlling changes to the version 1.2 CMMI for Development models. This group was responsible for pr oduct integrity by reviewing all changes to the baselines and approv ing only changes that met the criteria for version 1.2. CCB Members • Atkinson, Shane (Borland/TeraQuest) • Bate, Roger (Software Engineering Institute) • Bernard, Tom (U.S. Air Force) • Chrissis, Mary Beth (Sof tware Engineering Institute) • Croll, Paul (Computer Sciences Corporation) • Gristock, Stephen (JPMorganChase) • Hefner, Rick (Northrop Grumman Corporation) • Jacobsen, Nils (Motorola) • Konrad, Mike (Software Engineering Institute)24 • Osiecki, Lawrence (U.S. Army) • Peterson, Bill (Software Engineering Institute) 24 Configuration Control Board Chair",
        "CMMI for Development Version 1.2 Project Participants 531• Phillips, Mike (Software Engineering Institute) • Rassa, Bob (Raytheon) • Richter, Karen (Institu te for Defense Analyses) • Sapp, Millee (U.S. Air Force) • Schoening, Bill (Boeing and INCOSE) • Schwomeyer, Warren (Lockheed Martin) • Smith, Katie (U.S. Navy) • Wolf, Gary (Raytheon) Non-Voting CCB Members • Brown, Rhonda (Software Engineering Institute) • Shrum, Sandy (Software Engineering Institute)",
        "CMMI for Development Version 1.2 532 Glossary D. Glossary The CMMI glossary defines the basic terms used in the CMMI models. Glossary entries are typically multiple-word terms consisting of a noun and one or more restrictive modifiers. (There are some exceptions to this rule that account for one- word terms in the glossary.) To formulate definitions approp riate for CMMI, we consult multiple sources. We first consult Merriam-Webster OnLine dictionary (www.m-w.com) and the source models (i.e., EIA 731, SW-CMM v2, draft C, and IPD-CMM v0.98). We also consult other standards as needed, including the following: • ISO 9000 [ISO 1987] • ISO/IEC 12207 [ISO 1995] • ISO/IEC 15504 [ISO 2006] • ISO/IEC 15288 [ISO 2002b] • IEEE [IEEE 1990] • SW-CMM v1.1 • EIA 632 [EIA 1994] • SA-CMM [SEI 2002c] • P-CMM [Curtis 2002] We developed the glossary recogni zing the importance of using terminology that all model us ers can understand. We also recognized that words and term s can have different meanings in different contexts and environments. The glossary in CMMI models is designed to document the meanings of words and terms that should have the widest use and understanding by users of CMMI products. acceptance criteria The criteria that a product or product component must satisfy to be accepted by a user, customer, or other authorized entity. acceptance testing Formal testing conducted to enable a user, customer, or other authorized entity to det ermine whether to accept a product or product component. (See also “unit testing.”)",
        "CMMI for Development Version 1.2 Glossary 533 achievement profile In the continuous representati on, a list of process areas and their corresponding capability le vels that represent the organization’s progress for each process area while advancing through the capability levels. (See also “capability level profile,” “target profile,” and “target staging.”) acquisition The process of obtaining pr oducts (goods and services) through contract. acquisition strategy The specific approach to acqui ring products and services that is based on considerati ons of supply sources, acquisition methods, requirem ents specification types, contract or agreement types, and the related acquisition risk. addition In the CMMI Product Suite, a clearly marked model component that contains informat ion of interest to particular users. In a CMMI model, all additions bearing the same name (e.g., the IPPD addition) may be optionally selected as a group for use. adequate This word is used so that you can interpret goals and practices in light of your or ganization’s business objectives. When using any CMMI model, you must interpret the practices so that they work fo r your organization. This term is used in goals and practices w here certain activities may not be done all of the time. (See also “appropriate” and “as needed.”) allocated requirement Requirement that levies all or part of the performance and functionality of a higher level requirement on a lower level architectural element or design component. alternative practice A practice that is a substitute for one or more generic or specific practices contained in CMMI models that achieves an equivalent effect toward sati sfying the generic or specific goal associated with model prac tices. Alternative practices are not necessarily one-for-one replacements for the generic or specific practices. amplification Amplifications are informat ive model components that contain information relevant to a particular discipline. For example, to find an amplificat ion for software engineering, you would look in the model for items labeled “For Software Engineering.” The same is true for other disciplines. appraisal In the CMMI Product Suite, an examination of one or more processes by a trained team of professionals using an appraisal reference model as t he basis for determining, at a minimum, strengths and weaknesses. (See also “assessment” and “capab ility evaluation.”)",
        "CMMI for Development Version 1.2 534 Glossary appraisal findings The results of an appraisal that identify the most important issues, problems, or opportuniti es for process improvement within the appraisal scope. Appr aisal findings are inferences drawn from corroborated objective evidence. appraisal participants Members of the organizational unit who participate in providing information during the appraisal. appraisal rating As used in CMMI appraisal materials, the value assigned by an appraisal team to (a) a CMMI goal or process area, (b) the capability level of a proce ss area, or (c) the maturity level of an organizational unit. The rating is determined by enacting the defined rating proc ess for the appraisal method being employed. appraisal reference model As used in CMMI appraisal materials, the CMMI model to which an appraisal team correlates implemented process activities. appraisal scope The definition of the boundaries of the appraisal encompassing the organizational limits and the CMMI model limits within which the processe s to be investigated operate. appropriate This word is used so that you can interpret goals and practices in light of your or ganization’s business objectives. When using any CMMI model, you must interpret the practices so that they work fo r your organization. This term is used in goals and practices w here certain activities may not be done all of the time. (See also “adequate” and “as needed.”) as needed This phrase is used so that you can interpret goals and practices in light of your or ganization’s business objectives. When using any CMMI model, you must interpret the practices so that they work fo r your organization. This term is used in goals and practices w here certain activities may not be done all the time. (See also “adequate” and “appropriate.”) assessment In the CMMI Product Suite, an appraisal that an organization does internally for the purpos es of process improvement. The word assessment is also used in the CMMI Product Suite in an everyday English s ense (e.g., risk assessment). (See also “appraisal” and “capability evaluation.”) assignable cause of process variation In CMMI, the term special caus e of process va riation is used in place of assignable cause of process variation to ensure consistency. The two terms are defined identically. (See “special cause of pr ocess variation.”)",
        "CMMI for Development Version 1.2 Glossary 535 audit In CMMI process improv ement work, an objective examination of a work produc t or set of work products against specific criteria (e.g., requirements). base measure A distinct property or charac teristic of an entity and the method for quantifying it. (See al so “derived measures.”) baseline A set of specifications or work products that has been formally reviewed and agreed on, which thereafter serves as the basis for further de velopment, and which can be changed only through change cont rol procedures. (See also “configuration baseline” and “product baseline.”) bidirectional traceability An association among two or mo re logical entities that is discernable in either direction (i.e., to and from an entity). (See also “requirements traceab ility” and “traceability.”) business objectives (See “organization’s business objectives.”) capability evaluation An appraisal by a trained team of professionals used as a discriminator to select supplie rs, to monitor suppliers against the contract, or to deter mine and enforce incentives. Evaluations are used to gain insight into the process capability of a supplier organiza tion and are intended to help decision makers make better ac quisition decisions, improve subcontractor performance, and provide insight to a purchasing organization. (See also “appraisal” and “assessment.”) capability level Achievement of process impr ovement within an individual process area. A capability level is defined by t he appropriate specific and generic practices fo r a process area. (See also “generic goal,” “generic practice,” “maturity level,” and “process area.”) capability level profile In the continuous representati on, a list of process areas and their corresponding capability levels. (See also “achievement profile,” “target profile,” and “target staging.”) The profile may be an achievement profile when it represents the organization’s progress for each process area while advancing through t he capability levels. Or, the profile may be a target profile when it represents an objective for process improvement. capability maturity model A model that contains the e ssential elements of effective processes for one or more disciplines and describes an evolutionary improvement path from ad hoc, immature processes to disciplined, matu re processes with improved quality and effectiveness.",
        "CMMI for Development Version 1.2 536 Glossary capable process A process that can satisfy it s specified pr oduct quality, service quality, and process- performance objectives. (See also “stable process,” “standard process,” and “statistically managed process.”) causal analysis The analysis of defects to determine their cause. change management Judicious use of means to effect a change, or a proposed change, on a product or service. (See also “configuration management.”) CMMI Framework The basic structure that organizes CMMI components, including common elements of t he current CMMI models as well as rules and methods for generating models, appraisal methods (including associated artifacts), and training materials. The framework enables new disciplines to be added to CMMI so that the new disciplines will integrate with the existing ones. (See also “CMMI model” and “CMMI Product Suite.”) CMMI model One from the entire collection of possible models that can be generated from the CMMI Framework. Since the CMMI Framework can generate different models based on the needs of the organization using it, there are multiple CMMI models. (See also “CMMI Framework” and “CMMI Product Suite.”) CMMI model component Any of the main architectu ral elements that compose a CMMI model. Some of the main elements of a CMMI model include specific practices, generi c practices, specific goals, generic goals, process areas, c apability levels, and maturity levels. CMMI Product Suite The complete set of products developed around the CMMI concept. These products incl ude the framework itself, models, appraisal methods, appraisal materials, and various types of training. (See also “CMMI Framework” and “CMMI model.”) common cause of process variation The variation of a pr ocess that exists because of normal and expected interactions among t he components of a process. (See also “special cause of process variation.”) concept of operations (See “operational concept.”)",
        "CMMI for Development Version 1.2 Glossary 537 configuration audit An audit conducted to verify that a configuration item, or a collection of configuration it ems that make up a baseline, conforms to a spec ified standard or requirement. (See also “audit,” “configuration item,” “functional configuration audit,” and “physical configuration audit.”) configuration baseline The configuration informati on formally designated at a specific time during a produc t’s or product component’s life. Configuration baselines, plus approved changes from those baselines, constitute the current configuration information. (See also “product lifecycle.”) configuration control An element of configuration management cons isting of the evaluation, coordination, approval or disapproval, and implementation of changes to configuration items after formal establishment of their configuration i dentification. (See also “configuration identific ation,” “configur ation item,” and “configurati on management.”) configuration control board A group of people responsible for evaluating and approving or disapproving proposed changes to configuration items, and for ensuring implementati on of approved changes. (See also “configuration item.”) Configuration control board s are also known as change control boards. configuration identification An element of configurati on management c onsisting of selecting the configuration items for a product, assigning unique identifiers to them, and recording their functional and physical characteristics in technical documentation. (See also “configuration item,” “configuration management,” and “product.”) configuration item An aggregation of work produc ts that is designated for configuration management and treat ed as a single entity in the configuration managem ent process. (See also “configuration management.”) configuration management A discipline applying technical and administrative direction and surveillance to (1) identif y and document the functional and physical characteristics of a configuration item, (2) control changes to t hose characteristics, (3) record and report change processing and implementation status, and (4) verify compliance with spec ified requirements. (See also “configuration audit,” “configurat ion control,” “configuration identification,” and “configur ation status accounting.”)",
        "CMMI for Development Version 1.2 538 Glossary configuration status accounting An element of configuration management cons isting of the recording and reporting of information needed to manage a configuration effectively. This information includes a listing of the approved configuration i dentification, the status of proposed changes to the configuration, and the implementation status of approved changes. (See also “configuration identificat ion” and “configuration management.”) continuous representation A capability maturity model st ructure wherein capability levels provide a recommended order for approaching process improvement within each specified process area. (See also “capability level, ” “process area,” and “staged representation.”) contractor (See “supplier.”) corrective action Acts or deeds used to remedy a situation, remove an error, or adjust a condition. COTS Items that can be purchased from a commercial vendor. (COTS stands for commercial off the shelf.) customer The party (individual, project, or organization) responsible for accepting the product or for authorizing payment. The customer is external to the project (except possibly when integrated teams are used, as in IPPD), but not necessarily external to the organization. The customer may be a higher level project. Customers are a subset of stakeholders. (See also “stakeholder.”) In most cases where this term is used, the preceding definition is intended; however, in some contexts, the term “customer” is intended to include other relevant stakeholders. (See also “customer requirement.”) customer requirement The result of elicit ing, consolidating, and resolving conflicts among the needs, expectations, c onstraints, and interfaces of the product's relevant st akeholders in a way that is acceptable to the customer . (See also “customer.”) data Recorded information, regardless of the form or method of recording, including technical data, computer software documents, financial informati on, management information, representation of facts, number s, or datum of any nature that can be communicated, stored, and processed. data management The disciplined processes and systems that plan for, acquire, and provide stewardship for business and technical data, consistent with data r equirements, throughout the data lifecycle.",
        "CMMI for Development Version 1.2 Glossary 539 defect density Number of defects per unit of product size (e.g., problem reports per thousand lines of code). defined process A managed process that is tailored from the organization’s set of standard processes according to the organization’s tailoring guidelines; has a maintained process description; and contributes work prod ucts, measures, and other process improvement information to the organizational process assets. (See also “managed process.”) derived measures Data resulting from the mathemat ical function of two or more base measures. (See al so “base measure.”) derived requirements Requirements that are not explic itly stated in the customer requirements, but are infe rred (1) from contextual requirements (e.g., applicable standards, laws, policies, common practices, and management decisions), or (2) from requirements needed to specify a product component. Derived requirements can also arise during analysis and design of components of the pr oduct or system. (See also “product requirements.”) design review A formal, documented, comp rehensive, and systematic examination of a design to ev aluate the design requirements and the capability of the design to meet these requirements, and to identify problems and propose solutions. development In the CMMI Product Suite, not only development activities but also maintenance activiti es may be included. Projects that benefit from the best prac tices of CMMI can focus on development, maintenance, or both. developmental plan A plan for guiding, implementi ng, and controlling the design and development of one or more products. (See also “product lifecycle” and “project plan.”) discipline In the CMMI Product Suite, the bodies of knowledge available to you when selecting a CMMI model (e.g., systems engineering). The CMMI Product Team envisions that other bodies of knowl edge will be integrated into the CMMI Framework in the future. document A collection of data, regardless of the medium on which it is recorded, that generally has permanence and can be read by humans or machines. So, documents include both paper and electronic documents. enterprise The full composition of com panies. Companies may consist of many organizations in many locations with different customers. (See also “organization.”)",
        "CMMI for Development Version 1.2 540 Glossary entry criteria States of being that must be present before an effort can begin successfully. equivalent staging A target staging, creat ed using the continuous representation, which is defined so that the results of using the target staging can be compar ed to the maturity levels of the staged representation. (See also “capability level profile,” “maturity level,” “tar get profile,” and “target staging.”) Such staging permits benchmarking of progress among organizations, enterprises, and projects, regardless of the CMMI representation used. The organization may implement components of CMMI models beyond those reported as part of equivalent staging. Equivalent staging is only a measure to relate how the organization is compared to other organizations in terms of maturity levels. establish and maintain In the CMMI Product Suite, you will encounter goals and practices that include the phr ase “establish and maintain.” This phrase means more t han a combination of its component terms; it includes documentation and usage. For example, “Establish and mainta in an organizational policy for planning and performing the organizational process focus process” means that not only must a policy be formulated, but it also must be documented, and it must be used throughout the organization. evidence (See “objective evidence.”) executive (See “senior manager.”) exit criteria States of being that must be present before an effort can end successfully. expected CMMI components CMMI components that explain what may be done to satisfy a required CMMI component. Model users can implement the expected components explicitly or implement equivalent alternative practices to t hese components. Specific and generic practices are ex pected model components. finding (See “appraisal findings.”) formal evaluation process A structured approach to evaluating alternative solutions against established criteria to determine a recommended solution to address an issue. framework (See “CMMI Framework.”)",
        "CMMI for Development Version 1.2 Glossary 541 functional analysis Examination of a defined f unction to identify all the subfunctions necessary to t he accomplishment of that function; identific ation of functional relationships and interfaces (internal and exter nal) and capturing these in a functional architecture; and flow down of upper level performance requirements and assignment of these requirements to lower level subfunctions. (See also “functional architecture.”) functional architecture The hierarchical arrangement of functions, their internal and external (external to the a ggregation itself) functional interfaces and external physical interfaces, their respective functional and performance r equirements, and their design constraints. functional configuration audit An audit conducted to verify that the development of a configuration item has been co mpleted satisfactorily, that the item has achieved th e performance and functional characteristics specified in the functional or allocated configuration identification, and that its operational and support documents are complete and satisfactory. (See also “configuration audit,” “conf iguration management,” and “physical configuration audit.”) generic goal A required model component that describes the characteristics that must be pr esent to institutionalize the processes that implement a process area. (See also “institutionalization.”) generic practice An expected model component t hat is considered important in achieving the associated generic goal. The generic practices associated with a generic goal describe the activities that are expected to result in achievement of the generic goal and contribute to the institutionalization of the processes associated with a process area. generic practice elaboration An informative model component that appears after a generic practice to provide guidance on how the generic practice should be applied to the process area. goal A required CMMI component that can be either a generic goal or a specific goal. When you see the word goal in a CMMI model, it always refers to a model component (e.g., generic goal and specific goal). (See also “generic goal,” “objective,” and “specific goal.”)",
        "CMMI for Development Version 1.2 542 Glossary hardware engineering The application of a systematic , disciplined, and quantifiable approach to transform a set of requirements representing the collection of stakeholder needs, expectations, and constraints using documented techniques and technology to design, implement, and mainta in a tangible product. (See also “software engineering” and “systems engineering.”) In CMMI, hardware engineering represents all technical fields (e.g., electrical or mechanical) that transform requirements and ideas into tangible and producible products. higher level management The person or persons who provide the policy and overall guidance for the process, but do not provide the direct day- to-day monitoring and contro lling of the process. Such persons belong to a level of management in the organization above the immediate level res ponsible for the process and can be (but are not necessarily ) senior managers. (See also “senior manager.”) incomplete process A process that is not performed or is performed only partially (also known as capability level 0). One or more of the specific goals of the proce ss area are not satisfied. informative CMMI components CMMI components that help model users understand the required and expected com ponents of a model. These components can contain examples , detailed explanations, or other helpful information. S ubpractices, notes, references, goal titles, practice titles, s ources, typical work products, amplifications, and generic pr actice elaborations are informative model components. institutionalization The ingrained way of doing bus iness that an organization follows routinely as part of its corporate culture. integrated product and process development A systematic approach to product development that achieves a timely collabora tion of relevant stakeholders throughout the product lifecycle to better satisfy customer needs. integrated team A group of people with complementary skills and expertise who are committed to delivering specified work products in timely collaboration. Integrat ed team members provide skills and advocacy appropriate to all phases of the work products’ life and are collectively responsible for delivering the work products as specifi ed. An integrated team should include empowered representat ives from organizations, disciplines, and functions that have a stake in the success of the work products.",
        "CMMI for Development Version 1.2 Glossary 543 interface control In configuration management, t he process of (1) identifying all functional and phys ical characteristics relevant to the interfacing of two or more c onfiguration items provided by one or more organizations, and (2) ensuring that the proposed changes to these c haracteristics are evaluated and approved prior to implementation. (See also “configuration item” and “c onfiguration management.”) lifecycle model A partitioning of the life of a product or project into phases. managed process A performed process that is planned and executed in accordance with policy; employs skilled people having adequate resources to produce c ontrolled outputs; involves relevant stakeholders; is m onitored, controlled, and reviewed; and is evaluated for adherence to its process description. (See also “performed process.”) manager In the CMMI Product Suite, a person who provides technical and administrative direction and control to those performing tasks or activities within t he manager’s area of responsibility. The traditional functions of a manager include planning, organizing, directing, and cont rolling work within an area of responsibility. maturity level Degree of process improvement across a predefined set of process areas in which all goals in the set are attained. (See also “capability level” and “process area.”) memorandum of agreement Binding documents of under standing or agreements between two or more parties. Also known as a “memorandum of understanding.” natural bounds The inherent process reflect ed by measures of process performance, sometimes referre d to as “voice of the process.” Techniques such as control charts, confidence intervals, and prediction intervals are used to determine whether the variation is due to common causes (i.e., the process is predictable or “stable”) or is due to some special cause that can and should be identified and removed. nondevelopmental item (NDI) An item of supply that was developed prior to its current use in an acquisition or development process. Such an item may require minor modifications to meet the requirements of its current intended use.",
        "CMMI for Development Version 1.2 544 Glossary nontechnical requirements Contractual provisions, commi tments, conditions, and terms that affect how products or services are to be acquired. Examples include products to be delivered, data rights for delivered commercial off-the-shelf (COTS) nondevelopmental items ( NDIs), delivery dates, and milestones with exit criteria. Other nontechnical requirements include training requirements, site requirements, and depl oyment schedules. objective When used as a noun in the CMMI Product Suite, the term objective replaces the word goal as used in its common everyday sense since the word goal is reserved for use when referring to the CMMI model components called specific goals and generic goals. (See also “goal.”) objective evidence As used in CMMI appraisal materials, documents or interview results used as indica tors of the implementation or institutionalization of model pr actices. Sources of objective evidence can include inst ruments, presentations, documents, and interviews. objectively evaluate To review activities and work products against criteria which minimize subjectivity and bias by the reviewer. An example of an objective evaluation is an audit against requirements, standards, or procedures by an independent quality assurance function. (See also “audit.”) observation As used in CMMI appraisal materi als, a written record that represents the appraisal team members’ understanding of information either seen or heard during the appraisal data collection activities. The written record may take the form of a statement or may take alte rnative forms as long as the information content is preserved. operational concept A general description of the way in which an entity is used or operates. (Also known as “concept of operations.”) operational scenario A description of an imagined sequence of events that includes the interaction of t he product with its environment and users, as well as inte raction among its product components. Operational scenario s are used to evaluate the requirements and design of the system and to verify and validate the system.",
        "CMMI for Development Version 1.2 Glossary 545 optimizing process A quantitatively managed proce ss that is improved based on an understanding of the comm on causes of variation inherent in the process. The fo cus of an optimizing process is on continually improv ing the range of process performance through both incremental and innovative improvements. (See also “c ommon cause of process variation,” “defined proce ss,” and “quantitatively managed process.”) organization An administrative structure in which people collectively manage one or more projects as a whole, and whose projects share a senior manager and operate under the same policies. However, the word organization as used throughout CMMI models can also apply to one person who performs a function in a sma ll organization that might be performed by a group of people in a large organization. (See also “enterprise” and “organizational unit.”) organizational maturity The extent to which an organization has explicitly and consistently deployed proce sses that are documented, managed, measured, controlled, and continually improved. Organizational maturity may be measured via appraisals. organizational policy A guiding principle typically established by senior management that is adopted by an organization to influence and determine decisions. organizational process assets Artifacts that relate to describing, implementing, and improving processes (e.g., polic ies, measurements, process descriptions, and process impl ementation support tools). The term process assets is used to indicate that these artifacts are developed or ac quired to meet the business objectives of the organiza tion, and they represent investments by the organization that are expected to provide current and future business value. (See also “process asset library.”) organizational unit The part of an organization t hat is the subject of an appraisal. An organizational unit deploys one or more processes that have a coherent process context and operates within a coherent set of business objectives. An organizational unit is typically part of a larger organization, although in a small organization, the organizational unit may be the whole organization.",
        "CMMI for Development Version 1.2 546 Glossary organization's business objectives Senior management developed strategies designed to ensure an organization’s cont inued existence and enhance its profitability, market shar e, and other factors influencing the organization’s success. (S ee also “quality and process- performance objectives” and “quantitative objective.”) Such objectives may include reducing the number of change requests during a system’s integration phase, reducing development cycle time, increas ing the number of errors found in a product’s first or second phase of development, and reducing the number of customer-reported defects, when applied to systems engineering activities. organization's measurement repository A repository used to collect and make available measurement data on proce sses and work products, particularly as they relate to the organization’s set of standard processes. This reposit ory contains or references actual measurement data and re lated information needed to understand and analyze t he measurement data. organization's process asset library A library of information used to store and make available process assets that are useful to those who are defining, implementing, and managing proc esses in the organization. This library contains proce ss assets that include process- related documentation such as policies, defined processes, checklists, lessons-learne d documents, templates, standards, procedures, plans, and training materials. organization's set of standard processes A collection of definitions of the processes that guide activities in an organization. These process descriptions cover the fundamental pr ocess elements (and their relationships to each other, such as ordering and interfaces) that must be incorporated into the defined processes that are implemented in projects across the organization. A standard process enables cons istent development and maintenance activities ac ross the organization and is essential for long-term stability and improvement. (See also “defined process” and “process element.”) outsourcing (See “acquisition.”) peer review The review of work products performed by peers during development of the work produc ts to identify defects for removal. The term peer review is used in the CMMI Product Suite instead of the term work product inspection. (See also “work product.”) performance parameters The measures of effectiveness and other key measures used to guide and control progressive development.",
        "CMMI for Development Version 1.2 Glossary 547 performed process A process that accomplishes the needed work to produce work products. The specific goals of the process area are satisfied. physical configuration audit An audit conducted to verify t hat a configuration item, as built, conforms to the techni cal documentation that defines and describes it. (See also, “configuration audit,” “configuration management,” and “functional configuration audit.”) planned process A process that is document ed by both a description and a plan. The description and plan should be coordinated, and the plan should include standards, requirements, objectives, resources, assignments, and so on. policy (See “organizational policy.”) process In the CMMI Product Suite, acti vities that can be recognized as implementations of practi ces in a CMMI model. These activities can be mapped to one or more practices in CMMI process areas to allow a model to be useful for process improvement and process appraisal. (See also “process area,” “subprocess,” and “process element.”) There is a special use of t he phrase “the process” in the statements and descriptions of the generic goals and generic practices. “The process,” as used in Part Two, is the process or processes that implement the process area. process action plan A plan, usually resulting fr om appraisals, that documents how specific improvements targeting the weaknesses uncovered by an appraisal will be implemented. process action team A team that has the responsib ility to develop and implement process improvement activities for an organization as documented in a process action plan. process and technology improvements Incremental and innovative improvements to processes and to process or product technologies. process architecture The ordering, interfaces, interdependencies, and other relationships among the proc ess elements in a standard process. Process architecture also describes the interfaces, interdependencies, and other relationships between process elements and external processes (e.g., contract management).",
        "CMMI for Development Version 1.2 548 Glossary process area A cluster of related practices in an area that, when implemented collectively, sati sfy a set of goals considered important for making improvement in that area. All CMMI process areas are common to both continuous and staged representations. process asset Anything that the organization c onsiders useful in attaining the goals of a process area. (See also “organizational process assets.”) process asset library A collection of process asset holdings that can be used by an organization or project. (See also “organization’s process asset library.”) process attribute A measurable characteristic of process capability applicable to any process. process capability The range of expected result s that can be achieved by following a process. process definition The act of defining and describing a process. The result of a process definition is a proc ess description. (See also “process description.”) process description A documented expression of a set of activities performed to achieve a given purpose. A process description provides an operational definition of the major components of a process. The description specifies, in a complete, precise, and verifiable manner, the requirements, design, behavior, or other characteristics of a process. It also may include procedures for determining whether these provisions have been satisfied. Process descriptions can be found at the activity, project, or organizational level. process element The fundamental unit of a pr ocess. A process can be defined in terms of subprocesses or process elements. A subprocess can be further dec omposed into subprocesses or process elements; a process element cannot. (See also “process” and “subprocess.”) Each process element covers a closely related set of activities (e.g., estimating element and peer review element). Process elements can be portrayed using templates to be completed, abs tractions to be refined, or descriptions to be modified or used. A process element can be an activity or task. process group A collection of specialists w ho facilitate the definition, maintenance, and improvement of the processes used by the organization.",
        "CMMI for Development Version 1.2 Glossary 549 process improvement A program of activities desi gned to improve the performance and maturity of the organization’s processes and the results of such a program. process improvement objectives A set of target characteristi cs established to guide the effort to improve an existing process in a specific, measurable way either in terms of result ant product characteristics (e.g., quality, performance, and confor mance to standards) or in the way in which the process is executed (e.g., elimination of redundant process steps, co mbination of process steps, and improvement of cycle time). (See also “organization’s business objectives” and “ quantitative objective.”) process improvement plan A plan for achieving organizational process improvement objectives based on a thorough understanding of the current strengths and weaknesses of t he organization’s processes and process assets. process measurement The set of definitions, methods , and activities used to take measurements of a process and its resulting products for the purpose of characte rizing and understanding the process. process owner The person (or team) res ponsible for defining and maintaining a process. At the organizational level, the process owner is the person (o r team) responsible for the description of a standard process; at the project level, the process owner is the person (o r team) responsible for the description of the defined proce ss. A process may therefore have multiple owners at differ ent levels of responsibility. (See also “defined proce ss” and “standard process.”) process performance A measure of actual results ac hieved by following a process. It is characterized by both pr ocess measures (e.g., effort, cycle time, and defect removal efficiency) and product measures (e.g., reliability, defect density, and response time). process- performance baseline A documented characterization of the actual results achieved by following a proc ess, which is used as a benchmark for comparing ac tual process performance against expected process perfo rmance. (See also “process performance.”) process-performance model A description of the relations hips among attributes of a process and its work products that is developed from historical process-performanc e data and calibrated using collected process and product m easures from the project and that is used to predict results to be achieved by following a process.",
        "CMMI for Development Version 1.2 550 Glossary process tailoring Making, altering, or adapting a process description for a particular end. For example, a project tailors its defined process from the organization’s set of standard processes to meet the objectives, constrai nts, and environment of the project. (See also “defined proc ess,” “organization’s set of standard processes,” and “process description.”) product In the CMMI Product Suite, a wo rk product that is intended for delivery to a customer or end user. The form of a product can vary in different contexts. (See also “customer,” “product component,” “service,” and “work product.”) product baseline In configuration management, the initial approved technical data package (including, for software, the source code listing) defining a configurati on item during the production, operation, maintenance, and logistic support of its lifecycle. (See also “configuration item” and “configuration management.”) product component In the CMMI Product Suite, a wo rk product that is a lower level component of the product. Product components are integrated to produce the product. There may be multiple levels of product components. (See also “product” and “work product.”) product component requirements A complete specification of a product component, including fit, form, function, performanc e, and any other requirement. product lifecycle The period of time, consisti ng of phases, which begins when a product is conceived and ends when the product is no longer available for use. Since an organization may be producing multiple products for multiple customers, one description of a product lifecycle may not be adequate. Therefore, the organization may define a set of approved product lifecycle models. T hese models are typically found in published literature and are likely to be tailored for use in an organization. A product lifecycle could consis t of the following phases: (1) concept/vision, (2) feasibilit y, (3) design/development, (4) production, and (5) phase out. product line A group of products shari ng a common, managed set of features that satisf y specific needs of a selected market or mission. product-related lifecycle processes Processes associated with a product throughout one or more phases of its life (e .g., from conception through disposal), such as t he manufacturing and support processes.",
        "CMMI for Development Version 1.2 Glossary 551 product requirements A refinement of the customer requirements into the developers’ language, making implicit requirements into explicit derived requirements. (See also “derived requirements” and “product component requirements.”) The developer uses the produc t requirements to guide the design and building of the product. product suite (See “CMMI Product Suite.”) profile (See “achievement profile” and “target profile.”) program (1) A project. (2) A collection of related projects and the infrastructure that supports them, including objectives, methods, activities, plans, and success measures. (See also “project.”) project In the CMMI Product Suite, a managed set of interrelated resources which delivers one or more products to a customer or end user. A pr oject has a definite beginning (i.e., project startup) and typi cally operates according to a plan. Such a plan is frequent ly documented and specifies what is to be delivered or implemented, the resources and funds to be used, the work to be done, and a schedule for doing the work. A project can be composed of projects. (See also “project startup.”) project manager In the CMMI Product Suite, the person responsible for planning, directing, controlling, structuring, and motivating the project. The project manager is responsible for satisfying the customer. project plan A plan that provides the basis for performing and controlling the project’s activities, whic h addresses the commitments to the project’s customer. Project planning includes estima ting the attributes of the work products and tasks, det ermining the resources needed, negotiating commitments, producing a schedule, and identifying and analyzing projec t risks. Iterating through these activities may be necessa ry to establish the project plan. project progress and performance What a project achieves wi th respect to implementing project plans, including effort, cost, schedule, and technical performance. project startup When a set of interrelated resources are directed to develop or deliver one or more products for a customer or end user. (See also “project.”)",
        "CMMI for Development Version 1.2 552 Glossary project's defined process The integrated and defined process that is tailored from the organization’s set of standard processes. (See also “defined process.”) prototype A preliminary type, form, or in stance of a product or product component that serves as a model for later stages or for the final, complete version of the product. This model (e.g., physical, electronic, digital, and analytical) can be used for the following (and other) purposes: • Assessing the feasibility of a new or unfamiliar technology • Assessing or mitigating technical risk • Validating requirements • Demonstrating cr itical features • Qualifying a product • Qualifying a process • Characterizing performance or product features • Elucidating physical principles quality The ability of a set of inherent characteristics of a product, product component, or process to fulfill requirements of customers. quality and process- performance objectives Objectives and requirements for product quality, service quality, and process perform ance. Process-performance objectives include quality; however, to emphasize the importance of quality in the C MMI Product Suite, the phrase quality and process-performance ob jectives is used rather than just process-performance objectives. quality assurance A planned and systematic m eans for assuring management that the defined standards, practices, procedures, and methods of the process are applied. quality control The operational techniques and ac tivities that are used to fulfill requirements for quality. (See also “quality assurance.”) quantitative objective Desired target value expre ssed as quantitat ive measures. (See also “process improvement objectives” and “quality and process-performance objectives.”)",
        "CMMI for Development Version 1.2 Glossary 553 quantitatively managed process A defined process that is c ontrolled using statistical and other quantitative techniques . The product quality, service quality, and process-performance attributes are measurable and controlled throughout the project. (See also “defined process,” “optimizing proce ss,” and “statistically managed process.”) rating (See “appraisal rating.”) reference An informative model component that points to additional or more detailed information in related process areas. reference model A model that is used as a benchmark for measuring some attribute. relevant stakeholder A stakeholder that is identified for involvement in specified activities and is included in a plan. (See also “stakeholder.”) representation The organization, use, an d presentation of a CMM’s components. Overall, two types of approaches to presenting best practices are evident: the staged representation and the continuous r epresentation. required CMMI components CMMI components that are ess ential to achieving process improvement in a given process area. These components are used in appraisals to determine process capability. Specific goals and generic goals are required model components. requirement (1) A condition or capability needed by a user to solve a problem or achieve an objectiv e. (2) A condition or capability that must be met or possessed by a product or product component to satisfy a contrac t, standard, specification, or other formally imposed doc uments. (3) A documented representation of a condition or capability as in (1) or (2). requirements analysis The determination of produc t-specific performance and functional characteristics bas ed on analyses of customer needs, expectations, and constr aints; operational concept; projected utilizati on environments for peopl e, products, and processes; and measures of effectiveness. requirements elicitation Using systematic techniques , such as prototypes and structured surveys, to proac tively identify and document customer and end-user needs. requirements management The management of all requirements received by or generated by the project, including both technical and nontechnical requirements as well as those requirements levied on the project by the organization.",
        "CMMI for Development Version 1.2 554 Glossary requirements traceability A discernable association between requirements and related requirements, implementations , and verifications. (See also “bidirectional traceability” and “traceability.”) return on investment The ratio of revenue from output (product) to production costs, which determines w hether an organization benefits from performing an action to produce something. risk analysis The evaluation, classification, and prioritization of risks. risk identification An organized, thorough approach to seek out probable or realistic risks in achieving objectives. risk management An organized, analytic process to identify what might cause harm or loss (identify risks); to assess and quantify the identified risks; and to devel op and, if needed, implement an appropriate approach to prevent or handle causes of risk that could result in significant harm or loss. risk management strategy An organized, technical approac h to identify what might cause harm or loss (identify risks); to assess and quantify the identified risks; and to develop and, if needed, implement an appropriate appr oach to prevent or handle causes of risk that could result in significant harm or loss. Typically, risk management is performed for project, organization, or product developing organizational units. root cause A source of a defect such that if it is removed, the defect is decreased or removed. senior manager In the CMMI Product Suite, a management role at a high enough level in an organization that the primary focus of the person filling the role is th e long-term vitality of the organization rather than short- term project and contractual concerns and pressures. A seni or manager has authority to direct the allocation or realloca tion of resources in support of organizational process impr ovement effectiveness. (See also “higher level management.”) A senior manager can be any manager who satisfies this description, including the head of the organization. Synonyms for “senior manager” include “executive” and “top-level manager.” However, to ensure consistency and usability, these synonyms are not used in CMMI models. service In the CMMI Product Suite, a service is a product that is intangible and non-storable. (See also “product,” “customer,” and “work product.”)",
        "CMMI for Development Version 1.2 Glossary 555 shared vision A common understanding of guiding principles including mission, objectives, expect ed behavior, values, and final outcomes, which are developed and used by a project. software engineering (1) The application of a syste matic, disciplined, quantifiable approach to the development, operation, and maintenance of software. (2) The study of approaches as in (1). (See also “hardware engineering,” and “systems engineering.”) solicitation The process of preparing a package to be used in selecting a supplier (contractor). special cause of process variation A cause of a defect that is specific to some transient circumstance and not an inheren t part of a process. (See also “common cause of process variation.”) specific goal A required model component that describes the unique characteristics that must be pr esent to satisfy the process area. (See also “capability level,” “generic goal,” “organization’s business obj ectives,” and “process area.”) specific practice An expected model component t hat is considered important in achieving the associated specific goal. The specific practices describe the activiti es expected to result in achievement of the specific goals of a process area. (See also “process area” and “specific goal.”) stable process The state in which all special causes of process variation have been removed and prevented from recurring so that only the common causes of proc ess variation of the process remain. (See also “capable pr ocess,” “common cause of process variation,” “special c ause of process variation,” “standard process,” and “stati stically managed process.”) staged representation A model structure wherein attaining the goals of a set of process areas establishes a matu rity level; each level builds a foundation for subsequent le vels. (See also “maturity level” and “process area.”) stakeholder In the CMMI Product Suite, a group or individual that is affected by or is in some way accountable for the outcome of an undertaking. Stakeholders may include project members, suppliers, customer s, end users, and others. (See also “customer” and “r elevant stakeholder.”)",
        "CMMI for Development Version 1.2 556 Glossary standard When you see the word standard used as a noun in a CMMI model, it refers to the fo rmal mandatory requirements developed and used to prescri be consistent approaches to development (e.g., ISO/IEC standards, IEEE standards, and organizational standards). Inst ead of using standard in its common everyday sense, we use another term that means the same thing (e.g., typica l, traditional, usual, or customary). standard process An operational definition of the basic process t hat guides the establishment of a common pr ocess in an organization. A standard process describes the fundamental process elements that are expected to be incorporated into any defined process. It also descr ibes the relationships (e.g., ordering and interfaces) am ong these process elements. (See also “defined process.”) statement of work A description of contracted work required to complete a project. statistical predictability The performance of a quantitative process that is controlled using statistical and other quantitative techniques. statistical process control Statistically based analysis of a process and measurements of process performance, which will identify common and special causes of variation in the process performance and maintain process performance within limits. (See also “common cause of process va riation,” “special cause of process variation,” and “stati stically managed process.”) statistical techniques An analytic technique that empl oys statistical methods (e.g., statistical process control, confidence intervals, and prediction intervals). statistically managed process A process that is managed by a statistically based technique in which processes are analyzed, special causes of process variation are identified, and per formance is contained within well-defined limits. (See also “capable process,” “special cause of process va riation,” “stable process,” “standard process,” and “statistic al process control.”) subpractice An informative model component that provides guidance for interpreting and implementing a specific or generic practice. Subpractices may be worded as if prescriptive, but are actually meant only to provide ideas that may be useful for process improvement.",
        "CMMI for Development Version 1.2 Glossary 557 subprocess A process that is part of a la rger process. A subprocess can be decomposed into subproc esses and/or process elements. (See also “process,” “process description,” and “process element.”) supplier (1) An entity delivering products or performing services being acquired. (2) An individual, partnership, company, corporation, association, or other service having an agreement (contract) with an acquirer for the design, development, manufacture, main tenance, modification, or supply of items under the terms of an agreement (contract). sustainment The processes used to ensure that a produc t can be utilized operationally by its end users or customers. Sustainment ensures that maintenance is done such that the product is in an operable condition whether or not the product is in use by customers or end users. systems engineering The interdisciplinary approach governing the total technical and managerial effort required to transform a set of customer needs, expectations, and constraints into a product solution and to support that solution throughout the product’s life. (See also “hardware engineering” and “software engineering.”) This includes the definition of technical performance measures, the integration of engineering specialties toward the establishment of a pr oduct architecture, and the definition of supporting lifecyc le processes that balance cost, performance, and schedule objectives. tailoring Tailoring a process makes, alters, or adapts the process description for a particular end. For example, a project establishes its defined proc ess by tailoring from the organization’s set of standard processes to meet the objectives, constraints, a nd environment of the project.",
        "CMMI for Development Version 1.2 558 Glossary tailoring guidelines Organizational guidelines t hat enable projects, groups, and organizational functions to appropriately adapt standard processes for their use. The organization’s set of standard processes is described at a general level that may not be directly usable to perform a process. Tailoring guidelines aid thos e who establish the defined processes for projects. Tailoring guidelines cover (1) selecting a standard process, (2) selecting an approved lifecycle model, and (3) ta iloring the selected standard process and lifecycle model to fit project needs. Tailoring guidelines describe what can and cannot be modified and identify process components that are candidates for modification. target profile In the continuous representati on, a list of process areas and their corresponding capability le vels that represent an objective for process improvem ent. (See also “achievement profile” and “capability level profile.”) target staging In the continuous representat ion, a sequence of target profiles that describes the pat h of process improvement to be followed by the organization. (See also “achievement profile,” “capability level pr ofile,” and “target profile.”)",
        "CMMI for Development Version 1.2 Glossary 559 technical data package A collection of items that c an include the following if such information is appropriate to the type of product and product component (e.g., material and manufacturing requirements may not be useful for product components associated with software services or processes): • Product architecture description • Allocated requirements • Product component descriptions • Product-related lifecycle pr ocess descriptions if not described as separate product components • Key product characteristics • Required physical charac teristics and constraints • Interface requirements • Materials requirements (bills of material and material characteristics) • Fabrication and manufactu ring requirements (for both the original equipment manuf acturer and field support) • Verification criteria used to ensure requirements have been achieved • Conditions of use (envir onments) and operating/usage scenarios, modes and states for operations, support, training, manufacturing, disposal, and verifications throughout the life of the product • Rationale for decisions an d characteristics (e.g., requirements, requirement allocations, and design choices) technical requirements Properties (attributes) of products or services to be acquired or developed. test procedure Detailed instructions for the se tup, execution, and evaluation of results for a given test. traceability A discernable association among two or more logical entities such as requirements, system elements, verifications, or tasks. (See also “bidirectional traceability” and “requirements traceability.”) trade study An evaluation of alternatives, based on criteria and systematic analysis, to sele ct the best alternative for attaining determined objectives.",
        "CMMI for Development Version 1.2 560 Glossary training Formal and informal learning options, which may include in- class training, informal me ntoring, Web-based training, guided self-study, and formalized on-the-job training programs. The learning options selected for each situation are based on an assessment of the need for training and the performance gap to be addressed. typical work product An informative model component that provides sample outputs from a specific practi ce. These examples are called typical work products because there are often other work products that are just as effective but are not listed. unit testing Testing of individual hardware or software units or groups of related units. (See also “acceptance testing.”) validation Confirmation that the product, as provided (or as it will be provided), will fulfill its in tended use. In other words, validation ensures that “you built the right thing.” (See also “verification.”) verification Confirmation that work pr oducts properly reflect the requirements specified for them. In other words, verification ensures that “you built it ri ght.” (See also “validation.”) version control The establishment and maint enance of baselines and the identification of changes to baselines that make it possible to return to the previous baseline. work breakdown structure (WBS) An arrangement of work elements and their relationship to each other and to the end product. work product In the CMMI Product Suite, a usef ul result of a process. This can include files, documents, products, parts of a product, services, process descriptions, specifications , and invoices. A key distinction between a work product and a product component is that a work produc t is not necessarily part of the product. (See also “product” and “product component.”) In CMMI models, you will see the phrase work products and services. Even though the defin ition of work product includes services, this phras e is used to emphasize the inclusion of services in the discussion. work product and task attributes Characteristics of products, se rvices, and project tasks used to help in estimating project work. These characteristics include items such as size, comp lexity, weight, form, fit, and function. They are typically used as one input to deriving other project and resource esti mates (e.g., effort, cost, and schedule).",
        "CMMI for Development Version 1.2 561 REPORT DOCUMENTATION PAGE Form Approved OMB No. 0704-0188 Public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments r egarding this burden estimate or any other aspect of this collection of information, including sugge stions for reducing this burden, to Washington Headquarters Serv ices, Directorate for information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington, VA 22202-4302, and to the Office of Management and Budget, Pa perwork Reduction Project (0704- 0188), Washington, DC 20503. 1. AGENCY USE ONLY (Leave Blank) 2. REPORT DATE August 2006 3. REPORT TYPE AND DATES COVERED Final 4. TITLE AND SUBTITLE CMMI for Development, Version 1.2 5. FUNDING NUMBERS FA8721-05-C-0003-00-C- 0003 6. AUTHOR (S) CMMI Product Team 7. PERFORMING ORGANIZATION NAME (S) AND ADDRESS (ES) Software Engineering Institute Carnegie Mellon University Pittsburgh, PA 15213 8. PERFORMING ORGANIZATION REPORT NUMBER CMU/SEI-2006-TR-008 9. SPONSORING /MONITORING AGENCY NAME (S) AND ADDRESS (ES) HQ ESC/XPK 5 Eglin Street Hanscom AFB, MA 01731-2116 10. SPONSORING /MONITORING AGENCY REPORT NUMBER 11. SUPPLEMENTARY NOTES 12A DISTRIBUTION /AVAILABILITY STATEMENT Unclassified/Unlimited, DTIC, NTIS 12B DISTRIBUTION CODE 13. ABSTRACT (MAXIMUM 200 WORDS ) CMMI® for Development (CMMI-DEV), Version 1.2 is an upgrade of CMMI-SE/SW/IPPD /SS, Version 1.1. The focus of the CMMI Version 1.2 effort is on improving t he quality of CMMI products and the consistency of how they are applied. This report repres ents the model portion of the CMMI Product Suite. Other portions of the CMMI Product Suite include the SCAMPI A appraisal me thod and the Introduction to CMMI training course. CMMI now includes the concept of CMMI “constellations.” A constellation is a set of CMMI components designed to meet the needs of a specific area of interest. A c onstellation can produce one or more related CMMI models and related appraisal and training materials. CMMI fo r Development is the first of these constellations. This report contains the two models that comprise t he CMMI for Development constellation: the CMMI for Development and CMMI for Development +IPPD models. T he report consists of three parts. Part one is the overview, which describes CMMI concepts, model compon ents, and guidance on using the CMMI Product Suite. Part two contains the generic goals and practices and process areas, which are used by organizations to improve their development processes. Part three cont ains references, acronyms, project participants, and a glossary. 14. SUBJECT TERMS CMMI, CMMI for Development, process improv ement, version 1.2, software process improvement, reference model, product development model, development model, CMM 15. NUMBER OF PAGES 560 16. PRICE CODE 17. SECURITY CLASSIFICATION OF REPORT Unclassified 18. SECURITY CLASSIFICATION OF THIS PAGE Unclassified 19. SECURITY CLASSIFICATION OF ABSTRACT Unclassified 20. LIMITATION OF ABSTRACT UL NSN 7540-01-280-5500 Standard Form 298 (Rev. 2-89) Prescribed by ANSI Std. Z39-18 298-102"
      ]
    },
    "DAMA.pdf": {
      "fingerprint": "DAMA.pdf|1764154257|13782365",
      "pages": [
        "Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DAMA -DMBOK DATA MANAGEMENT BODY OF KNOWLEDGE SECOND EDITION , REVISED DAMA International Technics Publications BASKING RIDGE, NEW JERSEY Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "Dedicated to the memory of Patricia Cupoli, MLS, MBA, CCP, CDMP (May 25, 1948 – July 28, 2015) for her lifelong commitment to the Data Management profession and her contributions to this publication. Published by: 115 Linda Vista Sedona , AZ 86336 USA https://www.TechnicsPub.com Senior Editor: Deborah Henderson , CDMP Editor: Susan Earley, CDMP Production Editor: Laura Sebastian -Coleman, CDMP, IQCP Bibliography Researcher: Elena Sykora, DGSP Collaboration Tool Manager: Eva Smith, CDMP For the revised edition: Senior Editor: Chris Bradley CDMP Collaboration Tool Manager: Michele Valentini CDMP Cover design by Lorena Molinari All rights reserved. No part of this book may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording or by any information storage and retrieval system, without written permission from the publisher, except for the inclusion of brief quotations in a review. The author and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connect ion with or arising out of the use of the information or programs contained herein. All trade and product names are trademarks, registered trademarks or service marks of their respective companies and are the property of their respective holders and should be treated as such. Second Edition First Printing 2017 Second Edition , Revised , First Printing 20 24 Copyright © 20 24 DAMA International ISBN, P rint ed. 9781634622349 ISBN, PDF ed. 9781634622363 ISBN, Server ed. 9781634622486 ISBN, Enterprise ed. 9781634622479 Library of Congress Control Number: 2017941854 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "1 Contents Preface _______________________________________________________ 15 Preface to the Revised Edition ____________________________________ 17 Chapter 1 : Data Management _____________________________________ 19 1. Introduction ______________________________________________________________________ 19 2. Essential Concepts ________________________________________________________________ 20 2.1 Data ___________________________________________________________________________ 20 2.2 Data and Information ____________________________________________________________ 22 2.3 Data as an Organizational Asset ___________________________________________________ 22 2.4 Data Management Principles ______________________________________________________ 23 2.5 Data Management Challenges _____________________________________________________ 25 2.6 Data Management Strategy _______________________________________________________ 34 3. Data Management Frameworks ____________________________________________________ 35 3.1 Strategic Alignment Model _______________________________________________________ 36 3.2 The Amsterdam Information Model _______________________________________________ 36 3.3 The DAMA -DMBOK Framework __________________________________________________ 37 3.4 DMBOK Pyramid (Aiken) ________________________________________________________ 41 3.5 DAMA Data Management Framework Evolved ______________________________________ 42 4. DAMA and the DMBOK ___________________________________________________________ 45 5. Works Cited / Recommended _____________________________________________________ 49 Chapter 2 : Data Handling Ethics __________________________________ 51 1. Introduction ______________________________________________________________________ 51 2. Business Drivers __________________________________________________________________ 53 3. Essential Concepts ________________________________________________________________ 54 3.1 Ethical Principles for Data ________________________________________________________ 54 3.2 Principles Behind Data Privacy Law _______________________________________________ 55 3.3 Online Data in an Ethical Context _________________________________________________ 58 3.4 Risks of Unethical Data Handling Practices _________________________________________ 59 3.5 Establishing an Ethical Data Culture _______________________________________________ 63 3.6 Data Ethics and Governance ______________________________________________________ 67 4. Works Cited / Recommended _____________________________________________________ 67 Chapter 3 : Data Governance _____________________________________ 69 1. Introduction ______________________________________________________________________ 69 1.1 Business Drivers ________________________________________________________________ 72 1.2 Goals and Principles _____________________________________________________________ 73 1.3 Essential Concepts ______________________________________________________________ 74 2. Activities _________________________________________________________________________ 81 2.1 Define Data Governance for the Organization _______________________________________ 81 2.2 Perform Readiness Assessment ___________________________________________________ 82 2.3 Perform Discovery and Business Alignment ________________________________________ 82 2.4 Develop Organizational Touch Points ______________________________________________ 83 2.5 Develop Data Governance Strategy ________________________________________________ 84 2.6 Define the Data Governance Operating Framework __________________________________ 84 2.7 Develop Goals, Principles, and Policies _____________________________________________ 86 2.8 Underwrite Data Management Projects ____________________________________________ 86 2.9 Engage Change Management ______________________________________________________ 87 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "2 • DMBOK2 2.10 Engage in Issue Management ____________________________________________________ 88 2.11 Assess Regulatory Compliance Requirements _____________________________________ 89 2.12 Implement Data Governance ____________________________________________________ 90 2.13 Sponsor Data Standards and Procedures __________________________________________ 91 2.14 Develop a Business Glossary ____________________________________________________ 92 2.15 Coordinate with Architecture Groups ____________________________________________ 93 2.16 Define Data Asset Valuation Method _____________________________________________ 93 2.17 Embed Data Governance _______________________________________________________ 93 3. Tools and Techniques ____________________________________________________________ 94 3.1 Online Presence / Websites ______________________________________________________ 94 3.2 Business Glossary _______________________________________________________________ 95 3.3 Workflow Tools ________________________________________________________________ 95 3.4 Document Management Tools ____________________________________________________ 95 3.5 Data Governance Scorecards _____________________________________________________ 95 4. Implementation Guidelines _______________________________________________________ 95 4.1 Organization and Culture ________________________________________________________ 96 4.2 Adjustment and Communication _________________________________________________ 96 5. Metrics __________________________________________________________________________ 96 6. Works Cited / Recommended ____________________________________________________ 97 Chapter 4 : Data Architecture ____________________________________ 99 1. Introduction _____________________________________________________________________ 99 1.1 Business Drivers _______________________________________________________________ 101 1.2 Data Architecture Outcomes and Practices ________________________________________ 101 1.3 Essential Concepts _____________________________________________________________ 103 2. Activities _______________________________________________________________________ 111 2.1 Establish Data Architecture Practice _____________________________________________ 111 2.2 Integrate with Enterprise Architecture ___________________________________________ 115 3. Tools ___________________________________________________________________________ 116 3.1 Data Modeling Tools ___________________________________________________________ 116 3.2 Asset Management Software ____________________________________________________ 116 3.3 Graphical Design Applications ___________________________________________________ 116 4. Techniques _____________________________________________________________________ 116 4.1 Lifecycle Projections ___________________________________________________________ 116 4.2 Diagramming Clarity ___________________________________________________________ 117 5. Implementation Guidelines ______________________________________________________ 117 5.1 Readiness Assessment / Risk Assessment _________________________________________ 118 5.2 Organization and Cultural Change _______________________________________________ 119 6. Data Architecture Governance ___________________________________________________ 120 6.1 Metrics _______________________________________________________________________ 120 7. Works Cited / Recommended ___________________________________________________ 121 Chapter 5 : Data Modeling and Design ____________________________ 123 1. Introduction ____________________________________________________________________ 123 1.1 Business Drivers _______________________________________________________________ 125 1.2 Goals and Principles ___________________________________________________________ 125 1.3 Essential Concepts _____________________________________________________________ 126 2. Activities _______________________________________________________________________ 150 2.1 Plan for Data Modeling _________________________________________________________ 150 2.2 Build the Data Model ___________________________________________________________ 151 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "CONTENTS • 3 2.3 Review the Data Models ________________________________________________________ 156 2.4 Maintain the Data Models _______________________________________________________ 157 3. Tools ____________________________________________________________________________ 157 3.1 Data Modeling Tools ____________________________________________________________ 157 3.2 Lineage Tools __________________________________________________________________ 157 3.3 Data Profiling Tools ____________________________________________________________ 158 3.4 Metadata Repositories __________________________________________________________ 158 3.5 Data Model Patterns ____________________________________________________________ 158 3.6 Industry Data Models ___________________________________________________________ 158 4. Best Practices ____________________________________________________________________ 159 4.1 Best Practices in Naming Conventions ____________________________________________ 159 4.2 Best Practices in Database Design ________________________________________________ 159 5. Data Model Governance _________________________________________________________ 160 5.1 Data Model and Design Quality Management ______________________________________ 160 5.2 Data Modeling Metrics __________________________________________________________ 162 6. Works Cited / Recommended ____________________________________________________ 164 Chapter 6 : Data Storage and Operations ___________________________ 166 1. Introduction _____________________________________________________________________ 166 1.1 Business Drivers _______________________________________________________________ 168 1.2 Goals and Principles ____________________________________________________________ 168 1.3 Essential Concepts _____________________________________________________________ 169 2. Activities ________________________________________________________________________ 188 2.1 Manage Database Technology ____________________________________________________ 188 2.2 Manage Database Operations ____________________________________________________ 190 3. Tools ____________________________________________________________________________ 203 3.1 Data Modeling Tools ____________________________________________________________ 203 3.2 Database Monitoring Tools ______________________________________________________ 203 3.3 Database Management Tools _____________________________________________________ 203 3.4 Developer Support Tools ________________________________________________________ 203 4. Techniques ______________________________________________________________________ 204 4.1 Test in Lower Environments ____________________________________________________ 204 4.2 Physical Naming Standards ______________________________________________________ 204 4.3 Script Usage for All Changes _____________________________________________________ 204 5. Implementation Guidelines ______________________________________________________ 204 5.1 Readiness Assessment / Risk Assessment _________________________________________ 204 5.2 Organization and Cultural Change ________________________________________________ 205 6. Data Storage and Operations Governance _________________________________________ 206 6.1 Metrics _______________________________________________________________________ 206 6.2 Information Asset Tracking _____________________________________________________ 207 6.3 Data Audits and Data Validation _________________________________________________ 207 7. Works Cited / Recommended ____________________________________________________ 207 Chapter 7 : Data Security ________________________________________ 209 1. Introduction _____________________________________________________________________ 209 1.1 Business Drivers _______________________________________________________________ 212 1.2 Goals and Principles ____________________________________________________________ 214 1.3 Essential Concepts _____________________________________________________________ 214 2. Activities ________________________________________________________________________ 235 2.1 Identify Data Security Requirements _____________________________________________ 235 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "4 • DMBOK2 2.2 Define Data Security Policy _____________________________________________________ 237 2.3 Define Data Security Standards __________________________________________________ 238 3. Tools ___________________________________________________________________________ 246 3.1 Anti -Virus Software / Security Software __________________________________________ 246 3.2 Web Page Security _____________________________________________________________ 246 3.3 Identity Management Technology _______________________________________________ 246 3.4 Intrusion Detection and Prevention Software _____________________________________ 246 3.5 Firewalls (Prevention) _________________________________________________________ 247 3.6 Metadata Tracking _____________________________________________________________ 247 3.7 Data Masking/Encryption ______________________________________________________ 247 4. Techniques _____________________________________________________________________ 247 4.1 CRUD Matrix Usage ____________________________________________________________ 248 4.2 Immediate Security Patch Deployment ___________________________________________ 248 4.3 Data Security Attributes in Metadata _____________________________________________ 248 4.4 Security Needs in Project Requirements __________________________________________ 248 4.5 Efficient Search of Encrypted Data _______________________________________________ 248 4.6 Document Sanitization _________________________________________________________ 249 5. Implementation Guidelines ______________________________________________________ 249 5.1 Readiness Assessment / Risk Assessment _________________________________________ 249 5.2 Organization and Cultural Change _______________________________________________ 250 5.3 Visibility into User Data Entitlement _____________________________________________ 250 5.4 Data Security in an Outsourced World ___________________________________________ 250 5.5 Data Security in Cloud Environments ____________________________________________ 251 6. Data Security Governance _______________________________________________________ 252 6.1 Data Security and Enterprise Architecture ________________________________________ 252 6.2 Metrics _______________________________________________________________________ 253 7. Works Cited / Recommended ___________________________________________________ 255 Chapter 8 : Data Integration and Interoperability ___________________ 257 1. Introduction ____________________________________________________________________ 257 1.1 Business Drivers _______________________________________________________________ 258 1.2 Goals and Principles ___________________________________________________________ 260 1.3 Essential Concepts _____________________________________________________________ 260 2. Data Integration Activities _______________________________________________________ 273 2.1 Plan and Analyze ______________________________________________________________ 273 2.2 Design Data Integration Solutions ________________________________________________ 276 2.3 Develop Data Integration Solutions ______________________________________________ 278 2.4 Implement and Monitor ________________________________________________________ 279 3. Tools ___________________________________________________________________________ 280 3.1 Data Transformation Engine/ETL Tool ___________________________________________ 280 3.2 Data Virtualization Server ______________________________________________________ 280 3.3 Enterprise Service Bus _________________________________________________________ 281 3.4 Business Rules Engine __________________________________________________________ 281 3.5 Data and Process Modeling Tools ________________________________________________ 281 3.6 Data Profiling Tool _____________________________________________________________ 282 3.7 Metadata Repository ___________________________________________________________ 282 4. Techniques _____________________________________________________________________ 282 5. Implementation Guidelines ______________________________________________________ 282 5.1 Readiness Assessment / Risk Assessment _________________________________________ 282 5.2 Organization and Cultural Change _______________________________________________ 283 6. DII Governance _________________________________________________________________ 284 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "CONTENTS • 5 6.1 Data Sharing Agreements _______________________________________________________ 284 6.2 DII and Data Lineage ___________________________________________________________ 285 6.3 Data Integration Metrics ________________________________________________________ 285 7. Works Cited / Recommended ____________________________________________________ 285 Chapter 9 : Document and Content Management ____________________ 287 1. Introduction _____________________________________________________________________ 287 1.1 Business Drivers _______________________________________________________________ 289 1.2 Goals and Principles ____________________________________________________________ 289 1.3 Essential Concepts _____________________________________________________________ 290 2. Activities ________________________________________________________________________ 306 2.1 Plan for Lifecycle Management __________________________________________________ 306 2.2 Manage the Lifecycle ___________________________________________________________ 309 2.3 Publish and Deliver Content _____________________________________________________ 312 3. Tools ____________________________________________________________________________ 313 3.1 Enterprise Content Management Systems _________________________________________ 313 3.2 Collaboration Tools ____________________________________________________________ 316 3.3 Controlled Vocabulary and Metadata Tools ________________________________________ 316 3.4 Standard Markup and Exchange Formats __________________________________________ 316 3.5 E- discovery Technology ________________________________________________________ 318 4. Techniques ______________________________________________________________________ 319 4.1 Litigation Response Playbook ____________________________________________________ 319 4.2 Litigation Response Data Map ___________________________________________________ 319 5. Implementation Guidelines ______________________________________________________ 320 5.1 Readiness Assessment / Risk Assessment _________________________________________ 320 5.2 Organization and Cultural Change ________________________________________________ 322 6. Documents and Content Governance _____________________________________________ 322 6.1 Information Governance Frameworks ____________________________________________ 322 6.2 Proliferation of Information _____________________________________________________ 324 6.3 Govern for Quality Content _____________________________________________________ 324 6.4 Metrics _______________________________________________________________________ 325 7. Works Cited / Recommended ____________________________________________________ 326 Chapter 10 : Reference and Master Data ___________________________ 329 1. Introduction _____________________________________________________________________ 329 1.1 Business Drivers _______________________________________________________________ 331 1.2 Goals and Principles ____________________________________________________________ 331 1.3 Essential Concepts _____________________________________________________________ 332 2. Activities ________________________________________________________________________ 351 2.1 MDM Activities ________________________________________________________________ 351 2.2 Reference Data Activities _______________________________________________________ 353 3. Tools and Techniques ____________________________________________________________ 355 4. Implementation Guidelines ______________________________________________________ 355 4.1 Adhere to Master Data Architecture ______________________________________________ 356 4.2 Monitor Data Movement ________________________________________________________ 356 4.3 Manage Reference Data Change __________________________________________________ 356 4.4 Data Sharing Agreements _______________________________________________________ 357 5. Organization and Cultural Change ________________________________________________ 358 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "6 • DMBOK2 6. Reference and Master Data Governance __________________________________________ 358 6.1 Metrics _______________________________________________________________________ 359 7. Works Cited / Recommended ___________________________________________________ 359 Chapter 11 : Data Warehousing and Business Intelligence ____________ 361 1. Introduction ____________________________________________________________________ 361 1.1 Business Drivers _______________________________________________________________ 363 1.2 Goals and Principles ___________________________________________________________ 363 1.3 Essential Concepts _____________________________________________________________ 364 2. Activities _______________________________________________________________________ 374 2.1 Understand Requirements ______________________________________________________ 374 2.2 Define and Maintain the DW/BI Architecture _____________________________________ 374 2.3 Develop the Data Warehouse and Data Marts _____________________________________ 375 2.4 Populate the Data Warehouse ___________________________________________________ 376 2.5 Implement the Business Intelligence Portfolio _____________________________________ 377 2.6 Maintain Data Products _________________________________________________________ 378 3. Tools ___________________________________________________________________________ 381 3.1 Metadata Repositories __________________________________________________________ 381 3.2 Data Integration Tools _________________________________________________________ 382 3.3 Business Intelligence Tools Types _______________________________________________ 382 4. Techniques _____________________________________________________________________ 386 4.1 Prototypes to Drive Requirements _______________________________________________ 386 4.2 Self -Service BI ________________________________________________________________ 386 4.3 Queryable Data ________________________________________________________________ 387 5. Implementation Guidelines ______________________________________________________ 387 5.1 Readiness Assessment / Risk Assessment _________________________________________ 387 5.2 Release Roadmap ______________________________________________________________ 388 5.3 Configuration Management _____________________________________________________ 388 5.4 Organization and Cultural Change _______________________________________________ 388 6. DW/BI Governance _____________________________________________________________ 389 6.1 Enabling Business Acceptance ___________________________________________________ 390 6.2 Customer / User Satisfaction ____________________________________________________ 391 6.3 Service Level Agreements ______________________________________________________ 391 6.4 Reporting Strategy _____________________________________________________________ 391 6.5 Metrics _______________________________________________________________________ 392 7. Works Cited / Recommended ___________________________________________________ 393 Chapter 12 : Metadata Management ______________________________ 395 1. Introduction ____________________________________________________________________ 395 1.1 Business Drivers _______________________________________________________________ 397 1.2 Goals and Principles ___________________________________________________________ 398 1.3 Essential Concepts _____________________________________________________________ 399 2. Activities _______________________________________________________________________ 411 2.1 Define Metadata Strategy _______________________________________________________ 411 2.2 Understand Metadata Requirements _____________________________________________ 412 2.3 Define Metadata Architecture ___________________________________________________ 412 2.4 Create and Maintain Metadata ___________________________________________________ 415 2.5 Query, Report, and Analyze Metadata ____________________________________________ 416 3. Tools ___________________________________________________________________________ 417 3.1 Metadata Repository Management Tools __________________________________________ 417 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "CONTENTS • 7 4. Techniques ______________________________________________________________________ 417 4.1 Data Lineage and Impact Analysis ________________________________________________ 417 4.2 Metadata for Big Data Ingest _____________________________________________________ 419 5. Implementation Guidelines ______________________________________________________ 420 5.1 Readiness Assessment / Risk Assessment _________________________________________ 420 5.2 Organizational and Cultural Change ______________________________________________ 420 6. Metadata Governance ____________________________________________________________ 421 6.1 Process Controls _______________________________________________________________ 421 6.2 Documentation of Metadata Solutions ____________________________________________ 421 6.3 Metadata Standards and Guidelines _______________________________________________ 422 6.4 Metrics _______________________________________________________________________ 422 7. Works Cited / Recommended ____________________________________________________ 423 Chapter 13 : Data Quality Management ____________________________ 424 1. Introduction _____________________________________________________________________ 424 1.1 Business Drivers ____________________________________________________________ 427 1.2 Goals and Principles _________________________________________________________ 427 1.3 Essential Concepts _____________________________________________________________ 428 2. Activities ________________________________________________________________________ 440 2.1 Define a Data Quality Framework ________________________________________________ 440 2.2 Define High Quality Data _______________________________________________________ 441 2.3 Identify Dimensions and Supporting Business Rules ________________________________ 442 2.4 Perform an Initial Data Quality Assessment ________________________________________ 443 2.5 Identify and Prioritize Potential Improvements ____________________________________ 443 2.6 Define Goals for Data Quality Improvement _______________________________________ 444 2.7 Develop and Deploy Data Quality Operations ______________________________________ 445 3. Tools ____________________________________________________________________________ 450 3.1 Data Profiling Tools ____________________________________________________________ 451 3.2 Business Rule Templates and Engines _____________________________________________ 451 3.3 Data Parsing and Formatting _____________________________________________________ 451 3.4 Data Transformation and Standardization _________________________________________ 452 3.5 Data Enrichment _______________________________________________________________ 452 3.6 Incident Management System ____________________________________________________ 453 4. Techniques ______________________________________________________________________ 453 4.1 Effective Metrics _______________________________________________________________ 454 4.2 Data Profiling __________________________________________________________________ 456 4.3 Preventive Actions _____________________________________________________________ 457 4.4 Effective Data Quality Metrics ___________________________________________________ 457 4.5 Root Cause Analysis ____________________________________________________________ 458 4.6 Corrections ___________________________________________________________________ 458 5. Implementation Guidelines ______________________________________________________ 459 5.1 Readiness Assessment / Risk Assessment _________________________________________ 460 5.2 Organization and Cultural Change ________________________________________________ 461 6. Data Quality and the other Knowledge Areas ______________________________________ 462 6.1 Data Quality and Data Modeling and Design _______________________________________ 462 6.2 Data Quality and Metadata Management __________________________________________ 463 6.3 Data Quality and Master and Reference Data Management ___________________________ 463 6.4 Data Quality and Data Integration and Interoperability ______________________________ 463 6.5 Data Quality and Data Governance _______________________________________________ 464 7. Works Cited / Recommended ____________________________________________________ 464 8. Appendix _______________________________________________________________________ 465 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "8 • DMBOK2 8.1 Data Quality ISO Standard ______________________________________________________ 465 8.2 Further Reading on Data Quality Dimensions ______________________________________ 466 8.3 Statistical Process Control ______________________________________________________ 469 Chapter 14 : Big Data and Data Science ____________________________ 471 1. Introduction ____________________________________________________________________ 471 1.1 Business Drivers _______________________________________________________________ 472 1.2 Principles _____________________________________________________________________ 472 1.3 Essential Concepts _____________________________________________________________ 474 2. Activities _______________________________________________________________________ 484 2.1 Define Big Data Strategy and Business Needs ______________________________________ 484 2.2 Choose Data Sources ___________________________________________________________ 485 2.3 Acquire and Ingest Data Sources _________________________________________________ 486 2.4 Develop Data Hypotheses and Methods __________________________________________ 487 2.5 Integrate / Align Data for Analysis _______________________________________________ 487 2.6 Explore Data Using Models _____________________________________________________ 487 2.7 Deploy and Monitor ___________________________________________________________ 489 3. Tools ___________________________________________________________________________ 490 3.1 MPP Shared -nothing Technologies and Architecture _______________________________ 491 3.2 Distributed File -based Databases ________________________________________________ 492 3.3 In -database Algorithms _________________________________________________________ 493 3.4 Big Data Cloud Solutions _______________________________________________________ 493 3.5 Statistical Computing and Graphical Languages ____________________________________ 493 3.6 Data Visualization Tools ________________________________________________________ 493 4. Techniques _____________________________________________________________________ 494 4.1 Analytic Modeling _____________________________________________________________ 494 4.2 Big Data Modeling _____________________________________________________________ 495 5. Implementation Guidelines ______________________________________________________ 495 5.1 Strategy Alignment ____________________________________________________________ 496 5.2 Readiness Assessment / Risk Assessment _________________________________________ 496 5.3 Organization and Cultural Change _______________________________________________ 497 6. Big Data and Data Science Governance ___________________________________________ 497 6.1 Visualization Channels Management _____________________________________________ 498 6.2 Data Science and Visualization Standards _________________________________________ 498 6.3 Data Security __________________________________________________________________ 499 6.4 Metadata _____________________________________________________________________ 499 6.5 Data Quality __________________________________________________________________ 499 6.6 Metrics _______________________________________________________________________ 500 7. Works Cited / Recommended ___________________________________________________ 501 Chapter 15: D ata Management Maturity Assessment ________________ 503 1. Introduction ____________________________________________________________________ 503 1.1 Business Drivers _______________________________________________________________ 505 1.2 Goals and Principles ___________________________________________________________ 505 1.3 Essential Concepts _____________________________________________________________ 506 2. Activities _______________________________________________________________________ 510 2.1 Plan Assessment Activities ______________________________________________________ 511 2.2 Perform Maturity Assessment ___________________________________________________ 513 2.3 Interpret Results ______________________________________________________________ 514 2.4 Create a Targeted Program for Improvements _____________________________________ 515 2.5 Re -assess Maturity _____________________________________________________________ 515 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "CONTENTS • 9 3. Tools ____________________________________________________________________________ 516 4. Techniques ______________________________________________________________________ 516 4.1 Selecting a DMM Framework ____________________________________________________ 516 4.2 DAMA -DMBOK Framework Use _________________________________________________ 517 5. Guidelines for a DMMA __________________________________________________________ 518 5.1 Readiness Assessment / Risk Assessment _________________________________________ 518 5.2 Organizational and Cultural Change ______________________________________________ 518 6. Maturity Management Governance _______________________________________________ 519 6.1 DMMA Process Oversight _______________________________________________________ 519 6.2 Metrics _______________________________________________________________________ 519 7. Works Cited / Recommended ____________________________________________________ 520 Chapter 16: D ata Management Organization and Role Expectations ____ 521 1. Introduction _____________________________________________________________________ 521 2. Understand Existing Organization and Cultural Norms ____________________________ 521 3. Data Management Organizational Constructs ______________________________________ 523 3.1 Decentralized Operating Model __________________________________________________ 523 3.2 Network Operating Model _______________________________________________________ 524 3.3 Centralized Operating Model ____________________________________________________ 524 3.4 Hybrid Operating Model ________________________________________________________ 525 3.5 Federated Operating Model _____________________________________________________ 526 3.6 Identifying the Best Model for an Organization _____________________________________ 527 3.7 DMO Alternatives and Design Considerations _____________________________________ 527 4. Critical Success Factors __________________________________________________________ 528 4.1 Executive Sponsorship __________________________________________________________ 528 4.2 Clear Vision ___________________________________________________________________ 529 4.3 Proactive Change Management __________________________________________________ 529 4.4 Leadership Alignment __________________________________________________________ 529 4.5 Communication ________________________________________________________________ 529 4.6 Stakeholder Engagement ________________________________________________________ 530 4.7 Orientation and Training ________________________________________________________ 530 4.8 Adoption Measurement _________________________________________________________ 530 4.9 Adherence to Guiding Principles _________________________________________________ 530 4.10 Evolution Not Revolution ______________________________________________________ 531 5. Build the Data Management Organization _________________________________________ 531 5.1 Identify Current Data Management Participants ____________________________________ 531 5.2 Identify Committee Participants _________________________________________________ 532 5.3 Identify and Analyze Stakeholders _______________________________________________ 532 5.4 Involve the Stakeholders ________________________________________________________ 533 6. Interactions Between the DMO and Other Data -oriented Bodies ___________________ 533 6.1 The Chief Data Officer __________________________________________________________ 534 6.2 Data Governance _______________________________________________________________ 534 6.3 Data Quality ___________________________________________________________________ 535 6.4 Enterprise Architecture _________________________________________________________ 535 6.5 Managing a Global Organization __________________________________________________ 536 7. Data Management Roles __________________________________________________________ 537 7.1 Organizational Roles ____________________________________________________________ 537 7.2 Individual Roles _______________________________________________________________ 537 8. Works Cited / Recommended ____________________________________________________ 539 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "10 • DMBOK2 Chapter 17: D ata Management & Organizational Change Management _ 541 1. Introduction ____________________________________________________________________ 541 2. Laws of Change _________________________________________________________________ 542 3. Not Managing a Change: Managing a Transition ___________________________________ 542 4. Kotter’s Eight Errors of Change Management _____________________________________ 545 4.1 Error #1: Allowing Too Much Complacency _______________________________________ 545 4.2 Error #2: Failing to Create a Sufficiently Powerful Guiding Coalition _________________ 546 4.3 Error #3: Underestimating the Power of Vision ____________________________________ 546 4.4 Error #4: Under Communicating the Vision by a Factor of 10, 100, or 1000 ___________ 547 4.5 Error #5: Permitting Obstacles to Block the Vision _________________________________ 547 4.6 Error #6: Failing to Create Short -Term Wins ______________________________________ 547 4.7 Error #7: Declaring Victory Too Soon ____________________________________________ 548 4.8 Error # 8: Neglecting to Anchor Changes Firmly in the Corporate Culture _____________ 549 5. Kotter’s Eight Stage Process for Major Change ____________________________________ 549 5.1 Establishing a Sense of Urgency _________________________________________________ 550 5.2 The Guiding Coalition __________________________________________________________ 553 5.3 Developing a Vision and Strategy ________________________________________________ 557 5.4 Communicating the Change Vision ______________________________________________ 560 6. The Formula for Change _________________________________________________________ 565 7. Diffusion of Innovations and Sustaining Change __________________________________ 565 7.1 The Challenges to be Overcome as Innovations Spread _____________________________ 567 7.2 Key Elements in the Diffusion of Innovation ______________________________________ 567 7.3 The Five Stages of Adoption ____________________________________________________ 567 7.4 Factors Affecting Acceptance or Rejection of an Innovation or Change _______________ 568 8. Sustaining Change _______________________________________________________________ 569 8.1 Sense of Urgency / Dissatisfaction _______________________________________________ 570 8.2 Framing the Vision ____________________________________________________________ 570 8.3 The Guiding Coalition __________________________________________________________ 570 8.4 Relative Advantage and Observability ____________________________________________ 571 9. Communicating Data Management Value _________________________________________ 571 9.1 Communications Principles _____________________________________________________ 571 9.2 Audience Evaluation and Preparation ____________________________________________ 572 9.3 The Human Element ___________________________________________________________ 573 9.4 Communication Plan ___________________________________________________________ 573 9.5 Keep Communicating __________________________________________________________ 574 10. Works Cited / Recommended __________________________________________________ 575 Acknowledgements ___________________________________________ 577 Index _______________________________________________________ 581 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "11 Figures Figure 1 Data Management Principles _______________________________________________ 24 Figure 2 Data Lifecycle Key Activities ______________________________________________ 31 Figure 3 Strategic Alignment Model ________________________________________________ 36 Figure 4 Amsterdam Information Model ____________________________________________ 37 Figure 5 The DAMA- DMBOK2 Data Management Framework (The DAMA Wheel) __________ 38 Figure 6 DAMA Environmental Factors Hexagon _____________________________________ 39 Figure 7 Knowledge Area Context Diagram __________________________________________ 40 Figure 8 Purchased or Built Database Capability _______________________________________ 42 Figure 9 DAMA Functional Area Dependencies _______________________________________ 43 Figure 10 DAMA Data Management Function Framework _______________________________ 44 Figure 11 DAMA Wheel Evolved __________________________________________________ 46 Figure 12 Context Diagram: Data Handling Ethics _____________________________________ 52 Figure 13 Ethical Risk Model for Sampling Projects ____________________________________ 66 Figure 14 Context Diagram: Data Governance and Stewardship __________________________ 71 Figure 15 Data Governance and Data Management ____________________________________ 74 Figure 16 Data Governance Organization Parts _______________________________________ 76 Figure 17 Enterprise Data Governance Operating Framework Examples ____________________ 77 Figure 18 CDO Organizational Touch Points _________________________________________ 83 Figure 19 An Example of an Operating Framework ____________________________________ 85 Figure 20 Data Issue Escalation Path ________________________________________________ 89 Figure 21 Context Diagram: Data Architecture _______________________________________ 102 Figure 22 Simplified Zachman Framework __________________________________________ 104 Figure 23 Enterprise Data Model __________________________________________________ 107 Figure 24 Subject Area Models Diagram Example _____________________________________ 108 Figure 25 Data Flow Depicted in a Matrix ___________________________________________ 110 Figure 26 Data Flow Diagram Example _____________________________________________ 110 Figure 27 The Data Dependencies of Business Capabilities _____________________________ 113 Figure 28 Context Diagram: Data Modeling and Design ________________________________ 124 Figure 29 Entities _____________________________________________________________ 129 Figure 30 Relationships _________________________________________________________ 130 Figure 31 Cardinality Symbols ___________________________________________________ 130 Figure 32 Unary Relationship - Hierarchy __________________________________________ 131 Figure 33 Unary Relationship - Network ___________________________________________ 131 Figure 34 Binary Relationship ____________________________________________________ 131 Figure 35 Ternary Relationship ___________________________________________________ 132 Figure 36 Foreign Keys _________________________________________________________ 132 Figure 37 Attributes ___________________________________________________________ 133 Figure 38 Dependent and Independent Entity _______________________________________ 134 Figure 39 IE Notation __________________________________________________________ 136 Figure 40 Axis Notation for Dimensional Models _____________________________________ 137 Figure 41 UML Class Model _____________________________________________________ 139 Figure 42 ORM Model __________________________________________________________ 140 Figure 43 FCO -IM Model ________________________________________________________ 141 Figure 44 Data Vault Model ______________________________________________________ 141 Figure 45 Anchor Model ________________________________________________________ 142 Figure 46 Relational Conceptual Model ____________________________________________ 144 Figure 47 Dimensional Conceptual Model __________________________________________ 144 Figure 48 Relational Logical Data Model ____________________________________________ 145 Figure 49 Dimensional Logical Data Model __________________________________________ 146 Figure 50 Relational Physical Data Model ___________________________________________ 146 Figure 51 Dimensional Physical Data Model _________________________________________ 147 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "12 • DMBOK2 Figure 52 Supertype and Subtype Relationships _______________________________________ 150 Figure 53 Modeling is Iterative ____________________________________________________ 151 Figure 54 Context Diagram: Data Storage and Operations _______________________________ 167 Figure 55 Centralized vs. Distributed _______________________________________________ 171 Figure 56 Federated Databases ____________________________________________________ 173 Figure 57 Coupling _____________________________________________________________ 173 Figure 58 CAP Theorem _________________________________________________________ 176 Figure 59 Database Organization Spectrum __________________________________________ 180 Figure 60 Log Shipping vs. Mirroring _______________________________________________ 187 Figure 61 SLAs for System and Database Performance _________________________________ 197 Figure 62 Sources of Data Security Requirements _____________________________________ 210 Figure 63 Context Diagram: Data Security ___________________________________________ 211 Figure 64 DMZ Example _________________________________________________________ 222 Figure 65 Security Role Hierarchy Example Diagram __________________________________ 241 Figure 66 Context Diagram: Data Integration and Interoperability ________________________ 259 Figure 67 ETL Process Flow ______________________________________________________ 262 Figure 68 ELT Process Flow ______________________________________________________ 262 Figure 69 Application Coupling ___________________________________________________ 269 Figure 70 Enterprise Service Bus __________________________________________________ 270 Figure 71 Context Diagram: Documents and Content __________________________________ 288 Figure 72 Document Hierarchy based on ISO 9001- 4.2 _________________________________ 301 Figure 73 Electronic Discovery Reference Model _____________________________________ 302 Figure 74 Information Governance Reference Model __________________________________ 323 Figure 75 Context Diagram: Reference and Master Data ________________________________ 330 Figure 76 Key Processing Steps for MDM ____________________________________________ 341 Figure 77 Master Data Sharing Architecture Example __________________________________ 350 Figure 78 Reference Data Change Request Process ____________________________________ 357 Figure 79 Context Diagram: DW/BI ________________________________________________ 362 Figure 80 The Corporate Information Factory ________________________________________ 367 Figure 81 Kimball's Data Warehouse Chess Pieces _____________________________________ 369 Figure 82 Conceptual DW/BI and Big Data Architecture ________________________________ 370 Figure 83 Release Process Example ________________________________________________ 379 Figure 84 Context Diagram: Metadata _______________________________________________ 396 Figure 85 Centralized Metadata Architecture _________________________________________ 408 Figure 86 Distributed Metadata Architecture _________________________________________ 409 Figure 87 Bi -Directional Metadata Architecture _______________________________________ 411 Figure 88 Example Metadata Repository Metamodel ___________________________________ 413 Figure 89 Sample Data Element Lineage Flow Diagram _________________________________ 418 Figure 90 Sample System Lineage Flow Diagram ______________________________________ 418 Figure 91 Context Diagram: Data Quality ____________________________________________ 426 Figure 92 The Shewhart Chart with the role of a Data Quality Team indicated _______________ 434 Figure 93 Sources of Data Quality Issues ____________________________________________ 436 Figure 94 Illustrations of data defects trends over time _________________________________ 447 Figure 95 Control Chart of a Process in Statistical Control _______________________________ 470 Figure 96 Abate Information Triangle ______________________________________________ 472 Figure 97 Context Diagram: Big Data and Data Science _________________________________ 473 Figure 98 Data Science Process ____________________________________________________ 475 Figure 99 Data Storage Challenges _________________________________________________ 477 Figure 100 Conceptual DW/BI and Big Data Architecture _______________________________ 477 Figure 101 Services -based Architecture _____________________________________________ 479 Figure 102 Columnar Appliance Architecture ________________________________________ 492 Figure 103 Context Diagram: Data Management Maturity Assessment _____________________ 504 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "FIGURES AND TABLES • 13 Figure 104 Data Management Maturity Model Example ________________________________ 507 Figure 105 Example of a Data Management Maturity Assessment Visualization _____________ 508 Figure 106 Assess Current State to Create an Operating Model __________________________ 522 Figure 107 Decentralized Operating Model _________________________________________ 523 Figure 108 Network Operating Model _____________________________________________ 524 Figure 109 Centralized Operating Model ___________________________________________ 525 Figure 110 Hybrid Operating Model _______________________________________________ 526 Figure 111 Federated Operating Model _____________________________________________ 527 Figure 112 Stakeholder Rating Map _______________________________________________ 533 Figure 113 Bridges’s Transition Phases _____________________________________________ 543 Figure 114 Kotter’s Eight Stage Process for Major Change ______________________________ 550 Figure 115 Sources of Complacency _______________________________________________ 552 Figure 116 Vision Breaks Through Status Quo _______________________________________ 557 Figure 117 Management/Leadership Contrast _______________________________________ 560 Figure 118 Everett Rogers Diffusion of Innovations ___________________________________ 566 Figure 119 The Stages of Adoption ________________________________________________ 568 Tables Table 1 GDPR Principles _________________________________________________________ 56 Table 2 Canadian Privacy Statutory Obligations _______________________________________ 57 Table 3 United States Privacy Program Criteria _______________________________________ 57 Table 4 Typical Data Governance Committees / Bodies _________________________________ 76 Table 5 Principles for Data Asset Accounting _________________________________________ 80 Table 6 Architecture Domains ___________________________________________________ 103 Table 7 Commonly Used Entity Categories _________________________________________ 127 Table 8 Entity, Entity Type, and Entity Instance _____________________________________ 128 Table 9 Modeling Schemes and Notations ___________________________________________ 135 Table 10 Scheme to Database Cross Reference _______________________________________ 136 Table 11 Data Model Scorecard® Template __________________________________________ 162 Table 12 ACID vs BASE _________________________________________________________ 175 Table 13 Sample Regulation Inventory Table ________________________________________ 236 Table 14 Role Assignment Grid Example ___________________________________________ 240 Table 15 Levels of Control for Documents per ANSI -859 ______________________________ 310 Table 16 Sample Audit Measures _________________________________________________ 311 Table 17 Simple Reference List ___________________________________________________ 335 Table 18 Simple Reference List Expanded __________________________________________ 335 Table 19 Cross -Reference List ____________________________________________________ 335 Table 20 Multi -Language Reference List ____________________________________________ 336 Table 21 UNSPSC (Universal Standard Products and Services Classification) _______________ 336 Table 22 NAICS (North America Industry Classification System) ________________________ 336 Table 23 Critical Reference Data Metadata Attributes _________________________________ 338 Table 24 Source Data as Received by the MDM System ________________________________ 342 Table 25 Standardized and Enriched Input Data ______________________________________ 343 Table 26 Candidate Identification and Identity Resolution _____________________________ 344 Table 27 DW -Bus Matrix Example ________________________________________________ 368 Table 28 CDC Technique Comparison _____________________________________________ 373 Table 29 Common Dimensions of Data Quality ______________________________________ 430 Table 30 Data Quality Activities __________________________________________________ 435 Table 31 Data Quality Monitoring Techniques _______________________________________ 447 Table 32 Data Quality Metric Examples ____________________________________________ 455 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "14 • DMBOK2 Table 33 Analytics Progression ____________________________________________________ 474 Table 34 Typical Risks and Mitigations for a DMMA ___________________________________ 518 Table 35 Bridges’s Transition Phases _______________________________________________ 543 Table 36 Complacency Scenarios __________________________________________________ 545 Table 37 Declaring Victory Too Soon Scenarios _______________________________________ 548 Table 38 Diffusion of Innovations Categories Adapted to Information Management __________ 566 Table 39 The Stages of Adoption (Adapted from Rogers, 1964) __________________________ 568 Table 40 Communication Plan Elements ____________________________________________ 573 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "15 Preface AMA International is pleased to release the second edition of the DAMA Guide to the Data Management Body of Knowledge (DAMA -DMBOK2). Since the publication of the first edition in 2009, significant developments have taken place in the field of data management. Data Governance has become a standard structure in many organizations, new technologies have enabled the collection and use of ‘Big Data’ (semi -structured and unstructured data in a wide range of formats), and the importance of data ethics has grown al ong with our ability to explore and exploit the vast amount of data and information produced as part of our daily lives. These changes are exciting. They also place new and increasing demands on our profession. DAMA has responded to these changes by reformulating the DAMA Data Management Framework (the DAMA Wheel), adding detail and clarification, and expanding the scope of the DMBOK: • Context diagrams for all Knowledge Areas have been improved and updated. • Data Integration and Interoperability has been added as a new Knowledge Area to highlight its importance (Chapter 8). • Data Ethics has been called out as a separate chapter due to the increasing necessity of an ethical approach to all aspects of data management (Chapter 2). • The role of governance has been described both as a function (Chapter 3) and in relation to each Knowledge Area. • A similar approach has been taken with organizational change management, which is described in Chapter 17 and incorporated into the Knowledge Area chapters. • New chapters on Big Data and Data Science (Chapter 14) and Data Management Maturity Assessment (Chapter 15) help organizations understand where they want to go and give them the tools to get there. • The second edition also includes a newly formulated set of data management principles to support the ability of organizations to manage their data effectively and get value from their data assets (Chapter 1). We hope the DMBOK2 will serve data management professionals across the globe as a valuable resource and guide. Nevertheless, we also recognize it is only a starting point. Real advancement will come as we apply and learn from these ideas. DAMA exists to enable members to learn continuously by sharing ideas, trends, problems, and solutions. Sue Geuens Laura Sebastian -Coleman President Publications Officer DAMA International DAMA International D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "17 Preface to the Revised Edition AMA International is pleased to release an updated edition of the DAMA Guide to the Data Management Body of Knowledge (DAMA -DMBOK2). Since the publication of the second edition in 2017, many data professionals have contacted us to point out areas that could be easily improved or corrected. We are so grateful for your inputs. In 2023, we spent time soliciting additional input from the data management professionals community worldwide. We compiled these into this revised version. This interim release represen ts a number of corrections and improvements to the 2017 edition. These range from typos to factually incorrect statements. With these, the DAMA -DMBOK2 can continue to serve as a go -to source of knowledge about the what’s of data management. The how’s (of course) are capable of being best implemented by you. DAMA exists to enable volunteer members to learn continuously by sharing ideas, trends, problems, and solutions. This is a considerable accomplishment, and the team deserves the community’s collective th anks. Peter Aiken, Ph D Chris Bradley, PhD President Editor in Chief To read about the specific changes between this version and the previous one, please consult the publication section on the dama.org website. D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "19 CHAPTER 1 Data Management 1. Introduction any organizations recognize that their data is a vital enterprise asset. Data and information can give them insight about their customers, products, and services. It can help them innovate and reach strategic goals. Despite that recognition, few organizations actively manage data as an asset from which they can derive ongoing value (Evans and Price, 2012) . Deriving value from data does not happen in a vacuum or by accident. It requires intention, planning, coordination, and commitment. It requires management and leadership. Data Management is the development, execution, and supervision of plans, policies, programs, and practices that deliver, control, protect, and enhance the value of data and information assets throughout their lifecycles. A Data Management Professional is any person who works in any facet of data management (from technical management of data throughout its lifecycle to ensuring that data is properly utilized and leveraged) to meet strategic organizational goals. Data management professionals fill numerous roles, from the highly technical (e.g., database administrators, network administrators, programmers) to strategic business (e.g., Data Stewards, Data Strategists, Chief Data Officers). Data management activities are wide -ranging. They include everything from the ability to make consistent decisions about how to get strategic value from data to the technical deployment and performance of databases. Thus data management requires both techn ical and non-technical (i.e., ‘business’) skills. Responsibility for managing data must be shared between business and information technology roles, and people in both areas must be able to collaborate to ensure an organization has high quality data that m eets its strategic needs. Data and information are not just assets in the sense that organizations invest in them in order to derive future value. Data and information are also vital to the day -to-day operations of most organizations. They have been called the ‘currency’, the ‘life blood’, and even the ‘new oil’ of the information economy. 1 Whether or not an organization gets value from its analytics, it cannot even transact business without data. To support the data management professionals who carry out the work, DAMA International (The Data Management Association) has produced this book, the second edition of The DAMA Guide to the Data 1 Google ‘data as currency’, ‘data as life blood’, and ‘the new oil’, for numerous references. M Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "20 • DMBOK2 Management Body of Knowledge (DMBOK2) . This edition builds on the first one, published in 2009, which provided foundational knowledge on which to build as the profession advanced and matured. This chapter outlines a set of principles for data management. It discusses challenges related to following those principles and suggests approaches for meeting these challenges. The chapter also describes the DAMA Data Management Framework, which provides the context for the work carried out by data management professionals within various Data Management Knowledge Areas. 1.1 Business Drivers Information and knowledge hold the key to competitive advantage . Organiz ations that have reliable, high quality data about their customers, products, services, and operations can make better decisions than those without data or with unreliable data. Failure to manage data is similar to failure to manage capital. It results in waste and lost opportunity. The primary driver for data management is to enable organizations to get value from their data assets, just as effective management of financial and physical assets enables organizations to get value from those assets. 1.2 Goals Within an organization, data management goals include: • Understanding and supporting the information needs of the enterprise and its stakeholders, including customers, employees, and business partners • Capturing, storing, protecting, and ensuring the integrity of data assets • Ensuring the quality of data and information • Ensuring the privacy and confidentiality of stakeholder data • Preventing unauthorized or inappropriate access, manipulation, or use of data and information • Ensuring data can be used effectively to add value to the enterprise 2. Essential Concepts 2.1 Data Long- standing definitions of data emphasize its role in representing facts about the world.2 In relation to information technology, data is also understood as information that has been stored in digital form (though 2 The New Oxford Ameri can Dictionary defines data as “facts and statistics collected together for analysis. ” The American Society for Quality (ASQ) defines data as “A set of collected facts” and describes two kinds of numerical data: measured or variable and counted or Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 21 data is not limited to information that has been digitized and data management principles apply to data captured on paper as well as in databases). Still, because today we can capture so much information electronically, we call many things ‘data’ that woul d not have been called ‘data’ in earlier times – things like names, addresses, birthdates, what one ate for dinner on Saturday, the most recent book one purchased. Such facts about individual people can be aggregated, analyzed, and used to make a profit, improve health, or influence public policy. Moreover , our technological capacity to measure a wide range of events and activities (from the repercussions of the Big Bang to our own heartbeats) and to collect, store, and analyze electronic versions of things that were not previously thought of as data (videos , pictures, sound recordings, documents) is close to surpassing our ability to synthesize these data into usable information. 3 To take advantage of the variety of data without being overwhelmed by its volume and velocity requires reliable, extensible data management practices. Most people assume that, because data represents facts, it is a form of truth about the world and that the facts will fit together. But ‘facts’ are not always simple or straightforward. Data is a means of representation. It stands for things other than its elf (Chisholm, 2010). Data is both an interpretation of the objects it represents and an object that must be interpreted (Sebastian -Coleman, 2013). This is another way of saying that we need context for data to be meaningful. Context can be thought of as d ata’s representational system; such a system includes a common vocabulary and a set of relationships between components. If we know the conventions of such a system, then we can interpret the data within it. 4 These conventions are often documented in a specific kind of data referred to as Metadata . However, because people often make different choices about how to represent concepts, they create different ways of representing the same concepts. From these choices, data takes on different shapes. Think of the range of ways we have to represent calendar dates, a concept about which there is an agreed -to definition. Now consider more complex concepts (such as customer or product), where the granularity and level of detail of what needs to be represented is not always self -evident, and the process of representation grows more complex, as does the process of managing that inform ation over time. (See Chapter 10). Even within a single organization, there are often multiple ways of representing the same idea. Hence the need for Data Architecture, modeling, governance, and stewardship, and Metadata and Data Quality management, all of which help people understand and u se data. Across organizations, the problem of multiplicity multiplies. Hence the need for industry -level data standards that can bring more consistency to data. Organizations have always needed to manage their data, but changes in technology have expanded the scope of this management need as they have changed people’s understanding of what data is. These changes have enabled organizations to use data in new ways t o create products, share information, create knowledge, and attributed. The International Standards Organization (ISO) defines data as “re -interpretable representation of information in a formalized manner suitable for communication, interpretation, or processing” (ISO 11179). This definition emphasizes the electro nic nature of data and assumes, correctly, that data requires standards because it is managed through information technology systems. That said, it does not speak to the challenges of formalizing data in a consistent way, across disparate systems. Nor does it account well for the concept of unstructured data. 3 http://ubm.io/2c4yPOJ (Accessed 20016 -12-04). http://bit.ly/1rOQkt1 (Accessed 20016- 12-04). 4 For additional information on the constructed -ness of data see: Kent, Data and Reality (2012) and Devlin, Business Unintelligence (2013). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "22 • DMBOK2 improve organizational success. But , the rapid growth of technology and with it, human capacity to produce, capture, and mine data for meaning has intensified the need to manage data effectively. 2.2 Data and Information Much ink has been spilled over the relationship between data and information . Data has been called the “raw material of information” and information has been called “data in context”.5 Often a layered pyramid is used to describe the relationship between data (at the base), information, knowledge, and wisdom (at the very top). While the pyramid can be helpful in describing why data needs to be well -managed, this representation presents several challenges for data management . • It is based on the assumption that data simply exists. But data does not simply exist. Data has to be created. • By describing a linear sequence from data through wisdom, it fails to recognize that it takes knowledge to create data in the first place. • It implies that data and information are separate things, when in reality, the two concepts are intertwined with and dependent on each other. Data is a form of information and information is a form of data. Within an organization, it may be helpful to draw a line between information and data for purposes of clear communication about the requirements and expectations of different uses by different stakeholders. (“Here is a sales report for the last quarter [in formation]. It is based on data from our data warehouse [data]. Next quarter , these results [data] will be used to generate our quarter -over -quarter performance measures [information]”). Recognizing data and information need to be prepared for different pu rposes drives home a central tenet of data management: Both data and information need to be managed. Both will be of higher quality if they are managed together with uses and customer requirements in mind. Throughout the DMBOK, the terms will be used inter changeably. 2.3 Data as an Organizational Asset An asset is an economic resource, that can be owned or controlled, and that holds or produces value. Assets can be converted to money. Data is widely recognized as an enterprise asset, though understanding of what it means to manage data as an asset is still evolving. In the early 1990s, some organizations found it questionable whether the value of goodwill should be given a monetary value. Now, the ‘value of goodwill’ commonly shows up as an item on the Profit and Loss Statement (P &L). Similarly, while not universally adopted, monetization of data is becoming increasingly common. It will not be too long before we see this as a feature of P& Ls. (See Chapter 3 .) 5 See English, 1999 and DAMA, 2009. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 23 Today’s organizations rely on their data assets to make more effective decisions and to operate more efficiently. Businesses use data to understand their customers, create new products and services, and improve operational efficiency by cutting costs and c ontrolling risks. Government agencies, educational institutions, and not -for- profit organizations also need high quality data to guide their operational, tactical, and strategic activities. As organizations increasingly depend on data, the value of data assets can be more clearly established. Many organizations identify themselves as ‘data -driven’. Businesses aiming to stay competitive must stop making decisions based on gut feelings or instincts, and instead use event triggers and apply analytics to gain actionable insight. Being data -driven includes the recognition that data must be managed efficiently and with professional discipline, through a partnership of business leadership and technical expertise. Furthermore, the pace of business today means that change is no longer optional; digital disruption is the norm. To react to this, business must co -create information solutions with technical data professionals working alongside line -of-business counterpar ts. They must plan for how to obtain and manage data that they know they need to support business strategy. They must also position themselves to take advantage of opportunities to leverage data in new ways. 2.4 Data Management Principles Data management shares characteristics with other forms of asset management, as seen in Figure 1. It involves knowing what data an organization has and what might be accomplished with it, then determining how best to use data assets to reach organizational goals. Like other management processes, it must balance strategic and operational needs. This balance can best be struck by following a set of principles that recognize salient features of data management and guide data management practice. • Data is an asset with unique properties : Data is an asset, but it differs from other assets in important ways that influence how it is managed. The most obvious of these properties is that data is not consumed when it is used, as are financial and physical assets. • The value of data can and should be expressed in economic terms : Calling data an asset implies that it has value. While there are techniques for measuring data’s qualitative and quantitative value, there are not yet standards for doing so. Organizations that want to make better decisions about their data should develo p consistent ways to quantify that value. They should als o measure both the costs of low qualit y data and the benefits of high quality data. • Managing data means managing the quality of data : Ensuring that data is fit for purpose is a primary goal of data management. To manage quality, organizations must ensure they understand stakeholders’ requirements for quality and measure data against these requirements. • It takes Metadata to manage data : Managing any asset requires having data about that asset (number of employees, accounting codes, etc.). The data used to manage and use data is called Metadata . Because data cannot be held or touched, to understand what it is and how to use it requires definition Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "24 • DMBOK2 and knowledge in the form of Metadata. Metadata originates from a range of processes related to data creation, processing, and use, including architecture, modeling, stewardship, governance, Data Quality management, systems development, IT and business ope rations, and analytics. Figure 1 Data Management Principles • It takes planning to manage data : Even small organizations can have complex technical and business process landscapes. Data is created in many places and is moved between places for use. To coordinate work and keep the end results aligned requires planning from an architectural and proce ss perspective. • Data management is cross- functional; it requires a range of skills and expertise: A single team cannot manage all of an organization’s data. Data management requires both technical and non-technical skills and the ability to collaborate. • Data management requires an enterprise perspective: Data management has local applications, but it must be applied across the enterprise to be as effective as possible. This is one reason why data management and Data Governance are intertwined. Data is valuable •Data is an asset with unique properties •The value of data can and should be expressed in economic terms Data Management requirements are Business Requirements •Managing data means managing the quality of data •It takes Metadata tomanage data •It takes planning to manage data •Data management requirements must drive Information T echnology decisions Data Management is lifecycle management •Different types of data have different lifecycle characteristics •Managing data includes managing the risks associated with dataEffective data management requires leadership commitmentDATA MANAGEMENT PRINCIPLES Data Management depends on diverse skills •Data management is cross -functional •Data management requires an enterprise perspective •Data management must account for a range of perspectives Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 25 • Data management must account for a range of perspectives : Data is fluid. Data management must constantly evolve to keep up with the ways data is created and used and the data consumers who use it. • Data management is lifecycle management and different types of data have different lifecycle characteristics : Data has a lifecycle and managing data requires managing its lifecycle. Because data begets more data, the data lifecycle itself can be very complex. Data management practices need to account for the data lifecycle. • Different types of data have different lifecycle characteristics , and, for this reason, they have different management requirements. Data management practices have to recognize these differences and be flexible enough to meet different kinds of data lifecycle requirements. • Managing data includes managing the risks associated with data : In addition to being an asset, data also represents risk to an organization. Data can be lost, stolen, or misused. Organizations must consider the ethical implications of their uses of data. Data -related risks must be managed as part of the data lifecycle. • Data management requirements must drive Information Technology decisions : Data and data management are deeply intertwined with information technology and information technology management. Managing data requires an approach that ensures technology serves, rather than drives, an organization’s strategic data needs. • Effective data management requires leadership commitment : Data management involves a complex set of processes that, to be effective, require coordination, collaboration, and commitment. Getting there requires not only management skills, but also the vision and purpose that come from committed leadership. 2.5 Data Management Challenges Because data management has distinct characteristics derived from the properties of data itself, it also presents challenges in following these principles. Details of these challenges are discussed in Sections 2.5.1 through 2.5.13. Many of these challenges refer to more than one principle. 2.5.1 Data Differs from Other Assets6 Physical assets can be pointed to, touched, and moved around. They can be in only one place at a time. Financial assets must be accounted for on a balance sheet. However, data is different. Data is not tangible. Yet it is durable; it does not wear out, though the value of data often changes as it ages. Data is easy to copy and transport. But it is not easy to reproduce if it is lost or destroyed. Because it is not consumed when used, it can even be stolen 6 This section derives from Redman, Thomas. Data Quality for the Information Age (1996) pp. 41- 42, 232 -36; and Data Driven (2008), Chapter One, “The Wondrous and Perilous Properties of Data and Information.” Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "26 • DMBOK2 without being gone. Data is dynamic and can be used for multiple purposes. The same data can even be used by multiple people at the same time – something that is impossible with physical or financial assets. Many uses of data beget more data. Most organiza tions must manage increasing volumes of data and the relation between data sets. These differences make it challenging to put a monetary value on data . Without this monetary value, it is difficult to measure how data contributes to organizational success. These differences also raise other issues that affect data management, such as defining data ownership, inventorying how much data an organization has, protecting against the misuse of data, managing risk associated with data redundancy, and defining and enforcing standards for Data Quality. Despite the challenges with measuring the value of data, most people recognize that data, indeed, has value. An organization’s data is unique to itself. Were organizationally unique data (such as customer lists, product inventories, or claim history) to be lost or destroyed, replacing it would be impossible or extremely costly. Data is also the means by which an organization knows itself – it is a meta- asset that describes other assets. As such, it provides the foundation for organizational insight. Within and between organizations, data and information are essential to conducting business. Most operational business transactions involve the exchange of information. Most information is exchanged electronically, creating a data trail. This data trail can serve purposes in addition to marking the exchanges that have taken place. It can provide information about how an organization functions. Because of the important role that data plays in any organization, it needs to be managed with care. 2.5.2 Data Valuation Value is the difference between the cost of a thing and the benefit derived from that thing. For some assets, like stock, calculating value is easy. It is the difference between what the stock cost when it was purchased and what it was sold for. But for data, these calculations are more complicated because neither the costs nor the benefits of data are standardized. Since each organization’s data is unique to itself, an approach to data valuation needs to begin by articulating general cost and benefit categories that can be applied consistently within an organization. Sample categories include 7: • Cost of obtaining and storing data • Cost of replacing data if it were lost • Impact to the organization if data were missing • Cost of risk mitigation and potential cost of risks associated with data • Cost of improving data 7 While the DMBOK2 was preparing to go to press, another means of valuing data was in the news: Wannacry ransomware attack (17 May 2017) impacted more than 100K organizations in 150 countries. The culprits used the software to hold data hostage until victi ms paid ransom to get their data released. http://bit.ly/2tNoyQ7 . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 27 • Benefits of higher quality data • What competitors would pay for data • What the data could be sold for • Expected revenue from innovative uses of data A primary challenge to data asset valuation is that the value of data is contextual (what is of value to one organization may not be of value to another) and often temporal (what was valuable yesterday may not be valuable today). That said, within an organization, certain types of data are likely to be consistently valuable over time. Take reliable customer information, for example. Customer information may even grow more valuable over time, as more data accumulates related to customer activity. In relation to data management, establishing ways to associate financial value with data is critical, since organizations need to understand assets in financial terms in order to make consistent decisions. Putting value on data becomes the basis of putting value on data management activities. 8 The process of data valuation can also be used as a means of change management. Asking data management professionals and the stakeholders they support to understand the financial meaning of their work can help an organization transform its understanding of its own data and, through that, its approach to d ata management. 2.5.3 Data Quality Ensuring that data is of high quality is central to data management. Organizations manage their data because they want to use it. If they cannot rely on it to meet business needs, then the effort to collect, store, secure, and enable access to it is wasted. To ensure data meets business needs, they must work with data consumers to define these needs, including characteristics that make data of high quality. Largely because data has been associated so closely with information technology, managing Data Quality has historically been treated as an afterthought. IT teams are often dismissive of the data that the systems they create are supposed to store. It was probably a programmer who first observed ‘garbage in, garbage out’ – and who no doubt wanted to let it go at that. But the people who want to use the data cannot afford to be dismissive of quality. They generally assume data is reliable and trustworthy, until they have a reason to doubt these things. Once they lose trust, it is difficult to regain it. Most uses of data involve learning from it in order to apply that learning and create value. Examples include understanding customer habits in order to improve a product or service and assessing organizational performance or market trends in order to devel op a better business strategy, etc. Poor quality data will have a negative impact on these decisions. As importantly, poor quality data is simply costly to any organization. Estimates differ, but experts think organizations spend between 10 -30% of revenue handling Data Quality issues. IBM estimated the cost of poor 8 For case studies and examples, see Aiken and Billings, Monetizing Data Management (2014). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "28 • DMBOK2 quality data in the US in 2016 was $3.1 Trillion.9 Many of the costs of poor quality data are hidden, indirect, and therefore hard to measure. Others, like fines, are direct and easy to calculate. Costs come from: • Scrap and rework • Workarounds and hidden correction processes • Organizational inefficiencies or low productivity • Organizational conflict • Low job satisfaction • Customer dissatisfaction • Opportunity costs, including inability to innovate • Compliance costs or fines • Reputational costs The corresponding benefits of high quality data include: • Improved customer experience • Higher productivity • Reduced risk • Ability to act on opportunities • Increased revenue • Competitive advantage gained from insights on customers, products, processes, and opportunities As these costs and benefits imply, managing Data Quality is not a one -time job. Producing high quality data requires planning, commitment, and a mindset that builds quality into processes and systems. All data management functions can influence Data Qualit y, for good or bad, so all of them must account for it as they exec ute their work. (See Chapter 13 ). 2.5.4 Planning for Better Data As stated in the chapter introduction, deriving value from data does not happen by accident. It requires planning in many forms. It starts with the recognition that organizations can control how they obtain and create data. If they view data as a product that they create, they will make better decisions about it throughout its lifecycle. These decisions require systems thinking because they involve: • The ways data connects business processes that might otherwise be seen as separate • The relationship between business processes and the technology that supports them • The design and architecture of systems and the data they produce and store • The ways data might be used to advance organizational strategy 9 Reported in Redman, Thomas. “Bad Data Costs U.S. $3 Trillion per Year.” Harvard Business Review. 22 September 2016. https://hbr.org/2016/09/bad -data- costs -the-u-s-3- trillion -per-year . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 29 Planning for better data requires a strategic approach to architecture, modeling, and other design functions. It also depends on strategic collaboration between business and IT leadership. And, of course, it depends on the ability to execute effectively on individual projects. The challenge is that there are usually organizational pressures, as well as the perennial pressures of time and money, that get in the way of better planning. Organizations must balance long- and short -term goals as they execute their strategies. Having clarity about the trade -offs leads to better decisions. 2.5.5 Metadata and Data Management Organizations require reliable Metadata to manage data as an asset. Metadata, in this sense , should be understood comprehensively. It includes not only the business, technical, and operational Metadata described in Chapter 12, but also the Metadata embedded in Data Architecture, data models, data security requirements, data integration standards, and data operational processes. (See Chapters 4 – 11.) Metadata describes what data an organization has, what it represents, how it is classified, where it came from, how it moves within the organization, how it evolves through use, who can and cannot use it, and whether it is of high quality. Data is abstract. Definitions and other descriptions of context enable it to be understood. They make data, the data lifecycle, and the complex systems that contain data comprehensible. The challenge is that Metadata is a form of data and needs to be managed as such. Organizations that do not manage their data well generally do not manage their Metadata at all. Metadata management often provides a starting point for improvements in data m anagement overall. 2.5.6 Data Management is Cross -functional Data management is a complex process. Data is managed in different places within an organization by teams responsible for different phases of the data lifecycle. Data management requires design skills to plan for systems, highly technical skills to administer hardware and build software, data analysis skills to understand issues and problems, analytic skills to interpret data, language skills to bring consensus to definitions and models, as well as strategic thinking to see opportunities to serve customers and meet goals. The challenge is getting people with this range of skills and perspectives to recognize how the pieces fit together so that they collaborate well as they work toward common goals. 2.5.7 Establishing an Enterprise Perspective Managing data requires understanding the scope and range of data within an organization. Data is one of the ‘horizontals’ of an organization. It moves across verticals, such as sales, marketing, and operations… Or at least it should. Data is not only unique to an organization; sometimes it is unique to a department or other sub -part of an organization. Because data is often viewed simply as a by- product of operational processes (for example, Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "30 • DMBOK2 sales transaction records are the by- product of the selling process), it is not always planned for beyond the immediate need. Even within an organization, data can be disparate. Data originates in multiple places within an organization. Different departments may have different ways of representing the same concept (e.g., customer, product, vendor). As anyone involved in a data integration or Master Data Management project can testify, subtle (or blatant) differences in representational choices present challenges in managing data across an organization. At the same time, stakeholders assume that an organization’s data should be coh erent, and a goal of managing data is to make it fit together in common sense ways so that it is usable by a wide range of data consumers. One reason Data Governance has become increasingly important is to help organizations make decisions about data across verticals. (See Chapter 3. ) 2.5.8 Accounting for Other Perspectives Today’s organizations use data that they create internally, as well as data that they acquire from external sources. They have to account for different legal and compliance requirements across national and industry lines. People who create data often forget that someone else will use t hat data later. Knowledge of the potential uses of data enables better planning for the data lifecycle and, with that, for better quality data. Data can also be misused. Accounting for this risk reduces the likelihood of misuse. 2.5.9 The Data Lifecycle Like other assets, data has a lifecycle . To effectively manage data assets, organizations need to understand and plan for the data lifecycle. Well -managed data is managed strategically, with a vision of how the organization will use its data. A strategic organization will define not only its data content requirements, but also its data management requirements. These include policies and expectations for use, quality, controls, and security; an enterprise approach to architecture and design; and a sustainable approach to both infrastructure and s oftware development. The data lifecycle is based on the product lifecycle. It should not be confused with the systems development lifecycle. Conceptually, the data lifecycle is easy to describe (see Figure 2). It includes processes that create or obtain data, those that move, transform, and store it and enable it to be maintained and shared, and those that use or apply it, as well as those that dispose of it. 10 Throughout its lifecycle, data may be cleansed, transformed, merged, enhanced, or aggregated. As data is used or enhanced, new data is often created, so the lifecycle has internal iterations that are not shown on the diagram. Data is rarely static. Managing data involves a set of interconnected processes aligned with the data lifecycle. The specifics of the data lifecycle within a given organization can be quite complicated, because data not only has a lifecycle, it also has lineage (i.e., a pathway along which it moves from its point of origin to its point of 10 See McGilvray (2008) and English (1999) for information on the product lifecycle and data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 31 usage, sometimes called the data chain ). Understanding the data lineage requires documenting the origin of data sets, as well as their movement and transformation through systems where they are accessed and used. Lifecycle and lineage intersect and can be understood in relation to each other. The better an organization understands the lifecycle and lineage of its data, the better able it will be to manage its data. The focus of data management on the data lifecycle has several important implications: • Creation and usage are the most critical points in the data lifecycle : Data management must be executed with an understanding of how data is produced or obtained, as well as how data is used. It costs money to produce data. Data is valuable only when it is consumed or applied. (See Chapters 5, 6, 8, 11, and 14.) Figure 2 Data Lifecycle Key Activities • Data Quality must be managed throughout the data lifecycle : Data Quality Management is central to data management. Low quality data represents cost and risk, rather than value. Organizations often find it challenging to manage the quality of data because, as described previously, data is often created as a by -product of operational processes and organizations often do not set explicit standards for quality. Because the quality of data can be impacted by a range of lifecycle events, quality must be planned f or as part of the data lifecycle (see Chapter 13 ). • Metadata Quality must be managed through the data lifecycle : Because Metadata is a form of data, and because organizations rely on it to manage other data, Metadata quality must be managed in the same way as the quality of other data (see Chapter 12 ). • Data Security must be managed throughout the data lifecycle: Data management also includes ensuring that data is secure and that risks associated with data are mitigated. Data that requires protection must be protected throughout its lifecycle, from creation to disposal (see Chapter 7 Data Security). PlanDesign & EnableCreate / Obtain Store / MaintainEnhance Use Dispose of Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "32 • DMBOK2 • Data Management efforts should focus on the most critical data : Organizations produce a lot of data, a large portion of which is never actually used. Trying to manage every piece of data is not possible. Lifecycle management requires focusing on an organization’s most critical data and minimizing data ROT (Data that is Redundant, Obsolete, Trivial) (Aiken , 2014). 2.5.10 Different Types of Data Managing data is made more complicated by the fact that there are different types of data that have different lifecycle management requirements. Any management system needs to classify the objects that are managed. Data can be classified by type of data (e.g., transactional data, Reference Data, Master Data, Metadata; alternatively category data, resource data, event data, detailed transaction data) or by content (e.g., data domains, subject areas) or by format or by the level of protection the data requires. Data can a lso be classified by how and where it is stored or accessed . (See Chapters 5 and 10.) Because different types of data have different requirements, are associated with different risks, and play different roles within an organization, many of the tools of data management are focused on aspects of classification and control (Bryce, 2005). For example, Master Data has different uses and consequently different management requirements than does transactional data. (See Chapters 9, 10, 12, and 14.) 2.5.11 Data and Risk Data not only represents value, it also represents risk. Low quality data (inaccurate, incomplete, or out-of -date) obviously represents risk because its information is not right. But data is also risky because it can be misunderstood and misused. Organizations get the most value from the highest quality data – available, relevant, complete, accurate, consistent, timely, usable, meaningful, and understood. Yet, for many important decisions, we have information gaps – the difference between what we know and what we need to know to make an effective decision. Information gaps represent enterprise liabilities with potentially profound impacts on operational effectiveness and profitability. Organizations that recognize the value of high quality data can take concrete, proactive steps to improve the quality and usability of data and information within regulatory and ethical cultural frameworks. The increased role of information as an organizational asset across all sectors has led to an increased focus by regulators and legislators on the potential uses and abuses of information. From Sarbanes-Oxley (focusing on controls over accuracy and validity of financial transaction data from transaction to balance sheet) to Solvency II (focusing on data lineage and quality of data underpinning risk models and capital adequacy in the insurance sector), to the rapid growth in the last decade of data privacy regulations (covering the processing of data about people across a wide range of i ndustries and jurisdictions), it is clear that, while we are still waiting for Accounting to put Information on the balance sheet as an asset, the regulatory environment increasingly expects to see it on the risk register, with appropriate mitigations and controls being applied. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 33 Likewise, as consumers become more aware of how their data is used, they expect not only smoother and more efficient operation of processes, but also protection of their information and respect for their privacy. This means the scope of who our strategic s takeholders are as data management professionals can often be broader than might have traditionally been the case . (See Chapters 2 , Data Handl ing Ethics , and 7 , Data Security.) Increasingly, the balance sheet impact of information management, unfortunately, all too often arises when these risks are not managed and shareholders vote with their share portfolios, regulators impose fines or restrictions on operations, and customers v ote with their wallets. 2.5.12 Data Management and Technology As noted in the chapter introduction and elsewhere, data management activities are wide -ranging and require both technical and business skills. Because almost all of today’s data is stored electronically, data management tactics are strongly influenced by technology. From its inception, the concept of data management has been deeply intertwined with management of technology. That legacy continues. In many organizations, there is ongoing tension between the drive to build new technology and the desire to have more reliable data – as if the two were opposed to each other instead of necessary to each other. Successful data management requires sound decisions about technology, but managing technology is not the same as managing data. Organizations need to understand the impact of technology on data, in order to prevent technological temptation from driving their decisions about data. Instead, data requirements aligned with business strategy should drive decisions about technology. 2.5.13 Effective Data Management Requires Leadership and Commitment The Leader’s Data Manifesto (2017) recognized that an “organization’s best opportunities for organic growth lie in data.” Although most organizations recognize their data as an asset, they are far from being data -driven. Many don’t know what data they have or what data is most critical to their business. They confuse data and information technology and mismanage both. They do not approach data strategically. And they underestimate the work involved with data management. These conditions add to the challenges of managing data and poin t to a factor critical to an organization’s potential for success: committed leadership and the involvement of everyone at all levels of the organization. 11 The challenges outlined here should drive this point home: Data management is neither easy nor simple. But , because few organizations do it well, it is a source of largely untapped opportunity. To become better at it requires vision, planning, and willingness to change. (See Chapters 15- 17.) Advocacy for the role of Chief Data Officer (CDO) stems from a recognition that managing data presents unique challenges and that successful data management must be business- driven, rather than IT -driven. A CDO can lead data management initiatives and enable an organization to leverage its data assets and gain competitive 11 The full text of The Leader’s Data Manifesto can be found at: http://bit.ly/2sQhcy7 . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "34 • DMBOK2 advantage from them. However, a CDO not only leads initiatives. He or she must also lead cultural change that enables an organization to have a more strategic approach to its data. 2.6 Data Management Strategy A strategy is a set of choices and decisions that together chart a high- level course of action to achieve high -level goals. In the game of chess, a strategy is a sequenced set of moves to win by checkmate or to survive by stalemate. A strategic plan is a high-level course of action to achieve high -level goals. A data strategy should include business plans to use information to competitive advantage and support enterprise goals. Data strategy must come from an understanding of the data needs inherent in the business strategy: what data the organization needs, how it will get the data, how it will manage it and ensure its reliability over time, and how it will utilize it. Typically, a data strategy requires a supporting Data Management program strategy – a plan for maintaining and improving the quality of data, data integrity, access, and security while mitigating known and implied risks. The strategy must also address known challenges related to da ta management. In many organizations, the data management strategy is owned and maintained by the CDO and enacted through a Data Governance team, supported by a Data Governance Council. Often, the CDO will draft an initial data strategy and data management strategy even before a Data Governance Council is formed, in order to gain senior management’s commitment to establishing data stewardship and governance. The components of a data management strategy should include: • A compelling vision for data management • A summary business case for data management, with selected examples • Guiding principles, values, and management perspectives • The mission and long -term directional goals of data management • Proposed measures of data management success • Short -term (12 -24 months) Data Management program objectives that are SMART (specific, measurable, actionable, realistic, time -bound) • Descriptions of data management roles and organizations, along with a summary of their responsibilities and decision rights • Descriptions of Data Management program components and initiatives • A prioritized program of work with scope boundaries • A draft implementation roadmap with projects and action items Deliverables from strategic planning for data management include: • A Data Management Charter : Overall vision, business case, goals, guiding principles, measures of success, critical success factors, recognized risks, operating model, etc. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 35 • A Data Management Scope Statement : Goals and objectives for some planning horizon (usually three years) and the roles, organizations, and individual leaders accountable for achieving these objectives. • A Data Management Implementation Roadmap : Identifying specific programs, projects, task assignm ents, and delivery milestones (see Chapter 15 ). The data management strategy should address all DAMA Data Management Framework Knowledge Areas relevant to the organization. (See Figure 5 and Sections 3.3 and 4. ) 3. Data Management Frameworks Data management involves a set of interdependent functions, each with its own goals, activities, and responsibilities. Data management professionals need to account for the challenges inherent in trying to derive value from an abstract enterprise asset while balancing strategic and operational goals, specific business and technical requirements, risk and compliance demands, and conflicting understandings of what the data represents and whether it is of high quality. There is a lot to keep track of, which is why it helps to have a framework to understand the data management comprehensively and see relationships between its component pieces. Because the functions depend on one another and need to be aligned, in any orga nization, people responsible for the different aspects of data management need to collaborate if the organization is to derive value from its data. Frameworks developed at different levels of abstraction provide a range of perspectives on how to approach data management. These perspectives provide insight that can be used to clarify strategy, develop roadmaps, organize teams, and align functions. The ideas and concepts presented in the DMBOK2 will be applied differently across organizations. An organization’s approach to data management depends on key factors such as its industry, the range of data it uses, its culture, maturity level, strategy, vision, and the specific challenges it is addressing. The frameworks described in this section provide some lenses through which to see data management and apply concepts presented in the DMBOK. • The first two, the Strategic Alignment Model and the Amsterdam Information Model , show high-level relationships that influence how an organization manages data . • The DAMA DMBOK Framework (The DAMA Wheel, Hexagon, and Context Diagram) describes Data Management Knowledge Areas, as defined by DAMA, and explains their visual representation within the DMBOK. • The final two take the DAMA Wheel as a starting point and rearrange the pieces in order to better understand and describe the relationships between them . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "36 • DMBOK2 3.1 Strategic Alignment Model The Strategic Alignment Model (Henderson and Venkatraman , 1999) abstracts the fundamental drivers for any approach to data management. At its center is the relationship between data and information. Information is most often associated with business strategy and the operational use of data. Data is associated with information technology and processes which support physical management of systems that make data accessible for use. Surr ounding this concept are the four fundamental domains of strategic choice: business strategy, information technology strategy, organizational infrastructure and processes, and information technology infrastructure and processes. The fully articulated Strategic Alignment Model is more complex than is illustrated in Figure 3. Each of the corner hexagons has its own underlying dimensions. For example, within both Business and IT strategy, there is a need to account for scope, competencies, and governance. Operations must account for infrastructure, processes, and skills. The relationships between the pieces help an organization understand both the strategic fit of the different components and the functional integration of the pieces. Even the high -level depiction of the model is useful in understanding the organizational factor s that influence decisions about data and data management. Figure 3 Strategic Alignment Model12 3.2 The Amsterdam Information Model The Amsterdam Information Model, like the Strategic Alignment Model, takes a strategic perspective on business and IT alignment (Abcouwer, Maes, and Truijens, 1997) .13 Known as the 9 -cell, it recognizes a middle 12 Adapted by Henderson and Venkatraman . 13 See also: Business IT Alignment Blog, The Amsterdam Information Model (AIM) 9 -Cells (posted 2010- 12-08). https://businessitalignment.wordpress.com/tag/amsterdam -information -model/ Frameworks for IT Management , Chapter 13. Van Haren Publishing, 2006. http://bit.ly/2sq2Ow1 . StrategyOperationsBusinessIT Business StrategyIT Strategy Organization And ProcessInformation SystemsInformation Data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 37 layer that focuses on structure and tactics, including planning and architecture. Moreover, it recognizes the necessity of information communication (expressed as the information governance and Data Quality pillar in Figure 4). The creators of both the SAM and AIM frameworks describe in detail the relation between the components, from both a horizontal (Business / IT strategy) and vertical (Business Strategy / Business Operations) perspective. Figure 4 Amsterdam Information Model14 3.3 The DAMA -DMBOK Framework The DAMA -DMBOK Framework goes into more depth about the Knowledge Areas that make up the overall scope of data management. Three visuals depict DAMA’s Data Management Framework : • The DAMA Wheel ( Figure 5) • The Environmental Factors hexagon ( Figure 6) • The Knowledge Area Context Diagram ( Figure 7) The DAMA Wheel defines the Data Management Knowledge Areas. It places Data Governance at the center of data management activities, since governance is required for consistency within and balance between the functions. The other Knowledge Areas (Data Architecture, Data Modeling, etc.) are balanced around the 14 Adapted from Maas . Business Strategy & GovernanceInformation Strategy & GovernanceIT Strategy & Governance Organization & ProcessesInformation Architecture & PlanningIT Architecture & Planning Business ExecutionInformation Management & UseIT Services & ExploitationInformation Governance Data QualityArchitectureBusiness IT Strategy Tactics Operations Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "38 • DMBOK2 Wheel. They are all necessary parts of a mature data management function, but they may be implemented at different times, depending on the requirements of the organization. These Knowledge Areas are the focus of Chapters 3 – 13 of the DMBOK2. (See Figure 5.) The Environmental Factors hexagon shows the relationship between people, process, and technology and provides a key for reading the DMBOK context diagrams. It puts goals and principles at the center, since these provide guidance for how people should execute activities and effectively use the tools required for successful data management. (See Figure 6.) Figure 5 The DAMA-DM BOK2 Data Management Framework (The DAMA Wheel) Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance PEOPLE PEOPLEGoals & PrinciplesActivitiesRoles & Responsibilities Deliverables T echniques Organization & CultureT ools Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 39 Figure 6 DAMA Environmental Factors Hexagon The Knowledge Area Context Diagrams (See Figure 7) describe the detail s of the Knowledge Areas, including detail s related to people, processes , and technology. They are based on the concept of a SIPOC diagram used for product management (Suppliers, Inputs, Processes, Outputs, and Consumers). Context Diagrams put activities at the center, since they produce the deliverables that meet the requireme nts of stakeholders. Each context diagram begins with the Knowledge Area’s definition and goals. Activities that drive the goals (center) are classified into four phases: Plan (P), Control (C), Develop (D), and Operate (O). On the left side (flowing into the activities) are the Inputs and Suppliers. On the right side (flowing out of the activities) are Deliverables and Consumers. Participants are listed below the Activities. On the bottom are Tools, Techniques, and Metrics that influence aspects of the Knowledge Area. Organization & Culture is an important recurring theme, and more explicitly covered in the chapters of Data Governance, Data Management Maturity Assessment, Data Management Organization and Role Expectations and Data Management and Organizational Change Management. To emphasize guidance in making Data Mana gement success measurable, the Context Diagram ( Figure 7) has instead added the Metrics dimension. Lists in the context diagram are illustrative, not exhaustive. Items will apply differently to different organizations. The high -level role lists include only the most important roles. Each organization can adapt this pattern to address its own needs. Consumers : •Role 1 •Role 2Participants : •Role 1 •Role 2Suppliers : •Supplier 1 •Supplier 2Definition : High -level description of the knowledge area Goals : Purposes of the Knowledge Area 1.Goal 1 2.Goal 2 Activities : 1.Planning Activity / Activity Group (P) 1.Sub activity 2.Sub activity 2.Control Activity / Activity Group (C) 3.Development Activity / Activity Group (D) 4.Operational Activity / Activity Group (O)Inputs : •Input 1 •Input 2 •Input 3 Inputs are generally outputs from other Knowledge AreasDeliverables : •Deliverable 1 •Deliverable 2 •Deliverable 3 Deliverables are generally inputs to other Knowledge Areas T echniques : • Methods and procedures to execute activitiesT ools : • Software package types to support activitiesMetrics : • Measurable results of the process (P) Planning, (C) Control, (D) Development, (O) OperationsGENERIC CONTEXT DIAGRAM Business Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "40 • DMBOK2 Figure 7 Knowledge Area Context Diagram The component pieces of the context diagram include: Definition: This section concisely defines the Knowledge Area. Goals describe the purpose of the Knowledge Area and the fundamental principles that guide the performance of activities within each Knowledge Area. Activities are the actions and tasks required to meet the goals of the Knowledge Area. Some activities are described in terms of sub -activities, tasks, and steps. Activities are classified into four phases: Plan, Control, Develop, and Operate. (P) Planning Activities set the strategic and tactical course for meeting data management goals. Planning activities occur on a recurring basis. (C) Control Activities ensure the ongoing quality of data and the integrity, reliability, and security of systems through which data is accessed and used (D) Development Activities are organized around the system development lifecycle (SDLC) (analysis, design, build, test, preparation, and deployment). (O) Operational Activities support the use, maintenance, and enhancement of systems and processes through which data is accessed and used. Inputs are the tangible things that each Knowledge Area requires to initiate its activities. Many activities require the same inputs. For example, many require knowledge of the Business Strategy as input. Deliverables are the outputs of the activities within the Knowledge Area, the tangible things that each function is responsible for producing. Deliverables may be ends in themselves or inputs into other activities. Several primary deliverables are created by multiple functions. Roles and Responsibilities describe how individuals and teams contribute to activities within the Knowledge Area. Roles are described conceptually, with a focus on groups of roles required in most organizations. Roles for individuals are defined in terms of skills and qualification requirements. Skills Framework for the Information Age (SFIA) was used to help align role titles. Many roles will be cross- functional. 15 (See Chapter 16). Suppliers are the people responsible for providing or enabling access to inputs for the activities. Participants are the people who perform, manage the performance of, or approve the activities in the Knowledge Area. Consumers are those who directly benefit from the primary deliverables created by the data management activities. 15 http://bit.ly/2sTusD0 . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 41 Tools are the applications and other technologies that enable the goals of the Knowledge Area.16 Techniques are the methods and procedures used to perform activities and produce deliverables within a Knowledge Area. Techniques include common conventions, best practice recommendations, standards and protocols, and, where applicable, emerging alternative approach es. Metrics are standards for measurement or evaluation of performance, progress, quality, efficiency, or other effect. The metrics sections identify measurable facets of the work that is done within each Knowledge Area. Metrics may also measure more abstract charact eristics, like improvement or value. While the DAMA Wheel presents the set of Knowledge Areas at a high level, the Hexagon recognizes components of the structure of Knowledge Areas, and the Context Diagrams present the detail within each Knowledge Area. None of the pieces of the existing DAMA Data Management framework describe the relationship between the different Knowledge Areas. Efforts to address that question have resulted in reformulations of the DAMA Framework, which are described in the next two sections. 3.4 DMBOK Pyramid (Aiken) If asked, many organizations would say that they want to get the most out of their data – striving for that golden pyramid of advanced practices (data mining, analytics, etc.). But that pyramid is only the top of a larger structure, a pinnacle on a foundation. Most organizations do not have the luxury of defining a data management strategy before they start having to manage data. Instead, they build toward that capability, most times under less than optimal conditions. Peter Aiken’s framework uses the DMBOK functional areas to describe the situation in which many organizations find themselves. An organization can use it to define a way forward to a state where they have reliable data and processes to support strategic business goals. In trying to reach this goal, many organizations go through a similar logical progression of steps (See Figure 8): • Phase 1 : The organization purchases an application that includes database capabilities. This means the organization has a starting point for data modeling / design, data storage, and data security (e.g., let some people in and keep others out). To get the system functioning within their environment and with their data requires work on integration and interoperability. • Phase 2 : Once they start using the application, they will find challenges with the quality of their data. But getting to higher quality data depends on reliable Metadata and consistent Data Architecture. These provide clarity on how data from different systems wo rks together. • Phase 3 : Disciplined practices for managing Data Quality, Metadata, and architecture require Data Governance that provides structural support for data management activities. Data Governance also enables the execution of strategic initiatives, such as Document and Content Management, Reference 16 DAMA International does not endorse specific tools or vendors. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "42 • DMBOK2 Data Management, Master Data Management, Data Warehousing, and Business Intelligence, fully enabling advanced practices within the golden pyramid. • Phase 4 : The organization leverages the benefits of well -managed data and advances its analytic capabilities. Figure 8 Purchased or Built Database Capability17 Aiken’s pyramid draws from the DAMA Wheel , but also informs it by showing the relation between the Knowledge Areas. They are not all interchangeable; they have various kinds of interdependencies. The Pyramid framework has two drivers. First, the idea of building on a foundation, using components that need to be in the right places to support each other. Second, the somewhat contradictory idea that these may be put in place in an arbitrary order. 3.5 DAMA Data Management Framework Evolved Aiken’s pyramid describes how organizations evolve toward better data management practices. Another way to look at the DAMA Knowledge Areas is to explore the dependencies between them. Developed by Sue Geuens, the framework in Figure 9 recognizes that Business Intelligence and Analytic functions have dependencies on all other data management functions. They depend directly on Master Data and data warehouse solutions. But those, in turn, are dependent on feeding systems and applications. Reliable Data Quality, data design, and data interoperability practices are at the foundation of reliable systems and applications. In addition, Data Governance , which within this model includes Metadata Management, Data 17 Golden Pyramid figure copyright Data BluePrint, used with permission. Phase 1 Phase 2Phase 3Phase 4 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 43 Security, Data Architecture , and Reference Data Management, provides a foundation on which all other functions are dependent. Figure 9 DAMA Functional Area Dependencies A third alternative to DAMA Wheel is depicted in Figure 10. This also draws on architectural concepts to propose a set of relationships between the DAMA Knowledge Areas. It provides additional detail about the content of some Knowledge Areas in order to clarify these relationships. The framework starts with the guiding purpose of data management: To enable organizations to get value from their data assets as they do from other assets. Deriving value requires lifecycle management, so data management functions related to the data lifec ycle are depicted in the center of the diagram. These include planning a nd designing for reliable, high quality data; establishing processes and functions through which data can be enabled for use and also maintained; and, finally, using the data in various types of analysis and , through those processes, enhancing its value. The lifecycle management section depicts the data management design and operational functions (modeling, architecture, storage and operations, etc.) that are required to support traditional uses of data (Business Intelligence, document and content management). It also recognizes emerging data management functions (Big BUSINESS INTELLIGENCE / ANALYTICS MASTER DATA DATA WAREHOUSE SYSTEMS / APPLICATION DATA QUALITY DATA DESIGNDATA INTEGRATION & INTEROPERABILITY DATA GOVERNANCE METADATADATA SECURITYDATA ARCHITECTUREREFERENCE DATA DEPENDENCIES OUTPUTS Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "44 • DMBOK2 Data storage) that support emerging uses of data (Data Science, predictive analytics, etc.). In cases where data is truly managed as an asset, organizations may be able to get direct value from their data by selling it to other organizations (data monetiza tion). Figure 10 DAMA Data Management Function Framework Organizations that focus only on direct lifecycle functions will not get as much value from their data as those that support the data lifecycle through foundational and oversight activities. Foundational activities, like data OVERSIGHT : Data Governance LIFECYCLE MANAGEMENT FOUNDATIONAL ACTIVITIESDATA MANAGEMENT FUNCTIONS Strategy Culture ChangePolicy Stewardship Data Valuation Principles & Ethics PLAN & DESIGN Architecture Data Modeling & Design ENABLE & MAINTAIN Data Storage & Operations Data Integration & Interoperability Master Data ManagementData Warehousing Big Data Storage Reference Data Management USE & ENHANCE Business Intelligence Master Data Usage Document & Content ManagementData Science Data Monetization Predictive Analytics Data Risk Management : Security, Privacy, Compliance Metadata Management Data Quality Management Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 45 risk management, Metadata, and Data Quality management, span the data lifecycle. They enable better design decisions and make data easier to use. If these are executed well, data is less expensive to maintain, data consumers have more confidence in it, and the opportunities for using it expand. To successfully support data production and use and to ensure that foundational activities are executed with discipline, many organizations establish oversight in the form of Data Governance. A Data Governance function enables an organization to be data -driven, by putting in place the strategy and supporting principles, policies, and stewardship practices that ensure the organization recognizes and acts on opportunities to get value from its data. A Data Governance function should also engage in organizational change management activities to educate the organization and encourage behaviors that enable strategic uses of data. Thus, the necessity of culture change spans the breadth of Data Governance responsibilities, espe cially as an organization matures its data management practices. The DAMA Data Management Framework can also be depicted as an evolution of the DAMA Wheel, with core activities surrounded by lifecycle and usage activities, contained within the strictures of governance. ( See Figure 11.) Core activities, including Metadata Management, Data Quality Management, and data structure definition (architecture) , are at the center of the framework. Lifecycle management activities may be defined from a planning perspective (risk management, modeling, data design, Reference Data Management) and an enablement perspective (Master Data Management, data technology development, data integration and interoperability, data warehousing, and data storage and operations). Usages emerge from the lifecycle management activities: Master data usage, Document and content management, Business Intelligence, Data Science, predictive analytics, and data visualization. Many of these create more data by enhancing or developing insights about existing data. Opportunities for data monetization may be identified as uses of data. Data Governance activities provide oversight and containment, through strategy, principles, policy, and stewardship. They enable consistency through data classification and data valuation. The intention in presenting different visual depictions of the DAMA Data Management Framework is to provide additional perspective and to open discussion about how to apply the concepts presented in DMBOK. As the importance of data management grows, such frameworks become useful communication tools both within the data management community and between the data management community and our stakeholders. 4. DAMA and the DMBOK While data management presents many challenges, few of them are new. Since at least the 1980s, organizations have recognized that managing data is central to their success. As our ability and desire to create and exploit data has increased, so too has the need for reliable data management practices. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "46 • DMBOK2 Figure 11 DAMA Wheel Evolved DAMA was founded to address these challenges. The DMBOK, an accessible, authoritative reference book for data management professionals, supports DAMA’s mission by: • Providing a functional framework for the implementation of enterprise data management practices; including guiding principles, widely adopted practices, methods and techniques, functions, roles, deliverables , and metrics. • Establishing a common vocabulary for data management concepts and serving as the basis for best practices for data management professionals. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 47 • Serving as the fundamental reference guide for the CDMP (Certified Data Management Professional) and other certification exams. The DMBOK is structured around the eleven Knowledge Areas of the DAMA -DMBOK Data Management Framework (also known as the DAMA Wheel, see Figure 5). Chapters 3 – 13 are focused on Knowledge Areas. Each Knowledge Area chapter follows a common structure: 1. Introduction o Business Drivers o Goals and Principles o Essential Concepts 2. Activities 3. Tools 4. Techniques 5. Implementation Guidelines 6. Relation to Data Governance 7. Metrics Knowledge Areas describe the scope and context of sets of data management activities. Embedded in the Knowledge Areas are the fundamental goals and principles of data management. Because data moves horizontally within organizations, Knowledge Area activities intersect with each other and with other organizational functions. 1. Data Governance provides direction and oversight for data management by establishing a system of decision rights over data that accounts for the needs of the enterprise. (Chapter 3 ) 2. Data Architecture defines the blueprint for managing data assets by aligning with organizational strategy to establish strategic data requirements and designs to meet these requirements. (Chapter 4 ) 3. Data Modeling and Design is the process of discovering, analyzing, representing, and communicating data requirements in a precise form called the data model . (Chapter 5 ) 4. Data Storage and Operations includes the design, implementation, and support of stored data to maximize its value. Operations provide support throughout the data lifecycle from planning for to disposal of data. (Chapter 6 ) 5. Data Security ensures that data privacy and confidentiality are maintained, that data is not breached, and that data is accessed appropriately. (Chapter 7 ) 6. Data Integration and Interoperability includes processes related to the movement and consolidation of data within and between data stores, applications, and organizations. (Chapter 8) 7. Document and Content Management includes planning, implementation, and control activities used to manage the lifecycle of data and information found in a range of unstructured media, especially documents needed to support legal and regulatory compliance requirements. (Chapter 9 ) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "48 • DMBOK2 8. Reference and Master Data includes ongoing reconciliation and maintenance of core critical shared data to enable consistent use across systems of the most accurate, timely, and relevant version of the truth about essential business entities. (Chapter 10) 9. Data Warehousing and Business Intelligence includes the planning, implementation, and control processes to manage decision support data and to enable knowledge workers to get value from data via analysis and reporting. (Chapter 11) 10. Metadata includes planning, implementation, and control activities to enable access to high quality, integrated Metadata, including definitions, models, data flows, and other information critical to understanding data and the systems through which it is created, m aintained, and accessed. ( Chapter 12) 11. Data Quality includes the planning and implementation of quality management techniques to measure, assess, and improve the fitness of data for use within an organization. (Chapter 13) In addition to chapters on the Knowledge Areas, the DAMA -DMBOK contains chapters on the following topics: • Data Handling Ethics describes the central role that data ethics plays in making informed, socially responsible decisions about data and its uses. Awareness of the ethics of data collection, analysis, and use should guide all data management professionals. (Chapter 2) • Big Data and Data Science describes the technologies and business processes that emerge as our ability to collect and analyze large and diverse data sets increases. (Chapter 14. ) • Data Management Maturity Assessment outlines an approach to evaluating and improving an organization’s data management capabilities. (Chapter 15) • Data Management Organization and Role Expectations provide s best practices and considerations for organizing data management teams and enabling successful data management practices. (Chapter 16) • Data Management and Organizational Change Management describe s how to plan for and successfully move through the cultural changes that are necessary to embed effective data management practices within an organization. (Chapter 17) How a particular organization manages its data depends on its goals, size, resources, and complexity, as well as its perception of how data supports its overall strategy. Most enterprises do not perform all the activities described in each Knowledge Area. However, understanding the wider context of data management will enable organizations to make better decisions about where to focus as they work to impr ove practices within and across these related functions. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT • 49 5. Works Cited / Recommended Abcouwer, A. W., Maes, R., Truijens, J.: “Contouren van een generiek Model voor Informatienmanagement.” Primavera Working Pap er 97-07, 1997. http://bit.ly/2rV5dLx . Adelman, Sid, Larissa Moss, and Majid Abai. Data Strategy . Addison -Wesley Professional, 2005. Print. Aiken, Peter and Billings, Juanita. Monetizing Data Management. Technics Publishing, LLC, 2014. Print. Aiken, Peter and Harbour, Todd. Data Strategy and the Enterprise Data Executive. Technics Publishing, LLC. 2017. Print. APRA (Australian Prudential Regulation Authority). Prudential Practice Guide CPG 234, Management of Security Risk in Information and Information Technology . May 2013. http://bit.ly/2sAKe2y . APRA (Australian Prudential Regulation Authority). Prudential Practice Guide CPG 235, Managing Data Risk . September 2013. http://bit.ly/2sVIFil . Borek, Alexander et al. Total Information Risk Management: Maximizing the Value of Data and Information Assets . Morgan Kaufmann, 2013. Print. Brackett, Michael. Data Resource Design: Reality Beyond Illusion . Technics Publishing, LLC. 2014. Print. Bryce, Tim. Benefits of a Data Taxonomy . Blog 2005- 07-11. http://bit.ly/2sTeU1U . Chisholm, Malcolm and Roblyn -Lee, Diane. Definitions in Data Management: A Guide to Fundamental Semantic Metadata . Design Media, 2008. Print. Devlin, Barry. Business Unintelligence. Technics Publishing, LLC. 2013. Print. English, Larry. Improving Data Warehouse and Business Information Quality: Methods For Reducing Costs And Increasing Profits . John Wiley and Sons, 1999. Print. Evans, Nina and Price, James. “Barriers to the Effective Deployment of Information Assets: An Executive Management Perspectiv e.” Interdisciplinary Journal of Information, Knowledge, and Management Volume 7, 2012. Accessed from http://bit.ly/2sVwvG4 . Fisher, Tony. The Data Asset: How Smart Companies Govern Their Data for Business Success . Wiley, 2009. Print. Wiley and SAS Business Ser. Henderson, J.C., H Venkatraman, H. “Leveraging information technology for transforming Organizations.” IBM System Journal . Volume 38, Issue 2.3, 1999. [1993 Reprint] http://bit.ly/2sV86Ay and http://bit.ly/1uW8jMQ . Kent, William. Data and Reality: A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World . 3d ed. Technics Publications, LLC, 2012. Print. Kring, Kenneth L. Business Strategy Mapping - The Power of Knowing How it All Fits Together . Langdon Street Press (a division of Hillcrest Publishing Group, Inc.), 2009. Print. Loh, Steve. Data -ism: The Revolution Transforming Decision Making, Consumer Behavior, and Almost Everything Else . HarperBusiness, 2015. Print. Loshin, David. Enterprise Knowledge Management: The Data Quality Approach . Morgan Kaufmann, 2001. Print. Maes, R.: “A Generic Framework for Information Management.” PrimaVera Working Paper 99- 02, 1999. McGilvray, Danette. Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information. Morgan Kaufmann, 2008. Print. McKnight, William. Information Management: Strategies for Gaining a Competitive Advantage with Data. Morgan Kaufmann, 2013. Print. The Savvy Manager's Guides. Moody, Daniel and Walsh, Peter. “Measuring The Value Of Information: An Asset Valuation Approach.” European Conference on Information Systems (ECIS), 1999. http://bit.ly/29JucLO . Olson, Jack E. Data Quality: The Accuracy Dimension . Morgan Kaufmann, 2003. Print. Redman, Thomas. “Bad Data Costs U.S. $3 Trillion per Year.” Harvard Business Review . 22 September 2016. Web. Redman, Thomas. Data Driven: Profiting from Your Most Important Business Asset. Harvard Business Review Press. 2008. Print. Redman, Thomas. Data Quality: The Field Guide . Digital Press, 2001. Print. Reid, Roger, Gareth Fraser- King, and W. David Schwaderer. Data Lifecycles: Managing Data for Strategic Advantage . Wiley, 2007. Print. Rockley, Ann and Charles Cooper. Managing Enterprise Content: A Unified Content Strategy . 2nd ed. New Riders, 2012. Print. Voices That Matter. Sebastian -Coleman, Laura. Measuring Data Quality for Ongoing Improvement: A Data Quality Assessment Framework . Morgan Kaufmann, 2013. Print. The Morgan Kaufmann Series on Business Intelligence. Simsion, Graeme. Data Modeling: Theory and Practice. Technics Publications, LLC, 2007. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "50 • DMBOK2 Surdak, Christopher. Data Crush: How the Information Tidal Wave is Driving New Business Opportunities . AMACOM , 2014. Print. Waclawski, Janine. Organization Development: A Data -Driven Approach to Organizational Change . Pfeiffer, 2001. Print. White, Stephen. Show Me the Proof: Tools and Strategies to Make Data Work for the Common Core State Standards . 2nd ed. Advanced Learning Press, 2011. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "51 CHAPTER 2 Data Handling Ethics 1. Introduction efined simply, ethics are principles of behavior based on ideas of right and wrong. Ethical principles often focus on ideas such as fairness, respect, responsibility, integrity, quality, reliability, transparency, and trust. Data handling ethics are concerned with how to procure, store, manage, use, and dispose of data in ways that are aligned with ethical principles. Handling data in an ethical manner is necessary for the long -term success of any organization that wants to get value from its data. Unethical data handling can result in the loss of reputation and customers, because it puts at risk people whose data is exposed. In some cases, unethical practices are also illegal. 18 Ultimately, for data management professionals and the organizations for which they work, data ethics are a matter of social responsibility. The ethics of data handling are complex, but they center on several core concepts: • Impact on people : Because data represents characteristics of individuals and is used to make decisions that affect people’s lives, there is an imperative to manage its quality and reliability. • Potential for misuse : Misusing data can negatively affect people and organizations, so there is an ethical imperative to prevent the misuse of data. • Economic value of data : Data has economic value. Ethics of data ownership should determine how that value can be accessed and by whom. Organizations protect data based largely on laws and regulatory requirements. Nevertheless, because data represents people (customers, employees, patients, vendors, etc.), data management professionals should recognize that there are ethical (as well as legal) reasons to protect data and ensure it is not misused. Even data that does not directly represent individuals can still be used to make decisions that affect people’s lives. There is an ethical imperative not only to protect data, but also to manage its quality. People making decisions, as well as those impacted by decisions, expect data to be complete and accurate. From both a business and a 18 HIPAA (Health Insurance Portability and Accountability Act) in the US, PIPEDA (Personal Information Protection and Electronic Documents Act) in Canada, the EU General Data Protection Regulation (GDPR) and other data protection / information privacy laws describe obligations toward the handling of personal identifying data (e.g., name, addresses, religious affiliation, or sexual orientation) and privacy (access or restriction to this information) . D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "52 • DMBOK2 technical perspective, data management professionals have an ethical responsibility to manage data in a way that reduces the risk that it may be misrepresented , misused, or misunderstood. This responsibility extends across the data lifecycle, from creation to destruction of data. Figure 12 Context Diagram: Data Handling Ethics Definition : Data handling ethics are concerned with how to procure, store, manage, interpret, analyze / apply and dispose of data in ways that are aligned with ethical principles, including potential impacts on people and organizations . Goals: 1. T o control risk by establishing acceptable practices for data handling. 2. T o change/instill preferred culture and behaviors on handling data. 3. T o maintain alignment with compliant practices in ethical handling of data. Deliverables : •Current Practices and Gaps •Ethical Data Handling Strategy •Communication Plan •Ethics Training Program •Ethical Corporate Statements on Data •Awareness to Ethical Data Issues •Aligned Incentives, KPIs, and Targets •Updated Policies •Ethical Data Handling Reporting Suppliers : •Executive s •Data Steward s •Executive Data Stewards •IT Executive s •Data Provider s •Regulator sConsumers : •Employee s •Executive s •Regulator s •CustomersParticipants : •Data Governance Bodies •CDO / CIO •Executives •Coordinating Data Steward s •Subject Matter Experts •Change Managers •DM Services T echniques : • Communication Plan Checklists • Annual Ethics Statement Affirmations • Ethical Risk ModelT ools : • Wikis, Knowledge Bases, Intranet Sites • Microblogs, other internal communication toolsMetrics : • Number of Employees Trained • Compliance /non - compliance Incidents • Corporate Executive Involvement (P) Planning, (C) Control, (D) Development, (O) OperationsData Handling Ethics Activities : 1.Review Data -Handling Practices (P) 2.Identify Principles, Practices, and Risk Factors (P) 3.Create an Ethical Data Handling Strategy (P) 4.Identify and Remediate Gaps in Existing Practices (D) 5.Execute the Training and Communications Plan (D) 6.Implement Risk Mitigation (D) 7.Implement Monitoring Plan (C)Inputs : •Existing and Preferred Organization Ethics •Business Strategy & Goals •Organizational Structure •Business Culture •Regulations •Existing Corporate Policies T echnical DriversBusiness Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 53 Unfortunately, many organizations fail to recognize and respond to the ethical obligations inherent in data management. They may adopt a traditional technical perspective and profess not to understand the data; or they assume that if they follow the letter of the law, they have no risk related to data handling. This is a dangerous assumption. The data environment is evolving rapidly. Organizations are using data in ways they would not have imagined even a few years ago. While laws codify some ethical principles, legislation cannot keep up with the risks associated with the evolution of the data environment. Organizations must recognize and respond to their ethical obligation to protect data entrusted to them by fostering and sustaining a culture that values the ethical handling of information. 2. Business Drivers Like W. Edward Deming’s statements on quality, ethics means “doing it right when no one is looking.” An ethical approach to data use is increasingly being recognized as a competitive business advantage (Hasselbalch and Tranberg, 2016). Ethical data handling can increase the trustworthiness of an organization and the organization’s data and process outcomes. This can create better relationships between the organization and its stakeholders. Creating an ethical culture entails introducing proper governance, including the institution of controls to ensure that both intended and resulting outcomes of data processing are ethical and do not violate trust or infringe on human dignity. Data handling doesn’t happen in a vacuum, and customers and stakeholders expect ethical behavior and outcomes from businesses and their data processes. Reducing the risk that data for which the organization is responsible will be misused by employees, customers, or partners is a primary reason for an organization to cultivate ethical principles for data handling. There is also an ethical responsibility to secure data from criminals (i.e., to protect against hacking and potential data b reaches. See Chapter 7 .) Different models of data ownership influence the ethics of data handling. For example, technology has improved the ability of organizations to share data with each other. This ability means organizations need to make ethical decisions about their responsibility for sharing data that does not belong to them. The emerging roles of Chief Data Officer, Chief Risk Officer, Chief Privacy Officer, and Chief Analytics Officer are focused on controlling risk by establishing acceptable practices for data handling. But responsibility extends beyond people in these roles . Handling data ethically requires organization -wide recognition of the risks associated with misuse of data and organizational commitment to handling data based on principles that protect individuals and respect the imperatives related to data ownership. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "54 • DMBOK2 3. Essential Concepts 3.1 Ethical Principles for Data The accepted tenets of bioethics, which focus on preserving human dignity, provide a good general starting point for principles of data ethics. For example, the Belmont Principles for medical research may be adapted in Information Management disciplines (US -HSS, 1979) . • Respect for Persons : This principle reflects the fundamental ethical requirement that people be treated in a way that respects their dignity and autonomy as human individuals. It also requires that in cases where people have ‘diminished autonomy’, extra care be taken to protect their dignity and rights. When we consider data as an asset , do we keep in mind that data also affects, represents, or touches people? Personal data is different from other raw ‘assets’ , like oil or coal. Unethical use of personal data can directly influence people’s interactions, employment opportunities, and place in the community. Do we design information systems in a way that limits autonomy or freedom of choice? Have we considered ho w processing data may affect people with mental or physical disabilities? Have we accounted for how they will access and utilize data? Does data processing take place on the basis of informed, valid consent? • Beneficence : This principle has two elements: first, do not harm; second, maximize possible benefits and minimize possible harms. The ethical principle of ‘do not harm’ has a long history in medical ethics, but also has clear application in the context of data and information management. Ethical data and information practitioners should identify stakeholders , consider the outcomes of data processing , and work to maximize benefits and minimize the risk of harm caused by the processes designed. Is a process designed in a way that assumes a zero -sum outcome rather than a win -win situation? Is data processing unnecessarily invasive and is there a less risky way to meet the requirements of the business need? Is the data handling in question lacking transparency in a way that might hide possible harm to people? • Justice : This principle considers the fair and equitable treatment of people. Some questions that might be asked regarding this principle: Are people or groups of people being treated unequally under similar circumstances? Does the outcome of a process or algorithm result in effects that disproportionately benefit or harm a certain group of people? Is machine learning being trained using datasets that contain data inadvertently reinforcing cultural prejudices? The United States Department of Homeland Security’s Menlo Report adapts the Belmont Principles to Information and Communication Technology Research , adding a fourth principle: Respect for Law and Public Interest (US -DHS, 2012). In 2015, the European Data Protection Supervisor published an opinion on digital ethics highlighting the “engineering, philosophical, legal, and moral implications” of developments in data processing and Big Data. It Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 55 called for a focus on data processing that upholds human dignity and set out four pillars required for an information ecosystem that ensures the ethical treatment of data (EDPS, 2015): • Future -oriented regulation of data processing and respect for the rights to privacy and to data protection • Accountable controllers who determine personal information processing • Privacy-conscious engineering and design of data processing products and services • Empowered individuals These principles map broadly to the principle set out in the Belmont Report, focusing on promoting human dignity and autonomy. The EDPS states that privacy is a fundamental human right. It challenges innovators to see dignity, privacy, and autonomy as a pl atform on which a sustainable digital environment is shaped, rather than an obstacle to development, and calls for transparency and communication with stakeholders. Data Governance is a vital tool for ensuring these principles are considered in deciding who can do what with which data and under what circumstances processing is appropriate or necessary. The ethical impacts and risks of data processing on all stakeholders must be considered by practitioners, and managed in a similar manner to Data Quality . 3.2 Principles Behind Data Privacy Law Public policy and law try to codify right and wrong based on ethical principles. But they cannot codify every circumstance. For example, privacy laws in the European Union, Canada, and the United States show different approaches to codifying data ethics. These principles can a lso provide a framework for organizational policy. Privacy law is not new. Privacy and information privacy as concepts are firmly linked to the ethical imperative to respect human rights. In 1890, American legal scholars Samuel Warren and Louis Brandeis described privacy and information privacy as human rights with protections in common law that underpin several rights in the US constitution. In 1973, a code of Fair Information Practice was proposed, and the concept of information privacy as a fundamental right was reaffirmed in the US Privacy Act of 1974, which states that “the right to privacy is a personal and fundamental right protected by the Constitution of the United States”. In the wake of human rights violations during the Second World War, the European Convention of Human Rights (1950) established both the general right to privacy and the specific right to information privacy (or the right to protection of one’s personal data) as human rights which are fundamental to upholding the right to Human Dignity. In 1980, the Organization for Economic Co- operation and Development (OECD) established Guidelines and Principles for Fair Information Processing that became the basis for the European Union’s data protection laws. OECD’s eight core principles, the Fair Information Processing Standards, are intended to ensure that personal data is processed in a manner that respects individuals’ right to privacy. They include: limitations on data collection; an expectation that data will be of high quality; the requirement that when data is collected, it is done for a specific purpose; limitations on data usage; security safeguards; an expectation of openness and Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "56 • DMBOK2 transparency; the right of an individual to challenge the accuracy of data related to himself or herself; and accountability for organizations to follow the guidelines. The OECD principles have since been superseded by principles underlying the General Data Protection Regulation of the EU, (GDPR, 2016) . See Table 1. Table 1 GDPR Principles GDPR Principle Description of Principle Fairness, Lawfulness, Transparency Personal data shall be processed lawfully, fairly, and in a transparent manner in relation to the data subject. Purpose Limitation Personal data must be collected for specified, explicit, and legitimate purposes, and not processed in a manner that is incompatible with those purposes. Data Minimization Personal data must be adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed. Accuracy Personal data must be accurate, and where necessary, kept up -to-date. Every reasonable step must be taken to ensure that personal data that are inaccurate, having regard to the purpose for which they are processed, are erased or rectified without delay. Storage Limitation Data must be kept in a form that permits identification of data subjects [individuals] for no longer than is necessary for the purposes for which the personal data are processed. Integrity and Confidentiality Data must be processed in a manner that ensures appropriate security of the personal data, including protection against unauthorized or unlawful processing and against accidental loss, destruction or damage, using appropriate technical or organizational measures. Accountability Data Controllers shall be responsible for, and be able to demonstrate compliance with [these principles]. These principles are balanced by and support certain qualified rights individuals have to their data, including the rights to access, the rectification of inaccurate data, portability, the right to object to the processing of personal data that may cause damage or distress, and erasure. When processing of personal data is done based on consent, that consent must be an affirmative action that is freely given, specific, informed, and unambiguous. The GDPR requires effective governance and documentation to enable and demonstrate compliance and mandates Privacy by Design. Canadian privacy law combines a comprehensive regime of privacy protection with industry self -regulation. PIPEDA (Personal Information Protection and Electronic Documents Act) applies to every organization that collects, uses, and disseminates personal information in the cour se of commercial activities. It stipulates rules, with exceptions, that organizations must follow in their use of consumers’ personal information. Table 2 describes statutory obligations based on PIPEDA. 19 19 http://bit.ly/2tNM53c . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 57 In Canada, the Federal Privacy Commissioner has the sole responsibility for handling privacy complaints against organizations. However, they fill an ombudsman role; their decisions are only recommendations (not legally binding and with no precedential value, even within the commissioner’s office). Table 2 Canadian Privacy Statutory Obligations PIPEDA Principle Description of Principle Accountability An organization is responsible for personal information under its control and must designate an individual to be accountable for the organization's compliance with the principle. Identifying Purposes An organization must identify the purposes for which personal information is collected at or before the tim e the information is collected. Consent An organization must obtain the knowledge and consent of the individual for the collection, use, or disclosure of personal information, except where inappropriate. Limiting Collection, Use, Disclosure, and Retention The collection of personal information must be limited to that which is necessary for the purposes identified by the organization. Information shall be collected by fair and lawful means. Personal information shall not be used or disclosed for purposes oth er than those for which it was collected, except with the consent of the individual or as required by law. Personal information shall be retained only as long as necessary for the fulfillment of those purposes. Accuracy Personal information must be as accurate, complete, and up -to-date as is necessary for the purpo ses for which it is to be used. Safeguards Personal information must be protected by security safeguards appropriate to the sensitivity of the information. Openness An organization must make specific information about its policies and practices relating to the management of their personal information readily available to individuals. Individual Access Upon request, an individual shall be informed of the existence, use, and disclosure of his or her personal information, and shall be given access to that information. An individual shall be able to challenge the accuracy and completeness of the information and have it amended as appropriate. Compliance Challenges An individual shall be able to address a challenge concerning compliance with the above principles to the designated individual or individuals accountable for the organization's compliance. In March 2012, the US Federal Trade Commission (FTC) issued a report recommending organizations design and implement their own privacy programs based on best practices described in the report (i.e., Privacy by Design) (FTC 2012). The report reaffirms the FTC’s focus on Fair Information Processing Prin ciples ( see Table 3). Table 3 United States Privacy Program Criteria Principle Description of Principle Notice / Awareness Data collectors must disclose their information practices before collecting person al information from consumers. Choice / Consent Consumers must be given options with respect to whether and how personal information collected from them may be used for purposes beyond those for whic h the information was provided. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "58 • DMBOK2 Principle Description of Principle Access / Participation Consumers should be able to view and contest the accuracy and completenes s of data collected about them. Integrity / Security Data collectors must take reasonable steps to ensure that information collected from consumers is accurate an d secure from unauthorized use. Enforcement / Redress The use of a reliable mechanism to impose sanctions for noncompliance with these fair information practices. These principles are developed to embody the concepts in the OECD Fair Information Processing Guidelines, including emphasis on data minimization (reasonable collection limitation) and storage limitation (sound retention), accuracy, and the requirement that companies must provide reasonable security for consumer data. Other focuses for fair information practices include: • Simplified consumer choice to reduce the burden placed on consumers • The recommendation to maintain comprehensive data management procedure s throughout the information lifecycle • Do Not Track option • Requirements for affirmative express consent • Concerns regarding the data collection capabilities of large platform providers; transparency and clear privacy notices and policies • Individuals’ access to data • Educating consumers about data privacy practices • Privacy by Design There is a global trend towards increasing legislative protection of individuals’ information privacy, following the standards set by EU legislation. Laws around the world place different kinds of restrictions on the movement of data across international boundaries. Even within a multinational organization, there will be legal limits to sharing information globally. It is , therefore, important that organizations have policies and guidelines that enable staff to follow legal requirements as well as use data within the risk appetite of the organization. 3.3 Online Data in an Ethical Context There are now emerging dozens of initiatives and programs designed to create a codified set of principles to inform ethical behaviors online in the United States (Davis, 2012). Topics include: • Ownership of data : The rights to control one’s personal data in relation to social media sites and data brokers. Downstream aggregators of personal data can embed data into deep profiles that individuals are not aware of. • The Right to be Forgotten : To have information about an individual be erased from the web, particularly to adjust online reputation. This topic is part of data retention practices in general. • Identity : Having the right to expect one identity and a correct identity, and to opt for a private identity. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 59 • Freedom of speech online : Expressing one’s opinions versus bullying, terror inciting, ‘trolling,’ or insulting. 3.4 Risks of Unethical Data Handling Practices Most people who work with data know that it is possible to use data to misrepresent facts. The classic book How to Lie with Statistics by Darrell Huff (1954) describes a range of ways that data can be used to misrepresent facts while creating a veneer of factuality. Methods include judicious data selection, manipulation of scale, and omission of some data points. These approaches are sti ll at work today. One way to understand the implications of ethical handling of data is to examine practices that most people would agree are unethical. Ethical data handling entails a positive duty to handle data according to ethical principles such as trustworthiness. Ensuring data is trustworthy may include measuring is against Data Qu ality dimensions such as accuracy and timeliness. It also includes a base level of truthfulness and transparency – not using data to lie or mislead, and being transparent regarding the sources, uses, and intent behind an organization’s data handling. The following scenarios describe unethical data practices that violate these principles among others. 3.4.1 Timing It is possible to lie through omission or inclusion of certain data points in a report or activity based on timing . Equity market manipulation through ‘end of day’ stock trades can artificially raise a stock price at closing of the market giving an artificial view of the stock’s worth. This is called market timing and is illegal. Business Intelligence staff may be the first to notice anomalies. In fact, they are now seen as valuable players in the stock trading centers of the world , recreating trading patterns , looking for such problems as well as analyzing reports , and reviewing and monitoring rules and alerts. Ethical Business Intelligence staff may need to alert appropriate governance or management functions to such anomalies. 3.4.2 Misleading Visualizations Charts and graphs can be used to present data in a misleading manner. For instance, changing scale can make a trend line look better or worse. Leaving data points out, comparing two facts without clarifying their relationship, or ignoring accepted visual conventions (such as that the numbers in a pie chart representing percentages must add up to 100 and only 100), can also be used to trick people into interpreting visualizations in ways that are not supported by the data itself. 20 20 How To Statistics (Website). Misleading Graphs: Real Life Examples . 24 January 2014. http://bit.ly/1jRLgRH See also io9 (Website). The Most Useless and Misleading Infographics on the Internet. http://bit.ly/1YDgURl See http://bit.ly/2tNktve Google “misleading data visualization” for additional examples. For counter examples, i.e., visuals with an ethical base, see Tufte (2001). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "60 • DMBOK2 3.4.3 Unclear Definitions or Invalid Comparisons A US news outlet reported, based on 2011 US Census Bureau data, that 108.6 million people in the US were on welfare yet only 101.7 million people had full time jobs, making it seem that a disproportionate percentage of the overall population was on welfare. 21 Media Matters explained the discrepancy: The 108.6 million figure for the number of “people on welfare” comes from a Census Bureau’s account ... of participation in means -tested programs, which include “anyone residing in a household in which one or more people received benefits” in the fourth quarter of 2011, thus including individuals who did not themselves receive government benefits. On the other hand, the “people with a full time job” figure ... included only individuals who worked, not individuals residing in a household where at least one person works. 22 The ethical thing to do, in presenting information, is to provide context that informs its meaning, such as a clear, unambiguous definition of the population being measured and what it means to be “on welfare.” When required context is left out, the surfac e of the presentation may imply meaning that the data does not support. Whether this effect is gained through the intent to deceive or through simply clumsiness, it is an unethical use of data. It is also simply necessary, from an ethical perspective, not to misuse statistics. Statistical ‘smoothing’ of numbers over a period could completely change perception of the number. ‘Data mining snooping’ is a recently coined term for a phenomenon in data mining statistical investigations where exhaustive correlations are performed on a data set, essentially over training a statistical model. Because of the behavior of ‘statistical significance’, it is rea sonable to expect some statistically significant -looking results that are actually random results. The untrained can be misled. This is common in the financial and medical sectors (Jensen, 2000; ma.utexas.edu, 2012). 23 3.4.4 Bias Bias refers to an inclination of outlook. On the personal level, the term is associated with unreasoned judgments or prejudices. In statistics, bias refers to deviations from expected values. These are often introduced through systematic errors in sampling or data selection. 24 Bias can be introduced at different points in the data lifecycle: when data is collected or created, when it is selected for inclusion in analysis, through the methods by which it is analyzed, and in how the results of analysis are presented. The ethical principle of justice creates a positive duty to be aware of possible biases that might influence data collection, processing, analysis, or interpretation. This is particularly important in the case of large -scale data 21 As of 2015, the overall population of the US is estimated to be 321.4 million people. http://bit.ly/2iMlP58 22 http://mm4a.org/2spKToU The example also demonstrates misleading visuals, as on the bar graph, the 108.6 million bar was shown as approximately 5 times larger than the 101.7 million column. 23 See also numerous articles by W. Edwards Deming at: http://bit.ly/2tNnlZh 24 http://bit.ly/2lOzJqU Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 61 processing that might disproportionately affect groups of people that have been historically subjected to prejudice or unfair treatment. Using data without addressing the ways in which bias may be introduced can compound prejudice while reducing transparency in process, giving the resulting outcomes the veneer of impartiality or neutrality when they are not neutral. There are several types of bias : • Data Collection for pre -defined result: The analyst is pressured to collect data and produce results in order to reach a pre -defined conclusion, rather than as an effort to draw an objective conclusion. • Biased use of data collected: Data may be collected with limited bias, but an analyst is pressured to use it to confirm a pre -determined approach. Data may even be manipulated to this end (i.e., some data may be discarded if it does not confirm the approach). • Hunch and search : The analyst has a hunch and wants to satisfy that hunch, but uses only the data that confirms the hunch and does not account for other possibilities that the data may surface. • Biased sampling methodology : Sampling is often a necessary part of data collection. But bias can be introduced by the method used to select the sample set. It is virtually impossible for humans to sample without bias of some sort. To limit bias, use statistical tools to select sample s and establish adequate sample sizes. Awareness of bias in data sets used for training is particularly important. • Context and Culture : Biases are often culturally or contextually -based, so stepping outside that culture or context is required for a neutral look at the situation. Questions of bias are dependent on many factors, such as the type of data processing in question, the stakeholders involved, how data sets are populated, the business need being fulfilled, and the expected outcomes of the process. However, it is not always possible or even desirable to remove all bias. Business bias against poor customers (customers with whom no further business is sought) is a foundational piece to many scenarios built by business analysts; they are de- selected from samples, or ignored in the analysis. In such a case, analysts should document the cr iteria they used to define the population they are studying. In contrast, predictive algorithms determining ‘criminal risk’ of individuals or predictive policing sending resources to specific neighborhoods would have a much higher risk of violating ethical principles of justice or fairness, and should have greater precautions to ensure algorithmic transparency and accountability and to counter bias in data sets training any predictive algorithms. 25 3.4.5 Transforming and Integrating Data Data integration presents ethical challenges because data is changed as it moves from system to system. If data is not integrated with care, it presents risk for unethical or even illegal data handling. These ethical risks intersect with fundamental problems in data management, including: 25 For examples of machine learning bias see Brennan (2015) and the Ford Foundation and ProPublica websites. In addition to bias , there is the problem of opaqueness. As predictive algorithms of learning machines become more complex, it is difficult to track the logic and lineage of their decisions. See Lewis and Monett (2017). http://bit.ly/1Om41ap ; http://bit.ly/2oYmNRu . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "62 • DMBOK2 • Limited knowledge of data’s origin and lineage : If an organization does not know where data came from and how it has changed as it has moved between systems, then the organization cannot prove that the data represents what they claim it represents. • Data of poor quality : Organizations should have clear, measurable standards for Data Quality, and should measure their data to confirm that it meets standards for quality. Without this confirmation, an organization cannot vouch for the data , and data consumers may be at risk or put others at risk when they use the data. • Unreliable Metadata : Data consumers depend on reliable Metadata, including consistent definitions of individual data elements, documentation of data’s origin, and documentation of lineage (e.g., rules by which data is integrated). Without reliable Metadata, data may be misunderstood and potentially misused. In cases where data may move between organizations and especially where it may move across borders, Metadata should include tags that indicate its provenance, who owns it, and if it requires specific protection. • No documentation of data remediation history : Organizations should also have auditable information related to the ways data has been changed. Even if the intention of data remediation is to improve the quality of data, doing so may be illegal. Data remediation should always follow a formal, auditabl e change control process. 3.4.6 Obfuscation / Redaction of Data Obfuscating or redacting data is the practice of making information anonymous, or removing sensitive information. But obfuscation alone may not be sufficient to protect data if a downstream activity (analysis or combination with other datasets) can expose the data. This risk is presen t in the following instances: • Data aggregation : When aggregating data across some set of dimensions, and removing identifying data, a dataset can still serve an analytic purpose without concern for disclosing personal identifying information (PII). Aggregations into geographic areas are a common practice (see Chapters 7 and 14). • Data marking : Data marking is used to classify data sensitivity (secret, confidential, personal, etc.) and to control release to appropriate communities such as the public or vendors, or even vendors from certain countries or other community considerations. • Data masking : Data masking is a practice where only appropriate submitted data will unlock processes. Operators cannot see what the appropriate data might be; they simply type in responses given to them, and if those responses are correct, further activities are permitted. Business processes using data masking include outsourced call centers, or sub -contractors who should only have partial access to information. The use of extremely large data sets in Data Science analyses raises practical rather than merely theoretical concerns about the effectiveness of anonymization. Within large data sets, it is possible to combine data in Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 63 ways that enable individuals to be specifically identified, even if input data sets have been anonymized. The first concern when data lands in a data lake is to analyze it for sensitive data and apply accepted protection methods. These alone may not offer enough safeguard, however; this is why it is vital that organizations have strong governance and a commitment to ethical data handling. ( See Chapter 14.) 3.5 Establishing an Ethical Data Culture Establishing a culture of ethical data handling requires understanding existing practices, defining expected behaviors, codifying these in policies and a code of ethics, and providing training and oversight to enforce expected behaviors. As with other initiatives related to governing data and to changing culture, this process requires strong leadership. Ethical handling of data obviously includes following the law, but also influences how data is analyzed and interpreted and how it is leveraged internally and externally. An organizational culture that clearly values ethical behavior will not only have codes of conduct, but will ensure that clear communication and governance controls are in place to support employees with quer ies and proper escalation paths so that if employees become aware of unethical practices or ethical risk s, they can highlight the pro blem or stop the process without fear of retaliation. Improving an organization’s ethical behavior regarding data requires a formal Organizational Change Management (OCM) process. (S ee Chapter 17.) 3.5.1 Review Current State Data Handling Practices The first step to improvement is understanding the current state. The purpose of reviewing existing data handling practices is to understand the degree to which they are directly and explicitly connected to ethical and compliance drivers. This review should also identify how well employees understand the ethical implic ations of existing practices in building and preserving the trust of customers, partners, and other stakeholders. The deliverable from the review should document ethical principles that underlie the organization’s collection, use, and oversight of data, th roughout the data lifecycle, including data sharing activities. 3.5.2 Identify Principles, Practices, and Risk Factors The purpose of formalizing ethical practices around data handling is to reduce the risk that data might be misused and cause harm to customers, employees, vendors, other stakeholders, or the organization as a whole. An organization trying to improve its practices should be aware of general principles, such as the necessity of protecting the privacy of individuals, as well as industry- specific concerns, such as the need to protect financial or health-related information. An organization’s approach to data ethics must align with legal and regulatory compliance requirements. For example, organizations that operate globally need to have a broad knowledge of the ethical principles at the Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "64 • DMBOK2 foundation of the laws of the countries in which they operate, as well as specific knowledge of the agreements between countries. In addition, most organizations have specific risks, which may be related to their technology footprint, their rate of employee turnover, the means by which they collect customer data, or other factors. Principles should be aligned with risks (bad things that can happen if the principles are not adhered to) and practices (the right ways of doing things so that risks are avoided). Practices should be supported by controls, as illustrated in the following e xample: • Guiding principle : People have a right to privacy with respect to information about their health. Therefore, the personal health data of patients should not be accessed , except by people who are authorized to access it as part of caring for patients. • Risk : If there is wide access to the personal health data of patients, then information about individuals could become public knowledge, thereby jeopardizing their right to privacy. • Practice : Only nurses and doctors will be allowed to access the personal health data of patients and only for the purposes of providing care. • Control : There will be an annual review of all users of the systems that contain personal health information of patients to ensure that only those people who need to have access do have access. 3.5.3 Create an Ethical Data Handling Strategy and Roadmap After a review of the current state and the development of a set of principles, an organization can formalize a strategy to improve its data handling practices. This strategy must express both ethical principles and expected behavior related to data, expressed in values statements and a code of ethical behavior. The component pieces of such a strategy include: • Values statements: Values statements describe what the organization believes in. Examples might include truth, fairness, or justice. These statements provide a framework for ethical handling of data and decision -making. • Ethical data handling principles : Ethical data handling principles describe how an organization approaches challenges presented by data; for example, how to respect the right of individuals to privacy. Principles and expected behaviors can be summarized in a code of ethics and supported through an ethics policy. Socialization of the code and policy should be included in the training and communications plan. • Compliance framework : A compliance framework includes factors that drive organizational obligations. Ethical behaviors should enable the organization to meet compliance requirements. Compliance requirements are influenced by geographic and sector concerns. • Risk assessments: Risk assessments identify the likelihood and the implications of specific problems arising within the organization. These should be used to prioritize actions related to mitigation, including employee compliance with ethical principles. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 65 • Training and communications : Training should include review of the code of ethics. Employee s must sign off that they are familiar with the code and the implications of unethical handling of data. Training needs to be ongoing; for example, through a requirement for an annual ethics statement affirmation. Communications should reach all employees. • Roadmap : The roadmap should include a timeline with activities that can be approved by management. Activities will include execution of the training and communications plan, identification and remediation of gaps in existing practices, risk mitigation, and monitor ing plans. Develop detailed statements that reflect the target position of the organization on the appropriate handling of data. I nclude roles, responsibilities, processes, and references to experts for more information. The roadmap should cover al l applicable laws and cultural factors. • Approach to auditing and monitoring : Ethical ideas and the code of ethics can be reinforced through training. It is also advisable to monitor specific activities to ensure that they are being executed in compliance with ethical principles. 3.5.4 Adopt a Socially Responsible Ethical Risk Model Data professionals involved in Business Intelligence, analytics, and Data Science are often responsible for data that describes: • Who people are, including their countries of origin and their racial, ethnic, and religious characteristics • What people do, including political, social, and potentially criminal activities • Where people live, how much money they have, what they buy, who they talk with or text or send email to • How people are treated, including outcomes of analysis, such as scoring and preference tracking that will tag them as ultimately privil eged or not for future business This data can be misused and counteract the principles underlying data ethics: respect for persons, beneficence, and justice. Executing BI, analytics, and Data Science activities requires an ethical perspective that looks beyond the boundaries of the organization for which people work and accounts for implications to the wider community. An ethical perspective is necessary not on ly because data can easily be misused but also because organizations have a social responsibility not to do harm with their data. For example, an organization might set criteria for what it considers ‘bad’ customers in order to stop doing business with those individuals. But if that organization has a monopoly on an essential service in a particular geographic area, then some of thos e individuals may find themselves without that essential service and they will be in harm’s way because of the organization’s decision. Projects that use personal data should have a disciplined approach to the use of that data. See Figure 13. They should account for: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "66 • DMBOK2 • How they select their populations for study (arrow 1) • How data will be captured (arrow 2) • What activities analytics will focus on (arrow 3) • How the results will be made accessible (arrow 4) Within each area of consideration, they should address potential ethical risks, with a particular focus on possible negative effects on customers or citizens. A risk model can be used to determine whether to execute the project. It will also influence how to execute the project. For example, the data will be made anonymous, the private information removed from the file, the security on the files tightened or confirmed, and a review of the local and other applicable privacy law reviewed with legal. Dropping customers may not be permitted under law if the organization is a monopoly in a jurisdiction, and citizens have no other provider options , such as energy or water. Because data analytics projects are complex, people may not see the ethical challenges. Organizations need to actively identify potential risks. They also need to protect whistleblowers who do see risks and raise concerns. Automated monitoring is not sufficient protection from unethical activities. People – the analysts themselves – need to reflect on possible bias. Cultural norms and ethics in the workplace influence corporate behavior – learn and use the ethical risk model. DAMA International encourages data professionals to take a professional stand, and present the risk situation to business leaders who may not have recognized the implications of particular uses of data and these implications in their work. Figure 13 Ethical Risk Model for Sampling Projects Results •Privileges granted or denied •Further engagement or not •Relationship removal •Benefit or sanction •Trust or lack of trust •Biased treatmentIdentification •Demographic required •Selection method Behavior capture •Content required •Capture method •Activities •Sentiment •Location •Date/Time •Combination datasets •Legal and ethical review BI/Analytics/Data Science •Profiling Prospects •Actual and forecast activitiesEthical Risks in a Sampling Project1 2 34 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA HANDLING ETHICS • 67 3.6 Data Ethics and Governance Oversight for the appropriate handling of data falls under both Data Governance and legal counsel. Together , they are required to keep up -to-date on legal changes, and reduce the risk of ethical impropriety by ensuring employees are aware of their obligations. Data Governance must set standards and policies for and provide oversight of data handling practices. Employees must expect fair handling, protection from reporting possible breaches, and non-interference in their personal lives. Data Governance has a particular oversigh t requirement to review plans and decisions proposed by BI, analytics , and Data Science studies. DAMA International’s Certified Data Management Professional (CDMP) certification requires that data management professionals subscribe to a formal code of ethics, including an obligation to handle data ethically for the sake of society beyond the organization that employs them. 4. Works Cited / Recommended Blann, Andrew. Data Handling and Analysis . Oxford University Press, 2015. Print. Fundamentals of Biomedical Science. Council for Big Data, Ethics, and Society (website) http://bit.ly/2sYAGAq . Davis, Kord. Ethics of Big Data: Balancing Risk and Innovation. O'Reilly Media, 2012. Print. European Data Protection Supervisor (EDPS). Opinion 4/2015 “Towards a new digital ethics: Data, dignity and technology.” http://bit.ly/2sTFVlI . Federal Trade Commission, US (FTC). Federal Trade Commission Report Protecting Consumer Privacy in an Era of Rapid Change . March 2012. http://bit.ly/2rVgTxQ and http://bit.ly/1SHOpRB . GDPR REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) . Hasselbalch, Gry and Pernille Tranberg. Data Ethics: The New Competitive Advantage. Publishare. 2016 . Huff, Darrell. How to Lie with Statistics. Norton, 1954. Print. Jensen, David. “Data Snooping, Dredging and Fishing: The Dark Side of Data Mining A SIGKDD99 Panel Report.” SIGKDD Explorations . ACM SIGKDD, Vol. 1, Issue 2. January 2000. http://bit.ly/2tNThMK. Johnson, Deborah G. Computer Ethics . 4th ed. Pearson, 2009. Print. Kaunert, C. and S. Leonard, eds. European Security, Terrorism and Intelligence: Tackling New Security Challenges in Europe. Palgrave Macmillan, 2013. Print. Palgrave Studies in European Union Politics. Kim, Jae Kwan and Jun Shao. Statistical Methods for Handling Incomplete Data . Chapman and Hall/CRC, 2013. Chapman and Hall/CRC Texts in Statistical Science. Lake, Peter. A Guide to Handling Data Using Hadoop: An exploration of Hadoop, Hive, Pig, Sqoop and Flume . Peter Lake, 2015. Lewis, Colin and Dagmar Monett. AI and Machine Learning Black Boxes: The Need for Transparency and Accountability . KD Nuggets (website), April 2017. http://bit.ly/2q3jXLr . Lipschultz, Jeremy Harris. Social Media Communication: Concepts, Practices, Data, Law and Ethics . Routledge, 2014. Print. Mayfield, M.I. On Handling the Data . CreateSpace Independent Publishing Platform, 2015. Print. Mazurczyk, Wojciech et al. Information Hiding in Communication Networks: Fundamentals, Mechanisms, and Applications . Wiley -IEEE Press, 2016. Print. IEEE Press Series on Information and Communication Networks Security. Naes, T. and E. Risvik eds. Multivariate Analysis of Data in Sensory Science. Volume 16. Elsevier Science, 1996. Print. Data Handling in Science and Technology (Book 16). Olivieri, Alejandro C. et al, eds. Fundamentals and Analytical Applications of Multi -way Calibration . Volume 29. Elsevier, 2015. Print. Data Handling in Science and Technology (Book 29). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "68 • DMBOK2 ProPublica ( website ). “Machine Bias: Algorithmic injustice and the formulas that increasingly influence our lives.” May 2016 http://bit.ly/2oYmNRu . Provost, Foster and Tom Fawcett. Data Science for Business: What you need to know about data mining and data- analytic thinking. O'Reilly Media, 2013. Print. Quinn, Michael J. Ethics for the Information Age . 6th ed. Pearson, 2014. Print. Richards, Lyn. Handling Qualitative Data: A Practical Guide. 3 Pap/Psc ed. SAGE Publications Ltd, 2014. Print. Thomas, Liisa M. Thomas on Data Breach: A Practical Guide to Handling Data Breach Notifications Worldwide . LegalWorks, 2015. Print. Tufte, Edward R. The Visual Display of Quantitative Information . 2nd ed. Graphics Pr., 2001. Print. University of Texas at Austin, Department of Mathematics ( website ). Common Misteaks Mistakes in Using Statistics. http://bit.ly/2tsWthM . Web . US Department of Health and Human Services. The Belmont Report. 1979. http://bit.ly/2tNjb3u (US-HSS, 2012) . US Department of Homeland Security. “Applying Principles to Information and Communication Technology Research: A Companion to the Department of Homeland Security Menlo Report”. January 3, 2012. http://bit.ly/2rV2mSR (US-DHS, 1979) . Witten, Ian H., Eibe Frank and Mark A. Hall. Data Mining: Practical Machine Learning Tools and Techniques . 3rd ed. Morgan Kaufmann, 2011. Print. Morgan Kaufmann Series in Data Management Systems. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "69 CHAPTER 3 Data Governance 1. Introduction ata Governance (DG) is defined as the exercise of authority and control (planning, implementation, monitoring, and enforcement) over the management of data assets. All organizations make decisions about data, regardless of whether they have a formal Data Governance function. Those that establish a formal Data Governance function exercise authority and control with greater intentionality (Seiner, 2014). Such organizations are better able to increase the value they get from their data assets . Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "70 • DMBOK2 The Data Governance function guides all other data management functions. The purpose of Data Governance is to ensure that data is managed properly, according to policies and best practices (Ladley, 2012). While the driver of data management overall is to ensure an organization gets value out of its data, Data Govern ance focuses on how decisions are made about data and how people and processes are expected to behave in relation to data. The scope and focus of a particular Data Governance function will depend on organizational needs, but most include : • Strategy : Defining, communicating, and driving execution of Data Strategy and Data Governance Strategy • Policy : Setting and enforcing policies related to data and Metadata lifecycle management (creation, access, usage, security, quality, documentation, and purging) • Standards : Setting and enforcing Data Management standards • Oversight : Providing hands -on observation, audit, and correction in key data management areas like quality, privacy, security, and data management policies and procedures • Compliance : Ensuring the organization can meet data -related regulatory compliance requirements • Issue management : Identifying, defining, escalating, and resolving issues related to data security, data access, Data Quality , regulatory compliance, data ownership, policy, standards, terminology, or Data Governance procedures • Data management projects : Sponsoring efforts to improve data management practices • Data asset valuation : Setting standards and processes to consistently define the business value of data assets To accomplish these goals, a Data Governance function will develop policies and procedures, cultivate data stewardship practices at multiple levels within the organization, and engage in organizational change management efforts that actively communicate to the organization the benefits of improved Data Governance and the behaviors necessary to successfully manage data as an asset. For most organizations, adopting formal Data Governance requires the support of org anizational change management (s ee Chapter 17), as well as sponsorship from a C -level executive, such as the Chief Risk Officer, Chief Financial Officer, or Chief Data Officer. The ability to create and share data and information has transformed our personal and economic interactions. Dynamic market conditions and a heightened awareness of data as a competitive differentiator are causing organizations to realign data management r esponsibilities. This type of change is clear in the financial, e - commerce, government, and retail sectors. Organizations increasingly strive to become data -driven – proactively considering data requirements as part of strategy development, program plannin g, and technology implementation. However, doing so often entails significant cultural challenges. Moreover, because culture can derail any strategy, Data Governance efforts need to include a cultural change component – again, supported by strong leadershi p. To benefit from data as an asset, the organizational culture must learn to value data and data management activities. Even with the best data strategy, Data Governance and data management plans will not succeed unless the organization accepts and manages change. For many organizations, cultural change is a major challenge. One of the foundational tenets of change management is that organizational change requires Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 71 individual change (Hiatt and Creasey, 2012). When Data Governance and data management demand significant behavioral changes, formal change management is required for success. Figure 14 Context Diagram: Data Governance and Stewardship Definition : The exercise of authority, control, and shared decision -making (planning, monitoring, and enforcement) over the management of data assets. Goals : 1. Enable an organization to manage its data as an asset. 2. Define, approve, communicate, and implement principles, policies, procedures, metrics, tools, and responsibilities for data management. 3. Monitor and guide policy compliance, data usage, and management activities. Activities : 1. Define the Data Governance Strategy (P) 1. Develop Data Governance Strategy 2. Perform Readiness Assessment 3. Perform Discovery and Business Alignment 4. Develop Organizational Touchpoints 2. Define the Data Governance Organization (P) 1. Define the Data Governance Operating Framework 2. Develop Goals, Principles, and Policies 3. Underwrite Data Management Projects 4. Engage Change Management 5. Engage in Issue Management 6. Assess Regulatory Compliance Requirements 3. Implement Data Governance (O) 1. Sponsor Data Standards and Procedures 2. Develop a Business Glossary 3. Co-ordinate with Architecture Groups 4. Sponsor Data Asset Valuation 4. Embed Data Governance (C,O)Inputs : •Business Strategies & Goals •IT Strategies & Goals •Data Management and Data Strategies •Organization Policies & Standards •Business Culture Assessment •Data Maturity Assessment •IT Practices •Regulatory RequirementsDeliverables : •Data Governance Strategy •Data Strategy •Business / Data Governance Strategy Roadmap •Data Principles, Data Governance Policies, Processes •Operating Framework •Roadmap and Implementation Strategy •Operations Plan •Business Glossary •Satisfaction Survey •Data Governance Scorecard •Data Governance Website •Communications Plan •Data Valuation •Data Management Maturity Assessment Suppliers : •Business Executives •Data Stewards •Data Owners •Subject Matter Experts •Maturity Assessors •Regulators •Enterprise ArchitectsConsumers : •Data Governance Bodies •Project Managers •Compliance Team •DMCommunities of Interest •DM Team •Business Management •Architecture Groups •Partner OrganizationsParticipants : •Steering Committees •CIO •CDO / Chief Data Stewards •Executive Data Stewards •Coordinating Data Stewards •Business Data Stewards •Data Governance Bodies T echniques : •Concise Messaging •Contact List •LogoT ools : •Websites •Business Glossary T ools •Workflow T ools •Document Management T ools •Data Governance ScorecardsMetrics : •Compliance to regulatory and internal data policies. •Value •Effectiveness •Sustainability (P) Planning, (C) Control, (D) Development, (O) OperationsData Governance and Stewardship •Compliance Team •DM Executives •Change Managers •Enterprise Data Architects •Project Management Office •Governance Bodies •Audit •Data ProfessionalsBusiness Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "72 • DMBOK2 1.1 Business Drivers The most common driver for Data Governance is often regulatory compliance , especially for heavily regulated industries, such as financial services and healthcare. Responding to evolving legislation requires strict Data Governance processes. The explosion in advanced analytics and Data Science has created an additional driving force. While compliance or analytics may drive Data Governance, many organizations use other business needs, such as lack of trusted data assets or operational problems caused by unreliable data , to support their Data Governance function. A typical scenario: a company needs better customer data, it chooses to develop Customer Master Data Management (MDM), and then it realizes successful MDM requires Data Governance . Data Governance is not an end in itself. It needs to align directly with organizational strategy. The more clearly it helps solve organizational problems, the more likely people will change behaviors and adopt Data Governance practices. Drivers for Data Go vernance most often focus on reducing risks or improving processes. • Reducing Risk o General risk management : Oversight of the risks data poses to finances or reputation, including response to legal (E -Discovery) and regulatory issues. o Data security : Protection of data assets through controls for the availability, usability, integrity, consistency, auditability, and security of data. o Privacy : Control of private / confidential / Personal Identifying Information (PII) through policy and compliance monitoring. • Improving Processes o Regulatory compliance : The ability to respond efficiently and consistently to regulatory requirements. o Data Quality improvement : The ability to contribute to improved business performance by making data more reliable. o Metadata Management : Establishment of a business glossary to define and locate data in the organization , ensuring the wide range of other Metadata is managed and made available to the organization. o Efficiency in development projects : SDLC improvements to address issues and opportunities in data management across the organization, including management of data -specific technical debt through governance of the data lifecycle. o Vendor management : Control of contracts dealing with data, such as cloud storage, external data purchase, sales of data as a product, and outsourcing data operations. It is essential to clarify the particular business drivers for Data Governance within an organization and to align them with the overall business strategy. Focusing on the ‘Data Governance organization’ often alienates leadership who perceive extra overhead without apparent benefits. Sensitivity to organizational culture is necessary to determine the right language, operating model, and roles for the function. While people sometimes claim it is difficult to understand what Data Governance is, g overnance itself is a common concept. Rather than invent new approaches, data management professionals can apply the concepts and principles of other types of governance to the governance of data. A common analogy is to equate Data Governance to auditing and accountin g. Auditors and controllers set the rules for managing financial assets. Data Governance professionals set rules for managing data assets. Other areas carry out these rules. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 73 Data Governance is not a one -time thing. Governing data requires an ongoing program focused on ensuring that an organization gets value from its data and reduces risks related to data. A Data Governance team can be a virtual organization or a line organization with specific accountabilities. To be effective, the roles and activities within Data Governance need to be well understood. They should be built around an operating framework that functions well in the organization. A Data Governance function should take into account distinctive organizational and cultural issues and the specific data management challenges and opportunities within the organization . (See Chapters 1 and 16.) The Data Governance function is separate from the IT governance function. IT governance makes decisions about IT investments, the IT application portfolio, and the IT project portfolio – in other words, hardware, software, and overall technical architecture. IT governance aligns the IT strategies and investments with enterprise goals and stra tegies. The COBIT (Control Objectives for Information and Related Technology) framework provides standards for IT governance, but only a small portion of the COBIT framework addresses managing data and information. Some critical topics, such as Sarbanes -Oxley compliance (U.S.A.), span the concerns of corporate governance, IT governance, and Data Governance. In contrast, Data Governance focuses exclusively on the management of data assets and of data as an asset . 1.2 Goals and Principles The goal of Data Governance is to enable an organization to manage data as an asset. Data Governance provides the principles, policy, processes, framework, metrics, and oversight to manage data as an asset and to guide data management activities at all levels. To achieve this overall goal, a Data Governance function must be: • Sustainable : The Data Governance function needs to be ‘sticky’. Data Governance is not a project with a defined end; it is an enterprise function that requires organizational commitment. Data Governance necessitates changes in how data is managed and used. This does not always mean massive new organizations and upheaval. It does mean managing change in a way that is sustainable beyond the initial implementation of any Data Governance component. Sustainable Data Governance depends on business leadership, sponsorship, and ownership. • Embedded: Data Governance is not an add -on process. Data Governance activities need to be incorporated into development methods for software, use of data for analytics, management of Master Data, and risk management , among other activities . • Measured : Data Governance done well has positive financial impact, but demonstrating this impact requires understanding the starting point and planning for measurable improvement. Implementing a Data Governance function requires a commitment to change. The following principles, developed since the early 2000s, can help set a strong foundation for Data Governance . 26 26 The Data Governance Institute. http://bit.ly/1ef0tnb . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "74 • DMBOK2 • Leadership and strategy : Successful Data Governance starts with visionary and committed leadership. Data management activities are guided by a data strategy that is itself driven by the enterprise business strategy. • Business- driven: Data Governance is a business function , and, as such, must govern IT decisions related to data as much as it governs business interaction with data. • Shared responsibility : Across all Data Management Knowledge Areas, Data Governance is a shared responsibility between business data stewards and technical data management professionals. • Multi -layered : Data Governance occurs at both the enterprise and local levels and often at levels in between. • Framework -based : Because Data Governance activities require coordination across functional areas, the Data Governance function must establish an operating framework that defines accountabilities and interactions. • Principle -based : Guiding principles are the foundation of Data Governance activities, and especially of Data Governance policy. Often, organizations develop policy without formal principles – they are trying to solve particular problems. Principles can sometimes be reverse -engineered from policy. However, it is best to articulate a core set of principles and best practices as part of policy work. Reference to principles can mitigate potential resistance. Additional guiding principles will emerge over time w ithin an organization. Publish them in a shared internal environment along with other Data Governance artifacts. 1.3 Essential Concepts Just as an auditor controls financial processes but does not actually execute financial management, Data Governance ensures data is properly managed without directly executing data management (see Figure 15). Data Governance represents an inherent separation of duty between oversight and execution. Figure 15 Data Governance and Data Management Data, Information, And Content LifecyclesData Governance Ensuring data is managed Data Management Managing data to achieve goals Oversight Execution Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 75 1.3.1 Data -centric Organization A data- centric organization values data as an asset and manages data through all phases of its lifecycle, including project development and ongoing operations. To become data -centric, an organization must change the way it translates strategy into action. Data is no longer treated a s a by -product of process and applications. Ensuring data is of high quality is a goal of business processes. As organizations strive to make decisions based on insights gained from analytics, effective data management becomes a very high priority. People tend to conflate data, systems , and infrastructure . To become data -centric, organizations need to think differently and recognize that managing data is different from managing IT. This shift is not easy. Existing culture, with its internal politics, ambiguity about ownership, budgetary competition, and legacy systems, can be a huge obstacle to establishing an enterprise vision of Data Governance and data management. While each organization needs to evolve its own principles, those that seek to get more value from their data are likely to share the following: • Data should be managed as a corporate asset • Data management best practices should be incented across the organization • Enterprise data strategy must be directly aligned with overall business strategy • Data management processes should be continuously improved 1.3.2 Data Governance Organization The core word in governance is govern . Data Governance can be understood in terms of political governance . It includes legislative -like functions (defining policies, standards, and the Enterprise Data Architecture), judicial -like functions (issue management and escalation), and executive functions (protecting and serving, administrative responsibilities). To better manage risk, most organizations adopt a representative form of Data Governance so that all stakeholders can be heard. Each organization should adopt a governance model that supports its business strategy and is likely to succeed within its own cultural context. Organizations should also be prepared to evolve that model to meet new challenges. Models differ with respect to their organizational structure, level of formality, and approach to decision -making. Some models are centrally organized, while others are distributed. Data Governance organizations may also have multiple layers to address concerns at different levels within large and complex enterprises – local, divisional, and enterprise -wide. The work of governance is often divided among multiple committees, each with a purpose and level of oversight different from the others. Figure 16 represents a generic Data Governance model, with activities at different levels within the organization (vertical axis), as well as separation of governance responsibilities within organizational functions and between technical (IT) and business areas. Table 4 describes the typical bodies that might be established within a Data Governance operating framework. Note this is not an organization chart. The diagram explains how various bodies work together to carry out the Data Governance function . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "76 • DMBOK2 Figure 16 Data Governance Organization Parts Table 4 Typical Data Governance Committees / Bodies Data Governance Body Description Data Governance Steering Committee The primary and highest authority organization for Data Governance in an organization, responsible for oversight, support, and funding of Data Governance activities. Consists of a cross -functional group of senior executives. Typically releases funding for Data Governance and Data Governance -sponsored activities as recommended by the DGC and CDO. This committee may, in turn , have oversight from higher -level funding or initiative -based steering committees. Data Governance Council (DGC) Manages Data Governance initiatives (e.g., development of policies or metrics), issues, and escalations. Consists of executives from either the organization (in a centralized or federated model) or corresponding business unit (in a replicated model) . See Figure 17. Data Governance Office (DGO) Ongoing focus on enterprise -level data definitions and data management standards across all DAMA -DMBOK Knowledge Areas. Consists of coordinating roles that are labeled as data stewards or custodians, and data owners . Data Stewardship Teams Communities of interest focused on one or more specific subject -areas or projects, collaborating or consulting with project teams on data definitions and data management standards related to the focus. Consists of business and technical d ata stewards and data analysts. Local Data Governance Committee Large organizations may have divisional or departmental Data Governance councils working under the auspices of an Enterprise DGC. Smaller organizations should try to avoid such complexity. Legislative & Judicial View Do the right thingsExecutive View Do things right IT Organizations Data Management ExecutivesData Governance Steering Committee Data Governance Council (DGC)Program Manage - ment Program Steering Committees Project Management Office ProjectsChief Data Officer Chief Information Officer Data Governance Office (DGO) Chief Data Steward Executive Data Stewards Coordinating Data Stewards Data Analysts Data Owners Business Data Stewards or SMEsData Management Services (DMS) Data Architects Coordinating Data +Stewards Data Analysts T echnical Data Stewards or SMEsEnterpriseDivisions & Programs Subject Area Subject Area Subject Area Subject Area Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 77 1.3.3 Data Governance Operating Model Types In a centralized model, one Data Governance organization oversees all activities in all subject areas. In a replicated model, the same Data Governance operating model and standards are adopted by each business unit. In a federated model, one Data Governance organization coordinates with multiple Business Units to maintain consistent definitions and standards. (See Figure 17 and Chapter 16.) Figure 17 Enterprise Data Governance Operating Framework Examples27 1.3.4 Data Stewardship Data Stewardship is the most common label to describe accountability and responsibility for data and processes that ensure effective control and use of data assets. Stewardship can be formalized through job titles and 27 Adapted from Ladley (2012). Data Governance Region Region Region Region Subject Area FSubject Area ESubject Area DSubject Area CSubject Area BSubject Area A Data Governance Data Governance Data Governance Business Unit 1 Business Unit 2 Business Unit 3 Subject Area ESubject Area DSubject Area ASubject Area CSubject Area BSubject Area ASubject Area BSubject Area ESubject Area F Data Governance Subject Area ESubject Area DSubject Area ASubject Area CSubject Area BSubject Area ASubject Area BSubject Area ESubject Area FBusiness Unit 1 Business Unit 2 Business Unit 3Centralized Business Unit Replicated Federated Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "78 • DMBOK2 descriptions, or it can be a less formal function driven by people trying to help an organization get value from its data. Often terms like owner , custodian , or trustee are synonyms for those who carry out steward -like functions. The focus of stewardship activities will differ from organization to organization, depending on organizational strategy, culture, the problems an organization is trying to solve, its level of data management maturity, and the formality of its stewardship function . However, in most cases, data stewardship activities will focus on some, if not all, of the following: • Creating and managing core Metadata : Definition and management of business terminology, valid data values, and other critical Metadata. Stewards are often responsible for an organization’s Business Glossary, which becomes the ultimate source for business terms related to data. • Documenting rules and standards : Definition/documentation of business rules, data standards, and Data Quality rules. Expectations used to define high quality data are often formulated in terms of rules rooted in the business processes that create or consume data. Stewards help surface these rules in order to ensure that there is consensus about them within the or ganization and that they are used consistently. • Managing Data Quality issues: Stewards are often involved with the identification and resolution of data related issues or in facilitating the process of resolution. • Executing operational Data Governance activities : Stewards are responsible for ensuring that, day - to-day and project -by-project, Data Governance policies and initiatives are adhered to. They should influence decisions to ensure that data is managed in ways that support the overall goals of the organization. 1.3.5 Types of Data Stewards A steward is a person whose job it is to manage the property of another person. Data Stewards manage data assets on behalf of others and in the best interests of the organization (McGilvray, 2008). Data Stewards represent the interests of all stakeholders and must t ake an enterprise perspective to ensure enterprise data is of high quality and can be used effectively. Effective Data Stewards are accountable and responsible for Data Governance activities and dedicate a portion of their time to these activities. Depending on the complexity of the organization and the goals of its Data Governance function , formally appointed Data Stewards may be differentiated by their place within an organization, by the focus of their work, or by both. For example: • Chief Data Stewards may chair Data Governance bodies in lieu of the CDO or may act as a CDO in a virtual (committee -based) or distributed Data Governance organization. They may also be Executive Sponsors. • Executive Data Stewards are senior managers who serve on a Data Governance Council. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 79 • Enterprise Data Stewards have oversight of a data domain across business functions. • Business Data Stewards are business professionals, most often recognized subject matter experts, accountable for a subset of data. They work with stakeholders to define and control data. • A Data Owner is a business person who is accountable for decisions about data within their domain . • Technical Data Stewards are IT professionals operating within one of the Knowledge Areas, such as Data Integration Specialists, Database Administrators, Business Intelligence Specialists, Data Quality Analysts, or Metadata Administrators. • Coordinating Data Stewards lead and represent teams of business and technical Data Stewards in discussions across teams and with executive Data Stewards. Coordinating Data Stewards is particularly important in large organizations. The first edition of the DAMA -DMBOK stated that “the best Data Stewards are often found, not made” (DAMA, 2009). This assertion acknowledges that in most organizations, there are people who steward data, even in the absence of a formal Data Governance function . Such individuals are already involved in helping the organization reduce data- related risks and get more value from its data. Formalizing their stewardship accountabilities and responsibilities recognizes the work they are doing and enables them to be more successful and to contribute more. All of that said, Data Stewards can be ‘made’; people can be trained to be Data Stewards. And people who are already stewarding data can develop their skills and knowledge so that they become better at the work of stewardship (Plotkin, 2014). 1.3.6 Data Policies Data policies are directives that codify principles and management intent into fundamental rules governing the creation, acquisition, integrity, security, quality, and use of data and information. Data policies are enterprise-wide . They support data standards, as well as expected behaviors related to key aspects of data management and use. Data policies vary widely across organizations. Data policies describe the ‘what’ of Data Governance (what to do and what not to do), while standards and procedures describe ‘how’ to do Data Governance . There should be relatively few data policies, and they should be stated briefly and directly. 1.3.7 Data Asset Valuation Data asset valuation is the process of understanding and calculating the economic value of data to an organization. Because data and even Business Intelligence are abstract concepts, people have difficulty aligning them with economic impact. The key to understanding the value of an intangible asset (like data) is understanding how it is used and the value brought by its usage (Redman, 1996). Unlike many other assets (e.g., money, physical equipment), data sets are not interchangeable (fungible). One organization’s customer data differs from another ’s in important ways , not only the customers themselves but also the associated data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "80 • DMBOK2 (purchasing history, preferences, etc.) . How an organization gets value from customer data (i.e., what it learns about its customers from this data and how it applies what it learns) can be a competitive differentiator. Most phases of the data lifecycle involve costs (including acquiring, storing, administering, and disposing of data). Data only brings value when it is used. When used, data also creates costs related to risk management. So, value comes when the economic benefit of using data outweighs the costs of acquiring and storing it, as well as managing risk related to usage. Some other ways to measure value include: • Replacement cost : The replacement or recovery cost of data lost in a disaster or data breach, including the transactions, domains, catalogs, documents, and metrics within an organization. • Market value : The value as a business asset at the time of a merger or acquisition. • Identified opportunities : The value of income that can be gained from opportunities identified in the data (in Business Intelligence) by using the data for transactions or by selling the data. • Selling data : Some organizations package data as a product or sell insights gained from their data. • Risk cost : A valuation based on potential penalties, remediation costs, and litigation expenses, derived from legal or regulatory risk from: o The absence of data that is required to be present. o The presence of data that should not be present (e.g., unexpected data found during legal discovery; data that is required to be purged but has not been purged). o Data that is incorrect, causing damage to customers, company finances, and reputation in addition to the above costs. o Reduction in risk and risk cost is offset by the operational intervention costs to improve and certify data. To describe the concept of data asset value, one can translate Generally Accepted Accounting Principles into Generally Accepted Information Principles 28 (see Table 5). Table 5 Principles for Data Asset Accounting Principle Description Accountability Principle An organization must identify individuals who are ultimately accountable for data and content of all types. Asset Principle Data and content of all types are assets and have characteristics of other assets. They should be managed, secured, and accounted for as other material or financial assets. Audit Principle The accuracy of data and content is subject to periodi c audit by an independent body. 28 Adapted from Ladley (2010). See pp 108- 09, Generally Accepted Information Principles. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 81 Principle Description Due Diligence Principle If a risk is known, it must be reported. If a risk is possible, it must be confirmed. Data risks include risks related to p oor data management practices. Going Concern Principle Data and content are critical to successful, ongoing business operations and management (i.e., they are not viewed as temporary means to achieve results or merely as a business by -product). Level of Valuation Principle Value the data as an asset at a level that makes the most sense or is the easiest to measure. Liability Principle There is a financial liability connected to data or content based on regulatory and ethical misuse or mismanagement. Quality Principle The meaning, accuracy, and lifecycle of data and content can affect the financ ial status of the organization. Risk Principle There is risk associated with data and content. This risk must be formally recognized, either as a liability or through incurring costs to manag e and reduce the inherent risk. Value Principle There is value in data and content, based on the ways these are used to meet an organization’s objectives, their intrinsic marketability, and/or their contribution to the organization’s goodwill (balance sheet) valuation. The value of information reflects its contribution to the organization offset by the co st of maintenance and movement. 2. Activities 2.1 Define Data Governance for the Organization Data Governance efforts must support business strategy and goals. An organization’s business strategy and goals inform both the enterprise data strategy and how Data Governance and data management activities need to be operationalized in the organization. Data Governance enables shared responsibility for data -related decisions. Data Governance activities cross organizational and system boundaries in support of an integrated view of data. Successful Data Governance requires a clear understanding of what is being governed and who is being governed, as well as who is governing. Data Governance is most effective when it is an enterprise effort, rather than isolated to a particular functional area. Defining the scope of Data Governance in an enterprise usually entails defining what enterprise means. Data Governance , in turn, governs that defined enterprise. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "82 • DMBOK2 2.2 Perform Readiness Assessment Assessments that describe the current state of an organization’s information management capabilities, maturity, and effectiveness are crucial to planning a Data Governance function . Because they can be used to measure a function’s effectiveness, assessments are also valuable in managing and sustaining a Data Governance function . Typical assessments include: • Data management maturity : Understand what the organization does with data; measure its current data management capabilities and capacity. The focus is on the impressions business personnel have about how well the company manages data and uses data to its advantage, as well as on o bjective criteria, such as use of tools, levels of reporting, etc. (See Chapter 15.) • Capacity to change : Since Data Governance requires behavioral change, it is important to measure the capacity of the organization to change the behaviors required for adapting Data Governance . Secondarily, this activity will help identify potential resistance points. Often , Data Governance requires formal organizational change management. In assessing the capacity to change, the change management process will evaluate the existing organizational structure, perceptions of culture, and change management process itself (Hiatt and Creasey, 2012). (See Chapter 17.) • Collaborative readiness : This assessment characterizes the organization’s ability to collaborate in the management and use of data. Since stewardship , by definition , crosses functional areas, it is collaborative in nature. If an organization does not know how to collaborate, culture will be an obstacle to stewardship. Never assume an organization knows how to collaborate. When done in conjunction with change capacity, this assessment offers insight into the cultural capacity for implementing Data Governance . • Business alignment : Sometimes included with the change capacity, a business alignment assessment examines how well the organization aligns uses of data with business strategy. It is often surprising to discover how ad hoc data -related activities can be. 2.3 Perform Discovery and Business Alignment A Data Governance function must contribute to the organization by identifying and delivering on specific benefits (e.g., reduce fines paid to regulators). Discovery activity will identify and assess the effectiveness of existing policies and guidelines – what risks they address, what behaviors they encourage, and how well they have been implemented. Discovery can also identify opportunities for Data Governance to improve the usefulness of data and content. Business alignment attaches business benefits to Data Governance function elements. Data Quality (DQ) analysis is part of discovery. DQ assessment will provide insight into existing issues and obstacles, as well as the impact and risks associated with poor quality data. DQ assessment can identify business processes that are at risk if executed using poor quality d ata, as well as the financial and other benefits of creating a Data Quality program as part of Data Governance efforts. (See Chapter 13.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 83 Assessment of data management practices is another key aspect of the Data Governance discovery process. For example, this might mean identifying power users to create an initial list of potential agents for ongoing Data Governance activity. Derive a list of Data Governance requirements from the discovery and alignment activities. For example, if regulatory risks generate a financial concern to the business, then specify Data Governance activities that support risk management. These requirements will drive Data Governance strategy and tactics. 2.4 Develop Organizational Touch Points Part of alignment includes developing organizational touchpoints for Data Governance work. Figure 18 illustrates examples of touch points that support alignment and cohesiveness of an enterprise Data Governance and data management approach in areas outside the direct authority of the Data Governance function . Figure 18 CDO Organizational Touch Points • Procurement and Contracts : The Data Governance function works with Vendor/Partner Management or Procurement to develop and enforce standard contract language vis -à-vis data management contracts. These could include Data -as-a-Service (DaaS) and cloud -related procurements, other outsourcing arrangements, third -party development efforts, or content acquisition/ licensing deals, and possibly data -centric IT tools acquisitions and upgrades. • Budget and Funding : If the Data Governance function is not directly in control of all data acquisi tion- related budgets, then the office can be a focal point for preventing duplicate efforts and ensuring optimization of acquired data assets. • Regulatory Compliance : The Data Governance function understands and works within required local, national, and international regulatory environments, and how these impact the organization and their data management activities. Ongoing monitoring is performed to identify and track new and potential impacts and requirements. CDO Budget & FundingProcurement & Contracts Policies, Procedures, & StandardsDG Management and OversightRegulatory Compliance SDLC and Agile Control PointsEAI Asset OptimizationData Quality Programs Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "84 • DMBOK2 • SDLC / development framework : The Data Governance function identifies control points where enterprise policies, processes, and standards can be developed in the system or application development lifecycles. The touch points that the Data Governance function influences support the organization’s cohesiveness in managing its data, therefore, increasing its nimbleness to use its data. In essence, this is a vision of how Data Governance will be perceived by the organization. 2.5 Develop Data Governance Strategy A Data Governance strategy defines the scope and approach to governance efforts. Data Governance strategy should be defined comprehensively and articulated in relation to the overall business strategy, as well as to data management and IT strategies. It should be implemented iteratively as the pieces are developed and approved. The specific content will be tailored to each organization, but the deliverables include: • Charter : Identifies the business drivers, vision, mission, and principles for Data Governance, including readiness assessment, internal process discovery, and current issues or success criteria • Operating framework and accountabilities : Defines structure and responsibility for Data Governance activities • Implementation roadmap : Timeframes for the roll out of policies and directives, business glossary, architecture, asset valuation, standards and procedures, expected changes to business and technology processes, and deliverables to support auditing activities and regulatory compl iance • Plan for operational success : Describing a target state of sustainable Data Governance activities 2.6 Define the Data Governance Operating Framework While developing a basic definition of Data Governance is easy, creating an operating model that an organization will adopt can be difficult. Consider these areas when constructing an organization’s operating model: • Value of data to the organization : If an organization sells data, obviously Data Governance has a huge business impact. Organizations that use data as a crucial commodity (e.g., Facebook, Amazon) will need an operating model that reflects the role of data. For organizations where data is an operational lubricant, the form of Data Governance will be less intense. • Business model : Decentralized business vs. centralized, local vs. international, etc. , are factors that influence how business occurs and , therefore, how the Data Governance operating model is defined. Links with specific IT strategy, Data Architecture, and application integration functions should be reflected in the target operating framework design (per Figure 16). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 85 • Cultural factors : Such as acceptance of discipline and adaptability to change. Some organizations will resist the imposition of governance by policy and principle. Data Governance strategy will need to advocate for an operating model that fits with organizational culture, while still progressing change. • Impact of regulation : Highly regulated organizations will have a different mindset and operating model of Data Governance than those less regulated. There may be links to the Risk Management group or Legal as well. Layers of Data Governance are often part of the solution. This means determining where accountability should reside for stewardship activities, who owns the data, etc. The operating model also defines the interaction between the governance organization and the people responsible for data management projects or initiatives, the engagement of change management activities to introduce this new function , and the model for issue management resolution pathways through governance. Figure 19 shows an example of an operating framework. The example is illustrative. This kind of artifact must be customized to meet the needs of a specific organization. Figure 19 An E xample of an Operating Framework Executive Council (COO, CFO) Enterprise Data Governance Oversight Data Governance Council (DGC) Consumer Partner Product Data Warehouse ERP Analytics Vision & Strategy Oversight Escalation Resolution Program Oversight Policy Development Alignment Sequencing AccountabilityData Lifecycles ResponsibilityRotating chairs Policy Issues Data Governance Operations (DGO) Domain Data Governance ForumsStakeholdersProjects & Other InitiativesStewardship Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "86 • DMBOK2 2.7 Develop Goals, Principles, and Policies The d evelopment of goals, principles, and policies derived from the Data Governance Strategy will guide the organization into the desired future state. Goals, principles, and policies are typically drafted by data management professionals, business policy staff, or a combination of these under the auspices of Data Governance. Next, Data Stewards and management review and refine them. Then, the Data Governance Council (or similar body) conducts the final review, revision, and adoption. Policies may take different shapes, as in the following examples: • The Data Governance Office (DGO) will certify data for use by the organization. • Data owners will be approved by the Data Governance Office. • Data owners will designate Data Stewards from their business capability areas. The Data Stewards will have day -to-day responsibility for coordinating Data Governance activities. • Whenever possible, standardized reporting and/or dashboards/scorecards will be made available to serve the majority of business needs. • Authorized Users will be granted access to specific data for ad hoc /non -standard reporting. • All critical data will be evaluated on a regular basis to assess its accuracy, completeness, consistency, accessibility, uniqueness, compliance, and efficiency. Data policies must be effectively communicated, monitored, enforced, and periodically re-evaluated. The Data Governance Council may delegate this authority to the Data Stewardship Steering Committee. 2.8 Underwrite Data Management Projects Initiatives to improve data management capabilities provide enterprise -wide benefits. These usually require cross -functional sponsorship or visibility from the DGC. They can be hard to sell because they can be perceived as obstacles to ‘just getting things done’. The key to promoting them is to articulate the ways they improve efficiency and reduce risk. Organizations that want to get more value from their data need to prioritize the development or improvement of data management capabilities. The DGC helps define the business case and oversees the status and progress of data management improvement projects. The DGC coordinates its efforts with a Project Management Office (PMO), if o ne exists. Data management projects may be considered part of the overall IT project portfolio. The DGC may also coordinate data management improvement efforts with large programs with enterprise - wide scope. Implementing Enterprise Resource Planning (ERP), Customer or Citizen Relationship Management (CRM), or Master Data Management (MDM), are large projects with significant impacts on Data Management. These projects require DGC coordination as they reach out to many business units. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 87 Data management activity in other projects must be accommodated by the internal SDLC, service delivery management, other Information Technology Infrastructure Library (ITIL) components, and PMO processes .29 Every project with a significant data component (and almost every project has these) should capture data management requirements early in the SDLC (planning and design phases). These include architecture, regulatory compliance, system -of-record identification and analysis, and Data Quality inspection and remediation. There may also be data management support activities, including requirements verification testing using standard test workbenches . 2.9 Engage Change Management Organizational Change Management (OCM ) is the vehicle for bringing about change in an organization’s systems and processes. The Change Management Institute posits that organizational change management is more than just the ‘people side of projects’. It should be viewed as the approach the whole organization uses to manage change well. Organizations often manage the transitions of projects rather than the evo lution of the organization (Anderson and Ackerson, 2012). An organization that is mature in its management of change builds a clear organizational vision, actively leads and monitors change from the top, and designs and manages smaller change efforts. It a dapts change initiatives based on the feedback and collaboration of the whole organization (Change Management Institute, 2012). (See Chapter 17.) For many organizations, the formality and discipline inherent in Data Governance differ from existing practices. Adopting them requires that people change their behaviors and interactions. A formal OCM program, with the right executive sponsor, is critical to driving the behavioral changes required to sustain the Data Governance activities . Organizations should create a team responsible for: • Planning : Planning change management, including performing stakeholder analysis, gaining sponsorship, and establishing a communications approach to overcome resistance to change . • Training : Creating and executing training plans for Data Governance function s. • Influencing systems development: Engaging with the PMO to add Data Governance steps the SDLC. • Policy implementation : Communicating data policies and the organization’s commitment to data management activities. • Communications: Increasing awareness of the role and responsibilities of Data Stewards and other Data Governance professionals, as well as the objectives and expectations for data management projects . Communications are vital to the change management process. A change management program supporting formal Data Governance should focus communications on: 29 http://bit.ly/2spRr7e . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "88 • DMBOK2 • Promoting the value of data as an asset : Educate and inform employees about the role data plays in achieving organizational goals. • Monitoring and acting on feedback about Data Governance activities : In addition to sharing information, communications plans should elicit feedback that can guide both the Data Governance function and the change management process. Actively seeking and using input from stakeholders can build commitment to the function’s goals, while also identifying successes and opportunities for improvement. • Implementing data management training : Training at all levels of the organization increases awareness of data management best practices and processes. • Measuring the effects of change management on in five key areas:30 o Awareness of the need to change o Desire to participate and support the change o Knowledge about how to change o Ability to implement new skills and behaviors o Reinforcement to keep the change in place • Implementing new metrics and KPIs : Employee incentives should be realigned to support behaviors connected to data management best practices. Since enterprise Data Governance requires cross - functional cooperation, incentives should encourage cross-unit activities and collaboration. 2.10 Engage in Issue Management Issue management is the process of identifying, quantifying, prioritizing, and resolving Data Governance - related issues, including: • Authority : Questions regarding decision rights and procedures • Change management escalations : Issues arising from the change management process • Compliance : Issues with meeting compliance requirements • Conflicts : Conflicting policies, procedures, business rules, names, definitions, standards, architecture, data ownerships , and conflicting stakeholder interests in data and information • Conformance : Issue related to conformance to policies, standards, architecture, and procedures • Contracts : Negotiation and review of data sharing agreements, buying and selling data, and cloud storage • Data security and identity : Privacy and confidentiality issues, including breach investigations • Data Quality : Detection and resolution of Data Quality issues, including disasters or security breaches Many issues can be resolved locally in Data Stewardship teams. Issues requiring communication and / or escalation must be logged, and may be escalated to the Business Unit Data Governance , or higher to the DGC , 30 http://bit.ly/1qKvLyJ. See also Hiatt and Creasey (2012). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 89 as shown in Figure 20. A Data Governance Scorecard can be used to identify trends related to issues, such as where within the organization they occur, what their root causes are, etc. Issues that cannot be resolved by the DGC should be escalated to the Data Governance Steering Committee . Figure 20 Data Issue Escalation Path Data Governance requires control mechanisms and procedures for: • Identifying, capturing, logging, tracking, and updating issues • Assignment and tracking of action items • Documenting stakeholder viewpoints and resolution alternatives • Determining, documenting, and communicating issue resolutions • Facilitating objective, neutral discussions where all viewpoints are heard • Escalating issues to higher levels of authority Data issue management is very important. It builds credibility for the Data Governance team, has direct, positive effects on data consumers, and relieves the burden on production support teams. Solving issues also proves that data can be managed and its quality improved. Successful issue management requires control mechanisms that demonstrat e the work effort and impact of resolution. 2.11 Assess Regulatory Compliance Requirements Every enterprise is affected by governmental and industry regulations, including regulations that dictate how data and information are to be managed. Part of the Data Governance function is to monitor and ensure regulatory compliance . Regulatory compliance is often the initial reason for implementing Data Governance . Data Governance guides the implementation of adequate controls to monitor and document compliance with data- related regulations. Several global regulations have significant implications on data management practices. For example: • Accounting Standards : The Government Accounting Standards Board (GASB) and the Financial Accounting Standards Board (FASB) accounting standards also have significant implications on how information assets are managed (in the US). • BCBS 239 (Basel Committee on Banking Supervision) and Basel II refer to Principles for Effective Risk Data Aggregation and risk reporting, a wide ranging set of regulations for banks. Since 2006, financial institutions doing business in European Union countries are required to report standard information proving liqu idity. Data Governance Steering Committee Data Governance Council Business Unit Data Governance Data Stewardship T eams <5% <20% 80-85% of conflicts resolved at this levelStrategic Strategic Tactical and Operational Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "90 • DMBOK2 • CPG 235: The Australian Prudential Regulation Authority (APRA) provides oversight of banking and insurance entities. It publishes standards and guides to assist in meeting these standards. Among these is CGP 235, a standard for managing data risk. It focuses on addressing the sources of data risk and on managing data throughout its lifecycle. • PCI-DSS: The Payment Card Industry Data Security Standards (PCI -DSS). • Solvency II : European Union regulations, similar to Basel II, for the insurance industry. • Privacy laws : Local, sovereign, and international laws all apply. Data Governance organizations work with other business and technical leadership to evaluate the implications of regulations. The organization must determine, for example, • In what ways is a regulation relevant to the organization? • What constitutes compliance? What policies and procedures will be required to achieve compliance? • When is compliance required? How and when is compliance monitored? • Can the organization adopt industry standards to achieve compliance? • How is compliance demonstrated? • What is the risk of and penalty for non -compliance? • How is non-compliance identified and reported? How is non -compliance managed and rectified? Data Governance monitors the organization’s response to regulatory requirements or audit undertakings involving data and data practices (for example, certifying the quality of data in regulatory reporting). (See Chapter 6 .) 2.12 Implement Data Governance Data Governance cannot be implemented overnight. It requires planning – not only to account for organizational change, but also simply because it includes many complex activities that need to be coordinated. It is best to create an implementation roadmap that illustrates the timeframes for and relationship between different activities. For example, if the Data Governance function is focused on improving compliance, priorities may be driven by specific regulatory requirements. In a federated Data Governance organization, implementation in various lines of business can occur on different schedules, based on their level of engagement and maturity, as well as funding. Some Data Governance work is foundational. Other work depends on it. This work has an initial release and ongoing cultivation. Prioritized activities in the early stages include: • Defining Data Governance procedures required to meet high priority goals • Establishing a business glossary and documenting terminology and standards • Coordinating with Enterprise Architecture and Data Architecture to support better understanding of the data and the systems • Assigning financial value to data assets to enable better decision -making and to increase understanding of the role that data plays in organizational success Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 91 2.13 Sponsor Data Standards and Procedures A standard is defined as “something that is very good and that is used to make judgments about the quality of other things” or as “something set up and established by authority as a rule for the measure of quantity, weight, extent, value, or quality.”31 Standards help define quality because they provide a means of comparison. They also offer the potential to simplify processes. By adopting a standard, an organization makes a decision once and codifies it in a set of assertions (the standard). It does not need to make the same decision all over again for each project. Enforcing standards should promote consistent results from the processes using them. Unfortunately, creating or adopting standards is often a politicized process and these goals get lost. Most organizations are not well- practiced at developing or enforcing data or Data Governance standards . In some cases, they have not recognized the value of doing so and , therefore, have not taken the time to do so. Other times , they simply don’t know how to. Consequently, ‘standards’ vary widely within and across organizations, as do expectations for conformance. Data Governance standards should be mandatory. Data standards can take different forms depending on what they describe: assertions about how a field must be populated, rules governing the relationships between fields, detailed documentation of acceptable and unacceptable values, format, etc. They are u sually drafted by data management professionals. Data standards should be reviewed, approved , and adopted by the DGC, or a delegated workgroup, such as a Data Standards Steering Committee. The level of detail in data standards documentation depends, in part, on organizational culture. Keep in mind that documenting data standards presents an opportunity to capture details and knowledge that otherwise may be lost. Recreating or reverse engineering to access this knowledge is very expensive, compared to documenting it up front. Data standards must be effectively communicated, monitored, and periodically reviewed and updated. Most importantly, there must be a means to enforce them. Data can be measured against standards. Data management activities can be audited for standards compliance by the DGC or the Data Standards Steering Committee on a defined schedule or as part of SDLC approval processes. Data management procedures are the documented methods, techniques, and steps followed to accomplish specific activities that produce certain outcomes and supporting artifacts. Like policies and standards, procedures vary widely across organizations. As is the case with data standards, procedural documents capture organizational knowledge in an explicit form. Procedural documentation is usually drafted by data management professionals. Examples of concepts that can be standardized within the Data Management Knowledge Areas include: • Data Architecture : Enterprise data models, tool standards, and system naming conventions • Data Modeling and Design : Data model management procedures, data modeling naming conventions, definition standards, standard domains, and standard abbreviations 31 http://bit.ly/2sTfugb Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "92 • DMBOK2 • Data Storage and Operations : Tool standards, standards for database recovery and business continuity, database performance, data retention, and external data acquisition • Data Security: Data access security standards, monitoring and audit procedures, storage security standards, and training requirements • Data Integration : Standard methods and tools used for data integration and interoperability • Documents and Content: Content management standards and procedures, including use of enterprise taxonomies, support for legal discovery, document and email retention periods, electronic signatures, and report distribution approaches • Reference and Master Data : Reference Data Management control procedures, systems of data record, assertions establishing and mandating use, standards for entity resolution • Data Warehousing and Business Intelligence : Tool standard, processing standards and procedures, report and visualization formatting standards, standards for Big Data handling • Metadata : Standard business and technical Metadata to be captured, Metadata integration procedures and usage • Data Quality : Data Quality rules, standard measurement methodologies, data remediation standards, and procedures • Big Data and Data Science : Data source identification, authority, acquisition, system of record, sharing and refresh 2.14 Develop a Business Glossary Business Data Stewards are generally responsible for business glossary content. A glossary is necessary because people use words differently. It is particularly important to have clear definitions for data, because data represents things other than itself (Chisholm, 2010). In addition, many organizations develop their own internal vocabulary. A glossary is a means of sharing this vocabulary within the organization. Developing and documenting standard data definitions reduces ambiguity and improves communication. Definitions must be clear, rigorous in wording, and explain any exceptions, synonyms, or variants. Approvers of terminology should include representatives from core user groups. Data Architecture can ofte n supply draft definitions and type breakouts from subject area models. Business glossaries have the following objectives: • Enable common understanding of the core business concepts and terminology • Reduce the risk that data will be misused due to inconsistent understanding of the business concepts • Improve the alignment between technology assets (with their technical naming conventions) and the business organization • Maximize search capability and enable access to documented institutional knowledge Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 93 A business glossary is not merely a list of terms and definitions. Each term will also be associated with other valuable Metadata: synonyms, metrics, lineage, security and privacy classification, business rules, the steward responsible for the term, etc. 2.15 Coordinate with Architecture Groups The Data Governance Council is a stakeholder (possibly sponsoring and approving) for Data Architecture artifacts, such as a business- oriented enterprise data model . The DGC may appoint or interact with an Enterprise Data Architecture Steering Committee or Architecture Review Board (ARB) to oversee the program and its iterative projects. The enterprise data model should be developed and maintained jointly by data architects and Data Stewards working together in subject area teams. Depending on the organization, this work can be coordinated either by the Enterprise Data Architect or by the steward. As business requirements evolve, the Data Stewardship teams should propose changes and develop extensions to the enterprise data model. The enterprise data model should be reviewed, approved, and formally adopted by the Data Governance Council , and the Data Governance Council is a stakeholder (possibly sponsoring and approving) for Data Architecture artifacts, such as a business- oriented enterprise data model. This model must align with key business strategies, processes, organizations, and systems. Data strategy and Data Architecture are central to coordination between the ‘Doing things right’ and ‘Doing the right things’ when managing data assets. 2.16 Define Data Asset Valuation Method Data and information are assets because they have or can create value. Today’s accounting practices consider data an intangible asset, much like documentation, expert knowledge, trade secrets, and other intellectual property. That said, organizations find it c hallenging to put monetary value on data. The DGC should organize the effort and set standards for doing so. Some organizations start by estimating the value of business losses due to inadequate information. Information gaps – the difference between what information is needed and what is available – represent business liabilities. The cost of closing or preventing gaps can be used to estimate the business value of the missing data. From there, the organization can develop models to estimate the value of the information that does exist. Value estimates can be built into the data strategy that will justify business cases for root cause solutions to quality issues, as well as for other Data Governance initiatives. 2.17 Embed Data Governance One goal of the Data Governance organization is to embed a range of processes and behaviors related to managing data as an asset. The ongoing operation of Data Governance functions requires planning. The Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "94 • DMBOK2 operations plan contains the list of events required to implement and operate Data Governance activities. It outlines activities, timing, and techniques necessary to sustain success. Sustainability means acting to ensure that processes and funding are in place to enable the continued performance of the Data Governance organizational framework. Central to this requirement is that the organization accepts the governance of data; that the function is managed, its results are monitored and measured, and the obstacles that so often cause Data Governance function s to falter or fail are overcome. In order to deepen the organization’s understanding of Data Governance in general, its application locally, and to learn from each other, create a Data Governance Community of Interest . This is particularly useful in the first years of governance, and will likely taper off as the Data Governance operations become mature. 3. Tools and Techniques Data Governance is fundamentally about organizational behavior . This is not a problem that can be solved through technology. However, there are tools that support the overall process. For example, Data Governance requires ongoing communication. A Data Governance function should take advantage of existing communications channels to communicate key messages in a consistent manner and to keep stakeholders informed about policies, standards, and requirements. In addition, a Data Governance function must manage its own work and its own data effectively. Tools help not only with these tasks but also with the metrics that support them. Before choosing a tool for a specific function, like a business glossary solution, an organization should define its o verall Data Governance goals and requirements with an eye to building out a tool set. For example, some glossary solutions include additional components for policy and workflow management. If such additional functiona lity is desired, requirements should be clarified and tested before a tool is adopted. Otherwise, the organization will have multiple tools, none of which may meet its needs. 3.1 Online Presence / Websites The Data Governance function should have an online presence. It can make core documents available via a central website or a collaboration portal. Websites can house documentation libraries, give access to search capabilities, and help manage simple workflow. A website can also help establish a brand for the function through logos and a consistent visual representation. A Data Governance function website should include: • The Data Governance strategy and function charter, including vision, benefits, goals, principles, and implementation roadmap • Data policies and data standards • Descriptions of data stewardship roles and responsibilities • Program news announcements • Links to forums for a Data Governance Community of Interest Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 95 • Links to executive messages regarding Data Governance topics • Reports on Data Quality measurements • Procedures for issue identification and escalation • Links to request services or capture issues • Documents, presentations, and training programs with links to related online resources • Data Governance function contact information 3.2 Business Glossary A Business Glossary is a core Data Governance tool. It houses agreed -upon definitions of business terms and relates these to data. There are many business glossary tools available, some as part of larger ERP systems, data integration tools, or Metadata management tools, and some as standalone tools. 3.3 Workflow Tools Larger organizations may want to consider a robust workflow tool to manage processes, such as the implementation of new Data Governance policies. These tools connect processes to documents, and can be useful in policy administration and issue resolution. 3.4 Document Management Tools Very often, a document management tool is used by governance teams to assist in managing policies and procedures. 3.5 Data Governance Scorecards The collection of metrics to track Data Governance activities and compliance with policies can be reported up to the Data Governance Council and Data Governance Steering Committees on an automated scorecard . 4. Implementation Guidelines Once the Data Governance function is defined, an operating plan developed, and an implementation roadmap prepared with the support of information gathered in the data maturity assessment (s ee Chapter 15), the organization can begin to implement processes and policies. Most rollout strategies are incremental, either deploying the Data Governance function as part of a large effort, such as MDM, or by a region or division. Rarely is Data Governance deployed enterprise -wide as a first effort . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "96 • DMBOK2 4.1 Organization and Culture As noted in Section 2.9, the formality and discipline inherent in Data Governance will be new and different for many organizations. Data Governance adds value by bringing about changes in behavior. There may be resistance to change s. New methods of making decisions and governing projects may require training and pass through an adoption curve . Effective and long -lasting Data Governance functions require a cultural shift in organizational thinking and behavior about data. An ongoing change management program is required to support the new thinking and behaviors as well as the implementation of ne w policies and processes targeting to achieve the desired future state. No matter how precise or exotic the Data Governance strategy is, ignoring business culture will diminish the chances for Data Governance success. A f ocus on Change Management must be p art of the strategy. Change Management is key for sustainability. Sustainability is a quality of a process that measures how easy it is for the process to continue to add value. Sustaining a Data Governance function requires planning for change. (See Chapter 17.) 4.2 Adjustment and Communication Data Governance functions are implemented incrementally within the context of a wider business and data management strategy. Success requires keeping the wider goals in mind while putting the pieces in place. The Data Governance team will need to be flexible and adjust its approach as conditions shift. Tools required to manage and communicate changes include: • Business / Data Governance strategy map : This map connects Data Governance activity with business needs. Periodically measuring and communicating how Data Governance is helping the business is vital to obtain ing ongoing support for the function . • Data Governance roadmap : The roadmap to Data Governance should not be rigid. It should be adapted to changes in business environment or priorities. • Ongoing business case for Data Governance : The business case must be adjusted periodically to reflect changing priorities and financial realities of the organization. • Data Governance metrics : Metrics will need to grow and change as the Data Governance function matures. 5. Metrics To counter the resistance or the challenge of a long learning curve, a Data Governance function must be able to measure progress and success through metrics that demonstrate how Data Governance has added business value and attained objectives. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA GOVERNANCE • 97 To manage the behavior changes required, it is important to measure progress of the rollout of Data Governance, compliance with the Data Governance requirements, and the value that Data Governance is bringing to the organization. Metrics that reinforce the Data Governance ’s value and those that verify that the organization has the resources required to support Data Governance after it is rolled out are also important to sustaining a Data Governance function . Sample metrics include: • Value • Contributions to business objectives • Reduction of risk • Improved efficiency in operations • Effectiveness • Achievement of goals and objectives • Extent stewards are using the relevant tools • Effectiveness of communication • Effectiveness of education/training • Speed of change adoption • Sustainability • Performance of policies and processes (i.e., are they working appropriately?) • Conformance to standards and procedures (i.e., are staff following the guidance and changing behavior as necessary?) 6. Works Cited / Recommended Adelman, Sid, Larissa Moss and Majid Abai. Data Strategy . Addison -Wesley Professional, 2005. Print. Anderson, Dean and Anderson, Linda Ackerson. Beyond Change Management . Pfeiffer, 2012. Avramov, Lucien and Maurizio Portolani. The Policy Driven Data Center with ACI: Architecture, Concepts, and Methodology . Cisco Press, 2014. Print. Networking Technology. Axelos Global Best Practice (ITIL website). http://bit.ly/1H6SwxC . Brzezinski, Robert. HIPAA Privacy and Security Compliance - Simplified: Practical Guide for Healthcare Providers and Practice Managers . CreateSpace Independent Publishing Platform, 2014. Print. Calder, Alan. IT Governance: Implementing Frameworks and Standards for the Corporate Governance of IT. IT Governance Publishing, 2009. Print. Change Management Institute and Carbon Group. Organizational Change Maturity Model , 2012. http://bit.ly/1Q62tR1 . Change Management Institute (website). http://bit.ly/1Q62tR1 . Chisholm, Malcolm and Roblyn -Lee, Diane. Definitions in Data Management: A Guide to Fundamental Semantic Metadata . Design Media, 2008. Print. Cokins, Gary et al. CIO Best Practices: Enabling Strategic Value with Information Technology , 2nd ed. Wiley, 2010. Print. De Haes, Steven and Wim Van Grembergen. Enterprise Governance of Information Technology: Achieving Alignment and Value, Featuring COBIT 5 . 2nd ed. Springer, 2015. Print. Management for Professionals. DiStefano, Robert S. Asset Data Integrity Is Serious Business . Industrial Press, Inc., 2010. Print. Doan, AnHai, Alon Halevy and Zachary Ives. Principles of Data Integration . Morgan Kaufmann, 2012. Print. Fisher, Tony. The Data Asset: How Smart Companies Govern Their Data for Business Success . Wiley, 2009. Print. Giordano, Anthony David. Performing Information Governance: A Step -by-step Guide to Making Information Governance Work . IBM Press, 2014. Print. IBM Press. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "98 • DMBOK2 Hiatt, Jeff and Creasey, Timothy. Change Management: The People Side of Change. Prosci, 2012. Huwe, Ruth A. Metrics 2.0: Creating Scorecards for High -Performance Work Teams and Organizations . Praeger, 2010. Print. Ladley, John. Data Governance: How to Design, Deploy and Sustain an Effective Data Governance Program . Morgan Kaufmann, 2012. Print. The Morgan Kaufmann Series on Business Intelligence. Ladley, John. Making Enterprise Information Management (EIM) Work for Business: A Guide to Understanding Information as an Asset. Morgan Kaufmann, 2010. Print. Marz, Nathan and James Warren. Big Data: Principles and best practices of scalable realtime data systems . Manning Publications, 2015. Print. McGilvray, Danette. Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information. Morgan Kaufmann, 2008. Print. Osborne, Jason W. Best Practices in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Yo ur Data . SAGE Publications, Inc , 2013. Print. Plotkin, David. Data Stewardship: An Actionable Guide to Effective Data Management and Data Governance . Morgan Kaufmann, 2013. Print. PROSCI (website). http://bit.ly/2tt1bf9 . Razavi, Behzad. Principles of Data Conversion System Design . Wiley -IEEE Press, 1994. Print. Redman, Thomas C. Data Driven: Profiting from Your Most Important Business Asset. Harvard Business Review Press, 2008. Print. Reinke, Guido. The Regulatory Compliance Matrix: Regulation of Financial Services, Information and Communication Technology, and Generally Related Matters . GOLD RUSH Publishing, 2015. Print. Regulatory Compliance. Seiner, Robert S. Non -Invasive Data Governance . Technics Publications, LLC, 2014. Print. Selig, Gad. Implementing IT Governance: A Practical Guide to Global Best Practices in IT Management . Van Haren Publishing, 2008. Print. Best Practice. Smallwood, Robert F. Information Governance: Concepts, Strategies, and Best Practices . Wiley, 2014. Print. Wiley CIO. Soares, Sunil. Selling Information Governance to the Business: Best Practices by Industry and Job Function. Mc Press, 2011. Print. Tarantino, Anthony. The Governance, Risk, and Compliance Handbook: Technology, Finance, Environmental, and International Guidance and Best Practices . Wiley, 2008. Print. The Data Governance Institute (website). http://bit.ly/1ef0tnb . The KPI Institute and Aurel Brudan, ed. The Governance, Compliance and Risk KPI Dictionary: 130+ Key Performance Indicator Definitions . CreateSpace Independent Publishing Platform, 2015. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "99 CHAPTER 4 Data Architecture 1. Introduction rchitecture refers to the art and science of building things (especially habitable structures) and to the results of the process of building – the buildings themselves. In a more general sense, architecture refers to an organized arrangement of component elements int ended to optimize the function, performance, feasibility, cost, and aesthetics of an overall structure or system. The term architecture has been adopted to describe several facets of information systems design. ISO/IEC 42010:2007 Systems and Software Engineering – Architecture Description (2011) defines architecture as “the fundamental organization of a system, embodied in its components, their relationships to each other and the environment, and the principles governing its design and evolution.” However, depending on context, the word architecture can refer to a description of the current state of systems, the components of a set of systems, the Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International A Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "100 • DMBOK2 discipline of designing systems (architecture practice), the intentional design of a system or a set of systems (future state or proposed architecture), the artifacts that describe a system (architecture documentation), or the team that does the design wor k (the Architects or the Architecture team). Architecture practice is carried out at different levels within an organization (enterprise, domain, project, etc.) and with different areas of focus (infrastructure, application, and data). Exactly what architects do can be confusing to people who are not architects and who do not recognize the distinctions implied by these levels and focus areas. One reason architectural frameworks are valuable is that they enable non- architects to understand these relationships. The discipline of Enterprise Architecture encompasses domain architectures, including business, data, application, and technology. Well- managed enterprise architecture practices help organizations understand the current state of their systems, promote desirable change toward future state, enable regulatory compliance, and improve effectiveness. Effective management of data and the systems in which data is stored and used is a common goal of the breadth of architecture disciplines. In this chapter, Data Architecture will be considered from the following perspectives: • Data Architecture outcomes , such as models, definitions, and data flows on various levels, usually referred to as Data Architecture artifacts • Data Architecture activities to form, deploy , and fulfill Data Architecture intentions • Data Architecture behavior , such as collaborations, mindsets, and skills among the various roles that affect the enterprise’s Data Architecture Together, these three form the essential components of Data Architecture. Data Architecture is fundamental to data management. Because most organizations have more data than individual people can comprehend, it is necessary to represent organizational data at different levels of abstraction so that it can be understood and management can make de cisions about it. Data Architecture artifacts include specifications used to describe existing state, define data requirements, guide data integration, and control data assets as put forth in a data strategy. An organization’s Data Architecture is described by an integrated collection of master design documents at different levels of abstraction, including standards that govern how data is collected, stored, arranged, used, and removed. It is also classified by descriptions of all the containers and paths that data takes thr ough an organization’s systems. The most detailed Data Architecture design document is a formal enterprise data model, containing data names, comprehensive data and Metadata definitions, conceptual and logical entities and relationships, and business rules. Physical data models are inclu ded, but as a product of data modeling and design rather than Data Architecture. Data Architecture is most valuable when it fully supports the needs of the entire enterprise. Enterprise Data Architecture enables consistent data standardization and integration across the enterprise. The artifacts that architects create constitute valuable Metadata. Ideally, architectural artifacts should be stored and managed in an enterprise architecture artifact repository. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 101 We are in the middle of the third wave of end customer digitalization. Banks and financial transactions came first; various digital service interactions were in the second wave; and the internet of things and telematics drive the third. Traditional industries, like automotive, health care equipment, and tooling, are going digital in this third wave. This happens in almost every industry. New Volvo cars have now on- call 24/7 service, not only for vehicle - related matters, but also to locate restaurants and shopping. Overhead cranes, pallet loaders, and anesthesia equipment are collecting and sending ope rational data that enables up -time services. Offerings have moved from suppling equipment to pay -per-use or availability contracts. Many of these companies have little if any experience in these areas, since they were previously taken care of by retailers or aftermarket service providers. Forward -looking organizations should include data management professionals (e.g., Enterprise Data Architects or strategic Data Stewards) when they are designing new market offerings because nowadays , these usually include hardware, software, and services that capture data, depend on data access, or both. 1.1 Business Drivers The goal of Data Architecture is to be a bridge between business strategy and technology execution. As part of Enterprise Architecture, Data Architects: • Strategically prepare organizations to quickly evolve their products, services, and data to take advantage of business opportunities inherent in emerging technologies • Translate business needs into data and system requirements so that processes consistently have the data they require • Manage complex data and information delivery throughout the enterprise • Facilitate alignment between Business and IT • Act as agents for change, transformation, and agility These business drivers should influence measures of the value of Data Architecture. Data architects create and maintain organizational knowledge about data and the systems through which it moves. This knowledge enables an organization to manage its data as an asset and increase the value it gets from its data by identifying opportunities for data usage, cost reduction, and risk mitigation. 1.2 Data Architecture Outcomes and Practices Primary Data Architecture outcomes include: • Data storage and processing requirements • Designs of structures and plans that meet the current and long -term data requirements of the enterprise Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "102 • DMBOK2 Figure 21 Context Diagram: Data Architecture Architects seek to design in a way that brings value to the organization. This value comes through an optimal technical footprint, operational and project efficiencies, and the increased ability of the organization to use its data. To get there requires good design, planning, and the ability to ensure that the designs and plans are executed effectively. Consumers : •Database Administrators •Software Developers •Project Managers •Support T eamsParticipants : •Enterprise Data Architects •Data ModelersSuppliers : •Enterprise Architects •Data Stewards •Subject Matter Experts •Data AnalystsDefinition : Identifying the data needs of the enterprise (regardless of structure) and designing and maintaining the master blueprints to meet those needs. Using master blueprints to guide data integration, control data assets, and align data investments with business strategy. Goals : 1.Identify data storage and processing requirements. 2.Strategically prepare organizations to quickly evolve their products, services, and data to take advantage of business opportunities inherent in emerging technologies 3.Design structures and plans to meet the current and long -term data requirements of the enterprise. Activities : 1.Establish Enterprise Data Architecture (P) 1. Evaluate Existing Data Architecture Specifications 2. Develop a Roadmap 2.Manage Enterprise Requirements within Projects (D) 3.Integrate with Enterprise Architecture (O)Inputs : •Enterprise Architecture •Business Architecture •IT Standards and Goals •Data StrategiesDeliverables : •Data Architecture Design •Data Flows •Data Value Chains •Enterprise Data Model •Implementation Roadmap T echniques : •Lifecycle Reviews •Diagramming ClarityT ools : •Data modeling tools •Asset management software •Graphical design applicationsMetrics : •Architecture standards compliance rates •Trends in implementation •Business value metrics (P) Planning, (C) Control, (D) Development, (O) OperationsData Architecture Business Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 103 To reach these goals, Data Architects define and maintain specifications that: • Define the current state of data in the organization • Provide a standard business vocabulary for data and components • Align Data Architecture with enterprise strategy and business architecture • Express strategic data requirements • Outline high -level integrated designs to meet these requirements • Integrate with overall enterprise architecture roadmap An overall Data Architecture practice includes: • Using Data Architecture artifacts (master blueprints) to define data requirements, guide data integration, control data assets, and align data investments with business strategy • Collaborating with, learning from , and influencing various stakeholders that are engaged with improving the business or IT systems development • Using Data Architecture to establish the semantics of an enterprise via a common business vocabulary 1.3 Essential Concepts 1.3.1 Enterprise Architecture Domains Data Architecture operates in context of other architecture domains, including business, application, and technical architecture. Table 6 describes and compares these domains. Architects from different domains must address development directions and requirements collaboratively, as each domain influences and put constraints on the other domains. (See also Figure 22.) Table 6 Architecture Domains Domain Enterprise Business Architecture Enterprise Data Architecture Enterprise Applications Architecture Enterprise Technology Architecture Purpose To identify how an enterprise creates value for customers and other stakeholders To describe how data should be organized and managed To describe the structure and functionality of applications in an enterprise To describe the physical technology needed to enable systems to function and deliver value Elements Business models, processes, capabilities, services, events, strategies, vocabulary Data models, data definitions, data mapping specifications, data flows, structured data APIs Business systems, software packages, databases Technical platforms, networks, security, integration tools Dependencies Establishes requirements for the other domains Manages data created and required by business architecture Acts on specified data according to business requirements Hosts and executes the application architecture Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "104 • DMBOK2 Domain Enterprise Business Architecture Enterprise Data Architecture Enterprise Applications Architecture Enterprise Technology Architecture Roles Business architects and analysts, business data stewards Data architects and modelers, data stewards Applications architects Infrastructure architects 1.3.2 Enterprise Architecture Frameworks An architecture framework is a foundational structure used to develop a broad range of related architectures. Architectural frameworks provide ways of thinking about and understanding architecture. They represent an overall ‘architecture for architecture.’ IEEE Computer Society maintains a standard for Enterprise Architecture Frameworks, ISO/IEC/IEEE 42010:2011, Systems and software engineering — Architecture description and a comparison table. 32 Common frameworks and methods include Data Architecture as one of the architectural domains. 1.3.2.1 Zachman Framework for Enterprise Architecture The most well -known enterprise architectural framework , the Zachman Framework, was developed by John A. Zachman in the 1980s. (See Figure 22.) It has continued to evolve. Zachman recognized that in creating buildings, airplanes, enterprises, value chains, projects, or systems, there are many audiences, and each has a different perspective about architecture. He applied this concept to the requirements for different types and levels of architecture within an enterprise. Figure 22 Simplified Zachman Framework 32http://bit.ly/2tNnD2j ; http://bit.ly/2rVinIq . Inventory IdentificationProcessIdentificationDistribution IdentificationResponsibilityIdentificationTiming IdentificationMotivationIdentification InventorydefinitionProcess DefinitionDistribution DefinitionResponsibilityDefinitionTiming DefinitionMotivation Definition InventoryRepresentationProcess RepresentationDistribution RepresentationResponsibility RepresentationTiming RepresentationMotivation Representation Inventory SpecificationProcess SpecificationDistribution SpecificationResponsibility SpecificationTiming SpecificationMotivation Specification Inventory ConfigurationProcess ConfigurationDistribution ConfigurationResponsibility ConfigurationTiming Configuration Motivation Configuration Inventory InstantiationsProcess InstantiationsDistributionInstantiationsResponsibility InstantiationsTiming InstantiationsMotivation Instantiations Executive Business Management Architect Engineer T echnician Enterprise Scope Context Business Concepts System Logic T echnology Physics T ool Components Operational Instances Inventory Sets What How Where Who When Why Process Flows Distribution Networks Responsibility Assignments Timing Cycles Motivation Intentions Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 105 The Zachman Framework is an ontology – the 6x6 matrix comprises the complete set of models required to describe an enterprise and the relationships between them. It does not define how to create the models. It simply shows what models should exist. The two dimensions in the matrix framework are the communication interrogatives (i.e., what, how, where, who, when, and why) as columns and the reification transformations (Identification, Definition, Representation, Specification, Configuration , and Instantiation) as rows. The framework classifications are represented by the cells (the intersection between the interrogatives and the transformations). Each cell in the Zachman Framework represents a unique type of design artifact. Communication interrogatives are the fundamental questions that can be asked about any entity. Translated to enterprise architecture, the columns can be understood as follows: • What (the inventory column): Entities used to build the architecture • How (the process column): Activities performed • Where (the distribution column): Business location and technology location • Who (the responsibility column): Roles and organizations • When (the timing column): Intervals, events, cycles, and schedules • Why (the motivation column): Goals, strategies, and means Reification transformations represent the steps necessary to translate an abstract idea into a concrete instance (an instantiation). These are represented in the rows: planner, owner, designer, builder, implementer, and user. Each has a different perspective on the overall process a nd different problems to solve. These perspectives are depicted as rows. For example, each perspective has a different relation to the What (inventory or data) column: • The executive perspective (business context): Lists of business elements defining scope in identification models. • The business management perspective (business concepts): Clarification of the relationships between business concepts defined by Executive Leaders as Owners in definition models. • The architect perspective (business logic): System logical models detailing system requirements and unconstrained design represented by Architects as Designers in representation models. • The engineer perspective (business physics): Physical models optimizing the design for implementation for specific use under the constraints of specific technology, people, costs, and timeframes specified by Engineers as Builders in specification models. • The technician perspective (component assemblies): A technology -specific, out- of-context view of how components are assembled and operate d, configured by Technicians as Implementers in configuration models. • The user perspective (operations classes): Actual functioning instances used by Workers as Participants. There are no models in this perspective. As noted previously, each cell in the Zachman Framework represents a unique type of design artifact, defined by the intersection of its row and column. Each artifact represents how the specific perspective answers the fundamental questions. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "106 • DMBOK2 1.3.3 Enterprise Data Architecture Enterprise Data Architecture defines standard terms and designs for the elements that are important to the organization. The design of an Enterprise Data Architecture includes a depiction of the business data as such, including the collection, storage, integration, movement, and distribution of data. As data flows in an organization through feeds or interfaces, it is secured, integrated, stored, recorded, cataloged, shared, reported on, analyzed, and delivered to stakeholders. Along the way, the data may be verified, enhanced, linked, certified, aggreg ated, anonymized, and used for analytics until archived or purged. The Enterprise Data Architecture descriptions must therefore include both Enterprise Data Models (e.g., data structures and data specifications), as well as Data Flow Design: • Enterprise Data Model (EDM) : The EDM is a holistic, enterprise -level, implementation -independent conceptual or logical data model providing a common consistent view of data across the enterprise. It is common to use the term to mean a high -level, simplified data model, but that is a question of abstraction for presentation. An EDM includes key enterprise data entities (i.e., business concepts), their relationships, critical guiding business rules, and some critical attributes. It sets forth the foundation f or all data and data- related projects. Any project -level data model must be based on the EDM. The EDM should be reviewed by stakeholders so that there is consensus that it effectively represents the enterprise. • Data Flow Design : Defines the requirements and master blueprint for storage and processing across databases, applications, platforms, and networks (the components). These data flows map the movement of data to business processes, locations, business roles, and technical c omponents. These two types of specifications need to fit well together. As mentioned, both need to be reflected in current state and target state (architecture perspective), and also in transition state (project perspective). 1.3.3.1 Enterprise Data Model Some organizations create an EDM as a stand -alone artifact. In other organizations, it is understood as composed of data models from different perspectives and at different levels of detail, that consistently describe an organization’s understanding of data entities, data attributes, and their relationships across the enterprise. An EDM includes both universal (Enterprise -wide Conceptual and Logical Models) and application - or project - specific data models, along with definitions, specifications, mappings, and business rules. Adopting an industry standard model can jumpstart the process of developing an EDM. These models provide a useful guide and references. However, even if an organization starts with a purchased data model, producing enterprise -wide data models requires a si gnificant investment. Work includes defining and documenting an organization’s vocabulary, business rules, and business knowledge. Maintaining and enriching an EDM requires an ongoing commitment of time and effort. An organization that recognizes the need for an enterprise data model must decide how much time and effort it can devote to building and maintaining it. EDMs can be built at different levels of detail, so resource availability will influence initial scope. Over time, as the needs of the enterprise demand, the scope and level of detail captured within an enterprise data model typically expands. Most successful enterprise data models Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 107 are built incrementally and iteratively, using layers. Figure 23 shows how different types of models are related and how conceptual models are ultimately linkable to physical application data models. It distinguishes: • A conceptual overview over the enterprise’s subject areas • Views of entities and relationships for each subject area • Detailed, partially attributed logical views of these same subject areas • Logical and physical models specific to an application or project Figure 23 Enterprise Data Model All levels are part of the Enterprise Data Model, and linkages create paths to trace an entity from top to bottom and between models in the same level. • Vertical : Models in each level map to models in other levels. Model lineage is created using these maps. For example, a table or file MobileDevice in a project -specific physical model may link to a 1 diagram: 12-20 significant business subject areas with relationships 1+ diagram per subject area: 50+ significant entities within subject area with relationships For each Subject Area Logical Model: Increase detail by adding attributes, and less- significant entities and relationships Conceptual Model Subject Area Model Subject Area Model Subject Area Model Subject Area Model Logical ModelLogical ModelLogical ModelLogical Model LDM LDM PDM PDMLDM LDM PDM PDMLDM LDM PDM PDMLDM PDMEnterprise Data Model Application or Project- SpecificLimit scope to the subject and 1 -step external relationships Limit scope to the subject physical objects and relationships Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "108 • DMBOK2 MobileDevice entity in the project- specific logical model, a MobileDevice entity in the Product subject area in the Enterprise Logical Model, a Product conceptual entity in the Product Subject Area Model, and to the Product entity in the Enterprise Conceptual Model. • Horizontal : Entities and relationships may appear in multiple models in the same level; entities in logical models centered on one topic may relate to entities in other topics, marked or noted as external to the subject area on the model images. A Product Part entity may appear in the Product subject area models and in the Sales Order, Inventory, and Marketing subject areas, related as external links. An enterprise data model at all levels is developed using data modeling techniques. (See Chapter 5.) Figure 24 depicts three Subject Area diagrams (simplified examples), each containing a Conceptual Data Model with a set of entities. Relationships may cross Subject Area borders; each entity in an enterprise data model should reside in only one Subject Area, but ca n be related to entities in any other Subject Area. Hence, the conceptual enterprise data model is built up by the combination of Subject Area models. The enterprise data model can be built using a top -down approach or using a bottom -up approach. The top- down approach means starting with forming the Subject Areas and then populating them with models. When using a bottom -up approach , the Subject Area structure is based on existing data models. A combination of the approaches is usually recommended; starting with bottom -up using existing models and completing the enterprise data model by populating the models by delegating Subject Area modeling to projects. Figure 24 Subject Area Models Diagram Example Product Platform Product Product PartMarket Offering Portfolio Sales Item Sales Bundle Bill-of-material (BOM) Sales Order Sales Order ItemProduct Group Part StructureProduct Design Bill-of-material (BOM)Belongs toUsesOffering Range Occurs in Specifies DetailsProduct Design Subject AreaCommercial Offer Subject AreaSales Subject Area Sales Order Item Configuration Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 109 The Subject Area discriminator (i.e., the principles that form the Subject Area structure) must be consistent throughout the enterprise data model. Frequently used subject area discriminator principles include using normalization rules, dividing Subject Areas from systems portfolios (i .e., funding), forming Subject Areas from Data Governance structure and data ownership (organizational), using top -level processes (based on the business value chains), or using business capabilities (enterprise architecture -based). The Subject Area struct ure is usually most effective for Data Architecture work if it is formed using normalization rules. The normalization process will establish the major entities that carry/constitute each Subject Area. 1.3.3.2 Data Flow Design Data flows are a type of data lineage documentation that depicts how data moves through business processes and systems. End -to-end data flows illustrate where the data originated, where it is stored and used, and how it is transformed as it moves inside and between diverse processes and systems. Data lineage analysis can help explain the state of data at a given point in the data flow. Data flows map and document relationships between data and : • Applications within a business process • Data stores or databases in an environment • Network segments (useful for security mapping) • Business roles, depicting which roles have responsibility for creating, updating, using, and deleting data (CRUD) • Locations where local differences occur Data flows can be documented at different levels of detail: Subject Area, business entity, or even the attribute level. Systems can be represented by network segments, platforms, common application sets, or individual servers. Data flows can be represented by two -dimensional matrices (Figure 25) or in data flow diagrams (Figure 26). A matrix gives a clear overview of what data the processes create and use. The benefit of showing the data requirements in a matrix is that it takes into consideration that data does not flow in only one direction . The data exchange between processes is many -to-many in a quite complex way, where any data may appear anywhere. In addition, a matrix can be used to clarify the processes’ data acquisition responsibilities and the data dependencies between the processes, which in turn improves the process docu mentation. Those who prefer working with business capabilities could show this in the same way – just exchang e the processes axis for capabilities. Building such matrices is a long -standing practice in enterprise modeling. IBM introduced this practice in its Business Systems Planning (BSP) method. James Martin later popularized it in his Information Systems Planning (ISP) method during the 1980’s. The data flow in Figure 26 is a traditional high -level data flow diagram depicting what kind of data flows between systems. Such diagrams can be described in many formats and detail levels. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "110 • DMBOK2 Figure 25 Data Flow Depicted in a M atrix Figure 26 Data Flow Diagram Example Product Product Part Manufacturing Plant Customer Sales Item Assembly Structure Sales Order Production Order Individual Product Shipping Customer’s Invoice Product developmentMarketing & SalesIndustrial preparationOrder managementManufacturing Logistics Invoicing Create Read/useMajor EntitiesBusiness Processes CRM Sales System Manufacturing Planning System PDM System Manufacturing Routing System Aftermarket System Product Design BOM Product properties Product design BOM Product properties Sourcing data Manufacturing BOM Production line routing Lead timesSales BOM Order Contents Serial numbers Individual product BOMService & repair statisticsService & repair statistics Material returns Customer data Customer agreement Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 111 2. Activities Data and enterprise architecture deal with complexity from two viewpoints: • Quality -oriented: Focus on improving execution within business and IT development cycles. Unless architecture is managed, architecture will deteriorate. Systems will gradually become more complex and inflexible, creating risk for an organization. Uncontrolled data delivery , data copies, and interface ‘spaghetti’ relationships make organizations less efficient and reduce trust in the data. • Innovation- oriented: Focus on transforming business and IT to address new expectations and opportunities. Driving innovation with disruptive technologies and data uses has become a role of the modern Enterprise Architect. These two drivers require separate approaches. The quality -oriented approach aligns with traditional Data Architecture work where architectural quality improvements are accomplished incrementally. The architecture tasks are distributed to projects, where architects participate or the project carries out by delegation. Typically, the architect keeps the entirety of architecture in mind and focuses on long -term goals directly connected to governance, standardization, and structured development. The innovation -oriented approach can have a shorter -term perspective and be using unproven business logic and leading edge technologies. This orientation often requires architects make contact with people within the organization with whom IT professionals do not usually interact (e.g., product development representatives and business designers). 2.1 Establish Data Architecture Practice Ideally, Data Architecture should be an integral part of enterprise architecture. If there is not an enterprise architecture function, a Data Architecture team can still be established. Under these conditions, an organization should adopt a framework that helps articulate the goals and drivers for Data Architecture. These drivers will influence the approach, scope, and priorities on the roadmap. Choose a framework applicable to the business type (e.g., use a government framework for a governmental organization). The views and taxonomy in the framework must be useful in communication with the various stakeholders. This is especially important for Data Architecture initiatives, as they address business and systems terminology. Data Architecture has an inherently close relationship to business architecture. An Enterprise Data Architecture practice generally includes the following work streams, executed serially or in parallel: • Strategy : Select frameworks, state approaches, develop roadmap • Acceptance and culture : Inform and motivate changes in behavior • Organization : Organize Data Architecture work by assigning accountabilities and responsibilities • Working methods : Define best practices and perform Data Architecture work within development projects in coordination with Enterprise Architecture • Results : Produce Data Architecture artifacts within an overall roadmap Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "112 • DMBOK2 Enterprise Data Architecture also influences the scope boundaries of projects and system releases: • Defining project data requirements : Data Architects provide enterprise data requirements for individual projects. • Reviewing project data designs : Design reviews ensure that conceptual, logical, and physical data models are consistent with architecture and in support of long -term organizational strategy. • Determining data lineage impact : Ensures that business rules in the applications along the data flow are consistent and traceable. • Data replication control : Replication is a common way to improve application performance and make data more readily available, but it can also create inconsistencies in the data. Data Architecture governance ensures that sufficient replication control (methods and mechanisms) are in place to achieve required consistency. (Not all applications need strict consistency.) • Enforcing Data Architecture standards : Formulating and enforcing standards for the Enterprise Data Architecture lifecycle. Standards can be expressed as principles , procedures, and guidelines , as well as blueprints with compliance expectations. • Guide data technology and renewal decisions : The Data Architect works with Enterprise Architects to manage data technology versions, patches, and policies each application uses as a roadmap for data technology. 2.1.1 Evaluate Existing Data Architecture Specifications Every organization has some form of documentation for its existing systems. Identify these documents and evaluate them for accuracy, completeness, and level of detail. If necessary, update them to reflect the current state. 2.1.2 Develop a Roadmap If an enterprise were developed from scratch (free from dependence on existing processes), an optimal architecture would be based solely on the data required to run the enterprise, priorities would be set by business strategy, and decisions could be made unencumbered by the past. Very few organizations are ever in this state. Even in an ideal situation, data dependencies would quickly arise and need to be managed. A roadmap provides a means to manage these dependencies and make forward -looking decisions. A roadmap helps an organization see trade-offs and formulate a pragmatic plan, aligned with business needs and opportunities, external requirements, and available resources. A roadmap for Enterprise Data Architecture describes the architecture’s 3 -5 year development path. Together with the business requirements, consideration of actual conditions, and technical assessments, the roadmap describes how the target architecture will become reality. The Enterprise Data Architecture roadmap must be integrated into an overall enterprise architecture roadmap that includes high- level milestones, resources Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 113 needed, and cost estimations, divided in to business capability work streams. The roadmap should be guided by a data m anagement maturity assessment. (S ee Chapter 15.) Most business capabilities require data as an input; others also produce data on which other business capabilities are dependent. The enterprise architecture and the Enterprise Data Architecture can be formed coherently by resolving this data flow in a cha in of dependencies between business capabilities. A business- data- driven roadmap starts with the business capabilities that are most independent (i.e., have the least dependency from other activities), and ends with those who are most dependent on others. Dealing with each business capability in sequence will follow an overall business data origination order. Figure 27 shows an example chain of dependency, with the lowest dependency at the top. Product Management and Customer Management do not depend on anything else and thus constitute Master Data. The highest dependency items are on the bottom where Customer’s Invoice Management depends on Customer Management and Sales Order Management, which in turn depends on two others. Figure 27 The Data Dependencies of Business Capabilities Therefore, the roadmap would ideally advise starting at Product Management and Customer Management capabilities and then resolve each dependency in steps from top to bottom. Product Management Product Part Management Sales Item Management Sales Order Management Customer Management Customer’s Invoice Management Production Order Management Assembly Structure Management Product DataProduct Data BOM Details BOM Details Assembly Structure DataSales Order DataSales Order DataCustomer Data Customer DataSales Item Data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "114 • DMBOK2 2.1.3 Manage Enterprise Requirements within Projects Architecture should not be locked into the limitations that prevail at the time it is developed. Data models and other specifications describing an organization’s Data Architecture must be flexible enough to accommodate future requirements. A data model at the architectural level should have a global view of the enterprise along with clear definitions that can be understood throughout the organization. Development projects implement solutions for capturing, storing, and distributing data based on business requirements and the standards established by the Enterprise Data Architecture. This process, by its nature, is accomplished incrementally. At the project level, the process of specifying requirements via a data model begins with reviewing business needs. Often , these needs will be specific to the goals of the project and will not have enterprise implications. The process should still include developing definitions of terms and other activities that support use of the data. Importantly, data architects must be able to understand requirements in relation to the overall architecture. When a project specification is completed, the data architects should determine: • Whether enterprise- wide entities represented in the specification conform to agreed -upon standards • What entities in the requirements specification should be included in the overall Enterprise Data Architecture • Whether entities and definitions in this specification need to be generalized or improved upon to handle future trends • Whether new data delivery architectures are indicated or whether to point the developers in the direction of reuse Organizations often wait to address Data Architecture concerns until projects need to design data storage and integration. However, it is preferable to include these considerations early in planning and throughout the entire project lifecycle. Enterprise Data Architecture project -related activities include: • Define scope : Ensure the scope and interface are aligned with the enterprise data model. Understand the project’s potential contribution to the overall Enterprise Data Architecture, with respect to what the project will model and design and in terms of what existing components should (or can) be reused. In those areas that should be designed, the project needs to determine dependencies with stakeholders outside the project scope, such as down- stream processes. The data artifacts that the project determines to be shareable or reusable need to be incorporated into the enterprise logical data model and designated repositories. • Understand business requirements : Capture data- related requirements such as entity, source(s), availability, quality, and pain points, and estimate the business value of meeting these requirements. • Design : Form detailed target specifications, including business rules in a data lifecycle perspective. Validate the outcome and, when needed, address needs for extended and improved standardized models. The enterprise logical data model and enterprise architecture repository are good places for Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 115 project data architects to look and reuse constructs that are shareable across the enterprise. Review and use data technology standards. • Implement: • When buying, reverse engineer purchased applications (Commercial Off the Shelf – COTS) and map against data structure. Identify and document gaps and differences in structures, definitions, and rules. Ideally, vendors will supply data models for their products; howeve r, many do not, as they consider these proprietary. If possible, negotiate for a model with in -depth definitions. • When reusing data, map application data models against common data structures and existing and new processes to understand CRUD operations. Enforce the use of system of record or other authoritative data. Identify and document gaps. • When building, implement data storage according to the data structure. Integrate according to standardized or designed specifications. (See Chapter 8.) The role of Enterprise Data Architects in projects depends on the development methodology. The process of building architectural activities into projects also differs between methodologies. • Waterfall methods : Understand the requirements and construct systems in sequential phases as part of an overall enterprise design. This method includes tollgates designed to control change. It is usually no problem to include Data Architecture activities in such models. Be sure to include an enterprise perspective. • Incremental methods : Learn and construct in gradual steps (i.e., mini -waterfalls). This method creates prototypes based on vague overall requirements. The initiation phase is crucial; it is best to create a comprehensive data design in early iterations. • Agile, iterative, methods : Learn, construct, and test in discrete delivery packages (called ‘sprints’) that are small enough that if work needs to be discarded, not much is lost. Agile methods (Scrum, Rapid Development, and Unified Process) promote object -oriented modeling that emp hasizes user interface design, software design, and systems behavior. Complete such methods with specifications for data models, data capture, data storage, and data distribution. Experience from DevOps, an emerging and popular agile approach, testifies ab out improved data design and effective design choices when programmers and data architects have a strong working relationship and both comply with standards and guidelines. 2.2 Integrate with Enterprise Architecture The work of developing Enterprise Data Architecture specifications from the subject area level to more detailed levels and in relation to other architecture domains is typically performed within funded projects. Funded projects generally drive architecture priorities. Nevertheless, enterprise -wide Data Architecture matters should be addressed proactively. In fact, Data Architecture may influence the scope of projects. It is best, therefore, to integrate Enterprise Data Architecture matters with project portfolio management. Doing so enables implementation of the roadmap and contributes to better project outcomes. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "116 • DMBOK2 Likewise, the Enterprise Data Architects need to be included with enterprise application development and integration planning. Apply the Data Architecture view on the target application landscape and the roadmap to that landscape. 3. Tools 3.1 Data Modeling Tools Data modeling tools and model repositories are necessary for managing the enterprise data model at all levels. Most data modeling tools include lineage and relation tracking functions, which enable architects to manage linkages between models created for different purposes and at di fferent levels of abstraction. (S ee Chapter 5.) 3.2 Asset Management Software Asset management software is used to inventory systems, describe their content, and track the relationships between them. Among other things, these tools enable an organization to ensure that it follows contractual obligations related to software licenses and to collect data related to assets that can be used to minimize costs and optimize their IT footprint. Because they compile an inventory of IT assets, such tools collect and contain valuable Metadata about systems and the data they contain. This Metadata is very helpful when creating data flows or researching current state. 3.3 Graphical Design Applications Graphical design applications are used to create architectural design diagrams , data flows, data value chains, and other architectural artifacts. 4. Techniques 4.1 Lifecycle Projections Architecture designs can be aspirational or future -looking, implemented and active, or plans for retirement. What they represent should be clearly documented. For example: • Current : Products currently supported and used • Deployment period : Products deployed for use in the next 1 -2 years Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 117 • Strategic period : Products expected to be available for use in the next 2+ years • Retirement : Products the organization has retired or intends to retire within a year • Preferred : Products preferred for use by most applications • Containment : Products limited to use by certain applications • Emerging : Products being researched and piloted for possible future deployment • Reviewed : Products that have been evaluated, the evaluation results and are currently not in any other status above See Chapter 6 for more about managing data technologies. 4.2 Diagramming Clarity Models and diagrams present information based on an established set of visual conventions. These need to be used consistently or they will be misunderstood and may, in fact, be incorrect. Characteristics that minimize distractions and maximize useful information include: • A clear and consistent legend: The legend should identify all objects and lines and what they signify. The legend should be placed in the same spot in all diagrams. • A match between all diagram objects and the legend : In legends that are used as templates, not all legend objects may appear in the diagram, but all diagram objects should match a legend object. • A clear and consistent line direction: All flows should start at one side or corner (generally the left) and flow toward the opposite side or corner as much as possible. Loops and circles will occur, so make the lines going backward flow out and around to be clear. • A consistent line cross display method: Lines can cross as long as it is clear that the crossing point is not a join. Use line jumps for all lines in one direction. Do not join lines to lines. Minimize the number of lines that cross. • Consistent object attributes : Any difference in sizes, colors, line thickness, etc. , should signify something, otherwise differences are distracting. • Linear symmetry: Diagrams with objects placed in lines and columns are more readable than those with random placement. While it is rarely possible to align all objects, lining up at least half (horizontally and/or vertically) will greatly improve readability of any diagra m. 5. Implementation Guidelines As stated in the chapter introduction, Data Architecture is about artifacts, activities , and behavior. Implementing Enterprise Data Architecture is therefore about: • Organizing the Enterprise Data Architecture teams and forums Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "118 • DMBOK2 • Producing the initial versions of Data Architecture artifacts, such as enterprise data model, enterprise -wide data flow , and road maps • Forming and establishing a data architectural way of working in development projects • Creating awareness throughout the organization of the value of Data Architecture efforts A Data Architecture implementation should include at least two of these as they benefit from being launched simultaneously, or at least as parallel activities. The implementation can begin in a part of the organization or in a data domain, such as product data or customer data. After learning and maturing, the implementation may grow wider. Data models and other Data Architecture artifacts are usually captured within development projects and then standardized and managed by data architects. Therefore, the first projects will have larger portions of Data Architecture work before there are any artifacts to reuse. These early projects could benefit from special architecture funding. The Enterprise Data Architect collaborates with other business and technology architects who share the common goal of improving organizational effectiveness and agility. The business drivers for the overall enterprise architecture also influence Enterprise Data Architecture implementation strategy significantly. Establishing an Enterprise Data Architecture in a solution -oriented culture where new inventions are tried using disruptive technology will require an agile implementation approach. This can include having an outlined subject area model on an overall level while participating on a detail level in agile sprints. Thus, the Enterprise Data Architecture will evolve incrementally. However, this agile approach needs to ensure that data architects are engaged early in development initiatives, as these evolve rapidly in an inventive culture. Having a quality driver for enterprise architecture may force some initial Data Architecture work on an enterprise level for planned development projects. Typically, the Enterprise Data Architecture starts with Master Data areas that are in great need for improvements and, once established and accepted, expands to include business event oriented data (i.e., transactional data). This is the traditional implementation approach where Enterprise Data Architects produce blueprints and templates to be used throughout the system landscape, and ensur e compliance using various governance means. 5.1 Readiness Assessment / Risk Assessment Architecture initiation projects expose more risks than other projects, especially during the first attempt within the organization. The most significant risks are: • Lack of management support: Any reorganization of the enterprise during the planned execution of the project will affect the architecture process. For example, new decision makers may question the process and be tempted to withdraw from opportunities for participants to continue the ir work on the Data Architecture. It is by establishing support among management that an architecture process can survive reorganization. Therefore, be certain to enlist into the Data Architecture development process more than one member of top -level management, or at least senior management, who understand the benefits of Data Architecture. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 119 • No proven record of accomplishment : Having a sponsor is essential to the success of the effort, as is his or her confidence in those carrying out the Data Architecture function. Enlist the help of a senior architect colleague to help carry out the most important steps. • Apprehensive sponsor: If the sponsor requires all communication to pass through them, it may be an indication that that person is uncertain of their role, has interests other than the objectives of the Data Architecture process, or is uncertain of the data architect’s capability. Regardless of the reason, the sponsor must allow the project manager and data architect to take the leading roles in the project. Try to establish independence in the workplace, along with the sponsor’s confidence. • Counter -productive executive decisions : It may be the case that although management understands the value of a well -organized Data Architecture, they do not know how to achieve it. Therefore, they may make decisions that counteract the data architect’s efforts. This is not a sign of disloyal management but rather an indication that the data architect needs to communicate more clearly or frequently with management. • Culture shock: Consider how the working culture will change among those who will be affected by the Data Architecture. Try to imagine how easy or difficult it will be for the employees to change their behavior within the organization. • Inexperienced project leader : Make sure that the project manager has experience with Enterprise Data Architecture , particularly if the project has a heavy data component. If this is not the case, encourage the sponsor to change or educate the project manager (Edvinsson, 2013). • Dominance of a one -dimensional view : Sometimes the owner(s) of one business application might tend to dictate their view about the overall enterprise -level Data Architecture (e.g., the owners of an ERP system) at the expense of a more well -balanced, all -inclusive view. 5.2 Organization and Cultural Change The speed with which an organization adopts architectural practices depends on how adaptive its culture is. The nature of design work requires that architects collaborate with developers and other creative thinkers throughout the organization. Often , such people are used to working in their own ways. They may embrace or resist the change required to adopt formal architecture principles and tools. Output -oriented, strategically aligned organizations are in the best position to adopt architectural practices. These organizations are most often goal- oriented, aware of customer and partner challenges, and capable of prioritizing based on common objectives. The ability of an organization to adopt Data Architecture practices depends on several factors: • Cultural receptivity to architectural approach (developing an architecture -friendly culture) • Organizational recognition of data as a business asset, not just an IT concern • Organizational ability to let go of a local perspective and adopt an enterprise perspective on data • Organizational ability to integrate architectural deliverables into project methodology • Level of acceptance of formal Data Governance Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "120 • DMBOK2 • Ability to look holistically at the enterprise, rather than being focused solely on project delivery and IT solutioning (Edvinsson, 2013) 6. Data Architecture Governance Data Architecture activities directly support the alignment and control of data. Data architects often act as business liaisons for Data Governance activities. Therefore, Enterprise Data Architecture and the Data Governance organization have to be well aligned. Ideally, both a data architect and a Data Steward should be assigned to each subject area and even to each entity within a subject area. In addition, business oversight should be aligned with process oversight. Business event subject areas should be aligned with business process governance, as each event entity usually corresponds to a business process. Data Architecture governance activities include: • Overseeing Project s: This includes ensuring that projects comply with required Data Architecture activities, use and improve architectural assets, and implement according to stated architectural standards. • Managing architectural designs, lifecycle, and tools : Architectural designs must be defined, evaluated , and maintained. Enterprise Data Architecture serves as a ‘zoning plan’ for long -term integration. Future state architecture affects project objectives and influences the priority of the projects in the project portfolio. • Defining standards : Setting the rules, guidelines, and specifications for how data is used within the organization. • Creating data -related artifacts : Artifacts that enable compliance with governance directives. 6.1 Metrics Performance metrics on Enterprise Data Architecture reflect the architectural goals: architectural compliance, implementation trends, and business value from Data Architecture. Data Architecture metrics are often monitored annually as part of overall business customer satisfaction with projects. • Architecture standard compliance rate measures how closely projects comply with established Data Architectures and how well projects adhere to processes for engaging with enterprise architecture. Metrics that track project exceptions may also be useful as a means of understanding obstacles to adoption. • Implementation trends track the degree to which enterprise architecture has improved the organization’s ability to implement projects along at least two lines: • Use/reuse/replace/retire measurements : Determine the proportion of new architecture artifacts versus reused, replaced, or retired artifacts. • Project execution efficiency measurements : These measure lead times for projects and their resource costs for delivery improvements with reusable artifacts and guiding artifacts. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA ARCHITECTURE • 121 • Business value measurements track progress toward expected business effects and benefits. • Business agility improvements: Measurements that account for the benefits of lifecycle improvements or alternative, the cost of delay. • Business quality : Measurements of whether business cases are fulfilled as intended; measuring whether projects actually deliver changes that lead to business improvements based on newly created or integrated data. • Business operation quality : Measurements of improved efficiency. Examples include improved accuracy, and reducing the time and expense of correcting mistakes due to data errors. • Business environment improvements: Examples include improved client retention rate related to reducing data errors, and reduced incidence of remarks from authorities on submitted reports. 7. Works Cited / Recommended Ahlemann, Frederik, Eric Stettiner, Marcus Messerschmidt, and Christine Legner, eds. Strategic Enterprise Architecture Management: Challenges, Best Practices, and Future Developments . Springer, 2012. Print. Management for Professionals. Bernard, Scott A. An Introduction to Enterprise Architecture. 2nd ed. Authorhouse, 2005. Print. Brackett, Michael H. Data Sharing Using a Common Data Architecture . John Wiley and Sons, 1994. Print. Carbone, Jane. IT Architecture Toolkit. Prentice Hall, 2004. Print. Cook, Melissa. Building Enterprise Information Architectures: Re -Engineering Information Systems . Prentice Hall, 1996. Print. Edvinsson, Hakan and Lottie Aderinne. Enterprise Architecture Made Simple Using the Ready, Set, Go Approach to Achieving Information Centricity. Technics Publications, LCC, 2013. Print. Executive Office of the President of the United States. The Common Approach to Federal Enterprise Architecture. whitehouse.gov, 2012. Web. Fong, Joseph. Information Systems Reengineering and Integration. 2nd ed. Springer, 2006. Print. Gane, Chris and Trish Sarson. Structured Systems Analysis: Tools and Techniques . Prentice Hall, 1979. Print. Hagan, Paula J., ed. EABOK: Guide to the (Evolving) Enterprise Architecture Body of Knowledge. mitre.org MITRE Corporation, 2004. Web. Harrison, Rachel. TOGAF Version 8.1.1 Enterprise Edition - Study Guide . The Open Group. 2nd ed. Van Haren Publishing, 2007. Print. TOGAF. Hoberman, Steve, Donna Burbank, and Chris Bradley. Data Modeling for the Business: A Handbook for Aligning the Business with IT using High -Level Data Models . Technics Publications, LLC, 2009. Print. Take It with You Guides. Hoberman, Steve. Data Modeling Made Simple: A Practical Guide for Business and Information Technology Professionals . 2nd ed. Technics Publications, LLC, 2009. Print. Hoogervorst , Jan A. P. Enterprise Governance and Enterprise Engineering . Springer, 2009. Print. The Enterprise Engineering Ser. ISO (website). http://bit.ly/2sTp2rA , http://bit.ly/2ri8Gqk . Inmon, W. H., John A. Zachman, and Jonathan G. Geiger. Data Stores, Data Warehousing and the Zachman Framework: Managing Enterprise Knowledge . McGraw -Hill, 1997. Print. Lankhorst, Marc. Enterprise Architecture at Work: Modeling, Communication and Analysis. Springer, 2005. Print. Martin, James and Joe Leben. Strategic Information Planning Methodologies , 2nd ed. Prentice Hall, 1989. Print. Osterwalder, Alexander and Yves Pigneur. Business Model Generation: A Handbook for Visionaries, Game Changers, and Challengers . Wiley, 2010. Print. Perks, Col and Tony Beveridge. Guide to Enterprise IT Architecture. Springer, 2003. Print. Springer Professional Computing. Poole, John, Dan Chang, Douglas Tolbert, and David Mellor. Common Warehouse Metamodel . Wiley, 2001. Print. OMG (Book 17) . Radhakrishnan, Rakesh. Identity and Security: A Common Architecture and Framework For SOA and Network Convergence. futuretext, 2007. Print. Ross, Jeanne W., Peter Weill, and David Robertson . Enterprise Architecture As Strategy: Creating a Foundation For Business Execution . Harvard Business School Press, 2006. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "122 • DMBOK2 Schekkerman, Jaap. How to Survive in the Jungle of Enterprise Architecture Frameworks: Creating or Choosing an Enterprise Architecture Framework . Trafford Publishing, 2006. Print. Spewak, Steven and Steven C. Hill. Enterprise Architecture Planning: Developing a Blueprint for Data, Applications, and Technology . 2nd ed. A Wiley -QED Publication, 1993. Print. Ulrich, William M. and Philip Newcomb. Information Systems Transformation: Architecture- Driven Modernization Case Studies . Morgan Kaufmann, 2010. Print. The MK/OMG Press. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "123 CHAPTER 5 Data Modeling and Design 1. Introduction ata modeling is the process of discovering, analyzing, and scoping data requirements, and then representing and communicating these data requirements in a precise form called the data model . Data modeling is a critical component of data management. The modeling process requires that organizations discover and document how their data fits together. The modeling process itself designs how data fits together (Simsion, 2013). Data models depict and enable an organization to understand its data assets. There are a number of different schemes used to represent data. The six most commonly used schemes are: Relational, Dimensional, Object -Oriented, Fact-Based, Time -Based, and NoSQL . Models of these schemes exist at three levels of detail: conceptual, logical, and physical. Each model contains a set of components. Examples Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "124 • DMBOK2 of components are entities, relationships, facts, keys, and attributes. Once a model is built, it needs to be reviewed and once approved, maintained. Figure 28 Context Diagram: Data Modeling and Design Definition : Data modeling is the process of discovering, analyzing, and scoping data requirements, and then representing and communicating these data requirements in a precise form called the data model. This process is iterative and involves conceptual, logical, and physical models. Goals : 1.T o confirm and document an understanding of different perspectives, which leads to applications that more closely align with current and future business requirements. 2.T o understand how data fits together and creates a foundation to successfully complete broad -scoped initiatives such as master data management and data governance. 3.T o lower support costs and reduce the costs of building new applications thanks to increased reusability opportunities. Activities : 1. Plan for Data Modeling (P) 2. Build the Data Model (D) 1. Conceptual Data Modeling 2. Logical Data Modeling 3. Physical Data Modeling 3. Review the Data Models (C) 4. Maintain the Data Models (O)Inputs : •Enterprise Data Model •Existing Data Models and Databases •Data Standards •Data Sets •Data Requirements •Data Architecture •Enterprise Taxonomy Deliverables : •Diagrams (different levels, schemes, and notations) •Definitions •Issues and outstanding questions •Lineage Suppliers : •Business Professionals •Business Analysts •Data Architects •Database Administrators and Developers •Subject Matter Experts •Data Stewards •Metadata AdministratorsConsumers : •Business Analysts •Data Modelers •Database Administrators and Developers •Software Developers •Data Stewards •Data Quality Analysts •Data ConsumersParticipants : •Business Analysts •Data Analysts •Data Modelers •Data Consumers T echniques : •Naming conventions •Forward/reverse engineering •Approach determinationT ools : •Data modeling tools •Lineage tools •Data profiling tools •Metadata repositories •Data model patterns •Industry data modelsMetrics : •Data model scorecard (P) Planning, (C) Control, (D) Development, (O) OperationsData Modeling and Design Business Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 125 Data models comprise and contain Metadata essential to data consumers. Much of this Metadata uncovered during the data modeling process is essential to other data management functions. For example, definitions for Data Governance and lineage for data warehousing and analytics. This chapter will describe the purpose of data models, the essential concepts and common vocabulary used in data modeling, and data modeling goals and principles. It will use a set of examples from data related to education to illustrate how data models wo rk and to show differences between them. 1.1 Business Drivers Data m odels are critical to effective management of data. They: • Provide a common vocabulary around data • Capture and document explicit knowledge about an organization’s data and systems • Serve as a primary communications tool during projects • Provide the starting point for customization, integration, or even replacement of an application 1.2 Goals and Principles The goal of data modeling is to confirm and document understanding of different perspectives, which leads to applications that more closely align with current and future business requirements, and creates a foundation to successfully complete broad -scoped initiatives such as Maste r Data Management and Data Governance function s. Proper data modeling leads to lower support costs and increases the reusability opportunities for future initiatives, thereby reducing the costs of building new applications. Data models are an important for m of Metadata. Confirming and documenting understanding of different perspectives facilitates: • Formalization: A data model documents a concise definition of data structures and relationships. It enables assessment of how data is affected by implemented business rules, for current as- is states or desired target states. Formal definition imposes a disciplined structure to data that reduces the possibility of data anomalies occurring when accessing and persisting data. By illustrating the structures and relationships in the data, a data model makes data easier to consume. • Scope definition: A data model can help explain the boundaries for data context and implementation of purchased application packages, projects, initiatives, or existing systems. • Knowledge retention/documentation : A data model can preserve corporate memory regarding a system or project by capturing knowledge in an explicit form. It serves as documentation for future projects to use as the as -is version. Data models help us understand an organization or business ar ea, an existing application, or the impact of modifying an existing data structure. The data model becomes a reusable map to help business professionals, project managers, analysts, modelers , and developers understand data structure within the environment. In much the same way as the mapmaker learned and documented a geographic landscape for others to use for navigation, the modeler enables others to understand an information landscape (Hoberman, 2009). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "126 • DMBOK2 1.3 Essential Concepts This section will explain the different types of data that can be modeled, the component pieces of data models, the types of data models that can be developed, and the reasons for choosing different types in different situations. This set of definitions is extensive, in part, because data modeling itself is about the process of definition. It is important to understand the vocabulary that supports the practice. 1.3.1 Data Modeling and Data Models Data modeling is most frequently performed in the context of systems development and maintenance efforts, known as the system development lifecycle (SDLC). Data modeling can also be performed for broad -scoped initiatives (e.g., Business and Data Architecture, Master Data Management, and Data Governance initiatives) where the immediate result is not a database but an understanding of organizational data. A model is a representation of something that exists or a pattern for something to be made. A model can contain one or more diagrams. Model diagrams make use of standard symbols that allow one to understand content. Maps, organization charts, and building blueprints are examples of models in use every day. A data model describes an organization’s data as the organization understands it, or as the organization wants it to be. A data model contains a set of symbols with text labels that attempts visually to represent data requirements as communicated to the da ta modeler, for a specific set of data that can range in size from small, for a project, to large, for an organization. The model is a form of documentation for data requirements and data definitions resulting from the modeling process. Data models are the main medium used to communicate data requirements from business to IT and within IT from analysts, modelers, and architects, to database designers and developers. 1.3.2 Types of Data that are Modeled Four main types of data can be modeled (Edvinsson, 2013). The types of data being modeled in any given organization reflect the priorities of the organization or the project that requires a data model: • Category information: Data used to classify and assign types to things. For example, customers classified by market categories or business sectors; products classified by color, model, size, etc.; orders classified by whether they are open or closed. • Resource information: Basic profiles of resources needed to conduct operational processes such as Product, Customer, Supplier, Facility, Organization, and Account. Among IT professionals, resource entities are sometimes referred to as Reference Data. • Business event information : Data created while operational processes are in progress. Examples include Customer Orders, Supplier Invoices, Cash Withdrawal, and Business Meetings. Among IT professionals, event entities are sometimes referred to as transactional business data. • Detail transaction information: Detailed transaction information is often produced through point- of- sale systems (either in stores or online). It is also produced through social media systems, other Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 127 Internet interactions (clickstream, etc.), and by sensors in machines, which can be parts of vessels and vehicles, industrial components, or personal devices (GPS, RFID, Wi -Fi, etc.). This type of detailed information can be aggregated, used to derive othe r data, and analyzed for trends, similar to how the business information events are used. This type of data (large volume and/or rapidly changing) is usually referred to as Big Data. These types refer to ‘data at rest’. Data in motion can also be modeled, for example, in schemes for systems, including protocols, and schemes for messaging and event -based systems. 1.3.3 Data Model Components As will be discussed later in the chapter, different types of data models represent data through different conventions (See Section 1.3.4 ). However, most data models contain the same basic building blocks: entities, relationships, attributes, and domains. 1.3.3.1 Entity Outside of data modeling, the definition of entity is a thing that exists separate from other things. Within data modeling, an entity is a thing about which an organization collects information. Entities are sometimes referred to as the nouns of an organization. An entity can be thought of as the answer to a fundamental question – who, what, when, where, why, or how – or to a combination of these questions (see Chapter 4 ). Table 7 defines and gives examples of commonly used entity categories (Hoberman , 2009). Table 7 Commonly U sed Entity Categories Category Definition Examples Who Person or organization of interest. That is, Who is important to the business? Often a ‘who’ is associated with a party generalization, or role such as Customer or Vendor. Persons or organizations can have multiple roles or be included in multiple parties. Employee, Patient, Player, Suspect, Customer, Vendor, Student, Passenger, Competitor, Author What Product or service of interest to the enterprise. It often refers to what the organization makes or what service it provides. That is, What is important to the business? Attributes for categories, types, etc. , are very important here. Product, Service, Raw Material, Finished Good, Course, Song, Photograph, Book When Calendar or time interval of interest to the enterprise. That is, When is the business in operation? Time, Date, Month, Quarter, Year, Calendar, Semester, Fiscal Period, Minute, Departure Time Where Location of interest to the enterprise. Location can refer to actual places as well as electronic places. That is, Where is business conducted? Mailing Address, Distribution Point, Website URL, IP Address Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "128 • DMBOK2 Category Definition Examples Why Event or transaction of interest to the enterprise. These events keep the business afloat. That is, Why is the business in business? Order, Return, Complaint, Withdrawal, Deposit, Compliment, Inquiry, Trade, Claim How Documentation of the event of interest to the enterprise. Documents provide the evidence that the events occurred, such as a Purchase Order recording an Order event. That is, How do we know that an event occurred? Invoice, Contract, Agreement, Account, Purchase Order, Speeding Ticket, Packing Slip, Trade Confirmation Measure ment Counts, sums, etc. of the other categories (what, where) at or over points in time (when). Sales, Item Count, Payments, Balance 1.3.3.1.1 Entity Aliases The generic term entity can go by other names. The most common is entity -type, as a type of something is being represented (e.g., Jane is of type Employee), therefore, Jane is the entity and Employee is the entity type. However, in widespread use today is using the term entity for Employee and entity instance for Jane. Table 8 Entity, Entity Type, and Entity Instance Usage Entity Entity Type Entity Instance Common Use Jane Employee Recommended Use Employee Jane Entity instances are the occurrences or values of a particular entity. The entity Student may have multiple student instances, with names Bob Jones, Joe Jackson, Jane Smith, and so forth. The entity Course can have instances of Data Modeling Fundamentals, Advanced Geology, and English Literature in the 17th Century. Entity aliases can also vary based on scheme. (Schemes will be discussed in Section 1.3.4.) In relational schemes, the term entity is often used . In dimensional schemes, the terms dimension and fact table are often used . In object -oriented schemes , the terms class or object are often used . In time -based schemes , the terms hub, satellite, and link are often used . In NoSQL schemes, terms such as document or node are used. Entity aliases can also vary based on the level of detail. (The three levels of detail will be discussed in Section 1.3.5 .) An entity at the conceptual level can be called a concept or term , an entity at the logical level is called an entity (or a different term depending on the scheme), and at the physical level the terms vary based on database technology, the most common term being table . 1.3.3.1.2 Graphic Representation of Entities In data models, entities are generally depicted as rectangles (or rectangles with rounded edges) with their names inside, such as in Figure 29, where there are three entities: Student, Course, and Instructor . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 129 Figure 29 Entities 1.3.3.1.3 Definition of Entities Entity definitions are essential contributors to the business value of any data model. They are core Metadat a. High quality definitions clarify the meaning of business vocabulary and provide rigor to the business rules governing entity relationships. They assist business and IT professionals in making intelligent business and application design decisions. High quality data definitions exhibit three essential characteristics: • Clarity : The definition should be easy to read and grasp. Simple, well -written sentences without obscure acronyms or unexplained ambiguous terms such as sometimes or normally . • Accuracy: The definition is a precise and correct description of the entity. Definitions should be reviewed by experts in the relevant business areas to ensure that they are accurate. • Completeness : All of the parts of the definition are present. For example, in defining a code, examples of the code values are included. In defining an identifier, the scope of uniqueness in included in the definition. 1.3.3.2 Relationship A relationship is an association between entities (Chen, 1976). A relationship captures the high- level interactions between conceptual entities, the detailed interactions between logical entities, and the constraints between physical entities. 1.3.3.2.1 Relationship Aliases The generic term relationship can go by other names. Relationship aliases can vary based on scheme. In relational schemes, the term relationship is often used . In dimensional schemes, the term navigation path is often used . In NoSQL schemes , terms such as edge or link are used, for example. Relationship aliases can also vary based on the level of detail. A relationship at the conceptual and logical levels is called a relationship . However, a relationship at the physical level may be called by other names, such as constraint or reference, depending on the database technology. 1.3.3.2.2 Graphic Representation of Relationships Relationships are shown as lines on the data modeling diagram. See Figure 30 for an Information Engineering example. Student Course Instructor Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "130 • DMBOK2 Figure 30 Relationships In this example, the relationship between Student and Course captures the rule that a Student may attend Courses. The relationship between Instructor and Course captures the rule that an Instructor may teach Courses. The symbols on the line (called cardinality) capture the rules in a precise syntax. (These will be explained in Section 1.3.3.2.3 .) A relationship is represented through foreign keys in a relational database and through alternative methods for NoSQL databases , such as through edges or links. 1.3.3.2.3 Relationship Cardinality In a relationship between two entities, cardinality captures how many of one entity (entity instances) participate in the relationship with how many of the other entity. Cardinality is represented by the symbols that appear on both ends of a relationship line. Data rules are specified and enforced through cardinality. Without cardinality, the most one can say about a relationship is that two entities are connected in some way. For cardinality, the choices are simple: zero, one, or many. Each side of a relationship can have any combination of zero, one, or many (‘many’ means could be more than ‘one’). Specifying zero or one allows us to capture whether an entity instance is required in a relationship. Specifying one or many allows us to capture how many of a particular instance participates in a given relationship. These cardinality symbols are illustrated in the following information engineering example of Student and Course . Figure 31 Cardinality Symbols The business rules are: • Each Student may attend one or many Courses. • Each Course may be attended by one or many Students . 1.3.3.2.4 Arity of Relationships The number of entities in a relationship is the ‘arity’ of the relationship. The most common are unary, binary, and ternary relationships. Instructor Student CourseTeach Attend Course StudentAttend Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 131 1.3.3.2.4.1 Unary (Recursive) Relationship A unary (also known as a recursive or self -referencing) relationship involves only one entity. A one -to-many recursive relationship describes a hierarchy, whereas a many -to-many relationship describes a network or graph. In a hierarchy, an entity instance has at most one parent (or higher -level entity). In relational modeling, child entities are on the many side of the relationship, with parent entities on the one side of the relationship. In a network, an entity instance can have more than one parent. For example, a Course can require prerequisites. If to take the Biology Workshop, one would first need to complete the Biology Lecture, the Biology Lecture is the prerequisite for the Biology Workshop. In the following relational data models, which use information engineering notation, one can model this recur sive relationship a s either a hierarchy or network: Figure 32 Unary Relationship - Hierarchy Figure 33 Unary Relationship - Network This first example ( Figure 32) is a hierarchy and the second (Figure 33) is a network. In the first example, the Biology Workshop requires first taking the Biology Lecture and the Chemistry Lecture. Once the Biolo gy Lecture is chosen as the pre requisite for the Biology Workshop, the Biology Lecture cannot be the prerequisite for any other courses. The second example allows the Biology Lecture to be the prerequisite for other courses as well. 1.3.3.2.4.2 Binary Relationship An arity of two is also known as binary. A binary relationship, the most common on a traditional data model diagram, involves two entities. Figure 34, a UML class diagram, shows that both Student and Course are entities participating in a binary relationship. Figure 34 Binary Relationship CourseRequire as a pre-requisite CourseRequire as a pre-requisite Student Course -Attend *-Be attended by * Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "132 • DMBOK2 1.3.3.2.4.3 Ternary Relationship An arity of three, known as ternary, is a relationship that includes three entities. An example in fact-based modeling (object- role notation) appears in Figure 35. Here Student can register for a particular Course in a given Semester . Figure 35 Ternary Relationship 1.3.3.2.5 Foreign K ey A foreign key is used in physical and sometimes logical relational data modeling schemes to represent a relationship. A foreign key may be created implicitly when a relationship is defined between two entities, depending on the database technology or data modeling tool, and whether the two entities involved have mutual dependencies. In the example shown in Figure 36, Registration contains two foreign keys, Student Number from Student and Course Code from Course . Foreign keys appear in the entity on the many side of the relationship, often called the child entity. Student and Course are parent entities and Registration is the child entity. Figure 36 Foreign Keys 1.3.3.3 Attribute An attribute is a property that identifies, describes, or measures an entity. Attributes may have domains, which will be discussed in Section 1.3.3.4 . The physical correspondent of an attribute in an entity is a column, field, tag, or node in a table, view, document, graph, or file. 1.3.3.3.1 Graphic Representation of Attributes In data models, attributes are generally depicted as a list within the entity rectangle, as shown in Figure 37, where the attributes of the entity Student include Student Number , Student First Name , Student Last Name , and Student Birth Date . Semester Course Student Registration Student Number (FK) Course Code (FK) Registration DateCourse Course Code Course NameStudent Student Number Student First Name Student Last Name Student Birth DateHave registeredRegister Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 133 Figure 37 Attributes 1.3.3.3.2 Identifiers An identifier (also called a key) is a set of one or more attributes that uniquely defines an instance of an entity. This section defines types of keys by construction (simple, compound, composite, surrogate) and function (candidate, primary, alternate). 1.3.3.3.2.1 Construction -type K eys A simple key is one attribute that uniquely identifies an entity instance. Universal Product Codes (UPCs) and Vehicle Identification Numbers (VINs) are examples of simple keys. A surrogate key is also an example of a simple key. A surrogate key is a unique identifier for a table. Often a counter and always system -generated without intelligence, a surrogate key is an integer whose meaning is unrelated to its face value. (In other words, a Month Identifier of 1 cannot be assumed to represent January.) Surrogate keys serve technical functions and should not be visible to end users of a database. They remain behind the scenes to help maintain uniqueness, allow for more efficient navigation across st ructures, and facilitate integration across applications. A composite key is a candidate key that consists of two or more attributes that together uniquely identify an entity. A compound key is a composite key for which each attribute makes up the key is a foreign key in its own right. 1.3.3.3.2.2 Function -type K eys A super key is any set of attributes that uniquely identify an entity instance. A candidate key is a minimal set of one or more attributes (i.e., a simple or compound key) that identifies the entity instance to which it belongs. Minimal means that no subset of the candidate key uniquely identifies the entity instance. An entity may have multiple can didate keys. Examples of candidate keys for a customer entity are email address, cell phone number, and customer account number. Candidate keys can be business keys (sometimes called natural keys ). A business key is one or more attributes that a business professional would use to retrieve a single entity instance. Business keys and surrogate keys are mutually exclusive. A primary key is the candidate key that is chosen to be the unique identifier for an entity. Even though an entity may contain more than one candidate key, only one candidate key can serve as the primary key for an entity. An alternate key is a candidate key that although unique, was not chosen as the primary key. An alternate key can still be used to find specific entity instances. Often , the primary key is a surrogate key and the alternate keys are business keys. Student Student Number Student First Name Student Last Name Student Birth Date Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "134 • DMBOK2 1.3.3.3.2.3 Identifying vs. Non- Identifying Relationships An independent entity is one where the primary key contains only attributes that belong to that entity. A dependent entity is one where the primary key contains at least one attribute from another entity. In relational schemes, most notations depict independent entities on the data modeling diagram as rectangles and dependent entities as rectangles with rounded corners. In the student example shown in Figure 38, Student and Course are independent entities and Registration is a dependent entity. Figure 38 Dependent and Independent Entity Dependent entities have at least one identifying relationship. An identifying relationship is one where the primary key of the parent (the entity on the one side of the relationship) is migrated as a foreign key to the child’s primary key, as can be seen w ith the relationship from Student to Registration , and from Course to Registration . In non- identifying relationships, the primary key of the parent is migrated as a non -primary foreign key attribute to the child. 1.3.3.4 Domain In data modeling, a domain is the complete set of possible values that an attribute can be assigned. A domain may be articulated in different ways (see points at the end of this section). A domain provides a means of standardizing the characteristics of the attributes. For example, the domain Date , which contains all possible valid dates, can be assigned to any date attribute in a logical data model or date columns/fields in a physical data model, such as: • EmployeeHireDate • OrderEntryDate • ClaimSubmitDate • CourseStartDate All values inside the domain are valid values. Those outside the domain are referred to as invalid values. An attribute should not contain values outside of its assigned domain. EmployeePreferredPronoun , for example, may be limited to the domain of He/Him, She/Her, They/Them. The domain for EmployeeHireDate may be defined simply as valid dates. Under this rule, the domain for EmployeeHireDate does not include February 30 of any year . One can restrict a domain with additional rules, called constraints . Rules can relate to format, logic, or both. For example, by restricting the EmployeeHireDate domain to dates earlier than today’s date, one would eliminate March 10, 2050 from the domain of valid values, even though it is a valid date. EmployeeHireDate could also be restricted to days in a typical workweek (e.g., dates that fall on a Monday, Tuesday, Wednesday, Thursday, or Friday). Registration Student Number (FK) Course Code (FK) Registration DateCourse Course Code Course NameStudent Student Number Student First Name Student Last Name Student Birth DateHave registeredRegister Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 135 Domains can be defined in different ways. • Data Type : Domains that specify the standard types of data one can have in an attribute assigned to that domain. For example, Integer, Character(30), and Date are all data type domains. • Data Format : Domains that use patterns including templates and masks, such as are found in postal codes and phone numbers, and character limitations (alphanumeric only, alphanumeric with certain special characters allowed, etc.) to define valid values. • List: Domains that contain a finite set of values. These are familiar to many people from functionality like dropdown lists. For example, the list domain for OrderStatusCode can restrict values to only {Open, Shipped, Closed, Returned}. • Range : Domains that allow all values of the same data type that are between one or more minimum and/or maximum values. Some ranges can be open-ended. For example, OrderDeliveryDate must be between OrderDate and three months in the future. • Rule-based : Domains defined by the rules that values must comply with in order to be valid. These include rules comparing values to calculated values or other attribute values in a relation or set. For example, ItemPrice must be greater than ItemCost . 1.3.4 Data Modeling Schemes The six most common schemes used to represent data are: Relational, Dimensional, Object -Oriented, Fact- Based, Time -Based, and NoSQL . Each scheme uses specific diagramming notations (see Table 9). Table 9 Modeling Schemes and Notations Scheme Sample Notations Relational Information Engineering (IE) Integration Definition for Information Modeling (IDEF1X) Barker Notation Chen Dimensional Dimensional Object -Oriented Unified Modeling Language (UML) Fact -Based Object Role Modeling (ORM or ORM2) Fully Communication Oriented Modeling (FCO -IM) Time -Based Data Vault Anchor Modeling NoSQL Document Column Graph Key-Value This section will briefly explain each of these schemes and notations. The use of schemes depends in part on the database being built, as some are suited to particular technologies, as shown in Table 10. For the relational scheme, all three levels of models can be built for RDBMS, but only conceptual and logical models can be built for the other types of databases. This is true for the fact -based scheme as well. For the dimensional scheme, all three levels of models can be built for both RDBMS and MDBMS databases. The object - Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "136 • DMBOK2 oriented scheme works well for RDBMS and object databases. The time -based scheme is a physical data modeling technique primarily for data warehouses in a RDBMS environment. The NoSQL scheme is heavily dependent on the underlying database structure (document, column, graph, or key -value), and is therefore a physical data modeling technique. Table 10 illustrates several important points including that even with a non - traditional database such as one that is document- based, a relational CDM and LDM can be built followed by a document PDM. Table 10 Scheme to Database Cross Reference Scheme Relational Database Management System (RDBMS) Multidimensional Database Management System (MDBMS) Object Databases Document Column Graph Key- Value Relational CDM LDM PDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM Dimensional CDM LDM PDM CDM LDM PDM Object -Oriented CDM LDM PDM CDM LDM PDM Fact -Based CDM LDM PDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM CDM LDM Time -Based PDM NoSQL PDM PDM PDM PDM PDM 1.3.4.1 Relational First articulated by Dr. Edward Codd in 1970, relational theory provides a systematic way to organize data to reflect its meaning (Codd, 1970). This approach had the additional effect of reducing redundancy in data storage. Codd’s insight was that data could most effectively be managed in terms of two -dimensional relations . The term relation was derived from the mathematics (set theory) upon which his approach was based. (See Chapter 6 .) The design objectives for the relational model are to have an exact expression of business data and to have one fact in one place (the removal of redundancy). Relational modeling is ideal for the design of operational systems, which require entering information quickly and having it stored accurately (Hay, 2011). There are several different kinds of notation to express the association between entities in relational modeling, including Information Engineering (IE), Integration Definition for Information Modeling (IDEF1X), Barker Notation, and Chen Notation. The most common form is IE syntax, with its familiar tridents or ‘crow’s feet’ to depict cardinality. (See Figure 39.) Figure 39 IE Notation Course StudentAttend Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 137 1.3.4.2 Dimensional The concept of dimensional modeling started from a joint research project conducted by General Mills and Dartmouth College in the 1960’s.33 In dimensional models, data is structured to optimize the query and analysis of large amounts of data. In contrast, operational systems that support transaction processing are optimized for fast processing of individual transactions. Dimensional data models capture business questions focused on a particular business process. The process being measured on the dimensional model in Figure 40 is Admissions. Admissions can be viewed by the Zone the student is from, School Name, Semester, and whether the student is receiving financial aid. Navigation can be made from Zone up to Region and Country, from Semester up to Year, and from School Name up to School Level. Figure 40 Axis Notation for Dimensional Models The diagramming notation used to build this model – the ‘axis notation’ – can be a very effective communication tool for those who prefer not to read traditional data modeling syntax. Both the relational and dimensional conceptual data models can be based on the same business process (as in this example with Admissions). The difference is in the meaning of the relationships, where i n the relational model , the relationship lines capture business rules, and i n the dimensional model, they capture the navigation paths needed to answer business questions. 1.3.4.2.1 Fact Tables Within a dimensional scheme, the rows of a fact table correspond to particular measurements and are numeric, such as amounts, quantities, or counts. Some measurements are the results of algorithms, in which c ase 33 http://bit.ly/2tsSP7w. AdmissionsZoneCountry Region Financial AidSemester Year Yes/NoSchoolGeography Calendar Name Level Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "138 • DMBOK2 Metadata is critical to proper understanding and usage. Fact tables take up the most space in the database (90% is a reasonable rule of thumb), and tend to have large numbers of rows. 1.3.4.2.2 Dimension Tables Dimension tables represent the important objects of the business and contain mostly textual descriptions. Dimensions serve as the primary source for ‘query by’ or ‘report by’ constraints, by acting as the entry points or links into the fact tables. Dimensions are typically highly denormalized and typically account for about 10% of the total data. Dimensions must have a unique identifier for each row. The two main approaches to identifying keys for dimension tables are surrogate keys and natural keys. Dimensions also have attributes that change at different rates. Slowly changing dimensions (SCDs) manage changes based on the rate and type of change. The three main types of change are sometimes known by ORC. • Overwrite (Type 1) : The new value overwrites the old value in place. • New Row (Type 2) : The new values are written in a new row, and the old row is marked as not current. • New Column (Type 3) : Multiple instances of a value are listed in columns on the same row, and a new value means writing the values in the series one spot down to make space at the front for the new value. The last value is discarded. 1.3.4.2.3 Snowflaking Snowflaking is the term given to normalizing the flat, single -table, dimensional structure in a star schema into the respective component hierarchical or network structures. 1.3.4.2.4 Grain The term grain stands for the meaning or description of a single row of data in a fact table; this is the most detail any row will have. Defining the grain of a fact table is one of the key steps in dimensional design. For example, if a dimensional model is measuring th e student registration process, the grain may be student, day, and class. 1.3.4.2.5 Conformed Dimensions Conformed dimensions are built with the entire organization in mind instead of just a particular project; this allows these dimensions to be shared across dimensional models, due to containing consistent terminology and values. For example, if Calendar is a conformed dimension, a dimensional model built to count student applicants by Semester will contain the same values and definition of Semester as a dimensional model built to count student graduates. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 139 1.3.4.2.6 Conformed Facts Conformed facts use standardized definitions of terms across individual marts. Different business users may use the same term in different ways. ‘Customer additions’ may be different from ‘gross additions’ or ‘adjusted additions.’ Developers need to be keenly aware of things that may be named the same but actually represent different concepts across organizations or , conversely , things that are named differently but are actually the same concept across organizations. 1.3.4.3 Object -Oriented (UML) The Unified Modeling Language (UML) is a graphical language for modeling software. The UML has a variety of notations of which one (the class model) concerns databases. The UML class model specifies classes (entity types) and their relationship types (Blah a, 2013). Student Stdntno : integer Strtdt : date Prgm : text ExpctGraddt : date ActlGr ad dt : dateClass Name Attributes Operations Figure 41 UML Class Model Figure 41 illustrates the characteristics of a UML Class Model: • A Class diagram resembles an ER diagram except that the Operations or Methods section is not present in ER. • In ER, the closest equivalent to Operations would be Stored Procedures. • Attribute types (e.g., Date, Minutes) are expressed in the implementable application code language and not in the physical database implementable terminology. • Default values can be optionally shown in the notation. • Access to data is through the class’ exposed interface. Encapsulation or data hiding is based on a ‘localization effect’. A class and the instances that it maintains are exposed through Operations. The c lass has Operations or Methods (also called its “behavior ”). Class behavior is only loosely connected to business logic because it still needs to be sequenced and timed. In ER terms, the table has stored procedures/triggers. Class Operations can be: • Public: Externally visible • Internally Visible: Visible to children Objects • Private: Hidden In comparison, ER Physical models only offer Public access; all data is equally exposed to processes, queries, or manipulations. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "140 • DMBOK2 1.3.4.4 Fact -Based Modeling (FBM) Fact -Based Modeling, a family of conceptual modeling languages, originated in the late 1970s. These languages are based o n the analysis of natural verbalization (plausible sentences) that might occur in the business domain. Fact -based languages view the world in terms of objects, the facts that relate or characterize those objects, and each role that each object plays in eac h fact. An extensive and powerful constraint system relies on fluent automatic verbalization and automatic checking against the concrete examples. Fact -based models do not use attributes, reducing the need for intuitive or expert judgment by expressing the exact relationships between objects (both entities and values). The most widely used of the FBM variants is Object Role Modeling (ORM) , which was formalized as a first -order logic by Terry Halpin in 1989. 1.3.4.4.1 Object -Role Modeling (ORM or ORM2) Object ‐Role Modeling (ORM) is a model ‐driven engineering approach that starts with typical examples of required information or queries presented in any external formulation familiar to users, and then verbalizes these examples at the conceptual level, in terms of simple facts expressed in a co ntrolled natural language. This language is a restricted version of natural language that is unambiguous, so the semantics are readily grasped by humans; it is also formal, so it can be used to automatically map the st ructures to lower levels for implementation (Halpin, 2015). Figure 42 illustrates an ORM model. Figure 42 ORM Model 1.3.4.4.2 Fully Communication Oriented Modeling (FCO -IM) FCO -IM is a member of the Fact Oriented Modeling family. As a method , it primarily focuses on the communication in the Universe of Discourse. The conceptual information models are made using natural language with concrete examples. Using so called elementary fact expressions, FCO -IM provides extensive support for different ways of communicating similar facts, synonyms, homonyms, generalized object types, and other conceptual modeling aspects. The information model can be transformed in artifacts through standardized algorithms . Figure 43 shows the steps from Fact Expression to Information Grammar to Information Model Diagram . “Student 928465 in semester 202001 enrolled in course Data Modeling. ” Semester Course Student … in … enrolled in … Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 141 Figure 43 FCO -IM Model 1.3.4.5 Time -Based Time -based patterns are used when data values must be associated in chronological order and with specific time values. 1.3.4.5.1 Data Vault The Data Vault is a detail -oriented, time -based, and uniquely linked set of normalized tables that support one or more functional areas of business. It is a hybrid approach, encompassing the best of breed between third normal form (3NF, to be discussed in Section 1.3.6) and star schema. Data Vaults are designed specifically to meet the needs of enterprise data warehouses. There are three types of entities: hubs, links, and satellites. The Data Vault design is focused around the functional areas of business with the hub representing the primary key. The links provide transaction integration between the hubs. The satellites provide the context of the hub primary key (Linstedt, 2012). In Figure 44, Student and Course are hubs, which represent the main concepts within a subject. Attendance is a link, which relates two hubs to each other. Student Contact , Student Characteristics , and Course Description are satellites that provide the descriptive information on the hub concepts and can support varying types of history. Figure 44 Data Vault Model Anchor Modeling is a technique suited for information that changes over time in both structure and content. It provides graphical notation used for conceptual modeling similar to traditional data modeling, with extensions for working with temporal data. Anchor Modeling has four basic modeling concepts: anchors, attributes, ties, and knots. Anchors model entities and events, attributes model properties of anchors, ties model the relationships between anchors, and knots are used to model shared properties, such as states. Student Course Attendance Student CharacteristicsStudent ContactCourse Description Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "142 • DMBOK2 1.3.4.5.2 Anchor Modeling On the anchor model in Figure 45, Student , Course , and Attendance are anchors, the gray diamonds represent ties, and the circles represent attributes. Figure 45 Anchor Model 1.3.4.6 NoSQL NoSQL is a name for the category of databases built on non -relational technology. Some believe that NoSQL is not a good name for what it represents, as it is less about how to query the database (which is where SQL comes in) and more about how the data is store d (which is where relational structures comes in). There are four main types of NoSQL databases: document, key -value, column-oriented, and graph. 1.3.4.6.1 Document Instead of taking a business subject and breaking it up into multiple relational structures, document databases frequently store the business subject in one structure called a document . For example, instead of storing Student , Course , and Registration information in three distinct relational structures, properties from all three will exist in a single document called Registration . 1.3.4.6.2 Key- value Key- value databases allow an application to store its data in only two columns (‘key’ and ‘value’), with the feature of storing both simple (e.g., dates, numbers, codes) and complex information (unformatted text, video, music, documents, photos) stored within the ‘value’ column. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 143 1.3.4.6.3 Column -oriented Out of the four types of NoSQL databases, column -oriented is closest to the RDBMS. Both have a similar way of looking at data as rows and values. The difference, though, is that RDBMSs work with a predefined structure and simple data types, such as amounts and dates, whereas column- oriented databases, such as Cassandra, can work with more complex data types , including unformatted text and imagery. In addition, column -oriented databases store each column in its own structure. 1.3.4.6.4 Graph A graph database is designed for data whose relations are well represented as a set of nodes with an undetermined number of connections between these nodes. Examples where a graph database can work best are social relations (where nodes are people), public transport links (where nodes could be bus or train stations), or roadmaps (where nodes could be street intersections or highway exits). Often requirements lead to traversing the graph to find the shortest routes, nearest neighbors, etc., all of which can be complex and time -consuming to navigate with a traditional RD BMS. Graph databases include Neo4J, Allegro, and Virtuoso. 1.3.5 Data Model Levels of Detail In 1975, the American National Standards Institute’s Standards Planning and Requirements Committee (SPARC) published their three -schema approach to database management. The three key components were: • Conceptual : This embodies the ‘real world’ view of the enterprise being modeled in the database. It represents the current ‘best model’ or ‘way of doing business’ for the enterprise. • External : The various users of the database management system operate on sub -sets of the total enterprise model that are relevant to their particular needs. These subsets are represented as ‘external schemas’. • Internal : The ‘machine view’ of the data is described by the internal schema. This schema describes the stored representation of the enterprise’s information (Hay, 2011). These three levels most commonly translate into the conceptual, logical, and physical levels of detail, respectively. Within projects, conceptual data modeling and logical data modeling are part of requirements planning and analysis activities, while physical data modeling is a design activity. This section provides an overview of conceptual, logical, and physical data modeling. In addition, each level will be illustrated with examples from two schemes: relational and dimensional. 1.3.5.1 Conceptual A conceptual data model captures the high -level data requirements as a collection of related concepts. It contains only the basic and critical business entities within a given realm and function, with a description of each entity and the relationships between entities. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "144 • DMBOK2 For example, if we were to model the relationship between students and a school, as a relational conceptual data model using the IE notation, it might look like Figure 46. Figure 46 Relational Conceptual Model Each School may contain one or many Students , and each Student must come from one School . In addition, each Student may submit one or many Applications , and each Application must be submitted by one Student. The relationship lines capture business rules on a relational data model. For example, Bob the student can attend County High School or Queens College, but cannot attend both when applying to this particular university. In addition, an application must be submitted by a single student, not two and not zero. Recall Figure 40, which is reproduced below as Figure 47. This dimensional conceptual data model using the Axis notation, illustrates concepts related to school: Figure 47 Dimensional Conceptual Model 1.3.5.2 Logical A logical data model is a detailed representation of data requirements, usually in support of a specific usage context, such as application requirements. Logical data models are still independent of any technology or specific implementation constraints. A logical data model often begins as an extension of a conceptual data model. School Application StudentContain Submit AdmissionsZoneCountry Region Financial AidSemester Year Yes/NoSchoolGeography Calendar Name Level Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 145 In a relational logical data model, the conceptual data model is extended by adding attributes. Attributes are assigned to entities by applying the technique of normalization (see Section 1.3.6 ), as shown in Figure 48. There is a very strong relationship between each attribute and the primary key of the entity in which it resides. For instance, School Name has a strong relationship to School Code . For example, each value of a School Code brings back at most one value of a School Name . Figure 48 Relational Logical Data Model A dimensional logical data model is in many cases a fully -attributed perspective of the dimensional conceptual data model, as illustrated in Figure 49. Whereas the logical relational data model captures the business rules of a business process, the logical dimensional captures the business questions to determine the health and performance of a business process. Admissions Count in Figure 49 is the measure that answers the business questions related to Admissions. The entities surrounding the Admissions provide the context to view Admissions Count at different levels of granularity, such as by Semester and Year . 1.3.5.3 Physical A physical data model (PDM) represents a detailed technical solution, often using the logical data model as a starting point and then adapted to work within a set of hardware, software, and network tools. Physical data models are built for a particular tec hnology. Relational DBMSs, for example, should be designed with the specific capabilities of a database management system in mind (e.g., IBM DB2, UDB, Oracle, Teradata, Sybase, Microsoft SQL Server, or Microsoft Access). Student Student Number School Code (FK) Student First Name Student Last Name Student Birth DateApplication Application Number Student Number (FK) Application Submission DateSchool School Code School Name SubmitContain Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "146 • DMBOK2 Figure 49 Dimensional Logical Data Model Figure 50 illustrates a relational physical data model. In this data model, School has been denormalized into the Student entity to accommodate a particular technology. Perhaps whenever a Student is accessed, their school information is as well and , therefore, storing school information with Student is a more performant structure than having two separate structures. Figure 50 Relational Physical Data Model STUDENT STUDENT_NUM STUDENT_FIRST_NAM STUDENT_LAST_NAM STUDENT_BIRTH_DT SCHOOL_CD SCHOOL_NAMAPPLICATION APPLICATION_NUM STUDENT_NUM (FK) APPLICATION_SUBMISSION_DTSubmit Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 147 Because the physical data model accommodates technology limitations, structures are often combined (denormalized) to improve retrieval performance, as shown in this example with Student and School . Figure 51 illustrates a dimensional physical data model (usually a star schema, meaning there is one structure for each dimension). Similar to the relational physical data model, this structure has been modified from its logical counterpart to work with a particular technology to ensure business questions can be answered with simplicity and speed. 1.3.5.3.1 Canonical A variant of a physical scheme is a Canonical Model, used for data in motion between systems. This model describes the structure of data being passed between systems as packets or messages. When sending data through web services, an Enterprise Service Bus (ESB), or through Enterprise Application Integration (EAI), the canonical model describes what data structure the sending service and any receiving services should use. These structures should be designed to be as generic as possible to enable re -use and simplify interface requirements. This structure may only be instantiated as a buffer or queue structure on an intermediary messaging system (middleware) to hold message contents temporarily. Figure 51 Dimensional Physical Data Model Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "148 • DMBOK2 1.3.5.3.2 Views A view is a virtual table. Views provide a means to look at data from one or many tables that contain or reference the actual attributes. A standard view runs SQL to retrieve data at the point when an attribute in the view is requested. An instantiated (of ten called ‘materialized’) view runs at a predetermined time. Views are used to simplify queries, control data access, and rename columns, without the redundancy and loss of referential integrity due to denormalization. 1.3.5.3.3 Partitioning Partitioning refers to the process of splitting a table. It is performed to facilitate archiving and to improve retrieval performance. Partitioning can be either vertical (separating groups of columns) or horizontal (separating groups of rows). • Vertically split : To reduce query sets, create subset tables that contain subsets of columns. For example, split a customer table in two based on whether the fields are mostly static or mostly volatile (to improve load / index performance), or based on whether the fields are commonly or uncommonly included in queries (to improve table scan performance). • Horizontally split : To reduce query sets, create subset tables using the value of a column as the differentiator. For example, create regional customer tables that contain only customers in a specific region. 1.3.5.3.4 Denormalization Denormalization is the deliberate transformation of normalized logical data model entities into physical tables with redundant or duplicate data structures. In other words, denormalization intentionally puts one attribute in multiple places. There are several reasons to d enormalize data. The first is to improve performance by: • Combining data from multiple other tables in advance to avoid costly run -time joins • Creating smaller, pre -filtered copies of data to reduce costly run -time calculations and/or table scans of large tables • Pre-calculating and storing costly data calculations to avoid run -time system resource competition Denormalization can also be used to enforce user security by segregating data into multiple views or copies of tables according to access needs. This process does introduce a risk of data errors due to duplication. Therefore, denormalization is frequently chosen if structures such as views and partitions fall short in producing an efficient physical design. It is good practice to implement Data Quality checks to ensure that the copies of the attributes are correctly stored. In general, denormalize only to improve database query performance or to facilitate enforcement of user security. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 149 Although the term denormalization is used in this section, the process does not apply just to relational data models. For example, one can denormalize in a document database, but it would be called something different – such as embedding . In dimensional data modeling, denormalization is called collapsing or combining . If each dimension is collapsed into a single structure, the resulting data model is called a Star Schema (see Figure 51). If the dimensions are not collapsed, the resulting data model is called a Snowflake (See Figure 49). 1.3.6 Normalization Normalization is the process of applying rules in order to organize business complexity into stable data structures. The basic goal of normalization is to keep each attribute in only one place to eliminate redundancy and the inconsistencies that can result from redundancy. The process requires a deep understanding of each attribute and each attribute’s relationship to its primary key. Normalization rules sort attributes according to primary and foreign keys. Normalization rules sort into levels, with each level applying granularity and specificity in search of the correct primary and foreign keys. Each level comprises a separate normal form, and each successive level does not need to include previous levels. Normalization levels include: • First normal form (1NF) : Ensures each entity has a valid primary key, and every attribute depends on the primary key; removes repeating groups, and ensures each attribute is atomic (not multi-valued). 1NF includes the resolution of many -to-many relationships with an additional entity, often called an associative entity. • Second normal form (2NF) : Ensures each entity has the minimal primary key and that every attribute depends on the complete primary key. • Third normal form (3NF) : Ensures each entity has no hidden primary keys and that each attribute depends on no attributes outside the key (“the key, the whole key, and nothing but the key”). • Boyce / Codd normal form (BCNF) : Resolves overlapping composite candidate keys. A candidate key is either a primary or an alternate key. ‘Composite’ means more than one (i.e., two or more attributes in an entity’s primary or alternate keys), and ‘overlapping’ means there are hidden busin ess rules between the keys. • Fourth normal form (4NF) : Resolves all many -to-many -to-many relationships (and beyond) in pairs until they cannot be broken down into any smaller pieces. • Fifth normal form (5NF) : Resolves inter -entity dependencies into basic pairs, and all join dependencies use parts of primary keys. The term normalized model usually means the data is in 3NF. Situations requiring BCNF, 4NF, and 5NF occur rarely. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "150 • DMBOK2 1.3.7 Abstraction Abstraction is the removal of details in such a way as to broaden applicability to a wide class of situations while preserving the important properties and essential nature from concepts or subjects. An example of abstraction is the Party/Role structure, which can be used to capture how people and organizations play certain roles (e.g., employee and customer). Not all modelers or developers are comfortable with, or have the ability to work with abstraction. The modeler needs to weigh the cost o f developing and maintaining an abstract structure versus the amount of rework required if the unabstracted structure needs to be modified in the future (Giles 2011). Abstraction includes generalization and specialization . Generalization groups the common attributes and relationships of entities into supertype entities, while specialization separates distinguishing attributes within an entity into subtype entities. This specialization is usually based on attribute values within an entity instance. Subtypes can also be created using roles or classification to separate instances of an entity into groups by function. An example is Party , which can have subtypes of Individual and Organization . The subtyping relationship implies that all the properties from the supertype are inherited by the subtype. In the relational example shown in Figure 52, University and High School are subtypes of School . Figure 52 Supertype and Subtype Relationships Subtyping reduces redundancy on a data model. It also makes it easier to communicate similarities across what otherwise would appear to be distinct and separate entities. 2. Activities This section will briefly cover the steps for building conceptual, logical, and physical data models, as well as maintaining and reviewing data models. Both forward engineering and reverse engineering will be discussed. 2.1 Plan for Data Modeling A plan for data modeling contains tasks such as evaluating organizational requirements, creating standards, and determining data model storage. The deliverables of the data modeling process include: School University High School StudentApplicationContain Submit Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 151 • Diagram : A data model contains one or more diagrams. The diagram is the visual that captures the requirements in a precise form. It depicts a level of detail (e.g., conceptual, logical, or physical), a scheme (relational, dimensional, object -oriented, fact- based, time -based, or NoSQL ), and a notation within that scheme (e.g., information engineering, unified modeling language, object- role modeling). • Definitions : Definitions for entities, attributes, and relationships are essential to maintaining the precision on a data model. • Issues and outstanding questions : Frequently, the data modeling process raises issues and questions that may not be addressed during the data modeling phase. In addition, often the people or groups responsible for resolving these issues or answering these questions reside outside of the group buildin g the data model. Therefore, often a document is delivered that contains the current set of issues and outstanding questions. An example of an outstanding issue for the student model might be, “If a Student leav es and then returns, are they assigned a different Student Number or do they keep their original Student Number ?” • Lineage : For physical and sometimes logical data models, it is important to know the data lineage, that is, where the data comes from. Often lineage takes the form of a source/target mapping, where one can capture the source system attributes and how they populate the target system attributes. Lineage can also trace the data modeling components from conceptual to logical to physical within the same modeling effort. There are two reasons why lineage is important to capture during the data modeling. First, the data modeler will obtain a very strong understanding of the data requirements and, therefore, is in the best position to determine the source attributes. Second, determining the source attributes can be an effective tool to validate the accuracy of the model and the mapping (i.e., a reality check). 2.2 Build the Data Model To build the models, modelers often rely heavily on previous analysis and modeling work. They may study existing data models and databases, refer to published standards, and incorporate any data requirements. After studying these inputs, they start building the model. Modeling is a very iterative process ( Figure 53). Modelers draft the model, and then return to business professionals and business analysts to clarify terms and business rules. They then update the model and ask more questions (Hoberman, 2014). Figure 53 Modeling is Iterative Elicit Document Increase precision Ask more questions Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "152 • DMBOK2 2.2.1 Forward Engineering Forward engineering is the process of building a new application beginning with the requirements. The CDM is completed first to understand the scope of the initiative and the key terminology within that scope. Then the LDM is completed to document the business solution, followed by the PDM to document the technical solution. 2.2.1.1 Conceptual Data Modeling Creating the CDM involves the following steps: • Select Scheme : Decide whether the data model should be built following a relational, dimensional, fact-based, or NoSQL scheme. Refer to the earlier discussion on scheme and when to choose each scheme ( see Section 1.3.4 ). • Select Notation : Once the scheme is selected, choose the appropriate notation, such as information engineering or object role modeling. Choosing a notation depends on standards within an organization and the familiarity of users of a particular model with a particular no tation. • Complete Initial CDM : The initial CDM should capture the viewpoint of a user group. It should not complicate the process by trying to figure out how their viewpoint fits with other departments or with the organization as a whole. Collect the highest -level concepts (nouns) that exist for the organization. Common concepts are Time , Geography , Customer/Member/Client , Product/Service , and Transaction . Then collect the activities (verbs) that connect these concepts. Relationships can go both ways, or involve mor e than two concepts. Examples are: Customers have multiple Geographic Locations (home, work, etc.), Geographic Locations have many Customers. Transactions occur at a Time , at a Facility , for a Customer , selling a Product . • Incorporate Enterprise Terminology: Once the data modeler has captured the users’ view in the boxes and lines, the data modeler next captures the enterprise perspective by ensuring consistency with enterprise terminology and rules. For example, there would be some reconciliation work invol ved if the audience conceptual data model had an entity called Client , and the enterprise perspective called this same concept Customer . • Obtain Sign -off: After the initial model is complete, make sure the model is reviewed for data modeling best practices as well as its ability to meet the requirements. Usually , email verification that the model looks accurate will suffice. 2.2.1.2 Logical Data Modeling A logical data model (LDM) captures the detailed data requirements within the scope of a CDM. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 153 2.2.1.2.1 Analyze Information Requirements To identify information requirements, one must first identify business information needs, in the context of one or more business processes. As their input, business processes require information products that are themselves the output from other business processes. The names of these information products often identify an essential business vocabulary that serves as the basis for data modeling. Regardless of whether processes or data are modeled sequentially (in either order), or concurrently, effective analysis and design should ensure a relatively balanced view of data (nouns) and processes (verbs), with equal emphasis on both process and data modeling. Requirements analysis includes the elicitation, organization, documentation, review, refinement, approval, and change control of business requirements. Some of these requirements identify business needs for data and information. Express requirement specifications in both words and diagrams. Logical data modeling is an important means of expressing business data requirements. For many people, as the old saying goes, ‘a picture is worth a thousand words’. However, some people do not relate easily to pictures; they relate better to reports and tables created by data modeling tools. Many organizations have formal requirements. Management may guide drafting and refining formal requirement statements, such as “The system shall…” Written data requirement specification documents may be maintained using requirements management tools. The s pecifications gathered through the contents of any such documentation should carefully synchronize with the requirements captured with data models to facilitate impact analysis so one can answer questions like “Which parts of my data models represent or im plement Requirement X?” or “Why is this entity here?” 2.2.1.2.2 Analyze Existing Documentation It can often be a great jump -start to use pre -existing data artifacts, including already- built data models and databases. Even if the data models are out -of-date, parts can be useful to start a new model. Make sure , however, that any work done based on existing artifacts is validated by the SMEs for accuracy and currency. Companies often use packaged applications, such as Enterprise Resource Planning (ERP) systems, that have their own data models. Creation of the LDM should take into account these data mode ls and either use them, where applicable, or map them to the new enterprise data model. In addition, there could be useful data modeling patterns, such as a standard way of modeling the Party Role concept. Numerous industry data models capture how a generi c industry, such as retail or manufacturing, should be modeled. These patterns or industry data models can then be customized to work for the particular project or initiative. 2.2.1.2.3 Add Associative Entities Associative entities are used to describe Many- to-Many (or Many -to-Many -to-Many, etc.) relationships. An associative entity takes the identifying attributes from the entities involved in the relationship, and puts them into a new entity that just describes the relationship between the entities. This allows the addition of attributes to describe that relationship, such as effective and expiration dates. Associative entities may have more than Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "154 • DMBOK2 two parents. Associative entities may become nodes in graph databases. In dimensional modeling, associative entities usually become fact tables. 2.2.1.2.4 Add Attributes Add attributes to the conceptual entities. An attribute in a logical data model should be atomic. It should contain one and only one piece of data (fact) that cannot be divided into smaller pieces. For example, a conceptual attribute called phone number divides into several logical attributes for phone type code (home, office, fax, mobile, etc.), coun try code, (1 for US and Canada), area code, prefix, base phone number, and extension. 2.2.1.2.5 Assign Domains Domains, which were discussed in Section 1.3.3.4 , allow for consistency in format and value sets within and across projects. Student Tuition Amount and Instructor Salary Amount can both be assigned the Amount domain, for example, which will be a standard currency domain. 2.2.1.2.6 Assign Keys Attributes assigned to entities are either key or non -key attributes. A key attribute helps identify one unique entity instance from all others, either fully (by itself) or partially (in combination with other key elements). Non- key attributes describe the entity instance but do not help uniquely identify it. Identify primary and alternate keys. 2.2.1.3 Physical Data Modeling Logical data models require modifications and adaptations in order to have the resulting design perform well within storage applications. For example, changes required to accommodate Microsoft Access would be different from changes required to accommodate Teradata. Going forward, the term table will be used to refer to tables, files, and schemas; the term column to refer to columns, fiel ds, and elements; and the term row to refer to rows, records, or instances. 2.2.1.3.1 Resolve Logical Abstractions Logical abstraction entities (supertypes and subtypes) become separate objects in the physical database design using one of two methods. • Subtype absorption : The subtype entity attributes are included as nullable columns into a table representing the supertype entity. • Supertype partition : The super type entity’s attributes are included in separate tables created for each subtype. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 155 2.2.1.3.2 Add Attribute Details Add details to the physical model, such as the technical name of each table and column (relational databases), or file and field (non -relational databases), or schema and element (XML databases). Define the physical domain, physical data type, and length of each column or field. Add appropriate constraints (e.g., nullability and default values) for columns or fields, especially for NOT NULL constraints. 2.2.1.3.3 Add Reference Data Objects Small Reference Data value sets in the logical data model can be implemented in a physical model in three common ways: • Create a matching separate code table : Depending on the model, these can be unmanageably numerous. • Create a master shared code table: For models with a large number of code tables, this can collapse them into one table; however, this means that a change to one reference list will change the entire table. Take care to avoid code value collisions as well. • Embed rules or valid codes into the appropriate object’s definition: Create a constraint in the object definition code that embeds the rule or list. For code lists that are only used as reference for one other object, this can be a good solution. 2.2.1.3.4 Assign Surrogate Keys Assign unique key values that are not visible to the business and have no meaning or relationship with the data with which they are matched. This is an optional step and depends primarily on whether the natural key is large, composite, and whose attributes are assigned values that could change over time. If a surrogate key is assigned to be the primary key of a table, make sure there is an alternate key on the original primary key. For example, if on the LDM, the primary key for Student was Student First Name , Student Last Name , and Student Birth Date (i.e., a composite primary key), and on the PDM, the primary key for Student may be the surrogate key Student ID . In this case, there should be an alternate key defined on the original primary key of Student First Name , Student Last Name , and Student Birth Date . 2.2.1.3.5 Denormalize for Performance In some circumstances, denormalizing or adding redundancy can improve performance so much that it outweighs the cost of the duplicate storage and synchronization processing. Dimensional structures are the main means of denormalization. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "156 • DMBOK2 2.2.1.3.6 Index for Performance An index is an alternate path for accessing data in the database to optimize query (data retrieval) performance. Indexing can improve query performance in many cases. The database administrator or database developer must select and define appropriate indexes for database tables. Major RDBMS products support many types of indexes. Indexes can be unique or non -unique, clustered or non- clustered, partitioned or non -partit ioned, single column or multi -column, b -tree or bitmap or hashed. Without an appropriate index, the DBMS will revert to reading every row in the table (table scan) to retrieve any data. On large tables, this is very costly. Try to build indexes on large ta bles to support the most frequently run queries, using the most frequently referenced columns, particularly keys (primary, alternate, and foreign). 2.2.1.3.7 Partition for Performance Great consideration must be given to the partitioning strategy of the overall data model (dimensional) , especially when facts contain many optional dimensional keys (sparse). Ideally, partitioning on a date key is recommended; when this is not possible, a study is required based on profiled results and workload analysis to propose and refine the subsequent partitioning model. 2.2.1.3.8 Create Views Views can be used to control access to certain data elements, or to embed common join conditions or filters to standardize common objects or queries. Views themselves should be requirements- driven. In many cases, they will need to be developed via a proces s that mirrors the development of the LDM and PDM. 2.2.2 Reverse Engineering Reverse engineering is the process of documenting an existing database. The PDM is completed first to understand the technical design of an existing system, followed by an LDM to document the business solution that the existing system meets, followed by the CDM to document the scope and key terminology within the existing system. Most data modeling tools support reverse engineering from a variety of databases; however, creating a readable layout of the model elements still requires a modeler. There are several common layouts (orthogonal, dimensional, and hierarchical) which can be selected to get the process started, but contextual organization (grouping entities by subject area or function) is still largely a manual process. 2.3 Review the Data Models As do other areas of IT, models require quality control. Continuous improvement practices should be employed. Techniques such as time -to-value, support costs, and data model quality validators such as the Data Model Scorecard ® (Hoberman , 2009), can all be used to evaluate the model for correctness, completeness, and consistency. Once the CDM, LDM, and PDM are complete, they become very useful tools for any roles that need to understand the model, ranging from business analysts through dev elopers. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 157 2.4 Maintain the Data Models Once the data models are built, they need to be kept current. Updates to the data model need to be made when requirements change and frequently when business processes change. Within a specific project, often when one model level needs to change, a corresponding higher level of model needs to change. For example, if a new column is added to a physical data model, that column frequently needs to be added as an attribute to the corresponding logical data model. A good practice at the end of each development i teration is to reverse engineer the latest physical data model and make sure it is still consistent with its corresponding logical data model. Many data modeling tools help automate this process of comparing physical with logical. 3. Tools There are many types of tools that can assist data modelers in completing their work, including data modeling, lineage, data profiling tools, and Metadata repositories. 3.1 Data Modeling Tools Data modeling tools are software that automate many of the tasks the data modeler performs. Entry- level data modeling tools provide basic drawing functionality, including a data modeling pallet, so that the user can easily create entities and relationships. These entry -level tools also support rubber banding, which is the automatic redrawing of relationship lines when entities are moved. More sophisticated data modeling tools support forward engineering from conceptual to logical to physical to database structures, allowing the generation of database data definition language (DDL). Most will also support reverse engineering from database up to conceptual data model. These more sophisticated tools often support functionality such as naming standards validatio n, spellcheckers, a place to store Metadata (e.g., definitions and lineage), and sharing features (such as publishing to the Web). 3.2 Lineage Tools A lineage tool is software that allows the capture and maintenance of the source structures for each attribute on the data model. These tools enable impact analysis; that is, one can use them to see if a change in one system or part of a system affects another system. For example, the attribute Gross Sales Amount might be sourced from several applications and require a calculation to populate – lineage tools would store this information. Microsoft Excel® is a frequently -used lineage tool. Although easy to u se and relatively inexpensive, Excel does not enable real impact analysis and leads to manually managing Metadata. Lineage is also frequently captured in a data modeling tool, Metadata repository, or data integration tool. (S ee Chapters 11 and 12.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "158 • DMBOK2 3.3 Data Profiling Tools A data profiling tool can help explore the data content, validate it against existing Metadata, and identify Data Quality gaps/deficiencies, as well as deficiencies in existing data artifacts, such as logical and physical models, DDL, and model description s. For example, if the business expects that an Employee can have only one job position at a time, but the system shows Employees have more than one job position in the same timeframe, this wil l be logged as a data anomaly. (S ee Chapters 8 and 13.) 3.4 Metadata Repositories A Metadata repository is a software tool that stores descriptive information about the data model, including the diagram and accompanying text such as definitions, along with Metadata imported from other tools and processes (software development and BPM to ols, system catalogs, etc.). The repository itself should enable Metadata integration and exchange. Even more important than storing the Metadata is sharing the Metadata. Metadata repositories must have an easily accessible way for people to view and navig ate the contents of the repository. Data modeling tools generally include a limited repository. (S ee Chapter 13.) 3.5 Data Model Patterns Data model patterns are reusable modeling structures that can be applied to a wide class of situations. There are elementary, assembly, and integration data model patterns. Elementary patterns are the ‘nuts and bolts’ of data modeling. They include ways to resolve many -to-many relationships, and to construct self -referencing hierarchies. Assembly patterns represent the building blocks that span the business and data modeler worlds. Business people can understand them – assets, documents, people and organizations, and the like. Equally importantly, they are often the subject of published data model patterns that can give the modeler proven, robust, extensible, and implementab le designs. Integration patterns provide the framework for linking the assembly patterns in common ways (Giles, 2011). 3.6 Industry Data Models Industry data models are data models pre- built for an entire industry, such as healthcare, telecom, insurance, banking, or manufacturing. These models are often both broad in scope and very detailed. Some industry data models contain thousands of entities and attributes. Industry data models can be purchased through vendors or obtained through industry groups such as ARTS (for retail), SID (for communications), or ACORD (for insurance). Any purchased data model will need to be customized to fit an organization, as it will have been developed from multiple other organizations’ needs. The level of customization required will depend on how close the model is to an organization’s needs, and h ow detailed the most important parts are. In some cases, it can be a reference for an organization’s in -progress efforts to help the modelers make models that are more complete. In others, it can merely save the data modeler some data entry effort for anno tated common elements. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 159 4. Best Practices 4.1 Best Practices in Naming Conventions The ISO 11179 Metadata Registry, an international standard for representing Metadata in an organization, contains several sections related to data standards, including naming attributes and writing definitions. Data modeling and database design standards serve as the guiding principles to effectively meet business data needs, conform to Enterprise and Data Architecture (see Chapter 4), and ensure the quality o f data (see Chapter 14). Data architects, data analysts, and database administrators must jointly develop these standards. They must complement and not conflict with related IT standards. Publish data model and database naming standards for each type of modeling object and database object. Naming standards are particularly important for entities, tables, attributes, keys, views, and indexes. Names should be unique and as descriptive as possible. Logical names should be meaningful to business users, using full words as much as possible and avoiding all but the most familiar abbreviations. Physical names must conform to the maximum length allowed by the DBMS, so use abbreviations where necessary. While logical names use blank spaces as separators between words, physical names typically use underscores as word separators. Naming standards should minimize name changes across environments. Names should not reflect their specific environment, such as test, QA, or production. Class words, which are the last terms in attribute names such as Quantity, Name, and Code, can be used to distinguish attributes from entities and column names from table names. They can also show which attributes and columns are quantitative rather than qualitative, which can be important when analyzing the contents of those columns. 4.2 Best Practices in Database Design In designing and building the database, the DBA should keep the following design principles in mind (remember the acronym PRISM): • Performance and ease of use : Ensure quick and easy access to data by approved users in a usable and business- relevant form, maximizing the business value of both applications and data. • Reusability : The database structure should ensure that, where appropriate, multiple applications can use the data and that the data can serve multiple purposes (e.g., business analysis, quality improvement, strategic planning, customer relationship management, and pro cess improvement). Avoid coupling a database, data structure, or data object to a single application. • Integrity : The data should always have a valid business meaning and value, regardless of context, and should always reflect a valid state of the business. Enforce data integrity constraints as close to the data as possible, and immediately detect and report violations of data integrity constraints. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "160 • DMBOK2 • Security: True and accurate data should always be immediately available to authorized users, but only to authorized users. The privacy concerns of all stakeholders, including customers, business partners, and government regulators, must be met. Enforce data securit y, like data integrity, as close to the data as possible, and immediately detect and report security violations. • Maintainability : Perform all data work at a cost that yields value by ensuring that the cost of creating, storing, maintaining, using, and disposing of data does not exceed its value to the organization. Ensure the fastest possible response to changes in business processes and new business requirements. 5. Data Model Governance 5.1 Data Model and Design Quality Management Data analysts and designers act as intermediaries between information consumers (the people with business requirements for data) and the data producers who capture the data in usable form. Data professionals must balance the data requirements of the information consumers and the application requirements of data producers. Data professionals must also balance the short -term versus long -term business interests. Information consumers need data in a timely fashion to meet short -term business obligations and to take advantage of current business opportunities. System -development project teams mus t meet time and budget constraints. However, they must also meet the long -term interests of all stakeholders by ensuring that an organization’s data resides in data structures that are secure, recoverable, sharable, and reusable, and that this data is as correct, timely, relevant, and usable as possible. Therefore, data models and database designs should be a reasonable balance between the short -term needs and the long -term needs of the enterprise. 5.1.1 Develop Data Modeling and Design Standards As previously noted (in Section 4.1) , data modeling and database design standards provide guiding principles to meet business data requirements, conform to Enterprise and Data Architecture standards, and ensure the quality of data. Data modeling and database design standards should include the following: • A list and description of standard data modeling and database design deliverables • A list of standard names, acceptable abbreviations, and abbreviation rules for uncommon words, that apply to all data model objects • A list of standard naming formats for all data model objects, including attribute and column class words • A list and description of standard methods for creating and maintaining these deliverables • A list and description of data modeling and database design roles and responsibilities Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 161 • A list and description of all Metadata properties captured in data modeling and database design, including both business Metadata and technical Metadata. For example, guidelines may set the expectation that the data model captures lineage for each attribute. • Metadata quality expectations and requirements (see Chapter 13) • Guidelines for how to use data modeling tools • Guidelines for preparing for and leading design reviews • Guidelines for versioning of data models • Practices that are discouraged 5.1.2 Review Data Model and Database Design Quality Project teams should conduct requirements reviews and design reviews of the conceptual data model, logical data model, and physical database design. The agenda for review meetings should include items for reviewing the starting model (if any), the changes made to the model and any other options that were considered and rejected, and how well the new model conforms to any modeling or architecture standards in place. Conduct design reviews with a group of subject matter experts representing different backgrounds, skills, expectations, and opinions. It may require executive mandate to get expert resources allocated to these reviews. Participants must be able to discuss different viewpoints and reach group consensus without personal conflict, as all participants share the common goal of promoting the most practical, best performing , and most usable design. Chair each design review with one leader who facilitates the meeting. The leader creates and follows an agenda, ensures all required documentation is available and distributed, solicits input from all participants, maintains order and keeps the meeting moving, and summarizes the group’s consensus findings. Many design reviews also use a scribe to capture points of discussion. In reviews where there is no approval, the modeler must rework the design to resolve the issues. If there are issues that the modeler cannot resolve on their own, the final say should be given by the owner of the system reflected by the model. 5.1.3 Manage Data Model Versioning and Integration Data models and other design specifications require careful change control, just like requirements specifications and other SDLC deliverables. Note each change to a data model to preserve the lineage of changes over time. If a change affects the logical data model, such as a new or changed business data requirement, the data analyst or architect must review and approve the change to the model. Each change should note: • Why the project or situation required the change • What and How the object(s) changed, including which tables had columns added, modified, or removed, etc. • When the change was approved and when the change was made to the model (not necessarily when the change was implemented in a system) • Who made the change Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "162 • DMBOK2 • Where the change was made (in which models) Some data modeling tools include repositories that provide data model versioning and integration functionality. Otherwise, preserve data models in DDL exports or XML files, checking them in and out of a standard source code management system just like application code. 5.2 Data Modeling Metrics There are several ways of measuring a data model’s quality, and all require a standard for comparison. One method that will be used to provide an example of data model validation is The Data Model Scorecard ®, which provides 11 data model quality metrics: one for each of ten categories that make up the Scorecard and an overall score across all ten categories (Hoberman, 2015). Table 11 contains the Scorecard template. Table 11 Data Model Scorecard® Template # Category Total score Model score % Comments 1 How well does the model capture the requirements? 15 2 How complete is the model? 15 3 How well does the model match its scheme? 10 4 How structurally sound is the model? 15 5 How well does the model leverage generic structures? 10 6 How well does the model follow naming standards? 5 7 How well has the model been arranged for readability? 5 8 How good are the definitions? 10 9 How consistent is the model with the enterprise? 5 10 How well does the Metadata match the data? 10 TOTAL SCORE 100 The model score column contains the reviewer’s assessment of how well a particular model met the scoring criteria, with a maximum score being the value that appears in the total score column. For example, a reviewer might give a model a score of 10 on “How well does the model capture the requirements?” The % column presents the Model Score for the category divided by the Total Score for the category. For example, receiving 10 out of 15 would lead to 66%. The comments column should document information that explains the score in more detail or captures the action items required to fix the model. The last row contains the overall score assigned to the model, a sum of each of the columns. A brief description of each category follows: 1. How well does the model capture the requirements? Here we ensure that the data model represents the requirements. If there is a requirement to capture order information, in this category we check the model to make sure it captures order information. If there is a requirement to view Student Count by Semester and Major , in this category we make sure the data model supports this query. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 163 2. How complete is the model? Here completeness means two things: completeness of requirements and completeness of Metadata. Completeness of requirements means that each requirement that has been requested appears on the model. It also means that the data model only contains what is be ing asked for and nothing extra. It’s easy to add structures to the model anticipating that they will be used in the near future; we note these sections of the model during the review. The project may become too hard to deliver if the modeler includes some thing that was never asked for. We need to consider the likely cost of including a future requirement in the case that it never eventuates. Completeness of Metadata means that all the descriptive information surrounding the model is present as well; for example, if we are reviewing a physical data model, we would expect formatting and nullability to appear on the data model. 3. How well does the model match its scheme? Here we ensure that the model level of detail (conceptual, logical, or physical), and the scheme (e.g., relational, dimensional, NoSQL ) of the model being reviewed matches the definition for this type of model. 4. How structurally sound is the model? Here we validate the design practices employed to build the model to ensure one can eventually build a database from the data model. This includes avoiding design issues such as having two attributes with the same exact name in the same entity or having a null attribute in a primary key. 5. How well does the model leverage generic structures? Here we confirm an appropriate use of abstraction. Going from Customer Location to a more generic Location, for example, allows the design to more easily handle other types of locations such as warehouses and distribution centers. 6. How well does the model follow naming standards? Here we ensure correct and consistent naming standards have been applied to the data model. We focus on naming standard structure, term, and style. Structure means that the proper building blocks are being used for entities, relationships, and attributes. For example, a building block for an attribute would be the subject of the attribute , such as ‘Customer’ or ‘Product’. Term means that the proper name is given to the attribute or entity. Term also includes proper spelling and abbreviation. Style means that the appearance, such as upper case or camel case, is consistent with standard pract ices. 7. How well has the model been arranged for readability? Here we ensure the data model is easy to read. This question is not the most important of the ten categories. However, if your model is hard to read, you may not accurately address the more important categories on the scorecard. Placing parent entities abo ve their child entities, displaying related entities together, and minimizing relationship line length all improve model readability. 8. How good are the definitions? Here we ensure the definitions are clear, complete, and accurate. 9. How consistent is the model with the enterprise? Here we ensure the structures on the data model are represented in a broad and consistent context, so that one set of terminology and rules can be spoken in the organization. The structures that appear in a data model should be consistent in terminology an d usage with structures that appear in related data models, and ideally with the enterprise data model (EDM), if one exists. 10. How well does the Metadata match the data? Here we confirm that the model and the actual data that will be stored within the resulting structures are consistent. Does the column Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "164 • DMBOK2 Customer_Last_Name really contain the customer’s last name, for example? The Data category is designed to reduce these surprises and help ensure the structures on the model match the data these structures will be holding. The scorecard provides an overall assessment of the quality of the model and identifies specific areas for improvement. 6. Works Cited / Recommended Ambler, Scott. Agile Database Techniques: Effective Strategies for the Agile Software Developer. Wiley and Sons, 2003. Print. Avison, David and Christine Cuthbertson. A Management Approach to Database Applications . McGraw -Hill Publishing Co., 2002. Print. Information systems ser. Blaha, Michael. UML Database Modeling Workbook . Technics Publications, LLC, 2013. Print. Brackett, Michael H. Data Resource Design: Reality Beyond Illusion . Technics Publications, LLC, 2012. Print. Brackett, Michael H. Data Resource Integration: Understanding and Resolving a Disparate Data Resource . Technics Publications, LLC, 2012. Print. Brackett, Michael H. Data Resource Simplexity: How Organizations Choose Data Resource Success or Failure . Technics Publications, LLC, 2011. Print. Bruce, Thomas A. Designing Quality Databases with IDEF1X Information Models . Dorset House, 1991. Print. Burns, Larry. Building the Agile Database: How to Build a Successful Application Using Agile Without Sacrificing Data Management . Technics Publications, LLC, 2011. Print. Carlis, John and Joseph Maguire. Mastering Data Modeling - A User -Driven Approach . Addison -Wesley Professional, 2000. Print. Codd, Edward F. “A Relational Model of Data for Large Shared Data Banks”. Communications of the ACM , 13, No. 6 (June 1970). DAMA International. The DAMA Dictionary of Data Management. 2nd Edition: Over 2,000 Terms Defined for IT and Business Professionals. 2nd ed. Technics Publications, LLC, 2011. Print. Daoust, Norman. UML Requirements Modeling for Business Analysts: Steps to Modeling Success. Technics Publications, LLC, 2012. Print. Date, C. J. An Introduction to Database Systems . 8th ed. Addison -Wesley, 2003. Print. Date, C. J. and Hugh Darwen. Databases, Types and the Relational Model . 3d ed. Addison Wesley, 2006. Print. Date, Chris J. The Relational Database Dictionary: A Comprehensive Glossary of Relational Terms and Concepts, with Illustrative Examples . O'Reilly Media, 2006. Print. Dorsey, Paul. Enterprise Data Modeling Using UML . McGraw -Hill Osborne Media, 2009. Print . Edvinsson, Håkan and Lottie Aderinne. Enterprise Architecture Made Simple: Using the Ready, Set, Go Approach to Achieving Information Centricity. Technics Publications, LLC, 2013. Print. Fleming, Candace C. and Barbara Von Halle. The Handbook of Relational Database Design. Addison Wesley, 1989. Print. Giles, John. The Nimble Elephant: Agile Delivery of Data Models using a Pattern -based Approach . Technics Publications, LLC, 2012. Print. Golden, Charles. Data Modeling 152 Success Secrets - 152 Most Asked Questions On Data Modeling - What You Need to Know. Emereo Publishing, 2015. Print. Success Secrets. Halpin, Terry, Ken Evans, Pat Hallock, and Bill McLean. Database Modeling with Microsoft Visio for Enterprise Architects . Morgan Kaufmann, 2003. Print. The Morgan Kaufmann Series in Data Management Systems. Halpin, Terry. Information Modeling and Relational Databases . Morgan Kaufmann, 2001. Print. The Morgan Kaufmann Series in Data Management Systems. Halpin, Terry. Information Modeling and Relational Databases: From Conceptual Analysis to Logical Design . Morgan Kaufmann, 2001. Print. The Morgan Kaufmann Series in Data Management Systems. Harrington, Jan L. Relational Database Design Clearly Explained . 2nd ed. Morgan Kaufmann, 2002. Print. The Morgan Kaufmann Series in Data Management Systems. Hay, David C. Data Model Patterns: A Metadata Map. Morgan Kaufmann, 2006. Print. The Morgan Kaufmann Series in Data Management Systems. Hay, David C. Enterprise Model Patterns: Describing the World (UML Version) . Technics Publications, LLC, 2011. Print . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MODELING AND DESIGN • 165 Hay, David C. Requirements Analysis from Business Views to Architecture . Prentice Hall, 2002. Print. Hay, David C. UML and Data Modeling: A Reconciliation . Technics Publications, LLC, 2011. Print. Hernandez, Michael J. Database Design for Mere Mortals: A Hands -On Guide to Relational Database Design . 2nd ed. Addison -Wesley Professional, 2003. Print. Hoberman, Steve, Donna Burbank, Chris Bradley, et al. Data Modeling for the Business: A Handbook for Aligning the Business with IT using High -Level Data Models . Technics Publications, LLC, 2009. Print. Take It with You Guides. Hoberman, Steve. Data Model Scorecard . Technics Publications, LLC, 2015. Print. Hoberman, Steve. Data Modeling Made Simple with ER/Studio Data Architect. Technics Publications, LLC, 2013. Print. Hoberman, Steve. Data Modeling Made Simple: A Practical Guide for Business and IT Professionals . 2nd ed. Technics Publications, LLC, 2009. Print. Hoberman, Steve. Data Modeling Mas ter Class Training Manual . 7th ed. Technics Publications, LLC, 201 7. Print. Hoberman, Steve. The Data Modeler's Workbench. Tools and Techniques for Analysis and Design . Wiley, 2001. Print. Hoffer, Jeffrey A., Joey F. George, and Joseph S. Valacich. Modern Systems Analysis and Design . 7th ed. Prentice Hall, 2013. Print. IIBA and Kevin Brennan, ed. A Guide to the Business Analysis Body of Knowledge (BABOK Guide). International Institute of Business Analysis, 2009. Print. Kent, William. Data and Reality: A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World . 3d ed. Technics Publications, LLC, 2012. Print. Krogstie, John, Terry Halpin, and Keng Siau, eds. Information Modeling Methods and Methodologies: Advanced Topics in Database Research . Idea Group Publishing, 2005. Print. Advanced Topics in Database Research. Linstedt, Dan. Super Charge Your Data Warehouse: Invaluable Data Modeling Rules to Implement Your Data Vault . Amazon Digital Services. 2012. Data Warehouse Architecture Book 1. Muller, Robert. J. Database Design for Smarties: Using UML for Data Modeling . Morgan Kaufmann, 1999. Print. The Morgan Kaufmann Series in Data Management Systems. Needham, Doug. Data Structure Graphs: The structure of your data has meaning . Doug Needham Amazon Digital Services, 2015. Kindle. Newton, Judith J. and Daniel Wahl, eds. Manual for Data Administration . NIST Special Publications, 1993. Print. Pascal, Fabian. Practical Issues in Database Management: A Reference for The Thinking Practitioner. Addison -Wesley Professional, 2000. Print. Reingruber, Michael. C. and William W. Gregory. The Data Modeling Handbook: A Best -Practice Approach to Building Quality Data Models . Wiley, 1994. Print. Riordan, Rebecca M. Designing Effective Database Systems . Addison -Wesley Professional, 2005. Print. Rob, Peter and Carlos Coronel. Database Systems: Design, Implementation, and Management. 7th ed. Cengage Learning, 2006. Print. Schmidt, Bob. Data Modeling for Information Professionals . Prentice Hall, 1998. Print . Silverston, Len and Paul Agnew. The Data Model Resource Book, Volume 3: Universal Patterns for Data Modeling . Wiley, 2008. Print. Silverston, Len. The Data Model Resource Book, Volume 1: A Library of Universal Data Models for All Enterprises . Rev. ed. Wiley, 2001. Print. Silverston, Len. The Data Model Resource Book, Volume 2: A Library of Data Models for Specific Industries . Rev. ed. Wiley, 2001. Print. Simsion, Graeme C. and Graham C. Witt. Data Modeling Essentials . 3rd ed. Morgan Kaufmann, 2004. Print. Simsion, Graeme. Data Modeling: Theory and Practice. Technics Publications, LLC, 2007. Print. Teorey, Toby, et al. Database Modeling and Design: Logical Design , 4th ed. Morgan Kaufmann, 2010. Print. The Morgan Kaufmann Series in Data Management Systems . Thalheim, Bernhard. Entity -Relationship Modeling: Foundations of Database Technology . Springer, 2000. Print. Watson, Richard T. Data Management: Databases and Organizations . 5th ed. Wiley, 2005. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "166 CHAPTER 6 Data Storage and Operations 1. Introduction ata Storage and Operations includes the design, implementation, and support of stored data, to maximize its value throughout its lifecycle, from creation/acquisition to disposal (see Chapter 1 ). Data Storage and Operations includes two sub -activities: • Database support focuses on activities related to the data lifecycle, from initial implementation of a database environment, through obtaining, backing up, and purging data. It also includes ensuring the database performs well. Monitoring and tuning are critical to databa se support. • Database technology support includes defining technical requirements that will meet organizational needs, defining technical architecture, installing and administering technology, and resolving issues related to technology. Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 167 Database administrators (DBAs) play key roles in both aspects of data storage and operations. The role of DBA is the most established and most widely adopted data professional role, and database administration practices are perhaps the most mature of all data management practices. DBAs also play dominant roles in data operations and data security. (S ee Chapter 7.) Figure 54 Context Diagram: Data Storage and Operations Definition : The design, implementation, and support of stored data to maximize its value. Goals : 1. Manage availability of data throughout the data lifecycle. 2. Ensure the integrity of data assets. 3. Manage performance of data transactions. Activities : 1.Manage Database T echnology 1. Understand Database T echnology (P) 2. Evaluate Database T echnology (D) 3. Manage and Monitor Database T echnology (O) 2.Manage Database Operations 1. Understand Requirements (P) 2. Plan for Business Continuity (P) 3. Develop Database Instances (D) 4. Manage Database Performance (C,O) 5. Manage T est Datasets (O) 6. Manage Data Migration (O)Inputs : •Data Architecture •Data Requirements •Data Models •Service Level AgreementsDeliverables : •Database T echnology Evaluation Criteria •Database Environments •Migrated / Replicated / Versioned Data •Disaster Recovery Plan for Data in the Business Continuity Plans •Database Operations Reports Suppliers : •Data Architect •Data Modeler •Software Developer •Application T esting Te a mConsumers : •Data Modeler •Software Developer •Application T esting T eam •Infrastructure Operations T eam •Executives and ManagersParticipants : •Database Administrator •Database T echnology Specialist •Data Modeler •Data Architect T echniques : •T est in Lower Environments •Physical Naming Standards •Script Usage for All ChangesT ools : •Data Modeling T ools •Database Monitoring T ools •Database Management T ools •Developer Support T oolsMetrics : •Data Storage Metrics •Performance Metrics •Operations Metrics •Service Metrics (P) Planning, (C) Control, (D) Development, (O) OperationsData Storage and Operations T echnical DriversBusiness Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "168 • DMBOK2 1.1 Business Drivers Companies rely on their information systems to run their operations. Data Storage and Operations activities are crucial to organizations that rely on data. Business continuity is the primary driver of these activities. If a system becomes unavailable, comp any operations may be impaired or stopped completely. A reliable data storage infrastructure for IT operations minimizes the risk of disruption. 1.2 Goals and Principles The goals of data storage and operations include: • Managing the availability of data throughout the data lifecycle • Ensuring the integrity of data assets • Managing the performance of data transactions Data Storage and Operations represent a highly technical side of data management. DBAs and others involved in this work can do their jobs better and help the overall work of data management when they follow these guiding principles: • Identify and act on automation opportunities : Automate database development processes, develop tools and processes that shorten each development cycle, reduce errors and rework, and minimize the impact on the development team. In this way, DBAs can adapt to more iterative (agile) approaches to application development. This improvement work should be done in collaboration with data modeling and Data Architecture. • Build with reuse in mind : Develop and promote the use of abstracted and reusable data objects that prevent applications from being tightly coupled to database schemas (the so -called ‘object -relational impedance mismatch’). A number of mechanisms exist to this end, including database views, triggers, functions and stored procedures, application data objects and data -access layers, XML and XSLT, ADO.NET typed data sets, and web services. The DBA should be able to assess the best approach to virtualizing data. The end goal is to make using the database as quick, easy, and painless as possible. • Understand and appropriately apply best practices : DBAs should promote database standards and best practices as requirements, but be flexible enough to deviate from them if given acceptable reasons for these deviations. Database standards should never be a threat to the success of a project. • Connect database standards to support requirements : For example, the Service Level Agreement (SLA) can reflect DBA -recommended and developer -accepted methods of ensuring data integrity and data security. The SLA should reflect the transfer of responsibility from the DBAs to the development team if the development team will be coding their own database u pdate procedures or data access layer. This prevents an ‘all or nothing’ approach to standards. • Set expectations for the DBA role in project work : Onboarding the DBA in the project definition phase can help throughout the SDLC. The DBA can understand project needs and support requirements up -front. This will improve communication by clarifying the project team’s expectations from the data group. Having a dedicated primary and secondary DBA during analysis and design clarify expectations about DBA tasks, standards, work effort, and timelines for development work. Teams should also clarify expectations for support after implementation. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 169 1.3 Essential Concepts 1.3.1 Database Terms Database terminology is specific and technical. In working as a DBA or with DBAs, it is important to understand the speci fics of this technical language: • Database : Any collection of stored data, regardless of structure or content. Some large databases refer to instances and schema. • Instance : An execution of database software controlling access to a certain area of storage. An organization will usually have multiple instances executing concurrently, using different areas of storage. Each instance is independent of all other instances. • Schema : A subset of database objects contained within the database or an instance. Schemas are used to organize objects into more manageable parts. Usually, a schema has an owner and an access list particular to the schema’s contents. Common uses of schemas are t o isolate objects containing sensitive data from the general user base, or to isolate read -only views from the underlying tables in relational databases. Schema can also be used to refer to a collection of database structures with something in common. • Node : An individual computer hosting either processing or data as part of a distributed database. • Database abstraction means that a common application interface (API) is used to call database functions, such that an application can connect to multiple different databases without the programmer having to know all function calls for all possible databases. ODBC (Open Database Connectivity) is an example of an API that enables database abstraction. Advantages include portability; disadvantages include an inability to use specific functions that are not common across databases. 1.3.2 Data Lifecycle Management DBAs maintain and assure the accuracy and consistency of data over its entire lifecycle through the design, implementation, and usage of any system that stores, processes, or retrieves data. The DBA is the custodian of all database changes. While many part ies may request changes, the DBA defines the precise changes to make to the database, implements the changes, and controls the changes. Data lifecycle management includes implementing policies and procedures for acquisition, migration, retention, expiration, and disposition of data. It is prudent to prepare checklists to ensure all tasks are performed at a high level of quality. DBAs shoul d use a controlled, documented, and auditable process for moving application database changes to the Quality Assurance or Certification (QA) and Production environments. A manager -approved service request or change request usually initiates the process. The DBA should have a back out plan to reverse changes in case of problems. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "170 • DMBOK2 1.3.3 Administrators The role of Database Administrator (DBA) is the most established and the most widely adopted data professional role. DBAs play the dominant roles in Data Storage and Operations, and critical roles in Data Security (see Chapter 7 ), the physical side of data modeling, and database design ( see Chapter 5 ). DBAs provide support for development, test, QA, and special use database environments. DBAs do not exclusively perform all the activities of Data Storage and Operations. Data stewards, data architects, network administrators, data analysts, and security analysts participate in planning for performance, retention, and recovery. These teams may also participate in obtaining and processing data from external sources. Many DBAs specialize as Production, Application, Procedural , or Development DBAs. Some organizations also have Network Storage Administrators (NSA) who specialize in supporting the data storage system separately from the data storage applications or structures. In some organizations, each specialized role reports to a different organization within IT. Production DBAs may be part of production infrastructure or application operations support groups. Application, Development, and Procedural DBAs are sometimes integ rated into application development organizations. NSAs usually are connected to Infrastructure organizations. 1.3.3.1 Production DBA Production DBAs take primary responsibility for data operations management, including: • Ensuring the performance and reliability of the database through performance tuning, monitoring, error reporting, and other activities • Implementing backup and recovery mechanisms to ensure data can be recovered if lost in any circumstance • Implementing mechanisms for clustering and failover of the database, if continual data availability data is a requirement • Executing other database maintenance activities, such as implementing mechanisms for archiving data As part of managing data operations, Production DBAs create the following deliverables: • A production database environment, including an instance of the DBMS (Database Management System) on the supporting server, of a sufficient size and capacity to ensure adequate performance, configured for the appropriate level of security, reliability, and availability. Database System Administration is responsible for the DBMS environment. • Mechanisms and processes for controlled implementation of changes to databases in the production environment • Mechanisms for ensuring the availability, integrity, and recoverability of data in response to all circumstances that could result in loss or corruption of data • Mechanisms for detecting and reporting any error that occurs in the database, the DBMS, or the data server • Database availability, recovery, and performance in accordance with service level agreements • Mechanisms and processes for monitoring database performance as workloads and data volumes vary Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 171 1.3.3.2 Application DBA An application DBA is responsible for one or more databases in all environments (development / test, QA, and production), as opposed to database systems administration for any of these environments. Sometimes, application DBAs report to the organizational units responsible for development and maintenance of the applications supported by their databases. There are pros and cons to staffing application DBAs. Application DBAs are viewed as integral members of an application support team. By focusing on a specific database, they can provide better service to application developers. However, application DBAs can easily become isolated and lose sight of the organization’s overall data needs and common DBA practices. Application DBAs collaborate closely with data analysts, modelers, and architects. 1.3.3.3 Procedural and Development DBAs Procedural DBAs lead the review and administration of procedural database objects. A procedural DBA specializes in developing and supporting procedural logic controlled and execute d by the DBMS: stored procedures, triggers, and user- defined functions (UDFs). The procedural DBA ensures this procedural logic is planned, implemented, tested, and shared (reused). Development DBAs focus on data design activities , including creating and managing special use databases, such as ‘sandbox’ or exploration areas. In many cases, these two functions are combined under one position. 1.3.3.4 NSA Network Storage Administrators are concerned with the hardware and software supporting data storage arrays. Multiple network storage array systems have different needs and monitoring requirements than simple database systems. 1.3.4 Database Architecture Types A database can be classified as either centralized or distributed. A centralized system manages a single database, while a distributed system manages multiple databases on multiple systems. A distributed system’s components can be classified depending on the autonom y of the component systems into two types: federated (autonomous) or non -federated (non -autonomous). Figure 55 illustrates the difference between centralized and distributed. Figure 55 Centralized vs. Distributed Centralized Distributed, not Federated User View User View Location A Location A Location B Location C Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "172 • DMBOK2 1.3.4.1 Centralized Databases Centralized databases have all the data in one system in one place. All users come to the one system to access the data. For certain restricted data, centralization can be ideal, but for data that needs to be widely available, centralized databases have risks. For example, if the centralized system is unavailable, there are no other alternatives for accessing the data. 1.3.4.2 Distributed Databases Distributed databases make quick access to data possible over a large number of nodes. Popular distributed database technologies are based on using commodity hardware servers. They are designed to scale out from single servers to thousands of machines, each offering local computation and storage. Rather than re ly on hardware to deliver high -availability, the database management software itself is designed to replicate data amongst the servers, thereby delivering a highly available service on top of a cluster of computers. Datab ase management software is also designed to detect and handle failures. While any given computer may fail, the system overall is unlikely to. Some distributed databases implement a computational paradigm named MapReduce to further improve performance. In MapReduce, the data request is divided into many small fragments of work, each of which may be executed or re- executed on any node in the cluster. In addition, data is co -located on the compute nodes, providing very high aggregate bandwidth across the cluster. Both the filesystem and the application are designed to automatically handle node failures. 1.3.4.2.1 Federated Databases Federation provisions data without additional persistence or duplication of source data. A federated database system maps multiple autonomous database systems into a single federated database. The constituent databases, sometimes geographically separated, are interconnected via a c omputer network. They remain autonomous yet participate in a federation to allow partial and controlled sharing of their data. Federation provides an alternative to merging disparate databases. There is no actual data integration in the constituent databas es because of data federation; instead, data interoperability manages the view of the federated databases as one large object (see Chapter 8 ). In contrast, a non -federated database system is an integration of component DBMS’s that are not autonomous; they are controlled, managed , and governed by a centralized DBMS. Federated databases are best for heterogeneous and distributed integration projects such as enterprise information integration, data virtualization, schema matching, and Master Data Management. Federated architectures differ based on levels of integration with the component database systems and the extent of services offered by the federation. A FDBMS can be categorized as either loosely or tightly coupled. Loosely coupled systems require component databases to construct their own federated schema. A user will typically access other component database systems by using a multi -database language, but this removes any levels of location transparency, forcing the user to have direct kn owledge of the federated schema. A user imports the data required from other component databases and integrates it with their own to form a federated schema. Tightly coupled systems consist of component systems that use independent processes to construct and publish an integrated federated schema, as illustrated in Figure 57. The same schema can apply to all parts of the federation with no data replication. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 173 Figure 56 Federated D atabases Figure 57 Coupling 1.3.4.2.2 Blockchain Database Blockchain databases are a type of federated database used to securely manage non-fungible transactions. They can also be used for contract management or exchange of health information. There are two types of structures: individual records and blocks. Each transaction has a record. The database creates chains of time -bound groups of transactions (blocks) that also contain information from the previous block in the chain. Hash algorithms are used to create information about transactions to store in blocks while the block is the end of the chain. Once a new block is created, the old block hash should never change, which means that no transactions contained within that block may change. Any change to transactions or blocks (tampering) will be apparent when the hash values no longer match. 1.3.4.3 Virtualization / Cloud Platforms Virtualization (also called ‘cloud computing’) provides computation, software, data access, and storage services that do not require end -user knowledge of the physical location and configuration of the system that delivers the service(s). Parallels are often drawn between the concept of cloud computin g and the electricity grid: end Federated User View Location A Location B Location Cmap map map F F F Tightly Coupled Loosely Coupled User ViewUser View Location A Location A Location B Location Bmap map map map Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "174 • DMBOK2 users consume power without needing to understand the component devices or infrastructure required to provide the service. However, virtualization can be on -premises or off-premises. Cloud computing is a natural evolution of the widespread adoption of virtualization, service -oriented architectures, and utility computing. Here are some methods for implementing databases on the cloud: • Virtual machine image : Cloud platforms allow users to purchase virtual machine instances for a limited time. It is possible to run a database on these virtual machines. Users can either upload their own machine image with a database installed on it or use ready -made machine images that already include an optimized installation of a database. • Database -as-a-service (DaaS) : Some cloud platforms offer options for using a database -as-a-service, without physically launching a virtual machine instance for the database. In this configuration, application owners do not have to install and maintain the database on their own. Inste ad, the database service provider is responsible for installing and maintaining the database, and application owners pay according to their usage. • Managed database hosting on the cloud: Here the database is not offered as a service; instead, the cloud provider hosts the database and manages it on the application owner’s behalf. DBAs, in coordination with network and system administrators, need to establish a systematic integrated project approach to include standardization, consolidation, virtualization, and automation of data backup and recovery functions, as well as security of these functions. • Standardization/consolidation: Consolidation reduces the number of data storage locations an organization has, including the number of data stores and processes within a data center. Based on Data Governance policy, Data Architects and DBAs may develop the standard procedures that include identifying mission critical data, duration of data retention, data encryption procedures, and data replication policies. • Server v irtualization : Virtualization technologies allow equipment, such as servers from multiple data centers, to be replaced or consolidated. Virtualization lowers capital and operational expenses and reduces energy consumption. Virtualization technologies are also used to c reate virtual desktops, which can then be hosted in data centers and rented out on a subscription basis. Gartner views virtualization as a catalyst for modernization (Bittman, 2009). Virtualization provides data storage operations much more flexibility in provisioning storage at local or cloud environment. • Automation : Data automation involves automating tasks such as provisioning, configuration, patching, release management, and compliance. • Security: The security of data on virtual systems needs to be integrated with the existing security of physical infrastructures (see Chapter 7 ). 1.3.5 Database Processing Types There are two basic types of database processing . ACID and BASE are on opposite ends of a spectrum, so the coincidental names matching ends of a pH spectrum are helpful. The CAP Theorem is used to define how closely a distributed system may match either ACID or BASE. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 175 1.3.5.1 ACID The acronym ACID was coined in the early 1980’s as the indispensable constraint for achieving reliability within database transactions. For decades, it has provided transaction processing with a reliable foundation on which to build.34 • Atomicity : All operations are performed, or none are, so that if one part of the transaction fails, the entire transaction fails. • Consistency : The transaction must meet all rules defined by the system at all times and must void half-completed transactions. • Isolation : Each transaction is independent unto itself. • Durability : Once complete, the transaction cannot be undone. Relational ACID technologies are the dominant tools in relational database storage; most use SQL as the interface. 1.3.5.2 BASE The unprecedented increase in data volumes and variability, the need to document and store unstructured data, the need for read -optimized data workloads, and the subsequent need for greater flexibility in scaling, design, processing, cost, and disaster recovery gave rise to the diametric opposite of ACID, appropriately termed BASE : • Basically Available : The system guarantees some level of availability to the data even when there are node failures. The data may be stale, but the system will still give and accept responses. • Soft State : The data is in a constant state of flux; while a response may be given, the data is not guaranteed to be current. • Eventual Consistency : The data will eventually be consistent through all nodes and in all databases, but not every transaction will be consistent at every moment. BASE- type systems are common in Big Data environments. Large online organizations and social media companies commonly use BASE implementations, as immediate accuracy of all data elements at all times is not necessary. Table 12 summarizes the differences between ACID and BASE. Table 12 ACID vs BASE Item ACID BASE Casting (data structure) Schema must exist Dynamic Table structure exists Adjust on the fly Columns data typed Store dissimilar data Consistency Strong Consistency Available Strong, Eventual, or None Processing Focus Transactional Key-value stores Processing Focus Row/Column Wide -column stores History 1970s application storage 2000s unstructured storage Scaling Product Dependent Automatically spreads data across commodity servers Origin Mixture Open -source Transaction Yes Possible 34 Jim Gray established the concept. Haerder and Rueter (1983) coined the term ACID. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "176 • DMBOK2 1.3.5.3 CAP The CAP Theorem (or Brewer’s Theorem ) was developed in response to a shift toward more distributed systems (Brewer, 2000). The theorem asserts that a distributed system cannot comply with all parts of ACID at all time s. The larger the system, the lower the compliance. A distributed system must instead trade -off between properties. • Consistency : The system must operate as designed and expected at all times. • Availability : The system must be available when requested and must respond to each request. • Partition Tolerance : The system must be able to continue operations during occasions of data loss or partial system failure. The CAP Theorem states that at most two of the three properties can exist in any shared -data system. This is usually stated with a ‘pick two’ statement, illustrated in Figure 58. An interesting use of this theorem drives the Lambda Architecture design discussed in Chapter 14. Lambda Architecture uses two paths for data: a Speed path where availability and partition tolerance are most important, and a Batch path where consistency and availability are most important. Figure 58 CAP Theorem 1.3.6 Data Storage Media Data can be stored on a variety of media, including disks, volatile memory, and flash drives. Some systems can combine multiple storage types. The most commonly used are Disk and Storage Area Networks (SAN), In - Memory, Columnar Compression Solutions, Virtual S torage Area Network VSAN, Cloud -based storage solutions, Radio Frequency Identification (RFID), Digital wallets, Data centers and Private, Public, and Hybrid Cloud Storage . (See Chapter 14.) 1.3.6.1 Disk and Storage Area Networks (SAN) Disk storage is a very stable method of storing data persistently. Multiple types of disk s can exist in the same system. Data can be stored according to usage patterns, with less- used data stored on slower -access disks, which are usually cheaper than high performance disk systems. Partition T olerance No System FailuresCAP Theorem “Pick T wo” CAP Theorem “Pick T wo”CAP Theorem “Pick T wo” Partition T olerance Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 177 Disk arrays can be collected into Storage Area Networks (SAN) . Data movement on a SAN may not require a network, as data can be moved on the backplane. 1.3.6.2 In-Memory In-Memory databases (IMDB) are loaded from permanent storage into volatile memory when the system is turned on, and all processing occurs within the memory array, providing a faster response time than disk -based systems. Most in -memory databases also have features to set and configure durability in case of unexpected shutdown. If the application can be reasonably assured to fit most/all data into memory, then significant optimization can be made available from in -memory database systems. These IMDB s provide more predictable access time to data than disk storage mechanisms, but they require a much larger investment. IMDBs provide functionality for real -time analytics processing and are generally reserved for this due to the investment required. 1.3.6.3 Columnar Compression Solutions Columnar -based databases are designed to handle data sets in which data values are repeated to a great extent. For example, in a table with 256 columns, a lookup for a value that exists in a row will retrieve all the data in the row (and be somewhat disk -bound). Columnar storage reduces this I/O bandwidth by storing column data using compression – where the state (for example) is stored as a pointer to a table of states, compressing the master table significantly. 1.3.6.4 Flash Memory Recent advances in memory storage have made flash memory or solid state drives (SSDs) an attractive alternative to disks. Flash memory combines the access speed of memory -based storage with the persistence of disk -based storage. 1.3.7 Database Environments Databases are used in a variety of environments during the systems development lifecycle. When testing changes, DBAs should be involved in designing the data structures in the Development environment . The DBA team should implement any changes to the QA environment and must be the only team implementing changes to the Production environment. Production changes must adhere strictly to standard processes and procedures. While most data technology is software running on general purpose hardware, occasionally specialized hardware is used to support unique data management requirements. Types of specialized hardware include data appliances – servers built specifically for data transformation and distribution. These servers integrate with existing infrastructure either directly as a plug- in, or peripherally as a network connection. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "178 • DMBOK2 1.3.7.1 Production Environment The production environment is the technical environment where all business processes occur. Production is mission -critical – if this environment ceases to operate, business processes will stop, resulting in bottom -line losses, as well as a negative impact on customers who are unable to access services. In an emergency, or for public service systems, unexpected loss of function can be disastrous. The production environment is the ‘real’ environment from a business perspective. However, in order to have a reliable production environment, other non -production environments must exist and be used appropriately. For example, production environments shou ld not be used for development and testing as these activities put production processes and data at risk. 1.3.7.2 Pre-production Environments Pre-production environments are used to develop and test changes before such changes are introduced to the production environment. In pre -production environments, issues with changes can be detected and addressed without affecting normal business processes. In order to detect potential issues, the configuration of pre - production environments must closely resemble the production environment. Due to space and cost, it is usually not possible to exactly replicate production in the pre -production environments. The closer on the development path the non- production environment is to the production environment, the more closely the non- production environment needs to match the production environment. Any deviation from the production system equipment and configuration can create issues or errors unrelated to the change, complicating issue research and resolution. Common types of pre -production environments include development, test, support, and special use environments. 1.3.7.2.1 Development The development environment is usually a slimmer version of the production environment. It generally has less disk space, fewer CPUs, less RAM, etc. Developers use this environment to create and test code for changes in separate environments, which then are combined in the QA environment for full integration testing. Development can have many copies of production data models, depending on how development projects are managed. Larger organizations may give individual developers their own environments to manage with all appropriate rights. The development environment should be the first place any patches or updates are applied for testing. This environment should be isolated from and on different physical hardware than the production environments. Due to the isolation, data from production systems may need to be copied to the development environments. However, in many industries, production data is protected through regulation. Do not move data from production environments without first determining what restrictions there are on doing so. (Se e Chapter 7.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 179 1.3.7.2.2 Test The test environment is used to execute quality assurance and user acceptance testing and, in some cases, stress or performance tests. In order to prevent test results from being distorted due to environmental differences, the test environment ideally also has the same softwa re and hardware as the production environment. This is especially important for performance testing. Test may or may not be connected via the network to production systems to read production data. Test environments should never write to production systems. Test environments serve many uses: • Quality Assurance Testing (QA): Used to test functionality against requirements . • Integration Testing : Used for testing as a whole multiple parts of a system that have been developed or upgraded independently. • User Acceptance Testing (UAT) : Used for testing the system functionality from a user perspective. Use Cases are the most common inputs for testing performed in this environment. • Performance Testing : Used to perform high- volume or high -complexity tests at any time, rather than having to wait for off hours, or adversely affecting production system peak time. 1.3.7.2.3 Sandboxes or Experimental Environments A sandbox is an alternate environment that allows read -only connections to production data and can be managed by the users. Sandboxes are used to experiment with development options and test hypotheses about data or merge production data with user -developed data or supplemental data obtained from external sources. Sandboxes are valuable, for example, when performing a Proof- of-Concept. A sandbox environment can either be a sub -set of the production system, walled off from production processing, or a completely separate environment. Sandbox users often have CRUD rights over their own space so that they can quickly validate ideas and optio ns for changes to the system. The DBAs usually have little to do with these environments other than setting them up, granting access, and monitoring usage. If the Sandbox areas are situated in production database systems, they must be isolated in order to avoid adversely affecting production operations. These environments should never write back to the production systems. Sandbox environments could be handled by virtual machines (VMs), unless licensing costs for separate instances become prohibitive. 1.3.8 Database Organization Data storage systems provide a way to encapsulate the instructions necessary to put data on disks and manage processing, so developers can simply use instructions to manipulate data. Databases are organized in three general ways: Hierarchical, Relational, and Non- Relational. These classes are not mutually exclusive (see Figure 59). Some database systems can read and write data organized in relational and non- relational structures. Hierarchical databases can be mapped to relational tables. Flat files with line delimiters can be read as tables with rows, and one or more columns can be defined to describe the row contents. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "180 • DMBOK2 Figure 59 Database Organization Spectrum 1.3.8.1 Hierarchical Hierarchical database organization is the oldest database model, used in early mainframe DBMS, and is the most rigid of structures. In hierarchical databases, data is organized into a tree -like structure with mandatory parent/child relationships: each parent can have many children, but each child has only one parent (also known as a 1 -to-many relationship). Directory trees are an example of a hierarchy. XML also uses a hierarchical model. It can be represented as a relational database, although the actual structure is that of a tree travers al path. 1.3.8.2 Relational People sometimes think that relational databases are named for the relation between tables. This is not the case. Relational databases are based on set theory and relational algebra, where data elements or attributes (columns) are related into tuples (rows). (See Chapter 5.) Tables are sets of relations with identical structure. Set operations (like union, intersect, and minus) are used to organize and retrieve data from relational databases, in the form of Structured Query Language (SQL) . In order to write data, the structure (schema) has to be known in advance (schema on write). Relational databases are row -oriented. The database management system (DBMS) of a relational database is called the RDBMS . A relational database is the predominant choice for storing data that constantly changes. Variations on relational databases include Multidimensional and Temporal. 1.3.8.2.1 Multidimensional Multidimensional database technologies store data in a structure that allows searching using several data element filters simultaneously. This type of structure is used most frequently in Data Warehousing and Business Intelligence. Some of these database types are proprietary, although most la rge databases have cube technology built in as objects. Access to the data uses a variant of SQL called MDX or Multidimensional eXpression. More Controlled Structure Less Controlled StructureRELATIONAL (Schema on Write)HIERARCHICAL (Tree Schema)NON - RELATIONAL (Schema on Read) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 181 1.3.8.2.2 Temporal A temporal database is a relational database with built -in support for handling data involving time. The temporal aspects usually include valid time and transaction time. These attributes can be combined to form bi- temporal data. • Valid time is the timeframe when a fact is true with respect to the entity it represents in the real world. • Transaction time is the period during which a fact stored in the database is considered true. It is possible to have timelines other than Valid Time and Transaction Time, such as Decision Time, in the database. In that case, the database is called a multi -temporal database as opposed to a bi -temporal database. Temporal databases enable application developers and DBAs to manage current, proposed, and historical versions of data in the same database. 1.3.8.3 Non -relational Non- relational databases can store data as simple strings or complete files. Data in these files can be read in different ways, depending on the need (this characteristic is referred to as ‘schema on read’). Non -relational databases may be row -oriented, but this is not required. A non- relational database provides a mechanism for storage and retrieval of data that employs less constrained consistency models than traditional relational databases. Motivations for this approach include simplicity of design, horizontal scaling, and fin er control over availability. Non -relational databases are usually referred to as NoSQL (which stands for “Not Only SQL”). The primary differentiating factor is the storage structure itself, where the data structure is no longer bound to a tabular relation al design. It could be a tree, a graph, a network, or a key -value pairing. The NoSQL tag emphasizes that some editions may , in fact , support conventional SQL directives. These databases are often highly optimized data stores intended for simple retrieval a nd appending operations. The goal is improved performance, especially with respect to latency and throughput. NoSQL databases are used increasingly in Big Data and real -time web applications. (See Chapter 5.) 1.3.8.3.1 Column -oriented Column -oriented databases are used mostly in Business Intelligence applications because they can compress redundant data. For example, a state ID column only has unique values, instead of one value for each of a million rows. There are trade- offs between column -oriented (non -relational) and row -oriented (usually relational) organization : • Column -oriented organization is more efficient when an aggregate needs to be computed over many rows. This only holds true for a notably smaller subset of all columns of data because reading that smaller subset of data can be faster than reading all data. • Column -oriented organization is more efficient when new values of a column are supplied for all rows at once, because that column data can be written efficiently to replace old column data without touching any other columns for the rows. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "182 • DMBOK2 • Row -oriented organization is more efficient when many columns of a single row are required at the same time, and when row-size is relatively small, as the entire row can be retrieved with a single disk seek. • Row -oriented organization is more efficient when writing a new row if all of the row data is supplied at the same time; the entire row can be written with a single disk seek. • In practice, row -oriented storage layouts are well suited for Online Transaction Processing (OLTP) - like workloads, which are more heavily loaded with interactive transactions. Column -oriented storage layouts are well suited for Online Analytical Processing (OLAP)- like workloads (e.g., data warehouses), which typically involve a smaller number of highly complex queries over all data (possibly terabytes). 1.3.8.3.2 Spatial A spatial database is optimized to store and query data that represents objects defined in a geometric space. Spatial databases support several primitive types (simple geometric shapes such as box, rectangle, cube, cylinder, etc.) and geometries composed of collections of points, lines, and shapes. Spatial database systems use indexes to quickly look up values; the way that most databases index data is not optimal for spatial queries. Instead, spatial databases use a spatial index to speed up database operations. Spatial databases can perform a wide variety of spatial operations. As per the Open Geospatial Consortium standard , a spatial database may perform one or more of the following operations: • Spatial Measurements : Computes line length, polygon area, the distance between geometries, etc. • Spatial Functions : Modifies existing features to create new ones; for example, by providing a buffer around them, intersecting features, etc. • Spatial Predicates : Allows true/false queries about spatial relationships between geometries. Examples include “Do two polygons overlap?” or “Is there a residence located within a mile of the area of the proposed landfill?” • Geometry Constructors : Creates new geometries, usually by specifying the vertices (points or nodes) which define the shape. • Observer Functions : Queries that return specific information about a feature , such as the location of the center of a circle. 1.3.8.3.3 Object / M ulti-media A multimedia database includes a Hierarchical Storage Management system for the efficient management of a hierarchy of magnetic and optical storage media. It also includes a collection of object classes, which represents the foundation of the system. 1.3.8.3.4 Flat File Database A flat file database describes any of various means to encode a data set as a single file. A flat file can be a plain text file or a binary file. Strictly, a flat file database consists of nothing but data, and contains records that may Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 183 vary in length and delimiters. More broadly, the term refers to any database that exists in a single file in the form of rows and columns, with no relationships or links between records and fields except the structure. Plain text files usually contain one record per line. A list of names, addresses, and phone numbers, written by hand on a sheet of paper, is an example of a flat file database. Flat files are used not only as data storage tools in DBMS systems, but also as data transfer tools. Hadoop database s use flat file storage. 1.3.8.3.5 Key- Value Pair Key- Value pair databases contain sets of two items: a key identifier and a value. There are a few specific uses of these types of databases. • Document Databases : Document -oriented databases contain collections of files, including both structure and data. Each document is assigned a key. More advanced document -oriented databases also can store attributes for the document’s contents, such as dates or tags. This type of database can store both complete and incomplete docume nts. Document databases may use XML or JSON (Java Script Object Notation) structures. • Graph Databases : Graph databases store key -value pairs where the focus is on the relationship between the nodes, rather than on the nodes themselves. 1.3.8.3.6 Triplestore A data entity composed of subject- predicate -object is known as a triplestore . In Resource Description Framework (RDF) terminology, a triplestore is composed of a subject that denotes a resource, the predicate that expresses a relationship between the subject and the object, and the object itself. A triplestore is a purpose - built d atabase for the storage and retrieval of triples in the form of subject -predicate -object expressions. Triplestores can be broadly classified into three categories: Native triplestores, RDBMS -backed triplestores , and NoSQL triplestores. • Native triplestores are those that are implemented from scratch and exploit the RDF data model to efficiently store and access the RDF data. • RDBMS -backed triplestores are built by adding an RDF -specific layer to an existing RDBMS. • NoSQL Triplestores are currently being investigated as possible storage managers for RDF. Triplestore databases are best for taxonomy and thesaurus management, linked data integration, and knowledge portals. 1.3.9 Specialized Databases Some specialized situations require specialized types of databases that are managed differently from traditional relational databases. Examples include: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "184 • DMBOK2 • Computer Assisted Design and Manufacturing (CAD / CAM) applications require an Object database, as will most embedded real -time applications. • Geographical Information Systems (GIS) make use of specialized geospatial databases, which have at least annual updates to their Reference Data. Some specialized GIS are used for utilities (electric grid, gas lines, etc.), for telecom in network management, or for ocean navigation. • Shopping- cart applications found on most online retail websites make use of XML databases to initially store the customer order data, and may be used real -time by social media databases for ad placement on other websites. Some of this data is then copied into one or more traditional OLTP (Online Transaction Processing) databases or data warehouses. In addition, many off -the-shelf vendor applications may use their own proprietary databases. At the very least, their schemas will be proprietary and mostly concealed, even if they sit on top o f traditional relational DBMSs. 1.3.10 Common Database Processes All databases , no matter the type, share the following processes in some way. 1.3.10.1 Archiving Archiving is the process of moving data off immediately accessible storage media and onto media with lower retrieval performance. Archives can be restored to the originating system for short- term use. Data that is not actively needed to support application processe s should be moved to an archive on less-expensive disk, tape, or a CD / DVD jukebox. Restoring from an archive should be a matter of simply copying the data from the archive back into the system. Archival processes must be aligned with the partitioning strategy to ensure optimal availability and retention. A robust approach involves: • Creating a secondary storage area, preferably on a secondary database server • Partitioning existing database tables into archival blocks • Replicating the data that is needed less often to the separate database • Creating tape or disk backups • Creating database jobs that periodically purge unneeded data It is wise to schedule regular tests of archive restoration to avoid surprises in an emergency. When changes are made to the technology or structure of a production system, the archive also needs to be evaluated to ensure that data moved from the archive into current storage will be readable. There are several ways of handling out -of-synch archives: • Determine if or how much of the archive is required to be preserved. What is not required can be considered purged. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 185 • For major changes in technology, restore the archives to the originating system before the technology change, upgrade or migrate to the new technology, and re -archive the data using the new technology. • For high -value archives where the source database structures change, restore the archive, make any changes to the data structures, and re -archive the data with the new structure. • For infrequent- access archives where the source technology or structure changes, keep a small version of the old system running with limited access, and extract from the archives using the old system as needed. Archives that are not recoverable with current technology are useless, and keeping old machinery around to read archives that cannot be otherwise read, is not efficient or cost-effective. 1.3.10.2 Capacity and Growth Projections Think of a database as a box, the data as fruit, and overhead (indexes, etc.) as packing material. The box has dividers, and fruit and p acking material go in the cells: • First, decide the size of the box that will hold all the fruit and any packing material needed – that is the Capacity. • How much fruit goes into the box, and how quickly? • How much fruit comes out of the box, and how quickly? Decide if the box will stay the same size over time, or must be expanded over time to hold more fruit. This projection of how much and how quickly the box must expand to hold incoming fruit and packing material is the growth projection. If the box cannot expand, the fruit must be taken out as fast as it is put in, and the growth projection is zero. How long should the fruit stay in the cells? If the fruit in one cell gets dehydrated over time, or for any reason becomes not as useful, should that fruit be put in a separate box for longer term storage (i.e., archived)? Will there ever be a need to brin g that dehydrated fruit back into the main box? Moving the fruit to another box with the ability to move it back into the first box is an important part of archiving. This allows the box to not have to be expanded quite as often or as much. If a fruit becomes too stagnant to use, throw that fruit away (i.e., purge the data). 1.3.10.3 Change Data Capture (CDC) Change data capture refers to the process of detecting that data has changed and ensuring that information relevant to the change is stored appropriately. Often referred to as log -based replication, CDC is a non -invasive way to replicate data changes to a target with out affecting the source. In a simplified CDC context, one computer system has data that may have changed from a previous point in time, and a second computer system needs to reflect the same change. Rather than sending the entire database over the network to reflect just a few minor changes, the idea is to just send what changed (deltas) so that the receiving system can make appropriate updates. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "186 • DMBOK2 There are two different methods to detect and collect changes: data versioning, which evaluates columns that identify rows that have changed (e.g., last- update -timestamp columns, version- number columns, status - indicator columns), or by reading logs that do cument the changes and enable them to be replicated in secondary systems. 1.3.10.4 Purging It is incorrect to assume that all data will reside forever in primary storage. Eventually, the data will fill the available space, and performance will begin to degrade. At that point, data will need to be archived, purged, or both. Just as important, some data will degrade in value and become not worth keeping. Purging is the process of completely removing data from storage media such that it cannot be recovered. A principal goal of data management is that the cost of maintaining data should not exceed its value to the organization. Purging data reduces costs and risks. Data to be purged is generally deemed obsolete and unnecessary, even for regulatory purposes. Some data may become a liability if kept longer than necessary. Purging it reduces the risks that it may be misused. 1.3.10.5 Replication Data replication means the same data is stored on multiple storage devices. In some situations, having duplicate databases is useful, such as in a high -availability environment where spreading the workload among identical databases in different hardware or even data centers can pres erve functionality during peak usage times or disasters. Repli cation can be active or passive: • Active replication is performed by recreating and storing the same data at every replica from every other replica. • Passive replication involves recreating and storing data on a single primary replica and then transforming its resultant state to other secondary replicas. Replication has two dimensions of scaling : • Horizontal data scaling has more data replicas. • Vertical data scaling has data replicas located further away in distance geographically. Multi-master replication, where updates can be submitted to any database node and then ripple through to other servers is often desired but increases complexity and cost. Replication transparency occurs when data is replicated between database servers so that the information remains consistent throughout the database system and users cannot tell or even know which database copy they are using. The two primary replication patterns are mirroring and log shipping (see Figure 60). • In mirroring, updates to the primary database are replicated immediately (relatively speaking) to the secondary database, as part of a two -phase commit process. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 187 • In log shipping, a secondary server receives and applies copies of the primary database’s transaction logs at regular intervals. The choice of replication method depends on how critical the data is, and how important it is that failover to the secondary server be immediate. Mirroring is usually a more expensive option than log shipping. For one secondary server, mirroring is effecti ve; log shipping may be used to update additional secondary servers. Figure 60 Log Shipping vs. Mirroring 1.3.10.6 Resiliency and Recovery Resiliency in databases is the measurement of how tolerant a system is to error conditions. If a system can tolerate a high level of processing errors and still function as expected, it is highly resilient. If an application crashes upon the first unexpected condition, that system is not resilient. If the database can detect and either abort or automatically recover from common processing errors (runaway query, for example), it is considered resilient. There are always some conditions that no system can detect in advance, such as a power failure, and those conditions are considered disasters. Three recovery types provide guidelines for how quickly recovery takes place and what it focuses on: • Immediate recovery from some issues sometimes can be resolved through design; for example, predicting and automatically resolving issues, such as those that might be caused by a failover to backup system. • Critical recovery refers to a plan to restore the system as quickly as possible in order to minimize delays or shut downs of business processes. • Non-critical recovery means that restoration of function can be delayed until systems that are more critical have been restored. Data processing errors include data load failures, query return failures, and obstacles to completing ETL or other processes. Common ways of increasing resilience in data processing systems are to trap and re -route data causing errors, detect and ignore data causing errors, and implement flags in processing for completed steps to avoid reprocessing data or repeating completed steps when restarting a process. Log Log Log Shipping Mirroring Location ALocation A Location B Location B data data datacreate synchapply Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "188 • DMBOK2 Each system should require a certain level of resiliency (high or low). Some applications may require that any error halts all processing (low resiliency), while others may only require that the errors be trapped and re - routed for review, if not outright i gnored. For extremely critical data, the DBA will need to implement a replication pattern in which data moves to another copy of the database on a remote server. In the event of database failure, applications can then ‘fail over’ to the remote database and continue processing. 1.3.10.7 Retention Data Retention refers to how long data is kept available. Data retention planning should be part of the physical database design. Retention requirements also affect capacity planning. Data Security also affects data retention plans, as some data needs to be retained for specific timeframes for legal reasons. Failure to retain data for the appropriate length of time can have legal consequences. Likewise, there are also regulations related to purging data. Data can become a liability if kept longer than specified. Organizations should formulate retention policies based on regulatory requirements and risk management guidelines. These policies should drive specifications for purging and archiving of data . 1.3.10.8 Sharding Sharding is a process where small chunks of the database are isolated and can be updated independently of other shards, so replication is merely a file copy. Because the shards are small, refreshes/overwrites may be optimal. 2. Activities The two main activities in Data Operations and Storage are Database Technology Support and Database Operations Support. Database Technology Support is specific to selecting and maintaining the software that stores and manages the data. Database Operations Support is specific to the data and processes that the software manages. 2.1 Manage Database Technology Managing database technology should follow the same principles and standards for managing any technology. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 189 The leading reference model for technology management is the Information Technology Infrastructure Library (ITIL), a technology management process model developed in the United Kingdom. ITIL principles apply to managing data technology.35 2.1.1 Understand Database Technology It is important to understand how technology works, and how it can provide value in the context of a particular business. The DBA, along with the rest of the data services teams, works closely with business users and managers to understand the data and information needs of the business. DBAs and Database Architects combine their knowledge of available tools with the business requirements in order to suggest the best possible applications of technology to meet organizational needs. Data professionals must first understand the characteristics of a candidate database technology before determining which to recommend as a solution. For example, database technologies that do not have transaction -based capabilities (e.g., commit and rollback) are not suitable for operational situations supporting Point-of -Sale processes. Do not assume that a single type of database architecture or DBMS works for every need. Most organizations have multiple database tools installed to perform a range of functions, from performance tuning to backups, to managing the database itself. Only a f ew of these tool sets have mandated standards. 2.1.2 Evaluate Database Technology Selecting strategic DBMS software is particularly important. DBMS software has a major impact on data integration, application performance, and business productivity. Some of the factors to consider when selecting DBMS software include: • Product architecture and complexity • Volume and velocity limits, including streaming rate • Application profile, such as transaction processing, Business Intelligence, and personal profiles • Specific functionality, such as temporal calculation support • Hardware platform and operating system support • Availability of supporting software tools • Performance benchmarks, including real -time statistics • Scalability • Software, memory, and storage requirements • Resiliency, including error handling and reporting Some factors are not directly related to the technology itself, but rather to the purchasing organization and to the tool vendors. For example: • Organizational appetite for technical risk • Available supply of trained technical professionals 35 http://bit.ly/1gA4mpr . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "190 • DMBOK2 • Cost of ownership, such as licensing, maintenance, and computing resources • Vendor reputation • Vendor support policy and release schedule • Customer references The expense of the product, including administration, licensing, and support, should not exceed the product’s value to the business. Ideally, the technology should be as user friendly, self -monitoring, and self- administering as possible. If it is not, then it may be necessary to bring in staff with experience using the tool. It is a good idea to start with a small pilot project or a proof -of-concept (POC), to get a good idea of the true costs and benefits before proceeding with a full -blown production implementation. 2.1.3 Manage and Monitor Database Technology DBAs often serve as Level 2 technical support, working with help desks and technology vendor support to understand, analyze, and resolve user problems. The key to effective understanding and use of any technology is training. Organizations should make sure they have training plans and budgets in place for everyone involved in implementing, supporting, and using data and database technology. Training plans should include appropriate levels of cross- training to better support application development, especially Agile development. DBAs should have working knowledge of application development skills, such as data modeling, use -case analysis, and application data access. The DBA will be responsible for ensuring databases have regular backups and for performing recovery tests. However, if data from these databases needs to be merged with other existing data in one or more databases, there may be a data integration challenge. DBAs should not simply merge data. Instead, they should work with other stakeholders to ensure that data can be integrated correctly and effectively. When a business requires new technology, the DBAs will work with business users and application developers to ensure the most effective use of the technology, to explore new applications of the technology, and to address any problems or issues that surface from its use. The DBAs then deploy new technology products in pre-production and production environments. They will need to create and document processes and procedures for administering the product with the least amount of effort and expense. 2.2 Manage Database Operations Database support, as provided by DBAs and Network Storage Administrators (NSAs), is at the heart of data management . Databases reside on managed storage areas. Managed storage can be as small as a disk drive on a personal computer (managed by the OS), or as large as RAID arrays on a storage area network or SAN . Backup media is also managed storage. DBAs manage various data storage applications by assigning storage structures, maintaining physical databases (including physical data models and physical layouts of the data, such as assignments to specific files or disk areas), and establishing DBMS environments on servers. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 191 2.2.1 Understand Requirements 2.2.1.1 Define Storage Requirements DBAs establish storage systems for DBMS applications and file storage systems to support NoSQL . NSAs and DBAs together play a vital role in establishing file storage systems. Data enters the storage media during normal business operations and, depending on the requirements, can stay permanently or temporarily. It is important to plan for adding additional space well in advance of when that space is actually needed. Doing any sort of maintenance in an emergency is a risk. All projects should have an initial capacity estimate for the first year of operations, and a growth projection for the following few years. Capacity and growth should be estimated not only for the space the data itself holds, but also for indexes, logs, and any redundant images such as mirrors. Data storage requirements must account for regulation s related to data retention. For legal reasons, organizations are required to retain some data for set periods (see Chapter 9 ). In some cases, they may also be required to purge data after a defined period. It’s a good idea to discuss data retention needs with the data owners at design time and reach an agreement on how to treat data through its lifecycle. The DBAs will work with application developers and other operations staff, including server and storage administrators, to implement the approved data retention plan. 2.2.1.2 Identify Usage Patterns Databases have predictable usage patterns. Basic types of patterns include: • Transaction -based • Large data set write - or retrieval -based • Time -based (heavier at month end, lighter on weekends, etc.), • Location -based (more densely populated areas have more transactions, etc.) • Priority-based (some departments or batch IDs have higher priority than others) Some systems will have a combination of these basic patterns. DBAs need to be able to predict ebbs and flows of usage patterns and have processes in place to handle peaks (such as query governors or priority management) as well as to take advantage of vall eys (delay processes that need large amounts of resources until a valley pattern exists). This information can be used to maintain database performance. 2.2.1.3 Define Access Requirements Data access includes activities related to storing, retrieving, or acting on data housed in a database or other repository. Data Access is simply the authorization to access different data files. Various standard languages, methods, and formats exist for accessing data from databases and other repositories: SQL, ODBC, JDBC, XQJ, ADO.NET, XML, X Query, X Path, and Web Services for ACID -type Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "192 • DMBOK2 systems. BASE -type access method standards include C, C++, REST, XML, and Java36. Some standards enable the translation of data from unstructured (such as HTML or free -text files) to structured (such as XML or SQL ). Data architects and DBAs can assist organizations in selecting appropriate methods and tools required for data access. 2.2.2 Plan for Business Continuity Organizations need to plan for business continuity in the event of disaster or adverse events that impact their systems and their ability to use their data. DBAs must make sure a recovery plan exists for all databases and database servers, covering scenarios that could result in loss or corruption of data, such as: • Loss of the physical database server • Loss of one or more disk storage devices • Loss of a database, including the DBMS Master Database, temporary storage database, transaction log segment, etc. • Corruption of database index or data pages • Loss of the database or log segment filesystems • Loss of database or transaction log backup files Each database should be evaluated for criticality so that its restoration can be prioritized. Some databases will be essential to business operations and will need to be restored immediately. Less critical databases will not be restored until primary syste ms are up and running. Still, other s may not need to be restored at all; for example, if they are merely copies that are refreshed when loaded. Management and the organization’s business continuity group , if one exists, should review and approve the data recovery plan. The DBA group should regularly review the plans for accuracy and comprehensiveness. Keep a copy of the plan, along with all the software needed to install and configure the DBMS, instructions, and security codes (e.g., the administrator password) , in a secure, off- site location in the event of a disaster. No system can be recovered from a disaster if the backups are unavailable or unreadable. Regular backups are essential to any recovery effort, but if they are unreadable, they are worse than useless; processing time making the unreadable backups will have been wasted, along with the opportunity for fixing the issue that made the backups unreadable. Keep all backups in a secure, off- site location. 2.2.2.1 Make Backups Make backups of databases and, if appropriate, the database transaction logs. The system’s Service Level Agreement (SLA) should specify backup frequency. Balance the importance of the data against the cost of protecting it. For large databases, frequent backups can consume large amounts of disk storage and server resources. In addition to incremental backups, periodically make a complete backup of each database. Furthermore, databases should reside on a managed storage area, ideally a RAID array on a storage area network 36 http://bit.ly/1rWAUxS (accessed 2/28/2016) has a list of all data access methods for BASE -type systems. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 193 or SAN, with daily back up to separate storage media. For OLTP databases, the frequency of transaction log backups will depend on the frequency of updating, and the amount of data involved. For frequently updated databases, more frequent log dumps will not only provide greater protection, but will also reduce the impact of the backups on server resources and applications. Backup files should be kept on a separate filesystem from the databases, and should be backed up to some separate storage medium as specified in the SLA. Store copies of the daily backups in a secure off- site facility. Most DBMSs support hot backups of the database – backups taken while applications are running. When some updates occur in transit, they will roll either forward to completion or roll back when the backup reloads. The alternative is a cold backup taken when the database is off -line. However, a cold backup may not be a viable option if applications need to be continuously available. 2.2.2.2 Recover Data Most backup software includes the option to read from the backup into the system. The DBA works with the infrastructure team to re -mount the media containing the backup and to execute the restoration. The specific utilities used to execute the restoration of the data depend o n the type of database. Data in file system databases may be easier to restore than those in relational database management systems, which may have catalog information that needs to be updated during the data recovery, especially if the recovery is from logs instead of a full bac kup. It is critical to periodically test the recovery of data. Doing so will reduce bad surprises during a disaster or emergency. Practice runs can be executed on non -production system copies with identical infrastructure and configuration, or if the system has a failover, on the secondary system. 2.2.3 Develop Database Instances DBAs are responsible for the creation of database instances. Related activities include: • Installing and updating DBMS software : DBAs install new versions of the DBMS software and apply maintenance patches supplied by the DBMS vendor in all environments (from development to production) as indicated by the vendor and vetted by and prioritized by DBA specialists, security specialists, and management. This is a critical activity to ensure against vulnerability to attacks, as well as to ensure ongoing data integrity in centralized and decentralized installations. • Maintaining multiple environment installations, including different DBMS versions : DBAs may install and maintain multiple instances of DBMS software in sandbox, development, testing, user acceptance testing, system acceptance testing, quality assurance, pre -production, hot -fix, disaster recovery environments, and production, and manage migration of the DBMS software versions through environments relative to applications and systems versioning and changes. • Installing and administering related data technology : DBAs may be involved in installing data integration software and third party data administration tools. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "194 • DMBOK2 2.2.3.1 Manage the Physical Storage Environment Storage environment management needs to follow traditional Software Configuration Management (SCM) processes or Information Technology Infrastructure Library (ITIL) methods to record modification to the database configuration, structures, constraints, permissions, thresholds, etc. DBAs need to update the physical data model to reflect the changes to the storage objects as part of a standard configuration management process. With agile development and extreme programming methods, updates to the physical data model play important roles in preventing design or development errors. DBAs need to apply the SCM process to trace changes and to verify that the databases in the development, test, and production environments have all of the enhancements included in each release – even if the changes are cosmetic or only in a virtualized data layer. The four procedures required to ensure a sound SCM process are configuration identification, configuration change control, configuration status accounting, and configuration audits. • During the configuration identification process, DBAs will work with data stewards, data architects, and data modelers to identify the attributes that define every aspect of a configuration for end -user purposes. These attributes are recorded in configuration documentation and baselined. Once a n attribute is baselined , a formal configuration change control process is required to change the attribute. • Configuration change control is a set of processes and approval stages required to change a configuration item’s attributes and to re -baseline them. • Configuration status accounting is the ability to record and report on the configuration baseline associated with each configuration item at any point in time. • Configuration audits occur both at delivery and when effecting a change. There are two types. A physical configuration audit ensures that a configuration item is installed in accordance with the requirements of its detailed design documentation, while a functional configuration audit ensures that performance attributes of a configuration item are achieved. To maintain data integrity and traceability throughout the data lifecycle, DBAs communicate the changes to physical database attributes to modelers, developers, and Metadata managers. DBAs must also maintain metrics on data volume, capacity projections, and query performance, as well as statistics on physical objects, in order to identify data replication needs, data migration volumes, and data recovery checkpoints. Larger databases will also have object partitioning, which must be monitored and maintained over time to ensure that the object maintains the desired distribution of data. 2.2.3.2 Manage Database Access Controls DBAs are responsible for managing the controls that enable access to the data . DBAs oversee the following functions to protect data assets and data integrity: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 195 • Controlled environment: DBAs work with NSAs to manage a controlled environment for data assets; this includes network roles and permissions management, 24x7 monitoring and network health monitoring, firewall management, patch management, and Microsoft Baseline Security Analyzer (MBSA) integration. • Physical security: The physical security of data assets is managed by Simple Network Management Protocol (SNMP) -based monitoring, data audit logging, disaster management, and database backup planning. DBAs configure and monitor these protocols. Monitoring is especially imp ortant for security protocols. • Monitoring : Database systems are made available by continuous hardware and software monitoring of critical servers. • Controls : DBAs maintain information security by access controls, database auditing, intrusion detection, and vulnerability assessment tools. Concepts and activities involved in setting up data security are discussed in Chapter 7. 2.2.3.3 Create Storage Containers All data must be stored on a physical drive and organized for ease of load, search, and retrieval. Storage containers themselves may contain storage objects, and each level must be maintained appropriate to the level of the object. For example, relational databases have schemas that contain tables, and non- relational databases have filesystems that contain files. 2.2.3.4 Implement Physical Data Models DBAs are typically responsible for creating and managing the complete physical data storage environment based on the physical data model. The physical data model includes storage objects, indexing objects, and any encapsulated code objects required to enforce Data Quality rules, connect database objects, and achieve database performance. Depending on the organization, data modelers may provide the data model and the DBAs implement the physical layout of the data model in storage. In other organizations, DBAs may take a skeleton of a physical model and add all the database -specific implementation details, including indexes, c onstraints, partitions or clusters, capacity estimates, and storage allocation details. For third -party database structures provided as part of an application, most data modeling tools allow reverse engineering of Commercial Off the Shelf (COTS) or Enterprise Resource Planning (ERP) system databases , as long as the modeling tool can read the storage tool catalog. These can be used to develop a Physical Model. DBAs or data modelers will still need to review and potentially update the physical model for application -based constraints or relationships; not all constraints and relationships are installed in database catalogs, especially for older applications where database abstraction was desired. Well -maintained physical models are necessary when DBAs are providing Data -as-a-Service. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "196 • DMBOK2 2.2.3.5 Load Data When first built, databases are empty. DBAs fill them. If the data to be loaded has been exported using a database utility, it may not be necessary to use a data integration tool to load it into the new database. Most database systems have bulk load capabilities, requiring that the data be in a format that matches the ta rget database object, or having a simple mapping function to link data in the source to the target object. Most organizations also obtain some data from external third -party sources , such as lists of potential customers purchased from an information broker, postal and address information, or product data provided by a supplier. The data can be licensed or provided as an open data service, free of charge; provided in a number of different formats (CD, DVD, EDI, XML, RSS feeds, text files); or provided upon request or regularly updated via a subscription service. Some acquisitions require legal agreements. DBAs need to be aware of these restrictions before loading data. DBAs may be asked to handle these types of loads or to create the initial load map. Limit manual execution of these loads to installations or other one -time situations, or ensure they are automated and scheduled. A managed approach to data acquisition centralizes responsibility for data subscription services with data analysts. The data analyst will need to document the external data source in the logical data model and data dictionary. A developer may design and create scripts or programs to read the data and load it into a database. The DBA will be responsible for implementing the necessary processes to load the data into the database and / or make it available to the application. 2.2.3.6 Manage Data Replication DBAs can influence decisions about the data replication process by advising on: • Active or passive replication • Distributed concurrency control from distributed data systems • The appropriate methods to identify updates to data through either timestamp or version numbers under Change Data Control process For small systems or data objects, complete data refreshes may satisfy the requirements for concurrency. For larger objects where most of the data does NOT change, merging changes into the data object is more efficient than completely copying all data for every change. For large objects where most of the data is changed, it may still be better to do a refresh than to incur the overhead of so many updates. 2.2.4 Manage Database Performance The Database performance depends on two interdependent facets: availability and speed. Performance includes ensuring availability of space, query optimization, and other factors that enable a database to return data in an efficient way. Performance cannot be measured without availability. An unavailable database has a performance measure of zero. DBAs and NSAs manage database performance by: • Setting and tuning operating system and application parameters. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 197 • Managing database connectivity. NSAs and DBAs provide technical guidance and support for IT and business users requiring database connectivity based on policies enforced through standards and protocols of the organization. • Working with system programmers and network administrators to tune operating systems, networks, and transaction processing middleware to work with the database. • Dedicating appropriate storage and enabling the database to work with storage devices and storage management software. Storage management software optimizes the use of different storage technologies for cost -effective storage of older, less -frequently referenced data, by migrating that data to less expensive storage devices. This results in more rapid retrieval time for core data. DBAs work with storage administrators to set up and monitor effective storage management procedures. • Providing volumetric growth studies to support storage acquisition and general data lifecycle management activities of retention, tuning, archiving, backup, purging, and disaster recovery. • Working with system administrators to provide operating workloads and benchmarks of deployed data assets that support SLA management, charge -back calculations, server capacity, and lifecycle rotation within the prescribed planning horizon. 2.2.4.1 Set Database Performance Service Levels System performance, data availability and recovery expectations, and expectations for teams to respond to issues are usually governed through Service Level Agreements (SLAs) between IT data management services organizations and data owners ( Figure 61). Figure 61 SLAs for System and Database Performance Typically, an SLA will identify the timeframes during which the database is expected to be available for use. Often an SLA will identify a specified maximum allowable execution time for a few application transactions (a mix of complex queries and updates). If the database is not available as agreed to, or if process execution times violate the SLA, the data owners will ask the DBA to identify and remediate the causes of the problem. Service Level Agreement System Performance System Availability System Recovery Service Level Agreement Database Performance Database Availability Data Owners Data Stewards IT Data Management Services DBA NSA Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "198 • DMBOK2 2.2.4.2 Manage Database Availability Availability is the percentage of time that a system or database can be used for productive work. As organizations increase their uses of data, availability requirements increase, as do the risks and costs of unavailable data. To meet higher demand, maintenance windows are shrinking. Four related factors affect availability: • Manageability : The ability to create and maintain an environment • Recoverability : The ability to reestablish service after interruption, and correct errors caused by unforeseen events or component failures • Reliability : The ability to deliver service at specified levels for a stated period • Serviceability : The ability to identify the existence of problems, diagnose their causes, and repair / solve them Many things may prevent databases from being available, including: • Planned outages o For maintenance o For upgrades • Unplanned outages o Loss of the server hardware o Disk hardware failure o Operating system failure o DBMS software failure o Data center site loss o Network failure • Application problems o Security and authorization problems o Severe performance problems o Recovery failures • Data problems o Corruption of data (due to bugs, poor design, or user error) o Loss of database objects o Loss of data o Data replication failure • Human error DBAs are responsible for doing everything possible to ensure databases stay online and operational, including: • Running database backup utilities • Running database reorganization utilities • Running statistics gathering utilities • Running integrity checking utilities • Automating the execution of these utilities Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 199 • Exploiting table space clustering and partitioning • Replicating data across mirror databases to ensure high availability 2.2.4.3 Manage Database Execution DBAs also establish and monitor database execution, use of data change logs, and synchronization of duplicated environments. Log sizes and locations require space and , in some cases, can be treated like file -based databases on their own. Other applications that consume logs must also be managed, to ensure use of the correct logs at the required logging level. The more detail that is logged, the more space and processing required, whic h may adversely affect performance. 2.2.4.4 Maintain Database Performance Service Levels DBAs optimize database performance both proactively and reactively, by monitoring performance and by responding to problems quickly and competently. Most DBMSs provide the capability of monitoring performance , allowing DBAs to generate analysis reports. Most server operating systems have similar monitoring and reporting capabilities. DBAs should run activity and performance reports against both the DBMS and the server on a regular basis, including during perio ds of heavy activity. They should compare these reports to previous reports to identify any negative trends and save them to help analyze problems over time. 2.2.4.4.1 Transaction Performance vs. Batch Performance Data movement may occur in real time through online transactions. However, many data movement and transformation activities are performed through batch programs, which may move data between systems, or merely perform operations on data within a system. These batch jobs must complete within specified windows in the operating schedule. DBAs and data integration specialists monitor the performance of batch data jobs, noting exceptional completion times and errors, determining the root cause of errors, and resolving these issues. 2.2.4.4.2 Issue Remediation When performance problems occur, the DBA, NSA, and Server Administration teams should use the monitoring and administration tools of the DBMS to help identify the source of the problem. Common reasons for poor database performance include: • Memory allocation or contention : A buffer or cache for data. • Locking and blocking : In some cases, a process running in the database may lock up database resources, such as tables or data pages, and block another process that needs them. If the problem persists, the DBA can kill the blocking process. In some cases, two processes may ‘dea dlock’, with each process locking resources needed by the other. Most DBMSs will automatically terminate one of Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "200 • DMBOK2 these processes after an interval of time. These types of problems are often the result of poor coding, either in the database or in the application. • Inaccurate database statistics : Most relational DBMSs have a built- in query optimizer, which relies on stored statistics about the data and indexes to make decisions about how to execute a given query most effectively. These statistics should be updated frequently, especially in active databases. Failure to do so will result in poorly performing queries. • Poor coding : Perhaps the most common cause of poor database performance is poorly coded SQL. Query coders need a basic understanding of how the SQL query optimizer works. They should code SQL in a way that takes maximum advantage of the optimizer’s capabilities. Some systems allow encapsulation of complex SQL in stored procedures, which can be pre- compiled and pre -optimized, rather than embedded in application code or in script files. • Inefficient complex table joins : Use views to pre -define complex table joins. In addition, avoid using complex SQL (e.g., table joins) in database functions; unlike stored procedures, these are opaque to the query optimizer. • Insufficient indexing : Code complex queries and queries involving large tables to use indexes built on the tables. Create the indexes necessary to support these queries. Be careful about creating too many indexes on heavily updated tables, as this will slow down update processi ng. • Application activity : Ideally, applications should be running on a server separate from the DBMS, so that they are not competing for resources. Configure and tune database servers for maximum performance. In addition, the new DBMSs allow application objects, such as Java and . NET classes, to be encapsulated in database objects and executed in the DBMS. Be careful about making use of this capability. It can be very useful in certain cases, but executing application code on the database server may affect the interoperability, application architecture, and performance of database processes. • Overloaded servers : For DBMSs that support multiple databases and applications, there may be a breaking point where the addition of more databases has an adverse effect on the performance of existing databases. In this case, create a new database server. In addition, relocate databases that have grown very large, or that are being used more heavily than before, to a different server. In some cases, address problems with large databases by archiving less- used data to another location or by deleting expired o r obsolete data. • Database volatility : In some cases, large numbers of table inserts and deletes over a short while can create inaccurate database distribution statistics. In these cases, turn off updating database statistics for these tables, as the incorrect statistics will adversely affect the query optimizer. • Runaway queries : Users may unintentionally submit queries that use a majority of the system’s shared resources. Use rankings or query governors to kill or pause these queries until they can be evaluated and improved. After the cause of the problem is identified, the DBA will take whatever action is needed to resolve the problem, including working with application developers to improve and optimize the database code, and archiving or deleting data that is no longer actively needed by application processes. In exceptional cases for OLTP- type databases, the DBA may consider working with the data modeler restructure the affected portion of the Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 201 database. Do this only after other measures (e.g., the creation of views and indexes and the rewriting of SQL code) have been tried, and only after careful consideration of the possible consequences, such as loss of data integrity or the increase in compl exity of SQL queries against denormalized tables. For read- only reporting and analytical databases, de normalization for performance and ease of access is the rule rather than the exception, and poses no threat or risk. 2.2.4.5 Maintain Alternate Environments Databases do not appear once and remain unchanged. Business rules change, business processes change, and technology changes. Development and test environments enable changes to be tested before they are brought into a production environment. DBAs can make whole or subset copies of database structures and data onto other environments to enable development and testing of system changes. There are several types of alternate environments . • Development environments are used to create and test changes that will be implemented in production. Development must be maintained to closely resemble the production environment, though with scaled down resources. • Test environments serve several purposes: QA, integration testing, UAT, and performance testing. The test environment ideally also has the same software and hardware as production. In particular, environments used for performance testing should not be scaled down in resour ces. • Sandboxes or experimental environments are used to test hypotheses and develop new uses of data. The DBAs generally set up, grant access to, and monitor usage of these environments. They should also ensure that sandboxes are isolated and do not adversely affect pro duction operations. • Alternate production environments are required to support offline backups, failover, and resiliency support systems. These systems should be identical to the production systems, although the backup (and recovery) system can be scaled down in compute capacity, since it is mostly dedicated to I/O activities. 2.2.5 Manage Test Data Sets Software testing is labor -intensive and accounts for nearly half of the cost of the system development. Efficient testing requires high quality test data, and this data must be managed. Test data generation is a critical step in software testing. Test data is data that has been specifically identified to test a system. Testing can include verifying that a given set of input produces expected output or challenging the ability of programming to respond to unusual, extreme, exceptional, or unexpected input. Test data can be completely fabricated or generated using meaningless values or it can be sample data. Sample data can be a subset of actual production data (by either content or structure), or generated from production data. Production data can be filtered or aggregated to create multiple sample data sets, depending on the need. In cases where production data contains protected or restricted data, sample data must be masked. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "202 • DMBOK2 Test data may be produced in a focused or systematic way (as is typically the case in functionality testing) using statistics or filters, or by using other, less -focused approaches (as is typically the case in high -volume randomized automated tests). Test data may be produced by the tester, by a program or function that aids the tester, or by a copy of production data that has been selected and screened for the purpose. Test data may be recorded for short -term re -use, created and managed to support regression tests, or used once and then removed – although in most organizations, cleanup after projects does not include this step. DBAs should monitor project test data and ensure that obsolete test data is purged regularly to preserve capacity. It is not always possible to produce enough data for some tests, especially performance tests. The amount of test data to be generated is determined or limited by considerations such as time, cost, and quality. It is also impacted by regulation that limits the use of production data in a test environment. (S ee Chapter 7.) 2.2.6 Manage Data Migration Data migration is the process of transferring data between storage types, formats, or computer systems, with as little change as possible. Changing data during migration is discussed in Chapter 8. Data migration is a key consideration for any system implementation, upgrade, or consolidation. It is usually performed programmatically, being automated based on rules. However, people need to ensure that the rules and programs are executed correctly. Data migration occurs for a variety of reasons, including server or storage equipment replacements or upgrades, website consolidation, server maintenance, or data center relocation. Most implementations allow this to be done in a non -disruptive manner, such a s concurrently while the host continues to perform I/O to the logical disk (or LUN). The mapping granularity dictates how quickly the Metadata can be updated, how much extra capacity is required during the migration, and how quickly the previous location is marked as free. Smaller granularity means faster update, less space required, and quicker freeing up of old storage. Many day- to-day tasks a storage administrator has to perform can be simply and concurrently completed using data migration techniques: • Moving data off an over -used storage device to a separate environment • Moving data onto a faster storage device as needs require • Implementing an Information Lifecycle Management policy • Migrating data off older storage devices (either being scrapped or off- lease) to offline or cloud storage Automated and manual data remediation is commonly performed in migration to improve the quality of data, eliminate redundant or obsolete information, and match the requirements of the new system. Data migration phases (design, extraction, remediation, load, verification) for applications of moderate to high complexity are commonly repeated several times before the new system is deployed. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 203 3. Tools In addition to the database management systems themselves, DBAs use multiple other tools to manage databases. For example, modeling and other application development tools, interfaces that allow users to write and execute queries, data evaluation and modification tools for Data Quality improvement, and performance load monitoring tools. 3.1 Data Modeling Tools Data modeling tools automate many of the tasks the data modeler performs. Some data modeling tools allow the generation of database data definition language (DDL). Most support reverse engineering from database into a data model. Tools that are more sophisticated validate na ming standards, check spelling, store Metadata such as definitions and lineage, and even enable publishing to the web. (See Chapter 5.) 3.2 Database Monitoring Tools Database monitoring tools automate monitoring of key metrics, such as capacity, availability, cache performance, user statistics, etc., and alert DBAs and NSAs to database issues. Most such tools can simultaneously monitor multiple database types. 3.3 Database Management Tools Database systems have often included management tools. In addition, several third -party software packages allow DBAs to manage multiple databases. These applications include functions for configuration, installation of patches and upgrades, backup and restore, database cloning, test management, and data clean -up routines. 3.4 Developer Support Tools Developer Support tools contain a visual interface for connecting to and executing commands on a database. Some are included with the database management software. Others include third -party applications. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "204 • DMBOK2 4. Techniques 4.1 Test in Lower Environments For upgrades and patches to operating systems, database software, database changes, and code changes, install and test on the lowest level environment first – usually development. Once tested on the lowest level, install on the next higher levels, and install on the production environment last. This ensures that the installers have experience with the upgrade or patch, and can minimize disruption to the production environments. 4.2 Physical Naming Standards Consistency in naming speeds understanding. Data architects, database developers, and DBAs can use naming standards for defining Metadata or creating rules for exchanging documents between organizations. ISO/IEC 11179 – Metadata registries (MDR) address the semantics of data, the representation of data, and the registration of the descriptions of that data. It is through these descriptions that an accurate understanding of the semantics and a useful depiction of the data are found. The significant section for physical databases within naming standards is Part 5 – Naming and Identification Principles, which describes how to form conventions for naming data elements and their components. 4.3 Script Usage for All Changes It is extremely risky to directly change data in a database . However, there may be a need, such as an annual change in the chart of accounts structures, or in mergers and acquisitions, or emergencies, where these are indicated due to the ‘one -off’ nature of the request and/or the lack of appropriate tools for these circumstances. It is helpful to place changes to be made into update script files and test them thoroughly in non -production environments before applying to production. 5. Implementation Guidelines 5.1 Readiness Assessment / Risk Assessment A risk and readiness assessment revolves around two central ideas: risk of data loss and risks related to technology readiness . • Data loss : Data can be lost through technical or procedural errors, or through malicious intent. Organizations need to put in place strategies to mitigate these risks. Service Level Agreements often Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 205 specify the general requirements for protection. SLAs need to be supported by well -documented procedures. Ongoing assessment is required to ensure robust technical responses are in place to prevent data loss through malicious intent, as cyber threats are ever evolving. SLA and data audits are recommended to assess and plan risk mitigations. • Technology readiness : Newer technologies such as NoSQL, Big Data, triple stores, and FDMS require skills and experience readiness in IT. Many organizations do not have the skill sets needed to take advantage of these new technologies. DBAs, systems engineers and application developers, and business users must be ready to use the benefits from these in the BI and other applications. 5.2 Organization and Cultural Change DBAs often do not effectively promote the value of their work to the organization . They need to recognize the legitimate concerns of data owners and data consumers, balance short -term and long -term data needs, educate others in the organization about the importance of good data management practices, and optimize data development practices to ensure maximum benefit to the organization and minimal impact on data consumers. By regarding data work as an abstract set of principles and practices, and disregarding the human elements involved, DBAs risk propagating an ‘us versus them’ mentality, and being regarded as dogmatic, impractical, unhelpful, and obstructionist. Many disconnects – mostly clashes in frames of reference – contribute to this problem. Organizations generally regard information technology in terms of specific applications, not data, and usually see data from an application -centric point of view. The lo ng-term value to organizations of secure, reusable, high quality data, such as data as a corporate resource, is not as easily recognized or appreciated. Application development often sees data management as an impediment to application development, as something that makes development projects take longer and cost more without providing additional benefit. DBAs have been slow to adapt to changes in technolo gy (e.g., XML, objects, and service -oriented architectures) and new methods of application development (e.g., Agile Development, XP, and Scrum). Developers, on the other hand, often fail to recognize how good data management practices can help them achieve their long -term goals of object and application reuse, and true service- oriented application architecture. DBAs and other data management practitioners can help overcome these organizational and cultural obstacles. They can promote a more helpful and collaborative approach to meeting the organization’s data and information needs by following the guiding principles to identify and act on automation opportunities, building with reuse in mind, applying best practices, connecting database standards to support requirements, and setting expectations for DBAs in project work. In addition, they should: • Proactively communicate : DBAs should be in close communication with project teams, both during development and after implementation, to detect and resolve any issues as early as possible. They should review data access code, stored procedures, views, and database functions writt en by development teams and help surface any problems with database design. • Communicate with people on their level and in their terms : It is better to talk with business people in terms of business needs and ROI, and with developers in terms of object -orientation, loose coupling, and ease of development. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "206 • DMBOK2 • Stay business- focused: The objective of application development is to meet business requirements and derive maximum value from the project. • Be helpful: Always telling people ‘no’ encourages them to ignore standards and find another path. Recognize that people need to do whatever they need to do , and not helping them succeed becomes mutually detrimental. • Learn continually : Assess setbacks encountered during a project for lessons learned and apply these to future projects. If problems arise from having done things wrong, point to them later as reasons for doing things right. To sum up, understand stakeholders and their needs. Develop clear, concise, practical, business-focused standards for doing the best possible work in the best possible way. Moreover, teach and implement those standards in a way that provides maximum value to stakeholders and earns their respect. 6. Data Storage and Operations Governance 6.1 Metrics Data Storage metrics may include: • Count of databases by type • Aggregated transaction statistics • Capacity metrics, such as o Amount of storage used o Number of storage containers o Number of data objects in terms of committed and uncommitted block or pages • Data in queue • Storage service usage • Requests made against the storage services • Improvements to performance of the applications that use a service Performance metrics may be used to measure: • Transaction frequency and quantity • Query performance • API (application programming interface) service performance Operational metrics may consist of: • Aggregated statistics about data retrieval time • Backup size • Data Quality measurement • Availability Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA STORAGE AND OPERATIONS • 207 Service metrics may include • Issue submission, resolution, and escalation count by type • Issue resolution time DBAs need to discuss the need for metrics with data architects and Data Quality teams. 6.2 Information Asset Tracking Part of data storage governance includes ensuring that an organization complies with all licensing agreements and regulatory requirements. Carefully track and conduct yearly audits of software license and annual support costs, as well as server lease agreements and other fixed costs. Be ing out of compliance with licensing agreements poses serious financial and legal risks for an organization. Audit data can help determine the total cost- of-ownership (TCO) for each type of technology and technology product. Regularly evaluate technologies and products that are becoming obsolete, unsupported, less useful, or too expensive. 6.3 Data Audits and Data Validation A data audit is the evaluation of a data set based on defined criteria. Typically, an audit is performed to investigate specific concerns about a data set and is designed to determine whether the data was stored in compliance with contractual and methodological requir ements. The data audit approach may include a project- specific and comprehensive checklist, required deliverables, and quality control criteria. Data validation is the process of evaluating stored data against established acceptance criteria to determine its quality and usability. Data validation procedures depend on the criteria established by the Data Quality team (if one is in place) or other data consumer req uirements. DBAs support part of data audits and validation by: • Helping develop and review the approach • Performing preliminary data screening and review • Developing data monitoring methods • Applying statistical, geo -statistical, and bio -statistical techniques to optimize analysis of data • Supporting sampling and analysis • Reviewing data • Providing support for data discovery • Acting as SMEs for questions related to database administration 7. Works Cited / Recommended Amir, Obaid. Storage Data Migration Guide . 2012. Kindle. Armistead, Leigh. Information Operations Matters: Best Practices . Potomac Books Inc., 2010. Print. Axelos Global Best Practice (ITIL website). http://bit.ly/1H6SwxC . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "208 • DMBOK2 Bittman, Tom. “Virtualization with VMWare or HyperV: What you need to know.” Gartner Webinar, 25 November, 2009. http://gtnr.it/2rRl2aP , Web . Brewer, Eric. “Toward Robust Distributed Systems.” PODC Keynote 2000. http://bit.ly/2sVsYYv Web. Dunham, Jeff. Database Performance Tuning Handbook . McGraw -Hill, 1998. Print. Dwivedi, Himanshu. Securing Storage: A Practical Guide to SAN and NAS Security . Addison -Wesley Professional, 2005. Print. EMC Education Services, ed. Information Storage and Management: Storing, Managing, and Protecting Digital Information in Classic, Virtualized, and Cloud Environments . 2nd ed. Wiley, 2012. Print. Finn, Aidan, et al. Microsoft Private Cloud Computing . Sybex, 2013. Print. Finn, Aidan. Mastering Hyper -V Deployment . Sybex. 2010. Print. Fitzsimmons, James A. and Mona J. Fitzsimmons. Service Management: Operations, Strategy, Information Technology . 6th ed. Irwin/McGraw- Hill, 2007. Print with CDROM. Gallagher, Simon, et al. VMware Private Cloud Computing with vCloud Director . Sybex. 2013. Print. Haerder, T. and A Reuter. “Principles of transaction- oriented database recovery”. ACM Computing Surveys 15 (4) (1983). https://web.stanford.edu/class/cs340v/papers/recovery.pdf Web. Hitachi Data Systems Academy, Storage Concepts: Storing and Managing Digital Data . Volume 1. HDS Academy, Hitachi Data Systems, 2012. Print. Hoffer, Jeffrey, Mary Prescott, and Fred McFadden. Modern Database Management. 7th Edition. Prentice Hall, 2004. Print. Khalil, Mostafa. Storage Implementation in vSphere 5.0. VMware Press, 2012. Print. Kotwal, Nitin. Data Storage Backup and Replication: Effective Data Management to Ensure Optimum Performance and Business Continuity . Nitin Kotwal, 2015. Amazon Digital Services LLC. Kroenke, D. M. Database Processing: Fundamentals, Design, and Implementation . 10th Edition. Pearson Prentice Hall, 2005. Print. Liebowitz, Matt et al. VMware vSphere Performance: Designing CPU, Memory, Storage, and Networking for Performance -Intensive Workloads . Sybex, 2014. Print. Matthews, Jeanna N. et al. Running Xen: A Hands -On Guide to the Art of Virtualization. Prentice Hall, 2008. Print. Mattison, Rob. Understanding Database Management Systems . 2nd Edition. McGraw -Hill, 1998. Print. McNamara, Michael J. Scale -Out Storage: The Next Frontier in Enterprise Data Management . FriesenPress, 2014. Kindle. Mullins, Craig S. Database Administration: The Complete Guide to Practices and Procedures . Addison -Wesley, 2002. Print. Parsaye, Kamran and Mark Chignell. Intelligent Database Tools and Applications: Hyperinformation Access, Data Quality, Visualization, Automatic Discovery . John Wiley and Sons, 1993. Print. Pascal, Fabian. Practical Issues in Database Management: A Reference for The Thinking Practitioner. Addison -Wesley, 2000. Print. Paulsen, Karl. Moving Media Storage Technologies: Applications and Workflows for Video and Media Server Platforms . Focal Press, 2011. Print. Piedad, Floyd, and Michael Hawkins. High Availability: Design, Techniques and Processes . Prentice Hall, 2001. Print. Rob, Peter, and Carlos Coronel. Database Systems: Design, Implementation, and Management. 7th Edition. Course Technology, 2006. Print. Sadalage, Pramod J., and Martin Fowler. NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence. Addison -Wesley, 2012. Print. Addison -Wesley Professional. Santana, Gustavo A. Data Center Virtualization Fundamentals: Understanding Techniques and Designs for Highly Efficient Data Centers with Cisco Nexus, UCS, MDS, and Beyond . Cisco Press, 2013. Print. Fundamentals. Schulz, Greg. Cloud and Virtual Data Storage Networking . Auerbach Publications, 2011. Print. Simitci, Huseyin. Storage Network Performance Analysis . Wiley, 2003. Print. Tran, Duc A. Data Storage for Social Networks: A Socially Aware Approach. 2013 ed. Springer, 2012. Print. Springer Briefs in Optimization. Troppens, Ulf, et al. Storage Networks Explained: Basics and Application of Fibre Channel SAN, NAS, iSCSI, InfiniBand and FCoE . Wiley, 2009. Print. US Department of Defense. Information Operations: Doctrine, Tactics, Techniques, and Procedures . 2011. Kindle . VMware. VMware vCloud Architecture Toolkit (vCAT): Technical and Operational Guidance for Cloud Success . VMware Press, 2013. Print. Wicker, Stephen B. Error Control Systems for Digital Communication and Storage . US ed. Prentice -Hall, 1994. Print. Zarra, Marcus S. Core Data: Data Storage and Management for iOS, OS X, and iCloud . 2nd ed. Pragmatic Bookshelf, 2013. Print. Pragmatic Programmers. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "209 CHAPTER 7 Data Security 1. Introduction ata Security includes the planning, development, and execution of security policies and procedures to provide proper authentication, authorization, access, and auditing of data and information assets. The specifics of data security (which data needs to be protected, for example) differ between industries and countries. Nevertheless, the goal of data security practices is the same: To protect information assets in alignment with privacy and confidentiality regulations, contractual agreements, and business requirements. These requirements come from: • Stakeholders : Organizations must recognize the privacy and confidentiality needs of their stakeholders, including clients, patients, students, citizens, suppliers, or business partners. Everyone in an organization must be a responsible trustee of data about stakeholders. Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "210 • DMBOK2 • Government regulations : Government regulations are in place to protect the interests of some stakeholders. Regulations have different goals. Some restrict access to information, while others ensure openness, transparency, and accountability. • Proprietary business concerns : Each organization has proprietary data to protect. An organization’s data provides insight into its customers and, when leveraged effectively, can provide a competitive advantage. If confidential data is stolen or breached, an organization can lose competi tive advantage. • Legitimate access needs : When securing data, organizations must also enable legitimate access. Business processes require individuals in certain roles be able to access, use, and maintain data. • Contractual obligations : Contractual and non- disclosure agreements also influence data security requirements. For example, the PCI Standard, an agreement among credit card companies and individual business enterprises, demands that certain types of data be protected in defined wa ys (e.g., mandatory encryption for customer passwords). Effective data security policies and procedures ensure that the right people can use and update data in the right way, and that all inappropriate access and update is restricted (Ray, 2012) (see Figure 62). Understanding and complying with the privacy and confidentiality interests and needs of all stakeholders is in the best interest of every organization. Client, supplier, and constituent relationships all trust in, and depend on, the responsible use of d ata. Figure 62 Sources of Data Security Requirements •Regulations may restrict access to information •Acts to ensure openness and accountability •Provision of subject access rights •And more ,,,•Privacy and confidentiality of clients information •Trade secrets •Business partner activity •Mergers and acquisitions •Trade secrets •Research & other IP •Knowledge of customer needs •Business partner relationships and impending deals•Data security must be appropriate •Data security must not be too onerous to prevent users from doing their jobs •Goldilocks principleSTAKEHOLDER CONCERNSGOVERNMENT REGULATION NECESSARY BUSINESS ACCESS NEEDSLEGITIMATE BUSINESS CONCERNS Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 211 Figure 63 Context Diagram : Data Security Definition : The planning, development, and execution of security policies and procedures to provide proper authentication, authorization, access, and auditing of data and information assets within cultural and regulatory considerations. Goals : 1. Enable appropriate, and prevent inappropriate, access to enterprise data assets. 2. Understand and comply with all relevant regulations and policies for privacy, protection, and confidentiality. 3. Ensure that the privacy and confidentiality needs of all stakeholders are enforced and audited. Activities : 1.Identify Relevant Data Security Requirements (P) 2.Define Data Security Policy (C) 3.Define Data Security Standards (D) 4.Assess Current Security Risks (P) 5.Implement and Controls and Procedures (O)Inputs : •Business goals and strategy •Business rules and processes •Regulatory requirements •Enterprise Architecture standards •Enterprise Data Model •Cybersecurity updatesDeliverables : •Data security architecture •Data security policies •Data privacy and confidentiality standards •Data security access controls •Regulatory compliant data access views •Documented security classifications •Authentication and user access history •Data security audit reports Suppliers : •IT Steering Committee •Enterprise Architects •Government •Regulatory BodiesConsumers : •Business Users •Regulatory AuditorsParticipants : •Data Stewards •Information Security T eam •Internal Auditors •Process Analysts T echniques : •CRUD Matrix Usage •Immediate Security Patch •Data Security Attributes in Metadata •Security Needs in Projects •Efficient Search for Encrypted Data •Document SanitizationT ools : •Anti-virus and Security Software •Webpage Security •Identity Management T echnology •Intrusion Detection and Prevention Software •Firewalls •Metadata tracking •Data Masking / EncryptionMetrics : •Security Implementation Metrics •Security Awareness Metrics •Data Protection Metrics •Security Incident Metrics •Confidential Data Proliferation Rate (P) Planning, (C) Control, (D) Development, (O) OperationsData Security Business Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "212 • DMBOK2 1.1 Business Drivers Risk reduction and business growth are the primary drivers of data security activities. Ensuring that an organization’s data is secure reduces risk and adds competitive advantage. Security itself is a valuable asset. Data security risks are associated with regulatory compliance, fiduciary responsibility for the enterprise and stockholders, reputation, and a legal and moral responsibility to protect the private and sensitive information of employees, business partners, and customers. Organizations can be fined for failure to comply with regulations and contractual obligations. Data breaches can cause a loss of reputation and customer confidence. (See Chapter 2.) Business growth includes attaining and sustaining operational business goals. Data security issues, breaches, and unwarranted restrictions on employee access to data can directly impact operational success. The goals of mitigating risks and growing the business can be complementary and mutually supportive if they are integrated into a coherent strategy of information management and protection. 1.1.1 Risk Reduction As data regulations increase — usually in response to data thefts and breaches — so do compliance requirements. Security organizations are often tasked with managing not only IT compliance requirements, but also policies, practices, data classifications, and access authorization rules across the organization. As with other aspects of data management, it is best to address data security as an enterprise initiative. Without a coordinated effort, business units will find different solutions to security needs, increasing overall cost while potentially reducing secu rity due to inconsistent protection. Ineffective security architecture or processes can cost organizations through breaches and lost productivity. An operational security strategy that is properly funded, systems -oriented, and consistent across the enterprise will reduce these risks. Information security begins by classifying an organization’s data in order to identify which data requires protection. The overall process includes the following steps: • Identify and classify sensitive data assets : Depending on the industry and organization, there can be few or many assets, and a range of sensitive data (including personal identification, medical, financial, and more). • Locate sensitive data throughout the enterprise : Security requirements may differ, depending on where data is stored. A significant amount of sensitive data in a single location poses a high risk due to the damage possible from a single breach. • Determine how each asset needs to be protected : The measures necessary to ensure security can vary between assets, depending on data content and the type of technology. • Identify how this information interacts with business processes : Analysis of business processes is required to determine what access is allowed and under what conditions. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 213 In addition to classifying the data itself, it is necessary to assess external threats (such as those from hackers and criminals) and internal risks (posed by employees and processes). Much data is lost or exposed through the ignorance of employees who did not realize that the information was highly sensitive or who bypassed security policies. 37 The customer sales data left on a web server that is hacked, the employee database downloaded onto a contractor’s laptop that is subsequently stolen, and trade secrets left unencrypted in an executive’s computer that goes missing, all result from missing or unenforced security controls. The impact of security breaches on well -established brands in recent years has resulted in huge financial losses and a drop in customer trust. Not only are the external threats from the criminal hacking community becoming more sophisticated and targeted, the amount of damage done by external and internal threats, intentional or unintentional, has also been steadily increasing over the years (Kark, 2009). In a world of almost all- electronic, business infrastructure, trustworthy information systems have become a business differentiator. 1.1.2 Business Growth Globally, electronic technology is pervasive in the office, the marketplace, and the home. Desktop and laptop computers, smart phones, tablets, and other devices are important elements of most business and government operations. The explosive growth of e -commerce has changed how organiz ations offer goods and services. In their personal lives, individuals have become accustomed to conducting business online with goods providers, medical agencies, utilities, governmental offices, and financial institutions. Trusted e -commerce drives profit and growth. Product and service quality relate to information security in a quite direct fashion: Robust information security enables transactions and builds customer confidence. 1.1.3 Security as an Asset One approach to managing sensitive data is via Metadata. Security classifications and regulatory sensitivity can be captured at the data element and data set level. Technology exists to tag data so that Metadata travel with the information as it flows across the enterprise. Developing a master repository of data characteristics means all parts of the enterprise can know precisely what level of protection sensitive information requires. If a common standard is enforced, this approach enables multiple departments, business units, and vendors to use the same Metadata. Standard security Metadata can optimize data protection and guide business usage and technical support processes, leading to lower costs. This layer of information security can help prevent unauthorized access to and misuse of data assets. When sensitive data is correctly identified as such, organizations build trust with their customers and partners. Security- related Metadata itself becomes a strategic asset, increasing the quality of transactions, reporting, and business analysis, while reducing the cost of protection and associated risks that lost or stolen information cause. 37 One survey stated, “70 percent of IT professionals believe the use of unauthorized programs resulted in as many as half of their companies’ data loss incidents. This belief was most common in the United States (74 percent), Brazil (75 percent), and India (79 percent).” A report from the Ponomon group and Symantic Anti -Virus found that, “human errors and system problems caused two -thirds of da ta breaches in 2012. http://bit.ly/1dGChAz , http://symc.ly/1FzNo5l , http://bit.ly/2sQ68Ba , http://bit.ly/2tNEkKY . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "214 • DMBOK2 1.2 Goals and Principles 1.2.1 Goals The goals of data security activities include: • Enabling appropriate access and preventing inappropriate access to enterprise data assets • Enabling compliance with regulations and policies for privacy, protection, and confidentiality • Ensuring that stakeholder requirements for privacy and confidentiality are met 1.2.2 Principles Data security in an organization follows these guiding principles: • Collaboration: Data Security is a collaborative effort involving IT security administrators, data stewards/ Data Governance , internal and external audit teams, and the legal department. • Enterprise approach : Data Security standards and policies must be applied consistently across the entire organization. • Proactive management: Success in data security management depends on being proactive and dynamic, engaging all stakeholders, managing change, and overcoming organizational or cultural bottlenecks such as traditional separation of responsibilities between information security, information technology, data administration, and business stakeholders. • Clear accountability : Roles and responsibilities must be clearly defined, including the ‘chain of custody’ for data across organizations and roles. • Metadata -driven: Security classification for data elements is an essential part of data definitions. • Reduce risk by reducing exposure : Minimize sensitive/confidential data proliferation, especially to non-production environments. 1.3 Essential Concepts Information security has a specific vocabulary. Knowledge of key terms enables clearer articulation of governance requirements. 1.3.1 Vulnerability A vulnerability is a weaknesses or defect in a system that allows it to be successfully attacked and compromised – essentially a hole in an organization’s defenses. Some vulnerabilities are called exploits . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 215 Examples include network computers with out -of-date security patches, web pages not protected with robust passwords, users not trained to ignore email attachments from unknown senders, or corporate software unprotected against technical commands that will give the attacker control of the system. In many cases, non - production environments are more vulnerable to threats than production environments. Thus, it is critical to keep production data out of non -production environments. 1.3.2 Threat A threat is a potential offensive action that could be taken against an organization. Threats can be internal or external. They are not always malicious. An uniformed insider can take offensive actions again the organization without even knowing it. Threats may relate to specific vulnerabilities, which then can be prioritized for remediation. Each threat should match to a capability that either prevents the threat or limits the damage it might cause. An occurrence of a threat is also called an attack surface. Examp les of threats include virus -infected email attachments being sent to the organization, processes that overwhelm network servers and result in an inability to perform business transactions (also called denial- of-service attacks), and exploitation of known vulnerabilities. 1.3.3 Risk The term risk refers both to the possibility of loss and to the thing or condition that poses the potential loss. Risk can be calculated for each possible threat using the following factors. • Probability that the threat will occur and its likely frequency • The type and amount of damage created each occurrence might cause, including damage to reputation • The effect damage will have on revenue or business operations • The cost to fix the damage after an occurrence • The cost to prevent the threat, including by remediation of vulnerabilities • The goal or intent of the probable attacker Risks can be prioritized by potential severity of damage to the company, or by likelihood of occurrence, with easily exploited vulnerabilities creating a higher likelihood of occurrence. Often a priority list combines both metrics. Prioritization of risk must be a formal process among the stakeholders. 1.3.4 Risk Classifications Risk classifications describe the sensitivity of the data and the likelihood that it might be sought after for malicious purposes. Classifications are used to determine who (i.e. , people in which roles) can access the data. The highest security classification of any datum within a user entitlement determines the security classification of the entire aggregation. Example classifications include: • Critical Risk Data (CRD) : Personal information aggressively sought for unauthorized use by both internal and external parties due to its high direct financial value. Compromise of CRD would not Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "216 • DMBOK2 only harm individuals, but would result in financial harm to the company from significant penalties, costs to retain customers and employees, as well as harm to brand and reputation. • High Risk Data (HRD) : HRD is actively sought for unauthorized use due to its potential direct financial value. HRD provides the company with a competitive edge. If compromised, it could expose the company to financial harm through loss of opportunity. Loss of HRD can cause mi strust leading to the loss of business and may result in legal exposure, regulatory fines and penalties, as well as damage to brand and reputation. • Moderate Risk Data (MRD) : Company information that has little tangible value to unauthorized parties; however, the unauthorized use of this non -public information would likely have a negative effect on the company. 1.3.5 Data Security Organization Depending on the size of the enterprise, the overall Information Security function may be the primary responsibility of a dedicated Information Security group, usually within the Information Technology (IT) area. Larger enterprises often have a Chief Information Security Officer (CISO) who reports to either the CIO or the CEO. In organizations without dedicated Information Security personnel, responsibility for data security will fall on data managers. In all cases, data managers need to be involved in data security efforts. In large enterprises, the information security personnel may let specific Data Governance and user authorization functions be guided by the business managers. Examples include granting user authorizations and data regulatory compliance. Dedicated Information Security personnel are often most concerned with the technical aspects of information protection such as combating malicious software and system attacks. However, there is ample room for collaboration during development or an installation project. This opportunity for synergy is often missed when the two governance entities, IT and Data Management, lack an organized process to share regulatory and security requirements. They need a standard procedure to inform each other of data regulations, data lo ss threats, and data protection requirements, and to do so at the commencement of every software development or installation project. The first step in the NIST (National Institute of Standards and Technology) Risk Management Framework , for example, is to categorize all enterprise information. 38 Creating an enterprise data model is essential to this goal. Without clear visibility to the location of all sensitive information, it is impossible to create a comprehensive and effective data protection program. Data managers need to be actively engaged with information technology developers and cyber security professionals so that regulated data may be identified, sensitive systems can be properly protected, and user access controls can be designed to enforce confidentiality, integrity, and data regulatory compliance. The larger the enterprise, the more important becomes the need for teamwork and reliance on a correct and updated enterprise data model. 38 National Institute of Standards and Technology (US) http://bit.ly/1eQYolG . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 217 1.3.6 Security Processes Data security requirements and procedures are categorized into four groups, known as the four A’s: Access, Audit, Authentication, and Authorization. Recently an E, Entitlement, has been included, for effective data regulatory compliance. Information classification, access rights, role groups, users, and passwords are the means to implementing policy and satisfying the four A’s. Security Monitoring is also essential for proving the success of the other processes. Both monitoring and audit can be done continuously or inte rmittently. Formal audits must be done by a third party to be considered valid. The third party may be internal or external. 1.3.6.1 The Four A’s • Access : Enable individuals with authorization to access systems in a timely manner. Used as a verb, access means to actively connect to an information system and be working with the data. Used as a noun, access indicates that the person has a valid authorization to the data. • Audit : Review security actions and user activity to ensure compliance with regulations and conformance with company policy and standards. Information security professionals periodically review logs and documents to validate compliance with security regulations, policies, and standards. Results of these audits are published periodically. • Authentication : Validate users’ access. When a user tries to log into a system, the system needs to verify that the person is who he or she claims to be. Passwords are one way of doing this. More stringent authentication methods include the person having a security toke n, answering questions, or submitting a fingerprint. All transmissions during authentication are encrypted to prevent theft of the authenticating information. • Authorization : Grant individuals privileges to access specific views of data, appropriate to their role. After the authorization decision, the Access Control System checks each time a user logs in to see if they have a valid authorization token. Technically, this is an entry in a data field in the corporate Active Directory indicating that the person has been authorized by somebody to access the data. It further indicates that a responsible person made the decision to grant this authorization because the user is entitled to it by virtue of their job or corporate status. • Entitlement : An Entitlement is the sum total of all the data elements that are exposed to a user by a single access authorization decision. A responsible manager must decide that a person is ‘entitled’ to access this information before an authorization request is generated. An inventory of all the data exposed by each entitlement is necessary in determining regulatory and confidentiality requirements for Entitlement decision s. 1.3.6.2 Monitoring Systems should include monitoring controls that detect unexpected events, including potential security violations. Systems containing confidential information, such as salary or financial data, commonly implement active, real -time monitoring that alerts the security administrator to suspicious activity or inappropriate access. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "218 • DMBOK2 Some security systems will actively interrupt activities that do not follow specific access profiles. The account or activity remains locked until security support personnel evaluate the details. In contrast, passive monitoring tracks changes over time by taking snapshots of the system at regular intervals, and comparing trends against a benchmark or other criteria. The system sends reports to the data stewards or security administrator accountable for the data. While active monitoring is a detection mechan ism, passive monitoring is an assessment mechanism. 1.3.7 Data Integrity In security, data integrity is the state of being whole – protected from improper alteration, deletion, or addition. For example, in the U.S., Sarbanes -Oxley regulations are mostly concerned with protecting financial information integrity by identifying rules for how financial infor mation can be created and edited. 1.3.8 Encryption Encryption is the process of translating plain text into complex codes to hide privileged information, verify complete transmission, or verify the sender’s identity. Encrypted data cannot be read without the decryption key or algorithm, which is usually stored separ ately and cannot be calculated based on other data elements in the same data set. There are four main methods of encryption – hash, symmetric, private -key, and public -key – with varying levels of complexity and key structure. 1.3.8.1 Hash Hash encryption uses algorithms to convert data into a mathematical representation. The exact algorithms used and order of application must be known in order to reverse the encryption process and reveal the original data. Sometimes hashing is used as verification of transmission integrity or identity. Common hashing algorithms are Message Digest 5 (MD5) and Secure Hashing Algorithm (SHA). 1.3.8.2 Private -key Private -key encryption uses one key to encrypt the data. Both the sender and the recipient must have the key to read the original data. Data can be encrypted one character at a time (as in a stream) or in blocks. Common private -key algorithms include Data Encryption Standard (D ES), Triple DES (3DES), Advanced Encryption Standard (AES), and International Data Encryption Algorithm (IDEA). Cyphers Twofish and Serpent are also considered secure. The use of simple DES is unwise as it is susceptible to many easy attacks. 1.3.8.3 Public -key In public -key encryption, the sender and the receiver have different keys. The sender uses a public key that is freely available, and the receiver uses a private key to reveal the original data. This type of encryption is useful Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 219 when many data sources must send protected information to just a few recipients, such as when submitting data to clearinghouses. Public -key methods include Rivest- Shamir -Adelman (RSA) Key Exchange and Diffie- Hellman Key Agreement . PGP (Pretty Good Privacy) is a freely available application of public -key encryption. 1.3.9 Obfuscation or Masking Data can be made less available by obfuscation (making obscure or unclear) or masking , which removes, shuffles, or otherwise changes the appearance of the data, without losing the meaning of the data or the relationships the data has to other data sets, such as foreign key relationships to other objects or systems. The values within the attributes may change, but the new values are still valid for those attributes. Obfuscation is useful when displaying sensitive information on screens for reference, or creating test data sets from production data that comply with expected application logic. Data masking is a type of data -centric security. There are two types of data masking, persistent and dynamic. Persistent masking can be executed in -flight or in -place. 1.3.9.1 Persistent Data M asking Persistent data masking permanently and irreversibly alters the data. This type of masking is not typically used in production environments, but rather between a production environment and development or test environments. Persistent masking changes the data, but the data must s till be viable for use to test processes, application, report, etc. • In-flight persistent masking occurs when the data is masked or obfuscated while it is moving between the source (typically production) and destination (typically non -production) environment. In-flight masking is very secure when properly executed because it does not leave an intermed iate file or database with unmasked data. Another benefit is that it is re -runnable if issues are encountered part way through the masking. • In-place persistent masking is used when the source and destination are the same. The unmasked data is read from the source, masked, and then used to overwrite the unmasked data. In -place masking assumes the sensitive data is in a location where it should not exist and the risk needs to be mitigated, or that there is an extra copy of the data in a secure location to mask before moving it to the non -secure location. There are risks to this process. If the masking process fails mid -masking, it can be difficul t to restore the data to a useable format. This technique has a few niche uses, but in general, in -flight masking will more securely meet project needs. 1.3.9.2 Dynamic Data M asking Dynamic data masking changes the appearance of the data to the end user or system without changing the underlying data. This can be extremely useful when users need access to some sensitive production data, but not all of it. For example, in a database the social security number is stored as 123456789, but to the call center associate that needs to verify who they are speaking to, the data shows up as *** -**-6789. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "220 • DMBOK2 1.3.9.3 Masking Methods There are several methods for masking or obfuscating data. • Substitution : Replace characters or whole values with those in a lookup or as a standard pattern. For example, first names can be replaced with random values from a list. • Shuffling : Swap data elements of the same type within a record, or swap data elements of one attribute between rows. For example, mixing vendor names among supplier invoices such that the original supplier is replaced with a different valid supplier on an invoice. • Temporal v ariance : Move dates +/ – a number of days – small enough to preserve trends but significant enough to render them non -identifiable. • Value variance : Apply a random factor +/ – a percent, again small enough to preserve trends, but significant enough to be non -identifiable. • Nulling or deleting : Remove data that should not be present in a test system. • Randomization: Replace part or all of data elements with either random characters or a series of a single character. • Encryption : Convert a recognizably meaningful character stream to an unrecognizable character stream by means of a cipher code. An extreme version of obfuscation in -place. • Expression masking : Change all values to the result of an expression. For example, a simple expression would just hard code all values in a large free form database field (that could potentially contain confidential data) to be ‘This is a comment field’. • Key masking : Designate that the result of the masking algorithm/process must be unique and repeatable because it is being used to mask a database key field (or similar). This type of masking is extremely important for testing to maintain integrity around the organization. 1.3.10 Network Security Terms Data security includes both data -at-rest and data- in-motion. Data- in-motion requires a network in order to move between systems. It is no longer sufficient for an organization to wholly trust in the firewall to protect it from malicious software, poisoned email, or social engineering attacks. Each machine on the network needs to have a line of defense, and web servers need sophisticated protection as they are continually exposed to the entire world on the Internet. 1.3.10.1 Backdoor A backdoor refers to an overlooked or hidden entry into a computer system or application. It allows unauthorized users to bypass the password requirement to gain access. Backdoors are often created by Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 221 developers for maintenance purposes. Any backdoor is a security risk. Other backdoors are put in place by the creators of commercial software packages. Default passwords left unchanged when installing any software system or web page package is a backdoor and will undoubtedly be known to hackers. Any backdoor is a security risk. 1.3.10.2 Bot or Zombie A bot (short for robot) or Zombie is a workstation that has been taken over by a malicious hacker using a Trojan, a Virus, a Phish, or a download of an infected file. Remotely controlled, bots are used to perform malicious tasks, such as sending large amounts of spam, attacking legitimate businesses with network -clogging Internet packets, performing illegal money transfers, and hosting fraudulent website s. A Bot- Net is a network of robot computers (infected machines). 39 It was estimated in 2012 that globally 17% of all computers (approximately 187 million of 1.1 Billion computers) do not have anti- virus protection . 40 In the USA that year, 19.32% of users surfed unprotected. A large percentage of them are Zombies. Estimates are that two billion computers are in operation as of 2016 .41 Considering that desktop and laptop computers are being eclipsed in number by smart phones, tablets, wearables, and other devices, many of which are used for business transactions, the risks for data exposure will only increase. 42 1.3.10.3 Cookie A cookie is a small data file that a website installs on a computer’s hard drive, to identify returning visitors and profile their preferences. Cookies are used for Internet commerce. However, they are also controversial, as they raise questions of privacy because spyware sometimes uses them. 1.3.10.4 Firewall A firewall is software and/or hardware that filters network traffic to protect an individual computer or an entire network from unauthorized attempts to access or attack the system. A firewall may scan both incoming and outgoing communications for restricted or regu lated information and prevent it from passing without permissio n (Data Loss Prevention). Some firewalls also restrict access to specific external websites. 39 http://bit.ly/1FrKWR8 , http://bit.ly/2rQQuWJ. 40 http://tcrn.ch/2rRnsGr (17% globally lack AV), http://bit.ly/2rUE2R4 , http://bit.ly/2sPLBN4 , http://ubm.io/1157kyO (Windows 8 lack of AV) . 41 http://bit.ly/2tNLO0i (2016 number reaches 2 billion.) , http://bit.ly/2rCzDCV, http://bit.ly/2tNpwfg. 42 Cisco Corporation estimated that “By 2018, there will be 8.2 billion handheld or personal mobile -ready devices and 2 billion machine -to-machine connections (e.g., GPS systems in cars, asset tracking systems in shipping and manufacturing sectors, or medical applications making patient records and health status more readily available.)” http://bit.ly/Msevdw ( future numbers of computers and devices) . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "222 • DMBOK2 1.3.10.5 Perimeter A perimeter is the boundary between an organization’s environments and exterior systems. Typically, a firewall will be in place between all internal and external environments. 1.3.10.6 DMZ Short for de -militarized zone, a DMZ is an area on the edge or perimeter of an organization, with a firewall between it and the organization. A DMZ environment will always have a firewall between it and the internet (see Figure 64). DMZ environments are used to pass or temporarily store data moving between organizations. Figure 64 DMZ Example 1.3.10.7 Super User Account A Super User Account is an account that has administrator or root access to a system to be used only in an emergency. Credentials for these accounts are highly secured, only released in an emergency with appropriate documentation and approvals, and expire within a short time. For example, the staff assigned to production control might require access authorizations to multiple large systems, but these authorizations should be tightly controlled by time, user ID, location , or other requirement to prevent abus e. 1.3.10.8 Key Logger Key Loggers are a type of attack software that records all the keystrokes that a person types into their keyboard, then sends them elsewhere on the Internet. Thus, every password, memo, formula, document, and web address is captured. Often an infected website or mali cious software download will install a key logger. Some types of document downloads will allow this to happen as well. 1.3.10.9 Penetration Testing Setting up a secure network and website is incomplete without testing it to make certain that it truly is secure. In Penetration Testing (sometimes called the ‘pentest’), an ethical hacker, either from the organization itself or hired from an external security firm, attempts to break into the system from outside, as would a malicious DMZInternal Systems Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 223 hacker, in order to identify system vulnerabilities. Vulnerabilities found through penetration tests can be addressed before the application is released. Some people are threatened by ethical hacking audits because they believe these audits will result only in finger pointing. The reality is that in the fast- moving conflict between business security and criminal hacking, all purchased and internally -develop ed software contains potential vulnerabilities that were not known at the time of their creation. Thus, all software implementations must be challenged periodically. Finding vulnerabilities is an ongoing procedure and no blame should be applied – only secu rity patches. As proof of the need for continual software vulnerability mitigation, observe a constant stream of security patches arriving from software vendors. This continual security patch update process is a sign of due diligence and professional customer support fr om these vendors. Many of these patches are the result of ethical hacking performed on behalf of the vendors. 1.3.10.10 Virtual Private Network (VPN) VPN connections use the unsecured internet to create a secure path or ‘tunnel’ into an organization’s environment. The tunnel is highly encrypted. It allows communication between users and the internal network by using multiple authentication elements to connect with a firewall on the perimeter of an organization’s environment. Then it strongly encrypts all transmitted data. 1.3.11 Types of Data Security Data security involves not just preventing inappropriate access, but also enabling appropriate access to data. Access to sensitive data should be controlled by granting permissions (opt -in). Without permission, a user should not be allowed to see data or t ake action within the system. ‘Least Privilege’ is an important security principle. A user, process, or program should be allowed to access only the information allowed by its legitimate purpose. 1.3.11.1 Facility Security Facility security is the first line of defense against bad actors. Facilities should have, at a minimum, a locked data center with access restricted to authorized employees. Social threats to security (See Section 1.3.15) recognize humans as the weakest point in facility security. Ensure that employees have the tools and training to protect data in facilities. 1.3.11.2 Device Security Mobile devices, including laptops, tablets, and smartphones, are inherently insecure, as they can be lost, stolen, and physically and electronically attacked by criminal hackers. They often contain corporate emails, Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "224 • DMBOK2 spreadsheets, addresses, and documents that, if exposed, can be damaging to the organization, its employees, or its customers. With the explosion of portable devices and media, a plan to manage the security of these devices (both company -owned and personal) must be part of any company’s overall strategic security architecture. This plan should include both software and hardware to ols. Device security standards include: • Access policies regarding connections using mobile devices • Storage of data on portable devices such as laptops, DVDs, CDs, or USB drives • Data wiping and disposal of devices in compliance with records management policies • Installation of anti- malware and encryption software • Awareness of security vulnerabilities 1.3.11.3 Credential Security Each user is assigned credentials to use when obtaining access to a system. Most credentials are a combination of a User ID and a Password. There is a spectrum of how credentials are used across systems within an environment, depending on the sensitivity of the system’s data and the system’s capabilities to link to credential repositories. 1.3.11.3.1 Identity Management Systems Traditionally, users have had different accounts and passwords for each individual resource, platform, application system, or workstation. This approach requires users to manage several passwords and accounts. Organizations with enterprise user directories may have a synchronization mechanism established between the heterogeneous resources to ease user password management. In such cases, the user is required to enter the password only once, usually when logging into the workstation, after which all authenti cation and authorization execute through a reference to the enterprise user directory. An identity management system implementing this capability is known as ‘single -sign-on’ and is optimal from a user perspective. 1.3.11.3.2 User ID Standards for Email Systems User IDs should be unique within the email domain. Most companies use some first name or initial and a full or partial last name as the email or network ID, with a number to differentiate collisions. Names are generally known and are more useful for business contact reasons. Email or network IDs containing system employee ID numbers are discouraged, as that information is not generally available outside the organization and provides data that should be secure within the systems. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 225 1.3.11.3.3 Password Standards Passwords are the first line of defense in protecting access to data. Every user account should be required to have a password set by the user (account owner) with a sufficient level of password complexity defined in the security standards, commonly referred to as ‘strong’ passwords. When creating a new user account, the generated temporary password should be set to expire immediately after the first use and the user must choose a new password for subsequent access. Do not permit blank passwords. Most security experts recommend requiring users to change their passwords every 45 to 180 days, depending on the nature of the system, the type of data, and the sensitivity of the enterprise. However, changing passwords too frequently introduces risk since it often causes employees to write down their new passwords. 1.3.11.3.4 Multiple Factor Identification Some systems require additional identification procedures. These can include a return call to the user’s mobile device that contains a code, the use of a hardware item that must be used for login, or a biometric factor such as fingerprint, facial recognition, or retinal scan. Two -factor identification makes it much harder to break into an account or to log into a user’s device. All users with authorization entitlement to highly sensitive information should use two -factor identification to log into the netwo rk. 1.3.11.4 Electronic Communication Security Users must be trained to avoid sending their personal information or any restricted or confidential company information over email or direct communication applications. These insecure methods of communication can be read or intercepted by outside sources. Once a user sends an email, he or she no longer controls the information in it. It can be forwarded to other people without the sender’s knowledge or consent. Social media also applies here. Blogs, portals, wikis, forums, and other Internet or Intranet social media should be considered insecure and should not contain confidential or restricted information. 1.3.12 Types of Data Security Restrictions Two concepts drive security restrictions: the level of confidentiality of data and regulation related to data. • Confidentiality level: Confidential means secret or private. Organizations determine which types of data should not be known outside the organization or even within certain parts of the organization. Confidential information is shared only on a ‘need -to-know’ basis. Levels of confidentialit y depend on who needs to know certain kinds of information. • Regulation : Regulatory categories are assigned based on external rules, such as laws, treaties, customs agreements, and industry regulations. Regulatory information is shared on an ‘allowed -to-know’ basis. The ways in which data can be shared are governed by the details of the regulation. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "226 • DMBOK2 The main difference between confidential and regulatory restrictions is where the restriction originates: confidentiality restrictions originate internally, while regulatory restrictions are externally defined. Another difference is that any data set, such as a document or a database view, can only have one confidentiality level. This level is established based on the most sensitive (and highest classified) item in the data set. Regulatory categorizations, however, are additive. A single data set may have data restricted based on multiple regulatory categories. To assure regulatory compliance, enforce all actions required for each category, along with the confidentiality requirements. When applied to the user entitlement (the aggregation of the particular data elements to which a user authorization provides access), all protection policies must be followed, regardless of whether they originated internally or externally. 1.3.12.1 Confidential Data Confidentiality requirements range from high (very few people have access, for example, to data about employee compensation) to low (everyone has access to product catalogs). A typical classification schema might include two or more of the five confidentiality cl assification levels listed here: • For general audiences : Information available to anyone, including the public. • Internal use only : Information limited to employees or members, but with minimal risk if shared. For internal use only; may be shown or discussed, but not copied, outside the organization. • Confidential : Information that cannot be shared outside the organization without a properly executed non -disclosure agreement or similar in place. Client confidential information may not be shared with other clients. • Restricted confidential : Information limited to individuals performing certain roles with the ‘need to know.’ Restricted confidential may require individuals to qualify through clearance. • Registered confidential : Information so confidential that anyone accessing the information must sign a legal agreement to access the data and assume responsibility for its secrecy. The confidentiality level does not imply any details about restrictions due to regulatory requirements. For example, it does not inform the data manager that data may not be exposed outside its country of origin, or that some employees are prohibited from seeing certain information based on regulations like HIPAA. 1.3.12.2 Regulated Data Certain types of information are regulated by external laws, industry standards, or contracts that influence how data can be used, as well as who can access it and for what purposes. As there are many overlapping regulations, it is easier to collect them by subject area into a few regulatory categories or families to better inform data managers of regulatory requirements. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 227 Each enterprise, of course, must develop regulatory categories that meet their own compliance needs. Further, it is important that this process and the categories be as simple as possible to allow for an actionable protection capability. When category protective actions are similar, they should be combined into a regulation ‘family’. Each regulatory category should include auditable protective actions. This is not an organizational tool but an enforcement method. Since different industries are affected by different types of regulations, the organization needs to develop regulatory groupings that meet their operational needs. For example, companies that do no business outside of their native land may not need to incorporate regulations pertaining to exports. However, since all nations have some mixture of personal data privacy laws, and customers are likely to be from anywhere in the world, it may be wise and easier to gather all customer data privacy regulations into a single regulatory family and comply with the requirements for all nations. Doing so ensures compliance everywhere and offers a single standard to enforce. An example of the possible detail of regulatory compliance is one that prohibits by law a single type of data element in the database to travel outside the physical borders of the originating nation. Several regulations, both domestic and international, have this as a requirement. An optimal number of regulatory action categories is nine or fewer. Sample regulatory categories follow . 1.3.12.2.1 Sample Regulatory Families Certain government regulations specify data elements by name and demand that they be protected in specific ways. Each element does not need a different category; instead, use a single family of actions to protect all specifically targeted data fields. Some PCI data may be included in these categories even though it is a contractual obligation and not a governmental regulation. PCI contractual obligations are mostly uniform around the globe. • Personal Identification Information (PII) : Also known as Personally Private Information (PPI) , includes any information that can personally identify the individual (individually or as a set), such as name, address, phone numbers, schedule, government ID number, account numbers, age, race, religion, ethnicity, birthday, family members’ names or friends’ names, employment information (HR data), and in many cases, remuneration. Highly similar protective actions will satisfy the EU Privacy Directives , Canadian Privacy law (PIPEDA) , PIP Act 2003 in Japan, PCI standards, US FTC requirements , GLB, FTC standards, and most Security Breach of Information Acts . • Financially Sensitive Data : All financial information, including what may be termed ‘shareholder’ or ‘insider’ data, including all current financial information that has not yet been reported publicly. It also includes any future business plans not made public, planned mergers, acq uisitions, or spin -offs, non-public reports of significant company problems, unexpected changes in senior management, comprehensive sales, orders, and billing data. All of these can be captured within this one category and protected by the same policies. I n the US, this is covered under Insider Trading Laws, SOX (Sarbanes -Oxley Act), or GLBA (Gramm -Leach -Bliley/Financial Services Modernization Act). Note: the Sarbanes -Oxley A ct restricts and manages who can change financial data, thus assuring data integrity, while Insider Trading laws affect all those who can see financial data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "228 • DMBOK2 • Medically Sensitive Data /Personal Health Information (PHI) : All information regarding a person’s health or medical treatments. In the US, this is covered by HIPAA (Health Information Portability and Accountability Act). Other nations also have restrictive laws regarding the protection of personal and medical information. As these are evolving, ensure Corporate Counsel is aware of the need to follow legal requirements in a nation in which the organization does business or has customers. • Educational Records : All information regarding a person’s education. In the US, this is covered by FERPA (Family Educational Rights and Privacy Act). 1.3.12.2.2 Industry or Contract -based Regulation Some industries have specific standards for how to record, retain, and encrypt information. Some also disallow deletion, editing, or distributing to prohibited locations. For example, regulations for pharmaceuticals, other dangerous substances, food, cosmetics, and advanced technology prevent the transmission or storage of certain information outside the country of origin, or require data to be encrypted during transport. • Payment Card Industry Data Security Standard (PCI -DSS) : PCI -DSS is the most widely known industry data security standard. It addresses any information that can identify an individual with an account at a financial organization, such as name, credit card number (any number on the card), bank account number, or account expiration date. Most of these data fields are regulated by laws and policies. Any data with this classification in its Metadata definition automatically should be carefully reviewed by data stewards when included in any database, applic ation, report, dashboard, or user view. • Competitive advantage or trade secrets : Companies that use proprietary methods, mixtures, formulas, sources, designs, tools, recipes, or operational techniques to achieve a competitive advantage may be protected by industry regulations and/or intellectual property laws. • Contractual restrictions : In its contracts with vendors and partners, an organization may stipulate how specific pieces of information may or may not be used, and which information can and cannot be shared. For example, environmental records, hazardous materials reports, batch numbers, cooking times, points of origin, customer passwords, account numbers, and certain national identity numbers of non -US nationals. Specific technical companies may need to include certain restricted products or ingredients in this category. 1.3.13 System Security Risks The first step in identifying risk is identifying where sensitive data is stored, and what protections are required for that data. It is also necessary to identify risks inherent in systems. System security risks include elements that can compromise a network or database. These threats all ow legitimate employees to misuse information, either intentionally or accidentally, and enable malicious hacker success. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 229 1.3.13.1 Abuse of Excessive Privilege In granting access to data, the principle of least privilege should be applied. A user, process, or program should be allowed to access only the information allowed by its legitimate purpose. The risk is that users with privileges that exceed the requirements of their job function may abuse these privileges for malicious purposes or by accident. Users may be granted more access than they should have (excessive privilege) simply because it is challenging to manage user entitlements. The DBA may not have the time or Metadata to define and update granular access privilege control mechanisms for each user entitlement. As a result, many users receive generic default access privileges that far exceed specific job requirements. This lack of oversight of user entitlements is one reason why many data regulations speci fy data management security. The solution to excessive privileges is query- level access control, a mechanism that restricts database privileges to minimum -required SQL operations and data. The granularity of data access control must extend beyond the table to specific rows and columns within a table. Query- level access control is useful for detecting excessive privilege abuse by malicious employees. Most database software implementations integrate some level of query -level access control (triggers, row -level security, table security, views), but the manual nature of these ‘built- in’ features make them impractical for all but the most limited deploymen ts. The process of manually defining a query -level access control policy for all users across database rows, columns, and operations is time consuming. To make matters worse, as user roles change over time, query policies must be updated to reflect those new roles. Most database administrators would have a hard time defining a useful query policy for a handful of users at a single point in time, much less hundreds of users over time. As a result, in a large number of organizations, automated tools are usually necessary to make real query -level access control functional. 1.3.13.2 Abuse of Legitimate Privilege Users may abuse legitimate database privileges for unauthorized purposes. Consider a criminally inclined healthcare worker with privileges to view individual patient records via a custom Web application. The structure of corporate Web applications normally limits users to viewing an individual patient’s healthcare history, where multiple records cannot be viewed simultaneously and electronic copies are not allowed. However, the worker may circumvent these limitations by connecting to the database using an alternative system such as MS -Excel. Using MS -Excel and his legitimate login credentials, the worker might retrieve and save all patient records. There are two risks to consider: intentional and unintentional abuse. Intentional abuse occurs when an employee deliberately misuses organizational data. For example, an errant worker who wants to trade patient records for money or for intentional damage, such as releasing (or threatening to release) sensitive information publicly. Unintentional abuse is a more common risk: The diligent employee who retrieves and stores large amounts of patient information to a work machine for what he or she considers legitimate work purposes. Once the data exists on an endpoint machine, it becomes vulnerable to lapto p theft and loss. The partial solution to the abuse of legitimate privilege is database access control that not only applies to specific queries, but also enforces policies for end -point machines using time of day, location monitoring, and amount of information downloaded, and reduces the ability of any user to have unlimited access to all records Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "230 • DMBOK2 containing sensitive information unless it is specifically demanded by their job and approved by their supervisor. For example while it may be necessary for a field agent to access their customer’s personal records, they might not be allowed to download the entire customer database to their laptop just to ‘save time’. 1.3.13.3 Unauthorized Privilege Elevation Attackers may take advantage of database platform software vulnerabilities to convert access privileges from those of an ordinary user to those of an administrator. Vulnerabilities may occur in stored procedures, built- in functions, protocol implementations, and even SQL statements. For example, a software developer at a financial institution might take advantage of a vulnerable function to gain the database administrative privilege. With administrative privilege, the offending developer may turn off audit mechanisms, create bogus accounts, transfer funds, or close accounts. Prevent privilege elevation exploits with a combination of traditional intrusion prevention systems (IPS) and query -level access control intrusion prevention. These systems inspect database traffic to identify patterns that correspond to known vulnerabilities. For example, if a given function is vulnerable to an attack, an IPS may either block all access to t he procedure, or block those procedures allowing embedded attacks. Combine IPS with alternative attack indicators, such as query access control, to improve accuracy in identifying attacks. IPS can detect whether a database request accesses a vulnerable function while query access control detects whether the request matches normal user behavior. If a single request indicates both access to a vulnerable function and unusual behavior, then an attack is almost certainly occurring. 1.3.13.4 Service Account or Shared Account Abuse Use of service accounts (batch IDs) and shared accounts (generic IDs) increases the risk of data security breaches and complicates the ability to trace the breach to its source. Some organizations further increase their risk when they configure monitoring systems to ignore any alerts related to these accounts. Information security managers should consider adopting tools to manage service accounts securely. 1.3.13.4.1 Service Accounts Service accounts are convenient because they can tailor enhanced access for the processes that use them. However, if they are used for other purposes, they are untraceable to a particular user or administrator. Unless they have access to decryption keys, service accounts do not threaten encrypted data. This may be especially important for data held on servers storing legal documents, medical information, trade secrets, or confidential executive planning. Restrict the use of service accounts to specific tasks or commands on specific systems, and require documentation and approval for distributing the credentials. Consider assigning a new password every time distribution occurs, using processes such as those in place for Super User accounts. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 231 1.3.13.4.2 Shared Accounts Shared accounts are created when an application cannot handle the number of user accounts needed or when adding specific users requires a large effort or incurs additional licensing costs. For shared accounts, credentials are given to multiple users, and the password is rarely changed due to the effort to notify all users. Because they provide essentially ungoverned access, any use of shared accounts should be carefully evaluated. They should never be used by default. 1.3.13.5 Platform Intrusion Attacks Software updates and intrusion prevention protection of database assets require a combination of regular software updates (patches) and the implementation of a dedicated Intrusion Prevention System (IPS). An IPS is usually, but not always, implemented alongside an Intrusion Detection System (IDS). The goal is to prevent the vast majority of network intrusion attempts and to respond quickly to any intrusion that has succeeded in working its way past a prevention system. The most primitive form of intrusion protection is a firewall, but with mobile u sers, web access, and mobile computing equipment a part of most enterprise environments, a simple firewall, while still necessary, is no longer sufficient. Vendor- provided updates reduce vulnerabilities found in database platforms over time. Unfortunately, software updates are often implemented by enterprises according to periodic maintenance cycles rather than as soon as possible after the patches are made a vailable. In between update cycles, databases are not protected. In addition, compatibility problems sometimes prevent software updates altogether. To address these problems, implement IPS. 1.3.13.6 SQL Injection Vulnerability In a SQL injection attack, a perpetrator inserts (or ‘injects’) unauthorized database statements into a vulnerable SQL data channel, such as stored procedures and Web application input spaces. These injected SQL statements are passed to the database, where they are often executed as legitimate commands. Using SQL injection, attackers may gain unrestricted access to an entire database. SQL i njections are also used to attack the DBMS, by passing SQL commands as a parameter of a function or stored procedure. For example, a component that provides backup functionality usually runs at a hi gh privilege; calling a SQL injection vulnerable function in that specific component could allow a regular user to escalate their privileges, become a DBA and take over the database. Mitigate this risk by sanitizing all inputs before passing them back to the server. 1.3.13.7 Default Passwords It is a long -standing practice in the software industry to create default accounts during the installation of software packages. Some are used in the installation itself. Others provide users with a means to test the software out of the box. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "232 • DMBOK2 Default passwords are part of many demo packages. Installation of third party software creates others. For example, a CRM package might create several accounts in the backend database, for install, test, and admin and for regular users. SAP creates a number of default data base users at the time of installation. The DBMS industry also engages in this practice. Attackers are constantly looking for an easy way to steal sensitive data. Mitigate threats to sensitive data by creating the required username and password combinations, and ensuring the no default passwords are left in place in the DBMS. Eliminating the default passwords is an important security step after every implementation. 1.3.13.8 Backup Data Abuse Backups are made to reduce the risks associated with data loss, but backups also represent a security risk. The news offers many stories about lost backup media. Encrypt all database backups. Encryption prevents loss of a backup either in tangible media or in electronic transit. Securely manage backup decryption keys. Keys must be available off -site to be useful for disaster recovery. 1.3.14 Hacking / Hacker The term hacking came from an era when finding clever ways to perform some computer task was the goal. A hacker is a person who finds unknown operations and pathways within complex computer systems. Hackers can be good or bad. An ethical or ‘White Hat’ hacker works to improve a system. (‘White Hat’ refers to American western movies in which the hero always wore a white hat.) Without ethical hackers, system vulnerabilities that could be corrected would be discovered only by accident. The systematic patching (updating) of computers to increase security results from ethical hacking. A malicious hacker is someone who intentionally breaches or ‘hacks’ into a computer system to steal confidential information or to cause damage. Malicious Hackers usually look for financial or personal information in order to steal money or identities. They try to guess simple passwords, and seek to find undocumented weaknesses and backdoors in existing systems. They are sometimes called ‘Black Hat hackers’ . (In those same American westerns where the heroes wore white hats, the villains wore black hats.) 1.3.15 Social Threats to Security / Phishing Social threats to security often involve direct communications (whether in person, by phone, or over the internet) designed to trick people who have access to protected data into providing that information (or access to the information) to people who will use it for criminal or malicious purposes. Social engineering refers to how malicious hackers try to trick people into giving them either information or access. Hackers use any information they obtain to convince other employees that they have legitimate requests. Sometimes hackers will contact several people in sequence, collecting information at each step useful for gaining the trust of the next higher employee. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 233 Phishing refers to a phone call, instant message, or email meant to lure recipients into giving out valuable or private information without realizing they are doing so. Often these calls or messages appear to be from a legitimate source. For example, sometimes the y are framed as sales pitches for discounts or lowered interest rates. But they ask for personal information such as names, passwords, Social Security numbers, or credit card information. To reduce suspicion, these messages often request the recipient to ‘ update’ or ‘confirm’ information. Phishing instant messages and emails might also direct users to phony website s to trick them into providing personal information. Of special danger are fake emails specifically targeted to senior executives by name. This is called ‘Spear -phishing for whales’. In addition to phoning and spoofing, hackers have been known to physically go to target sites and speak directly with employees, sometimes using disguises or posing as vendors, in order to gain access to sensitive info rmation. 43 1.3.16 Malware Malware refers to any malicious software created to damage, change, or improperly access a computer or network. Computer viruses, worms, spyware, key loggers, and adware are all examples of malware. Any software installed without authorization can be considered malware, if for no other reason than that it takes up disk space and possibly processing cycles that the system owner did not authorize. Malware can take many forms, depending on its purpose (replication, destruction, information or processing theft, or beh avior monitoring). 1.3.16.1 Adware Adware is a form of spyware that enters a computer from an Internet download. Adware monitors a computer’s use, such as what websites are visited. Adware also may insert objects and toolbars in the user’s browser. Adware is not illegal, but is used to develop complete profiles of the user’s browsing and buying habits to sell to other marketing firms. It can also be easily leveraged by malicious software for identity theft. 1.3.16.2 Spyware Spyware refers to any software program that slips into a computer without consent in order to track online activity. These programs tend to piggyback on other software programs. When a user downloads and installs free software from a site on the Internet, spyware can also install, usually without the user’s knowledge. Different forms of spyware track different types of activity. Some programs monitor what website s are visited, while others record the user’s keystrokes to steal personal information, such as credit card numbers, bank account information, and passwords. Many legitimate websites, including search engines, install tracking spyware, which is a form of Adware. 43 The FBI report on Russian Hacking during the 2016 US Presidential Election outlines how these techniques were used in that instance. http://bit.ly/2iKStXO . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "234 • DMBOK2 1.3.16.3 Trojan Horse The Trojan horse was a large wooden ‘gift statue’ of a horse that the Greeks gave to the people of Troy, who quickly brought it inside the city walls. Unfortunately for them, it concealed Greek soldiers, who, once inside the Troy, slipped out and attacked the city. In computer security terms, a Trojan horse refers to a malicious program that enters a computer system disguised or embedded within legitimate software. Once installed, a Trojan horse will delete files, access personal information, install malware, reconfigure the computer, install a key logger, or even allow hackers to use the computer as a weapon (Bot or Zombie) against other computers on a network. 1.3.16.4 Virus A virus is a program that attaches itself to an executable file or vulnerable application and delivers a payload that ranges from annoying to extremely destructive. A file virus executes when an infected file opens. A virus always needs to accompany another progr am. Opening downloaded and infected programs can release a virus. 1.3.16.5 Worm A computer worm is a program built to reproduce and spread across a network by itself. A worm -infected computer will send out a continuous stream of infected messages. A worm may perform several different malicious activities, although the main function is to harm networ ks by consuming large amounts of bandwidth, potentially shutting the network down. 1.3.16.6 Malware Sources 1.3.16.6.1 Instant Messaging (IM) IM allows users to relay messages to each other in real time. IM is also becoming a new threat to network security. Because many IM systems have been slow to add security features, malicious hackers have found IM a useful means of spreading viruses, spyware, phishing scams, and a wide variety of worms. Typically, these threats infiltrate systems through contaminated attachments and messages. 1.3.16.6.2 Social Networking Sites Social networking sites , such as Facebook, Twitter, Vimeo, Google+, LinkedIn, Xanga, Instagram, Pinterest, or MySpace, where users build online profiles and share personal information, opinions, photographs, blog entries, and other information, have become targets of online pred ators, spammers, and identity thieves. In addition to representing a threat from malicious people, these sites pose risks from employees who may post information sensitive to the enterprise or ‘insider’ knowledge that might affect the price of a public Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 235 organization’s stock. Inform users of the dangers and the reality that whatever they post will become permanent on the Internet. Even if they later remove the data, many will have made copies. Some companies block these sites at their firewall. 1.3.16.6.3 Spam Spam refers to unsolicited, commercial email messages sent out in bulk, usually to tens of millions of users, in hopes that a few may reply. A return rate of 1% can net millions of dollars. Most email routing systems have traps to filter out known spam message patterns to reduce internal traffic. These exclusion patterns include: • Domains known for spam transmission • CC: or BCC: address count above certain limits • Email body has only an image as a hyperlink • Specific text strings or words Responding to a spam message will confirm to the sender that they have reached a legitimate email address and will increase future spam because lists of valid emails can be sold to other spammers. Spam messages may also be Internet hoaxes or include malware attachments, with attachment names and extensions, message text, and images giving the appearance of a legitimate communication. One way to detect spam email is to hover the pointer over any hype rlinks, which will show the actual link that has nothing in common with the company shown in the text. Another way is the lack of a way to unsubscribe. In the US, advertising emails are required to list an unsubscribe link to stop further emails. 2. Activities There is no one prescribed way of implementing data security to meet all necessary privacy and confidentiality requirements. Regulations focus on the ends of security, not the means for achieving it. Organizations should design their own security controls, demonstrate that the controls meet or exceed the requirements of the laws or regulations, document the implementation of those controls, and monitor and measure them over time. As in other Knowledge Areas, the activities include identifying requirements, assessing the current environment for gaps or risks, implementing security tools and processes, and auditing data security me asures to ensure they are effective. 2.1 Identify Data Security Requirements It is important to distinguish between business requirements, external regulatory restrictions, and the rules imposed by application software products. While application systems serve as vehicles to enforce business rules and procedures, it is common for these systems to have their own data security requirements over and above those required for business processes. These requirements are becoming more common with packaged and off -the-shelf systems. It is necessary, however, to see that they support organiza tional data security standards as well. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "236 • DMBOK2 2.1.1 Business Requirements Implementing data security within an enterprise begins with a thorough understanding of business requirements. The business needs of an enterprise, its mission, strategy , and size, and the industry to which it belongs define the degree of rigidity required for data security. For example, financial and securities enterprises in the United States are highly regulated and required to maintain stringent data security standards. In contrast, a small -scale retail enterprise may choose not to have the same kind of data security function tha t a large retailer has, even though both of them have similar core business activities. Analyze business rules and processes to identify security touch points. Every event in the business workflow may have its own security requirements. Data- to-process and data -to-role relationship matrices are useful tools to map these needs and guide definition of data security role -groups, parameters, and permissions. Plan to address short -term and long -term goals to achieve a balanced and effective data security function. 2.1.2 Regulatory Requirements Today’s fast changing and global environment requires organizations to comply with a growing set of laws and regulations. The ethical and legal issues facing organizations in the Information Age are leading governments to establish new laws and standards. These have all imposed strict security controls on information management. (See Chapter 2.) Create a central inventory of all relevant data regulations and the data subject area affected by each regulation. Add links to the corresponding security policies developed for compliance to these regulations ( see Table 13), and the controls implemented. Regulations, policies, required actions, and data affected will change over time, so this inventory should be in a format that is simple to manage and maintain. Table 13 Sample Regulation Inventory Table Regulation Subject Area Affected Security Policy Links Controls Implemented Examples of laws that influence data security include: • US o Sarbanes -Oxley Act of 2002 o Health Information Technology for Economic and Clinical Health (HITECH) Act, enacted as part of the American Recovery and Reinvestment Act of 2009 o Health Insurance Portability and Accountability Act of 1996 (HIPAA) Security Regulations o Gramm- Leach -Bliley I and II o SEC laws and Corporate Information Security Accountability Act o Homeland Security Act and USA Patriot Act o Federal Information Security Management Act (FISMA) o California: SB 1386, California Security Breach Information Act • EU o Data Protection Directive (EU DPD 95/46/) AB 1901, Theft of electronic files or databases Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 237 • Canada o Canadian Bill 198 • Australia o The CLERP Act of Australia Regulations that impact data security include: • Payment Card Industry Data Security Standard (PCI DSS) , in the form of a contractual agreement for all companies working with credit cards • EU: The Basel II Accord , which imposes information controls for all financial institutions doing business in its related countries • US: FTC Standards for Safeguarding Customer Info Compliance with company policies or regulatory restrictions will often require adjustments to business processes. For example, the need to authorize access to health information (regulated data elements) to multiple unique groups of users, in order to acco mmodate HIPAA. 2.2 Define Data Security Policy Organizations should create data security policies based on business and regulatory requirements. A policy is a statement of a selected course of action and high -level description of desired behavior to achieve a set of goals. Data security policies describe behaviors that are determined to be in the best interests of an organization that wishes to protect its data. For policies to have a measurable impact, they must be auditable and audited. Corporate policies often have legal implications. A court may consider a policy instituted to support a legal regulatory requirement to be an intrinsic part of the organization’s effort to comply with that legal requirement. Failure to comply with a corpor ate policy might have negative legal ramifications after a data breach. Defining security policy requires collaboration between IT security administrators, Security Architects, Data Governance committees, Data Stewards, internal and external audit teams, and the legal department. Data Stewards must also collaborate with all Privacy Officers (Sarbanes -Oxley supervisors, HIPAA Officers, etc.), and business managers having data expertise, to develop regulatory category Metad ata and apply proper security classifications consistently. All data regulation compliance actions must be coordinated to reduce cost, work instruction confusion, and needless turf battles. 2.2.1 Security Policy Contents Different levels of policy are required to govern behavior related to enterprise security. For example: • Enterprise Security Policy: Global policies for employee access to facilities and other assets, email standards and policies, security access levels based on position or title, and sec urity breach reporting policies • IT Security Policy: Directory structures standards, password policies, and an identity management framework Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "238 • DMBOK2 • Data Security Policy: Categories for individual application s, database roles, user groups, and information sensitivity Commonly, the IT Security Policy and Data Security Policy are part of a combined security policy. The preference, however, should be to separate them. Data security policies are more granular in nature, specific to content, and require different controls and procedures. The Data Governance Council should review and approve the Data Security Policy. The Data Management Executive owns and maintains the policy. Employees need to understand and follow security policies. Develop security policies so that the required processes and the reasons behind them are clearly defined and achievable. Compliance should be made easier than non-compliance. Policies need to protect and secure data without stifling user access. Security policies should be in a format easily accessible by the suppliers, consumers, and other stakeholders. They should be available and maintained on the company intranet or a similar collaboration portal. Data security policies, procedures, and activities should be periodically reevaluated to strike the best possible balance between the data security requirements of all stakeholders. 2.3 Define Data Security Standards Policies provide guidelines for behavior. They do not outline every possible contingency. Standards supplement policies and provide additional detail on how to meet the intention of the policies. For example, a policy may state that passwords must follow guidelines for strong passwords; the standards for strong passwords would be detailed separately; an d the policy would be enforced through technology that prevents passwords from being created if they do not meet the standards for strong passwords. 2.3.1 Define Data Confidentiality Levels Confidentiality classification is an important Metadata characteristic that guides how users are granted access privileges. Each organization should create or adopt a classification scheme that meets its business requirements. Any classification method should be clear and easy to apply. It will contain a range of levels, from the least to the most confidential (e.g., from “for general use” to “registered confidential”). (See Section 1.3.12.1.) 2.3.2 Define Data Regulatory Categories A growing number of highly publicized data breaches in which sensitive personal information has been compromised have resulted in data -specific laws being introduced. Financially -focused data incidents have spurred governments across the globe to implement additional regulations. This has created a new class of data, which might be called ‘Regulated Information ’. Regulatory requirements are an extension of information security. Additional measures are required to manage regulatory requirements effectively. Consultation with corporate counsel is often helpful in determining what actions certain regulations Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 239 require from the enterprise. Often the regulations imply a goal, and it is up to the corporation to determine the means for reaching that information protection goal. Actions that can be audited provide legal proof of compliance. A useful way to handle the data -specific regulations is by analyzing and grouping similar regulations into categories, as was been done by grouping various risks into a few security classifications. With more than one -hundred different data -specific ordinances around the world, it would be useless to develop a different category for each regulation. Most data regulations, imposed as they are by separate legal entities, seek to do the same thing. For example, the contractual obligations for protecting confidential customer data are remarkably similar to U.S., Japanese, and Canadian government regulations for protecting Personally Identifiable Information, and similar for compliance with EU privacy requirements. This pattern is easy to see when the auditable compliance actions for each regulation are listed and compared. Thus, they may all be managed properly by using the same protective action category. A key principle for both security classification and regulatory categorization is that most information can be aggregated so that it has greater or lesser sensitivity. Developers need to know how aggregations affect the overall security classification and regulatory categories. When a developer of a dashboard, report, or database view knows that some of the data that is required may be personally private or insider or related to competitive advantage, the system can then be designed to eliminate aspects of that from the entitlement, or, if the data must remain in the user -entitlement, to enforce all the security and regulatory requirements at the time of user authorization. The results of this classification work will be a formally approved set of security classifications and regulatory categories and a process for capturing this Metadata in a central repository so that employees, both business and technical, know the sensiti vity if the information they are handling, transmitting, and authorizing 2.3.3 Define Security Roles Data access control can be organized at an individual or group level, depending on the need. That said , granting access and update privileges to individual user accounts entails a great deal of redundant effort. Smaller organizations may find it acceptable to manage data access at the individual level. However, larger organizations will benefit greatly from role-based access control, granting permissions to role groups and thereby to each group member. Role groups enable security administrators to define privileges by role and to grant these privileges by enrolling users in the appropriate role group. While it is technically possible to enroll a user in more than one group, this practice may make it difficult to understand the privileges granted to a specific user. Whenever possible, try to assign each user to only one role group. This may require the creation of different user views of certain data entitlements to comply with regulations. Data consistency in user and role management is a challenge. User information such as name, title, and employee ID must be stored redundantly in several locations. These islands of data often conflict, representing multiple versions of the ‘truth’. To avoid data integrity issues, manage user identity data and role -group membership centrally. This is a requirement for the quality of data used for effective access control. Security administrators create, modify, and delete user accounts and role groups. Changes made to the group taxonomy Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "240 • DMBOK2 and membership should receive appropriate approval. Changes should be tracked via a change management system. Applying data security measures inconsistently or improperly within an organization can lead to employee dissatisfaction and significant risk to the organization. Role -based security depends on clearly defined, consistently assigned roles. There are two ways to define and organize roles: as a grid (starting from the data), or in a hierarchy (starting from the user). 2.3.3.1 Role Assignment Grid A grid can be useful for mapping out access roles for data, based on data confidentiality, regulations, and user functions. The Public User role can have access to all data ranked for General Audiences and not subject t o any regulations. A Marketing role may have access to some PII information for use in developing campaigns, but not to any restricted data, or Client Confidential data. Table 14 shows a very simplified example. Table 14 Role Assignment Grid Example Confidentiality Level General Audience Client Confidential Restricted Confidential Not Regulated Public User Role Client Manager Role Restricted Access Role PII Marketing Role Client Marketing Role HR Role PCI Financial Role Client Financial Role Restricted Financial Role 2.3.3.2 Role Assignment Hierarchy Construct group definitions at a workgroup or business unit level. Organize these roles in a hierarchy, so that child roles further restrict the privileges of parent roles. The ongoing maintenance of these hierarchies is a complex operation requiring reporting systems capable of granular drill down to individual user privileges. A security role hierarchy e xample is shown in Figure 65. 2.3.4 Assess Current Security Risks Security risks include elements that can compromise a network and/or database. The first step in identifying risk is identifying where sensitive data is stored and what protections are required for that data. Evaluate each system for the following: • The sensitivity of the data stored or in transit • The requirements to protect that data, and • The current security protections in place Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 241 Figure 65 Security Role Hierarchy Example Diagram Document the findings, as they create a baseline for future evaluations. This documentation may also be a requirement for privacy compliance, such as in the European Union. Gaps must be remediated through improved security processes supported by technology. The impact of improvements should be measured and monitored to ensure risks are mitigated. In larger organizations, white -hat hackers may be hired to assess vulnerabilities. A white hat exercise can be used as proof of an organization’s impenetrability, which can be used in publicity for market reputation. 2.3.5 Implement Controls and Procedures Implementation and administration of data security policy is primarily the responsibility of security administrators, in coordination with data stewards and technical teams. For example, database security is often a DBA responsibility. Organizations must implement proper controls to meet the security policy requirements. Controls and procedures should (at a minimum) cover: • How users gain and lose access to systems and/or applications • How users are assigned to and removed from roles • How privilege levels are monitored • How requests for access changes are handled and monitored • How data is classified according to confidentiality and applicable regulations • How data breaches are handled once detected Document the requirements for allowing original user authorizations so de -authorization may happen when these conditions no longer apply. For instance, a policy to ‘maintain appropriate user privileges’ could have a control objective of ‘Review DBA and User rights and privileges on a monthly basis ’. The organization’s procedure to satisfy this control might be to implement and maintain processes to : User A Work Unit B (A/R) * READ * UPDATE Work Unit C (FINANCE) * READ Work Unit A (CSR MGR) * CREATE * READ * UPDATE * DELETE Work Unit A (A/R MGR) * CREATE * READ * UPDATE User B User C User D Work Unit A (CSR) * CREATE * READ * UPDATE Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "242 • DMBOK2 • Validate assigned permissions against a change management system used for tracking all user permission requests • Require a workflow approval process or signed paper form to record and document each change request • Include a procedure for eliminating authorizations for people whose job status or department no longer qualifies them to have certain access rights Some level of management must formally request, track, and approve all initial authorizations and subsequent changes to user and group authorizations 2.3.5.1 Assign Confidentiality Levels Data Stewards are responsible for evaluating and determining the appropriate confidentiality level for data based on the organiz ation’s classification scheme. The classification for documents and reports should be based on the highest level of confidentiality for any information found within the document. (See Chapter 9. ) Label each page or screen with the classification in the header or footer. Information products classified as least confidential (e.g., “For General Audiences”) do not need labels. Assume any unlabeled products to be for General Audiences. Document authors and information product designers are responsible for evaluating, correctly classifying, and labeling the appropriate confidentiality level for each document, as well as each database, including relational tables, columns, and user entitle ment views. In larger organizations, much of the security classification and protective effort will be the responsibility of a dedicated information security organization. While Information Security will be happy to have the Data Stewards work with these classifications, they usually take responsibility for enforcement and for physically protecting the network. 2.3.5.2 Assign Regulatory Categories Organizations should create or adopt a classification approach to ensure that they can meet the demands of regulatory compliance. (See Section 3.3. ) This classification scheme provides a foundation for responding to internal and external audits. Once it is in place, information needs to be assessed and classified within the schema. Security staff may not be familiar with this concept, as they do not work with individual data regulations, but with infrastructure systems. They will need to have documented requirements for data protection relating to these categories defining actions they can implement. 2.3.5.3 Manage and Maintain Data Security Once all the requirements, policies, and procedures are in place, the main task is to ensure that security breaches do not occur and , if they do, to detect them as soon as possible. Continual monitoring of systems and auditing of the execution of security procedures are crucial to preserving data security. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 243 2.3.5.3.1 Control Data Availability / Data -centric Security Controlling data availability requires management of user entitlements and of the structures (data masking, view creation, etc.) that technically control access based on entitlements. Some databases are better than others in providing structures and processes to protect data in storage. (See Section 3.7. ) Security Compliance managers may have direct responsibility for designing user entitlement profiles that allow the business to function smoothly, while following relevant restrictions. Defining entitlements and granting authorizations requires an inventory of data, careful analysis of data needs, and documentation of the data exposed in each user entitlement. Often highly sensitive information is mixed with non -sensitive information. An enterprise data model is essential to identifying and locating sensitive data. (See Section 1.1.1.) Data masking can protect data even if it is inadvertently exposed. Certain data regulations require encryption, an extreme version of in -place masking. Authorization to the decryption keys can be part of the user authorization process. Users authorized to access the d ecryption keys can see the unencrypted data, while others only see random characters. Relational database views can used to enforce data security levels. Views can restrict access to certain rows based on data values or restrict access to certain columns, limiting access to confidential or regulated fields. 2.3.5.3.2 Monitor User Authentication and Access Behavior Reporting on access is a basic requirement for compliance audits. Monitoring authentication and access behavior provides information about who is connecting and accessing information assets. Monitoring also helps detect unusual, unforeseen, or suspicious transactions that warrant investigation. In this way, it compensates for gaps in data securi ty planning, design, and implementation. Deciding what needs monitoring, for how long, and what actions to take in the event of an alert, requires careful analysis driven by business and regulatory requirements. Monitoring entails a wide range of activities. It can be specific to certain data set s, users, or roles. It can be used to validate data integrity, configurations, or core Metadata. It can be implemented within a system or across dependent heterogeneous systems. It can focus on specific privileges, such as the ability to download large sets of data or to access data at off hours. Monitoring can be automated or executed manually or executed through a combination of automation and oversight. Automated monitoring does impose overhead on the underlying systems and may affect system performance. Periodic snapshots of activity can be use ful in understanding trends and comparing against standards criteria. Iterative configuration changes may be required to achieve the optimal parameters for proper monitoring. Automated recording of sensitive or unusual database transactions should be part of any database deployment. Lack of automated monitoring represents serious risks: • Regulatory risk : Organizations with weak database audit mechanisms will increasingly find that they are at odds with government regulatory requirements. Sarbanes -Oxley (SOX) in the financial services Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "244 • DMBOK2 sector and the Healthcare Information Portability and Accountability Act (HIPAA) in the healthcare sector are just two examples of US government regulation with clear database audit requirements. • Detection and recovery risk : Audit mechanisms represent the last line of defense. If an attacker circumvents other defenses, audit data can identify the existence of a violation after the fact. Audit data can also be used to link a violation to a particular user or as a guide to repair the system. • Administrative and audit duties risk : Users with administrative access to the database server – whether that access was obtained legitimately or maliciously – can turn off auditing to hide fraudulent activity. Audit duties should ideally be separate from both database administrators and the database server platform support staff. • Risk of reliance on inadequate native audit tools : Database software platforms often try to integrate basic audit capabilities but they often suffer from multiple weaknesses that limit or preclude deployment. When users access the database via Web applications (such as SAP, Oracle E -Business Suite, or Peo pleSoft), native audit mechanisms have no awareness of specific user identities and all user activity is associated with the Web application account name. Therefore, when native audit logs reveal fraudulent database transactions, there is no link to the r esponsible user. To mitigate the risks, implement a network -based audit appliance , which can address most of the weaknesses associated with native audit tools but does not take the place of regular audits by trained auditors. This kind of appliance has the following benefits: • High performance : Network- based audit appliances can operate at line speed with little impact on database performance. • Separation of duties : Network- based audit appliances should operate independently of database administrators making it possible to separate audit duties from administrative duties as appropriate. • Granular transaction tracking supports advanced fraud detection, forensics, and recovery. Logs include details such as source application name, complete query text, query response attributes, source OS, time , and source name. 2.3.5.4 Manage Security Policy Compliance Managing security policy compliance includes ongoing activities to ensure policies are followed and controls are effectively maintained. Management also includes providing recommendations to meet new requirements. In many cases, Data Stewards will act in conjunction with Information Security and Corporate Counsel so that operational policies and technical controls are aligned. 2.3.5.4.1 Manage Regulatory Compliance Managing regulatory compliance includes: • Measuring compliance with authorization standards and procedures Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 245 • Ensuring that all data requirements are measurable and therefore auditable (i.e., assertions like “be careful” are not measurable) • Ensuring regulated data in storage and in motion is protected using standard tools and processes • Using escalation procedures and notification mechanisms when potential non- compliance issues are discovered, and in the event of a regulatory compliance breach Compliance controls require audit trails. For example, if the policy states that users must take training before accessing certain data, then the organization must be able to prove that any given user took the training. Without an audit trail, there is no evidence of compliance. Controls should be designed to ensure they are auditable. 2.3.5.4.2 Audit Data Security and Compliance Activities Internal audits of activities to ensure data security and regulatory compliance policies are followed should be conducted regularly and consistently. Compliance controls themselves must be revisited when new data regulation is enacted, when existing regulation changes, and periodically to ensure usefulness. Internal or external auditors may perform audits. In all cases, auditors must be independent of the data and / or process involved in the audit to avoid any conflict of interest and to ensure the integrity of the auditing activity a nd results. Auditing is not a fault -finding mission. The goal of auditing is to provide management and the Data Governance council with objective, unbiased assessments, and rational, practical recommendations. Data security policy statements, standards documents, implementation guides, change requests, access monitoring logs, report outputs, and other records (electronic or hard copy) form the input to an audit. In addition to examining existing evidence, audits often include performing tests and checks, such as: • Analyzing policy and standards to en sure that compliance controls are defined clearly and fulfill regulatory requirements • Analyzing implementation procedures and user -authorization practices to ensure compliance with regulatory goals, policies, standards, and desired outcomes • Assessing whether authorization standards and procedures are adequate and in alignment with technology requirements • Evaluating escalation procedures and notification mechanisms to be executed when potential non - compliance issues are discovered or in the event of a regulatory compliance breach • Reviewing contracts, data sharing agreements, and regulatory compliance obligations of outsourced and external vendors that ensure business partners meet their obligations and that the organization meets its legal obligations for protecting regulated data • Assessing the maturity of security practices within the organization and reporting to senior management an d other stakeholders on the ‘ State of Regulatory Compliance’ • Recommending Regulatory Compliance policy changes and operational compliance improvements Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "246 • DMBOK2 Auditing data security is not a substitute for managing data security. It is a supporting process that objectively assesses whether management is meeting goals. 3. Tools The tools used for managing information security depend, in large part, on the size of the organization, the network architecture, and the policies and standards used by a security organization. 3.1 Anti-Virus Software / Security Software Anti-virus software protects computers from viruses encountered on the Web. New viruses and other malware appear every day, so it is important to update security software regularly. 3.2 Web Page Security If a Web address begins with https://, it indicates that the website is equipped with an encrypted security layer. Typically, users must provide a password or other means of authentication to access the site. Making payments online or accessing classified information uses this encryption protection. Train users to look for this in the URL address when they are performing sensitive operations over the Internet or even within the enterprise. Without encryption, people on the same network segment can read the plain text infor mation. 3.3 Identity Management Technology Identity management technology stores assigned credentials and shares them with systems upon request, such as when a user logs into a system. Some applications manage their own credential repository, although it is more convenient for users to have most or all applications use a centra l credential repository. There are protocols for managing credentials: Lightweight Directory Access Protocol (LDAP) is one. Some companies choose and provide an enterprise approved ‘Password Safe’ product that creates an encrypted password file on each user’s computer. Users only need to learn one long pass-phrase to open the program and they can store all their passwords safely in the encrypted file. A single -sign-on system also can perform this role. 3.4 Intrusion Detection and Prevention Software Tools that can detect incursions and dynamically deny access are necessary when hackers do penetrate firewalls or other security measures. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 247 An Intrusion Detection System (IDS) will notify appropriate people when an inappropriate incident happens. IDS should optimally be connected with an intrusion Prevention System (IPS) that automatically responds to known attacks and illogical combinations of user commands. Detection is often accomplished by analysis of patterns within the organization. Knowledge of expected patterns allows detection of out- of-the-ordinary events. When these take place, the system can send alerts. 3.5 Firewalls (Prevention) Secure and sophisticated firewalls, with capacity to allow full speed data transmission while still performing detailed packet analysis, should be placed at the enterprise gateway. For web servers exposed to the Internet, a more complex firewall structure is advised, as many malicious hacker attacks exploit legitimate appearing traffic that is intentionally malformed to exploit database and web server vulnerabilities. 3.6 Metadata Tracking Tools that track Metadata can help an organization track the movement of sensitive data. These tools create a risk that outside agents can detect internal information from Metadata associated with documents . Identification of sensitive information using Metadata provides the best way to ensure that data is protected properly. Since the largest number of data loss incidents result from the lack of sensitive data protection due to ignorance of its sensitivity, Metadata documentation completely overshadows any hypothetical risk that might occur if the Metadata were to be somehow exposed from the Metadata repository. This risk is made more negligible since it is trivial for an experienced hacker to locate unprotected sensitive data on the network. The people mos t likely unaware of the need to protect sensitive data appear to be employees and managers. 3.7 Data Masking/Encryption Tools that perform masking or encryption are useful for restricting movement of sensitive data. (See Section 1.3.9.) 4. Techniques Techniques for managing information security depend on the size of the organization, the architecture of the network, the type of data that must be secured, and the policies and standards used by a security organization. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "248 • DMBOK2 4.1 CRUD Matrix Usage Creating and using data -to-process and data -to-role relationship (CRUD –Create, Read, Update, Delete) matrices help map data access needs and guide the definition of data security role groups, parameters, and permissions. Some versions add an E for Execute to make CRUDE. 4.2 Immediate Security Patch Deployment A process for installing security patches as quickly as possible on all machines should be in place. A malicious hacker only needs root access to one machine in order to conduct his attack successfully on the network. Users should not be able to delay this update. 4.3 Data Security Attributes in Metadata A Metadata r epository is essential to ensure the integrity and consistent use of an Enterprise Data Model across business processes. Metadata should include security and regulatory classifications for data. (See Section 1.1.3. ) Having security Metadata in place protects an organization from employees who may not recognize the data as sensitive. When Data Stewards apply confidentiality and regulatory categories, category information should be documented in the Metadata repository and, if technology allows, tagged to the data. (S ee Sections 3.3.1 and 3.3.2 .) These classifications can be used to define and manage user entitlements and authorizations, as well as to inform development teams about risks related to sensitive data. 4.4 Security Needs in Project Requirements Every project that involves data must address system and data security. Identify detailed data and application security requirements in the analysis phase. Identification up front guides the design and prevents having to retrofit security processes. If implementation teams understand data protection requirements from the start, they can build compliance into the basic architecture of the system. This information can also be used for selecting appropriate vendor/purchased software packages. 4.5 Efficient Search of Encrypted Data Searching encrypted data obviously includes the need to decrypt the data. One way to reduce the amount of data that needs decryption is to encrypt the search criteria (such as a string) using the same encryption method used for the data, and then seek matches. The amount of data matching the encrypted search criteria will be much less, and therefore less costly (and risky) to decrypt. Then search using clear text on the result set to get exact matches. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 249 4.6 Document Sanitization Document sanitization is the process of cleaning Metadata, such as tracked change history, from documents before sharing. Sanitization mitigates the risk of sharing confidential information that might be embedded in comments. In contracts especially, access to this information may negatively affect negotiations. 5. Implementation Guidelines Implementation of data security practices depends on corporate culture, the nature of the risks, the sensitivity of what data the company manages, and the types of systems in place. Implementation system components should be guided by a strategic security plan and supporting architecture. 5.1 Readiness Assessment / Risk Assessment Keeping data secure is deeply connected to corporate culture. Organizations often end up reacting to crises rather than proactively managing accountability and ensuring auditability. While perfect data security is next to impossible, the best way to avoid data security breac hes is to build awareness and understanding of security requirements, policies, and procedures. Organizations can increase compliance through: • Training : Promotion of standards through training on security initiatives at all levels of the organization. Follow training with evaluation mechanisms such as online tests focused on improving employee awareness. Such training and testin g should be mandatory and a pre requisite for employee performance evaluation. • Consistent policies : Definition of data security policies and regulatory compliance policies for workgroups and departments that complement and align with enterprise policies. Adopting an ‘act local’ mindset helps engage people more actively. • Measure the benefits of security : Link data security benefits to organizational initiatives. Organizations should include objective metrics for data security activities in their balanced scorecard measurements and project evaluations. • Set security requirements for vendors : Include data security requirements in service level agreements and outsourcing contractual obligations. SLA agreements must include all data protection actions. • Build a sense of urgency : Emphasize legal, contractual, and regulatory requirements to build a sense of urgency and an internal framework for data security management. • Ongoing communications : Supporting a continual employee security- training program informing workers of safe computing practices and current threats. An ongoing program communicates that safe computing is important enough for management to support it. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "250 • DMBOK2 5.2 Organization and Cultural Change Organizations need to develop data policies that enable them to meet their goals while protecting sensitive and regulated information from misuse or unauthorized exposure. They must account for the interests of all stakeholders as they balance risks with ease of access. Often the technical architecture must accommodate the Data Architecture to balance these needs to create an effective and secure electronic environment. In most organizations, the behavior of both management and employees will need to change if they are to successfully protect their data. In many larger companies, the existing information security group will have in place policies, safeguards, security tools, access control systems, and information protection devices and systems. There should be a clear understanding and appreciation where these elements complement the work done by the Data Stewards and data administrators. Data Stewards are generally responsible for data categorization. Information security teams assist with compliance enforcement and establish operational procedures based on data protection policies, and security and regulatory categorization. Implementing data security measures without regard for the expectations of customers and employees can result in employee dissatisfaction, customer dissatisfaction, and organizational risk. To promote compliance, data security measures must account for the viewpoint of those who will be working with the data and systems. Well -planned and comprehensive technical security measures should make secure access easier for stakeholders. 5.3 Visibility into User Data Entitlement Each user data entitlement, which is the sum total of all the data made available by a single authorization, must be reviewed during system implementation to determine if it contains any regulated information. Knowing who can see which data requires management of Metadata that desc ribes the confidentiality and regulatory classifications of the data, as well as management of the entitlements and authorizations themselves. Classification of regulatory sensitivity should be a standard part of the data definition process. 5.4 Data Security in an Outsourced World Anything can be outsourced except liability. Outsourcing IT operations introduces additional data security challenges and responsibilities. Outsourcing increases the number of people who share accountability for data across organizational and geographic boundaries. Previously informal roles and respo nsibilities must be explicitly defined as contractual obligations. Outsourcing contracts must specify the responsibilities and expectations of each role. Any form of outsourcing increases risk to the organization, including some loss of control over the technical environment and the people working with the organization’s data. Data security measures and processes must look at the risk from the outsource ven dor as both an external and internal risk. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 251 The maturity of IT outsourcing has enabled organizations to re -look at outsourced services. A broad consensus has emerged that architecture and ownership of IT, which includes data security architecture, should be an in - sourced function. In other words, th e internal organization owns and manages the enterprise and security architecture. The outsourced partner may take the responsibility for implementing the architecture. Transferring control, but not accountability, requires tighter risk management and control mechanisms. Some of these mechanisms include: • Service level agreements • Limited liability provisions in the outsourcing contract • Right-to -audit clauses in the contract • Clearly defined consequences to breaching contractual obligations • Frequent data security reports from the service vendor • Independent monitoring of vendor system activity • Frequent and thorough data security auditing • Constant communication with the service vendor • Awareness of legal differences in contract law should the vendor be located in another country and a dispute arise s In an outsourced environment, it is critical to track the lineage, or flow, of data across systems and individuals to maintain a ‘chain of custody’. Outsourcing organizations especially benefit from developing CRUD (Create, Read, Update, and Delete) matrices that map data responsibilities across business processes, applications, roles, and organizations, tracing the transformation, lineage, and chain of custody for data. Additionally, the ability to execute business decisions or application functionality, such as approving checks or orders, must be included as part of the matrix. Responsible, Accountable, Consulted, and Informed (RACI) matrices also help clarify roles, the separation of duties, and the responsibilities of different roles, including their data security obligations. The RACI matrix can become part of the contractual agreements and data security policies. Defining responsibility matrices like RACI will establish clear accountability and ownership among the parties involved in the outsourcing engagement, leading to support of the overall data security policies and their implementation. In outsourcing information technology operations, the accountability for maintaining data still lies with the organization. It is critical to have appropriate compliance mechanisms in place and have realistic expectations from parties entering into the outsourcing agreements. 5.5 Data Security in Cloud Environments The rapid emergence of web computing and business -to-business and business- to-consumer interaction has caused the boundaries of data to extend beyond the four walls of the organization. The recent advances in cloud computing have extended the boundaries a step further. The ‘as-a- service’ nomenclature is now common across all stacks of technology and business. ‘Data -as-a-Service’, ‘Software -as-a-Service’, ‘Platform -as-a-Service’ are commonly used terms today. Cloud computing, or having resources distributed over the internet to process data and information, is complementing the ‘X -as-a-Service’ provisioning. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "252 • DMBOK2 Data security policies need to account for the distribution of data across the different service models. This includes the need to leverage external data security standards. Shared responsibility, defining the chain of custody of data and defining ownership and custodianship rights, is especially important in cloud computing. Infrastructure considerations (e.g., Who is responsible for the firewall when the cloud provider delivers the software over the web? Who i s accountable for access rights on the servers?) have direct impacts on data security management and data policies. Fine -tuning or even creating a new data security management policy geared towards cloud computing is necessary for organizations of all sizes. Even if an organization has not directly implemented resources in the cloud, business partners may. In a connected world of data, having a business partner use cloud computing means putting the organization’s data in the cloud. The same data proliferation security principles apply to sensitive/confidential production data. Internal cloud data -center architecture, including virtual machines , even though potentially more secure, should follow the same security policy as the rest of the enterprise. 6. Data Security Governance Securing enterprise systems and the data they store requires cooperation between IT and business stakeholders. Strong, clear policies and procedures are at the foundation of security governance. 6.1 Data Security and Enterprise Architecture Enterprise Architecture defines the information assets and components of an enterprise, their interrelationships, and business rules regarding transformation, principles, and guidelines. Data Security architecture is the component of enterprise architecture that describes how data security is implemented within the enterprise to satisfy the business rules and external regulations. Architecture influences: • Tools used to manage data security • Data encryption standards and mechanisms • Access guidelines to external vendors and contractors • Data transmission protocols over the internet • Documentation requirements • Remote access standards • Security breach incident- reporting procedures Security architecture is particularly important for the integration of data between: • Internal systems and business units • An organization and its external business partners • An organization and regulatory agencies Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 253 For example, an architectural pattern of a service -oriented integration mechanism between internal and external parties would call for a data security implementation different from traditional electronic data interchange (EDI) integration architecture. For a large enterprise, the formal liaison function between these disciplines is essential to protecting information from misuse, theft, exposure, and loss. Each party must be aware of elements that concern the others, so they can speak a common language and work toward shared goals. 6.2 Metrics It is essential to measure information protection processes to ensure that they are functioning as required. Metrics also enable improvement of these processes. Some metrics measure progress on processes: the number of audits performed, security systems installed, incidents reported, and the amount of unexamined data in systems. More sophisticated metrics will focus on findings from audits or the movement of the organization along a maturity model. In larger organizations with existing information security staff, a significant number of these metrics may already exist. It is helpful to reuse existing metrics as a part of an overall threat management measurement process and to prevent duplication of e ffort. Create a baseline (initial reading) of each metric to show progress over time. While a great number of security activities and conditions can be measured and tracked, focus on actionable metrics. A few key metrics in organized groups are easier to manage than pages of apparently unrelated indicators. Improvement actions may include a wareness training on data regulatory policies and compliance actions. Many organizations face similar data security challenges. The following lists may assist in selecting applicable metrics. 6.2.1 Security Implementation Metrics These general security metrics can be framed as positive value percentages: • Percentage of enterprise computers having the most recent security patches installed • Percentage of computers having up -to-date anti- malware software installed and running • Percentage of new -hires who have had successful background checks • Percentage of employees scoring more than 80% on annual security practices quiz • Percentage of business units for which a formal risk assessment analysis has been completed • Percentage of business processes successfully tested for disaster recovery in the event of fire, earthquake, storm, flood , explosion or other disaster • Percentage of audit findings that have been successfully resolved Trends can be tracked on metrics framed as lists or statistics: • Performance metrics of all security systems Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "254 • DMBOK2 • Background investigations and results • Contingency planning and business continuity plan status • Criminal incidents and investigations • Due diligence examinations for compliance and number of findings that need to be addressed • Informational risk management analysis performed and number of those resulting in actionable changes • Policy audit implications and results, such as clean desk policy checks, performed by evening -shift security officers during rounds • Security operations, physical security, and premises protection statistics • Number of documented, accessible security standards (a.k.a. policies) • The motivation of relevant parties to comply with security policies can also be measured • Business conduct and reputational risk analysis, including employee training • Business hygiene and insider risk potential based on specific types of data such as financial, medical, trade secrets, and insider information • Confidence and influence indicators among managers and employees as an indication of how data information security efforts and policies are perceived Select and maintain a reasonable number of actionable metrics in appropriate categories over time to assure compliance, spot issues before they become crises, and indicate to senior management a determination to protect valuable corporate information. 6.2.2 Security Awareness Metrics Consider these general areas to select appropriate metrics : • Risk assessment findings provide qualitative data that needs to be fed back to appropriate business units to make them more aware of their accountability. • Risk events and profiles identify unmanaged exposures that need correction. Determine the absence or degree of measurable improvement in risk exposure or conformance to policy by conducting follow -up testing of the awareness initiative to see how well the messages got across. • Formal feedback surveys and interviews identify the level of security awareness. Also, measure the number of employees who have successfully completed security awareness training within targeted populations. • Incident post mortems, lessons learned, and victim interviews provide a rich source of information on gaps in security awareness. Measures may include how much vulnerability has been mitigated. • Patching effectiveness audits involve specific machines that work with confidential and regulated information to assess the effectiveness of security patching. (An automated patching system is advised whenever possible.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA SECURITY • 255 6.2.3 Data Protection Metrics Requirements will dictate which of these are pertinent to an organization: • Criticality ranking of specific data types and information systems that, if made inoperable, would have profound impact on the enterprise. • Annualized loss expectancy of mishaps, hacks, thefts, or disasters related to data loss, compromise, or corruption. • Risk of specific data losses related to certain categories of regulated information, and remediation priority ranking. • Risk mapping of data to specific business processes. Risks associated with Point of Sale devices would be included in the risk profile of the financial payment system. • Threat assessments performed based on the likelihood of an attack against certain valuable data resources and the media through which they travel. • Vulnerability assessments of specific parts of the business process where sensitive information could be exposed, either accidentally or intentionally. Auditable list of locations where sensitive data is propagated throughout the organization. 6.2.4 Security Incident Metrics • Intrusion attempts detected and prevented • Return on Investment for security costs using savings from prevented intrusions 6.2.5 Confidential Data Proliferation The number of copies of confidential data should be measured in order to reduce this proliferation. The more places confidential data is stored, the higher the risk of a breach. 7. Works Cited / Recommended Andress, Jason. The Basics of Information Security: Understanding the Fundamentals of InfoSec in Theory and Practice . Syngress, 2011. Print. Calder, Alan, and Steve Watkins. IT Governance: An International Guide to Data Security and ISO27001/ISO27002 . 5th ed. Kogan Page, 2012. Print. Fuster, Gloria González. The Emergence of Personal Data Protection as a Fundamental Right of the EU . Springer, 2014. Print. Law, Governance and Technology Series / Issues in Privacy and Data Protection. Harkins, Malcolm. Managing Risk and Information Security: Protect to Enable (Expert's Voice in Information Technology) . Apress, 2012. Kindle. Hayden, Lance. IT Security Metrics: A Practical Framework for Measuring Security and Protecting Data . McGraw -Hill Osborne Media, 2010. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "256 • DMBOK2 Kark, Khalid. “Building A Business Case for Information Security”. Computer World . 2009- 08-10 http://bit.ly/2rCu7QQ Web. Kennedy, Gwen, and Leighton Peter Prabhu. Data Privacy: A Practical Guide . Interstice Consulting LLP, 2014. Kindle. Amazon Digital Services. Murdoch, Don GSE. Blue Team Handbook: Incident Response Edition: A condensed field guide for the Cyber Security Incident Responder . 2nd ed. CreateSpace Independent Publishing Platform, 2014. Print. National Institute for Standards and Technology (US Department of Commerce website) http://bit.ly/1eQYolG . Rao, Umesh Hodeghatta and Umesha Nayak. The InfoSec Handbook: An Introduction to Information Security . Apress, 2014. Kindle. Amazon Digital Services. Ray, Dewey E. The IT professional’s merger and acquisition handbook . Cognitive Diligence, 2012. Schlesinger, David. The Hidden Corporation: A Data Management Security Novel . Technics Publications, LLC, 2011. Print. Singer, P.W. and Allan Friedman. Cybersecurity and Cyberwar: What Everyone Needs to Know®. Oxford University Press, 2014. Print. What Everyone Needs to Know. Watts, John. Certified Information Privacy Professional Study Guide: Pass the IAPP's Certification Foundation Exam with Ease! CreateSpace Independent Publishing Platform, 2014. Print. Williams, Branden R., Anton Chuvakin Ph.D. PCI Compliance: Understand and Implement Effective PCI Data Security Standard Compliance . 4th ed. Syngress, 2014. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "257 CHAPTER 8 Data Integration and Interoperability 1. Introduction ata Integration and Interoperability (DII) describes processes related to the movement and consolidation of data within and between data stores, applications , and organizations. Integration consolidates data into consistent forms, either physical or virtual. Data Interoperability is the ability for multiple systems to communicate. DII solutions enable basic data management functions on which most organizations depend: • Data migration and conversion • Data consolidation into hubs or marts • Integration of vendor packages into an organization’s application portfolio • Data sharing between applications and across organizations • Distributing data across data stores and data centers Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "258 • DMBOK2 • Archiving data • Managing data interfaces • Obtaining and ingesting external data • Integrating structured and unstructured data • Providing operational intelligence and management decision support DII is dependent on these other areas of data management: • Data Governance : For governing the transformation rules and message structures • Data Architecture : For designing solutions • Data Security: For ensuring solutions appropriately protect the security of data, whether it is persistent, virtual, or in motion between applications and organizations • Metadata : For tracking the technical inventory of data (persistent, virtual, and in motion), the business meaning of the data, the business rules for transforming the data, and the operational history and lineage of the data • Data Storage and Operations : For managing the physical instantiation of the solutions • Data Modeling and Design : For designing the data structures , including physical persistence in databases, virtual data structures, and messages passing information between applications and organizations Data Integration and Interoperability is critical to Data Warehousing and Business Intelligence, as well as Reference Data and Master Data Management, because all of these focus on transforming and integrating data from source systems to consolidated data hubs and from hubs to the target systems where it can be delivered to dat a consumers, both system and human. Data Integration and Interoperability is central to the emerging area of Big Data management. Big Data seeks to integrate various types of data, including data structured and stored in databases, unstructured text data in documents or files, other types of unstructured data such as audio, video, and streaming data. This integrated data can be mined, used to develop predictive models, and deployed in operational intelligence activities. 1.1 Business Drivers The need to manage data movement efficiently is a primary driver for DII. Since most organizations have hundreds or thousands of databases and stores, managing the processes for moving data between the data stores within the organization and to and from other organizations has become a central responsibility of every information technology organization. If not managed properly, the process of moving data can overwhelm IT resources and capabilities and dwarf the support requirements of traditional application and data management areas. The advent of organizations purchasing applications from software vendors, rather than developing custom applications, has amplified the need for enterprise data integration and interoperability. Each purchased application comes with its own set of Master Data stores, transaction data stores, and reporting data stores that must integrate with the other data stores in the organization. Even Enterprise Resource Planning (ERP) systems that run the common functions of the organization, rarely, if ever, encompass all the data stores in the organization. They, too, have to have their data integrated with other organizational data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 259 Figure 66 Context Diagram: Data Integration and Interoperability Definition : Data Integration is the movement and consolidation of data within and between data stores, applications and organizations. Data Interoperability is the ability for multiple systems to communicate. Goals : 1. Make data available from disparate sources in the format and timeframe needed. 2. Lower cost and complexity of managing solutions by developing shared models and interfaces. 3. Identify meaningful events and automatically trigger alerts and actions. 4. Support business intelligence, analytics, master data management, and operational efficiency efforts. Deliverables : •DII Architecture •Data Exchange Specifications •Data Access Agreements •Data Services •Complex Event Processing Thresholds and Alerts Suppliers : •Data Producers •Architecture Board •IT Steering Committee •Executives and Managers •Subject Matter ExpertsConsumers : •Customers, Partners •Information Consumers •Knowledge Workers •Data ScientistParticipants : •Data Architects •Business and Data Analysts •Data Modelers •DII Developers •Project and Program Managers •Data Stewards T echniques : •Loosely coupled applications •Minimize the number of interfaces •Create standard canonical interfaces •Service OrchestrationT ools : •Data Transformation Engine •Business Rules Engine •Data Virtualization Server •Enterprise Service Bus •Data and Process Modeling T ools •Data Profiling T ool •Metadata RepositoryMetrics : •Data Availability •Data Volumes and Speed •Solution Costs and Complexity (P) Planning, (C) Control, (D) Development, (O) OperationsData Integration and Interoperability Activities : 1.Plan & Analyze (P) 1.Define data integration and lifecycle requirements 2.Perform Data Discovery 3.Document Data Lineage 4.Profile Data 5.Collect Business Rules 2.Design DII Solutions (P) 1.Design DII Architecture 2.Model Data Hubs, Interfaces, Messages, and Data Services 3.Map Data Sources to Targets 4.Design Data Orchestration 3.Develop DII Solutions (D) 1.Develop Data Services 2.Develop Data Flows 3.Develop Data Migration Approach 4.Develop a Publication Approach 5.Develop Complex Event Processing Flows 4.Implement and Monitor (O) 1. Maintain DII MetadataInputs : •Business Goals & Strategies •Data Needs & Standards •Regulatory, Compliance, & Security Requirements •Data, Process, Application, and T echnical Architectures •Data Semantics •Source Data T echnical DriversBusiness Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "260 • DMBOK2 The need to manage complexity and the costs associated with complexity are reasons to architect data integration from an enterprise perspective. An enterprise design of data integration is demonstrably more efficient and cost effective than distributed or point -to-point solutions. Developing point -to-point solutions between applications can result in thousands to millions of interfaces and can quickly overwhelm the capabilities of even the most effective and efficient IT support organization. Data hubs such as data warehouses and Master Data solutions help to alleviate this problem by consolidating the data needed by many applications and providing those applications with consistent views of the data. Similarly, the complexity of managing opera tional and transactional data that needs to be shared across the organization can be greatly simplified using enterprise data integration techniques such as hub -and-spoke integration and canonical message models. Another business driver is managing the cost of support. Moving data using multiple technologies, each requiring specific development and maintenance skills, can drive support costs up. Standard tool implementations can reduce support and staffing costs and improve the efficiency of troubleshooting efforts. Reducing the complexity of interface management can lower the cost of interface maintenance, and allow support resources to be more effectively deployed on other organizational priorities. DII also supports an organization’s ability to comply with data handling standards and regulations. Enterprise - level DII systems enable re -use of code to implement compliance rules and simplify verification of compliance. 1.2 Goals and Principles The implementation of Data Integration and Interoperability practices and solutions aims to: • Make data available in the format and timeframe needed by data consumers, both human and system • Consolidate data physically and virtually into data hubs • Lower cost and complexity of managing solutions by developing shared models and interfaces • Identify meaningful events (opportunities and threats) and automatically trigger alerts and actions • Support Business Intelligence, analytics, Master Data Management, and operational efficiency efforts When implementing DII, an organization should follow these principles: • Take an enterprise perspective in design to ensure future extensibility, but implement through iterative and incremental delivery • Balance local data needs with enterprise data needs, including support and maintenance. • Ensure business accountability for Data Integration and Interoperability design and activity. Business experts should be involved in the design and modification of data transformation rules, both persistent and virtual. 1.3 Essential Concepts 1.3.1 Extract, Transform, and Load Central to all areas in Data Integration and Interopera bility is the basic process of Extract, Transform, and L oad (ETL). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 261 Whether executed physically or virtually, in batch or real -time, these are the essential steps in moving data around and between applications and organizations. Depending on data integration requirements, ETL can be performed as a periodically scheduled event (batch) or whenever new or updated data is available (real -time or event -driven). Operational data processing tends to be real- time or near real -time, while data needed for analysis or reporting is often scheduled in batch jobs. Data integration requirements also determine whether the extracted and transformed data is physically stored in staging structures. Physical staging allows for an audit trail of steps that have occurred with the data and potential process restarts from an intermediate point. However, staging structures take up disk space and take time to write and read. Data integration needs that require very low latency will usually not include physical staging of the intermediate data integration results. 1.3.1.1 Extract The extract process includes selecting the required data and extracting it from its source. Extracted data is then staged, in a physical data store on disk or in memory. If physically staged on disk, the staging data store may be co-located with the source data store or with the target data store, or both. Ideally, if this process executes on an operational system, it is designed to use as few resources as possible, in order to avoid negatively affecting the operational processes. Batch processing during off -peak hours is an option for extracts that include complex processing to perform the selection or identify changed data to extract. 1.3.1.2 Transform The transform process makes the selected data compatible with the structure of the target data store. Transformation includes cases where data is removed from the source when it moves to the target, where data is copied to multiple targets, and where the data is used to trigger events but is not persisted. Examples of transformation may include • Format changes : Conversion of the technical format of the data; for example, from EBCDIC to ASCII format • Structure changes : Changes to the structure of the data; for example, from denormalized to normalized records • Semantic conversion : Conversion of data values to maintain consistent semantic representation. For example, the source gender codes might include 0, 1, 2, and 3, while the target gender codes might be represented as UNKNOWN, FEMALE, MALE, or NOT PROVIDED. • De-duping : Ensuring that if rules require unique key values or records, a means for scanning the target and detecting and removing duplicate rows is included • Re-ordering : Changing the order of the data elements or records to fit a defined pattern Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "262 • DMBOK2 Transformation may be performed in batch or real -time, either physically storing the result in a staging area, or virtually storing the transformed data in memory until ready to move to the load step. Data resulting from the transformation stage should be ready to integrate with data in the target structure. 1.3.1.3 Load The load step of ETL is physically storing or presenting the result of the transformations in the target system. Depending on the transformations performed, the target system’s purpose, and the intended use, the data may need further processing to be integrated wi th other data, or it may be in a final form, ready to present to consumers. Figure 67 ETL Process Flow 1.3.1.4 ELT If the target system has more transformation capability than either the source or an intermediary application system, the order of processes may be switched to ELT – Extract, Load, and Transform. ELT allows transformations to occur after the load to the target system, often as part of the process. Figure 68 ELT Process Flow Transform Process Load Process Lookups Mappings Extract Process Source DatastoreStaging DatastoreTarget Datastore Lookups Mappings Extract ProcessLoad ProcessTransform Process Source DatastoreTarget Datastore Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 263 ELT allows source data to be instantiated on the target system as raw data, which can be useful for other processes. This is common in Big Data environments where ELT loads the data lake. (See Chapter 14.) 1.3.1.5 Mapping A synonym for transformation, a mapping is both the process of developing the lookup matrix from source to target structures and the result of that process. A mapping defines the sources to be extracted, the rules for identifying data for extraction, targets to be loaded, rules for identifying target rows for update (if any), and any transformation rules or calculations to be applied. Many data integration tools offer visualizations of mappings that enable developers to use graphical interfaces to create transformation code. 1.3.2 Latency Latency is the time difference between when data is generated in the source system and when the data is available for use in the target system. Different approaches to data processing result in different degrees of data latency. Latency can be high (batch) or low (event- driven) to very low (real -time synchronous). 1.3.2.1 Batch Most data moves between applications and organizations in clumps or files either on request by a human data consumer or automatically on a periodic schedule. This type of interaction is called batch or ETL. Data moving in batch mode will represent either the full set of data at a given point in time, such as account balances at the end of a period, or data that has changed values since the last time the data was sent, such as address changes that have been ma de in a day. The set of changed data is called the delta , and the data from a point in time is called a snapshot . With batch data integration solutions, there is often a significant delay between when data changes in the source and when it is updated in the target, resulting in high latency. Batch processing is very useful for processing very high volumes of data in a short time window. It tends to be used for data warehouse data integration solutions, even when lower latency solutions are available. To achieve fast processing and lower latency, some data integration solutions use micro -batch processing which schedules batch processing to run on a much higher frequency than daily, such as every five minutes. Batch data integration is used for data conversions, migrations, and archiving, as well as for extracting from and loading data warehouses and data marts. There are risks associated with the timing of batch processing. To minimize issues with application u pdates, schedule data movement between applications at the end of logical processing for the business day, or after special processing of the data has occurred at night. To avoid incomplete data sets, jobs moving data to a data warehouse should be schedule d based on the daily, weekly, or monthly reporting schedule. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "264 • DMBOK2 1.3.2.2 Change Data Capture Change Data Capture is a method of reducing bandwidth by filtering to include only data that has been changed within a defined timeframe. Change data capture monitors a data set for changes (inserts, changes, deletes) and then passes those changes (the deltas) to other data sets, applications, and organizations that consume the data. Data may also be tagged with identifiers such as flags or timestamps as part of the process. Change data capture may be data- based or log -based. (See Chapter 6. ) There are three techniques for data -based change data capture. • The source system populates specific data elements, such as timestamps within a range, or codes or flags, which serve as change indicators. The extract process uses rules to identify rows to extract. • The source system processes add to a simple list of objects and identifiers when changing data, which is then used to control selection of data for extraction. • The source system processes copy data that has changed into a separate object as part of the transaction, which is then used for extract processing. This object does not need to be within the database management system. These types of extraction use capabilities built into the source application, which may be resource intensive and require the ability to modify the source application. In log -based change data captures, data activity logs created by the database management system are copied and processed, looking for specific changes that are then translated and applied to a target database. Complex translations may be difficult, but intermediary structures resembling the source object can be used as a way of staging the changes for further processing. 1.3.2.3 Near -real -time and Event -driven Most data integration solutions that are not performed in batches use a near -real-time or event -driven solution . Data is processed in smaller sets spread across the day in a defined schedule, or data is processed when an event happens, such as a data update. Near -real-time processing has a lower latency than batch processing and often a lower system load as the wor k is distributed over time, but it is usually slower than a synchronized data integration solution. Near -real-time data integration solutions are usually implemented using an enterprise service bus. State information and process dependencies must be monitored by the target application load process. Data coming into the target may not be available in the exact order that the target needs to build the correct target data. For example, process Master Dat a or dimensional data prior to transactional data that uses that Master Data. 1.3.2.4 Asynchronous In an asynchronous data flow , the system providing data does not wait for the receiving system to acknowledge update before continuing processing. Asynchronous implies that either the sending or receiving system could be off-line for some period without the other system also being of f-line. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 265 Asynchronous data integration does not prevent the source application from continuing its processing, or cause the source application to be unavailable if any of the target applications are unavailable. Since the data updates made to applications in an asynchronous configuration are not immediate, the integration is called near- real- time . The delay between updates made in the source and relayed to target data sets in a near -real-time environment is usually measured in seconds or minutes. 1.3.2.5 Real -time, Synchronous There are situation s where no time delay or other differences between source and target data is acceptable. When data in one data set must be kept perfectly in synch with the data in another data set, then a real -time, synchronous solution must be used. In a synchronous integration solution, an executing process waits to receive confirmation from other applications or processes prior to executing its next activity or transaction. This means that the solution can process fewer transactions because it has to spend time waiting for confirmation of data synchronization. If any of the applications that need the update are not availab le then the transaction cannot be completed in the primary application. This situation keeps data synchronized but has the potential to make strategic applica tions dependent on less critical applications. Solutions using this type of architecture exist on a continuum based on how much difference between data sets might be possible and how much such a solution is worth. Data sets may be kept in synch through database capabilities such as two -phase commits, w hich ensure that all updates in a business transaction are all successful, or none are made. F or example, financial institutions use two -phase commit solutions to ensure that financial transaction tables are absolutely synchronized with financial balance t ables . Most programming does not use two -phase commit. There is a very small possibility that if an application is interrupted unexpectedly then one data set may be updated but not another. Real -time, synchronous solutions require less state management than asynchronous solutions because the order in which transactions are processed is clearly managed by the updating applications. However, they also may lead to blocking and delay other transa ctions. 1.3.2.6 Low Latency or Streaming Tremendous advances have been made in developing extremely fast data integration solutions. These solutions require a large investment in hardware and software. The extra costs of low latency solutions are justified if an organization requires extremely fast data movement across large distances. ‘Streaming data’ flows from computer sy stems on a real-time continuous basis immediately as events occur. Data streams capture events like the purchase of goods or financial securities, social media comments, and readouts from sensors monitoring location, temperature, usage, or other values. Low latency data integration solutions are designed to minimize the response time to events. They may include the use of hardware solutions like solid -state disk or software solutions like in- memory databases so that the process does not have to slow down to read or write to traditional disk. The read and write processes to traditional disk drives is thousands of times slower than processing data in- memory or on solid -state disk Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "266 • DMBOK2 drives. Asynchronous solutions are usually used in low latency solutions so that transactions do not need to wait for confirmation from subsequent processes before processing the next piece of data. Massive multi - processing, or simultaneous processing, is also a common configuration in low latency solutions so that the processing of incoming data can be spread out over many processors simultaneously, and not bottlenecked by a single or small number of processors. 1.3.3 Replication To provide better response time for users located around the world, some applications maintain exact copies of data sets in multiple physical locations. Replication solutions minimize the performance impact of analytics and queries on the primary transactional operating environment. Such a solution must synchronize the physically distributed data set copies. Most database management systems have replication utilities to do this work. These utilities work best when the data sets are all maintained in the same database management system technology. Replication solutions usually monitor the log of changes to the data set, not the data set itself. They minimize the impact on any operational applications because they do not compete with the applications for access to the data set. Only data from the change log passes between replicated copies. Standard replication solutions are near -real-time; there is a small delay between a change in one copy of the data set and another. Because the benefits of replication solutions — minimal effect on the source data set and minimal amount of data being passed — are very desirable, replication is used in many data integration solutions, even those that do not include long distance physical distribution. The database management utilities do not require extensive programming, so there tend to be few programming bugs. Replication utilities work optimally when source and target data sets are exact copies of each other. Differences between source and target introduce risks to synchronization. If the ultimate target is not an exact copy of the source then it is necessary t o maintain a staging area to house an exact copy of the sources. This requires extra disk usage and possibly extra database technology. Data replication solutions are not optimal if changes to the data may occur at multiple copy sites. If it is possible that the same piece of data is changed at two different sites, then there is a risk that the data might get unsynchronized, or one of the sites may have their changes overwritten without warning. (See Chapter 6. ) 1.3.4 Archiving Data that is used infrequently or not actively used may be moved to an alternate data structure or storage solution that is less costly to the organization. ETL functions can be used to transport and possibly transform the archive data to the data structures in the archive environment. Use archives to store data from applications that are being retired, as well as data from production operational systems that have not been used for a long time, to improve operational efficiency. It is critical to monitor archive technology to ensure that the data is still accessible when technology changes. Having an archive in an older structure or format unreadable by newer technology can be a risk, especially for data that is still legally requ ired. (See Chapter 9. ) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 267 1.3.5 Enterprise Message Format / Canonical Model A canonical data model is a common model used by an organization or data exchange group that standardizes the format in which data will be shared. In a hub -and-spoke data interaction design pattern, all systems that want to provide or receive data interact only with a central information hub. Data is transformed from or to a sending or receiving system based on a common or enterprise message format for the organization (a canonical model). (See Chapter 5.) Use of a canonical model limits the number of data transformations needed by any system or organization exchanging data. Each system needs to transform data only to and from the central canonical model, rather than to the format of the multitude of systems with which it may want to exchange data. Although developing and agreeing on a shared message format is a major undertaking, having a canonical model can significantly reduce the complexity of data interoperability in an enterprise, and thus greatly lower the cost of support. The creation and management of the common canonical data model for all data interactions is a complex item of overhead that is required in the implementation of an enterprise data integration solution using a hub -and-spoke interaction model. It is justifiable in support of managing the data interactions between more than three systems and critical for managing data interactions in environments of more than 100 application syst ems. 1.3.6 Interaction Models Interaction models describe ways to make connections between systems in order to transfer data. 1.3.6.1 Point -to-point The vast majority of interactions between systems that share data do so ‘point- to-point’; they pass data directly to each other. This model makes sense in the context of a small set of systems. However, it becomes quickly inefficient and increases organizational risk when many systems require the same data from the same sources. • Impacts to processing: If source systems are operational, then the workload from supplying data could affect processing. • Managing interfaces : The number of interfaces needed in a point- to-point interaction model approaches the number of systems squared (s 2). Once they are built, these interfaces need to be maintained and supported. The workload to manage and support interfaces between the systems can quickly become greater than supporting the systems themselves. • Potential for inconsistency : Design issues arise when multiple systems require different versions or formats of the data. The use of multiple interfaces to obtain data will lead to inconsistencies in the data sent to downstream systems. 1.3.6.2 Hub-and-spoke The hub -and-spoke model , an alternative to point- to-point, consolidates shared data (either physically or virtually) in a central data hub that many applications can use. All systems that want to exchange data do so Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "268 • DMBOK2 through a central common data control system, rather than directly with one another (point -to-point). Data Warehouses, Data Marts, Operational Data Stores, and Master Data Management hubs are the most well- known examples of data hubs. The hubs provide consistent views of the data with limited performance impact on the source systems. Data hubs even minimize the number of systems and extracts that must access the data sources, thus minimizing the impact on the source system resources. Adding new systems to the portfolio only requires building interfaces to the data hub. Hub -and-spoke interaction is more efficient and can be cost- justified even if the number of systems involved is relatively small, but it becomes critical to managing a por tfolio of systems in the hundreds or thousands. Enterprise Service Buses (ESB) are the data integration solution for near real -time sharing of data between many systems, where the hub is a virtual concept of the standard format or the canonical model for sharing data in the organization. Hub -and-spoke may not always be the best solution. Some hub -and-spoke model latency is unacceptable or performance is insufficient. The hub itself creates overhead in a hub -and-spoke architecture. A point- to-point solution would not require the hub. Howeve r, the benefits of the hub outweigh the drawbacks of the overhead as soon as three or more systems are involved in sharing data. Use of the hub -and-spoke design pattern for the interchange of data can drastically reduce the proliferation of data transformation and integration solutions and thus dramatically simplify the necessary organizational support. 1.3.6.3 Publish - Subscribe A publish and subscribe model involves systems pushing data out (publish), and other systems pulling data in (subscribe). Systems providing data are listed in a catalog of data services, and systems looking to consume data subscribe to those services. When data is published, the data is automatically sent to the subscribers. When multiple data consumers want a certain set of data or data in a certain format, developing that data set centrally and making it available to all who need it ensures that all constituents receive a consistent data set in a timely manner. 1.3.7 DII Architecture Concepts 1.3.7.1 Application Coupling Coupling describes the degree to which two systems are entwined. Two systems that are tightly coupled usually have a synchronous interface, where one system waits for a response from the other. Tight coupling represents a riskier operation: if one system is unavailable then they are both effectively unavailable, and the business continuity plan for both have to be the same. (See Chapter 6.) Where possible, loose coupling is a preferred interface design, where data is passed between systems without waiting for a response and one system may be unavailable without causing the other to be unavailable. Loose Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 269 coupling can be implemented using various techniques with services, APIs, or message queues. Figure 69 illustrates a possible loose coupling design. Figure 69 Application Coupling Service -Oriented Architecture using an Enterprise Service Bus is an example of a loosely coupled data interaction design pattern. Where the systems are loosely coupled, replacement of systems in the application inventory can theoretically be performed without rewriting the systems with which they interact, because the interaction points are well -defined. 1.3.7.2 Orchestration and Process Controls Orchestration is the term used to describe how multiple processes are organized and executed in a system. All systems handling messages or data packets must be able to manage the order of execution of those processes, in order to preserve consistency and continuity. Process Controls are the components that ensure shipment, delivery, extraction, and loading of data are accurate and complete. An often -overlooked aspect of basic data movement architecture, controls include: • Database activity logs • Batch job logs • Alerts • Exception logs • Job dependence charts with remediation options, standard responses • Job ‘clock’ information, such as the timing of dependent jobs, the expected length of the jobs , and the computing (available) w indow time 1.3.7.3 Enterprise Application Integration (EAI) In an enterprise application integration model (EAI), software modules interact with one another only through well -defined interface calls (application programming interfaces – APIs). Data stores are updated only by their Process A Process B Process A Process BServiceTight Coupling Loose CouplingAPI API Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "270 • DMBOK2 own software modules and other software cannot reach in to the data in an application but only access through the defined APIs. EAI is built on object- oriented concepts, which emphasize reuse and the ability to replace any module without impact on any othe r. 1.3.7.4 Enterprise Service Bus (ESB) An Enterprise Service Bus is a system that acts as an intermediary between systems, passing messages between them. Applications can send and receive messages or files using the ESB, and are encapsulated from other processes existing on the ESB. An example of loose coupling, the ES B acts as the service between the applications. (See Figure 70.) Figure 70 Enterprise Service Bus 1.3.7.5 Service -Oriented Architecture (SOA) Most mature enterprise data integration strategies utilize the idea of service -oriented architecture (SOA), where the functionality of providing data or updating data (or other data services) can be provided through well -defined service calls between appli cations. With this approach, applications do not have to have direct interaction with or knowledge of the inner workings of other applications. SOA enables application independence and the ability for an organization to replace systems without needing to make significant changes to the systems that interfaced with them. The goal of service- oriented architecture is to have well -defined interaction between self- contained software modules. Each module performs functions (a.k.a. provides services) to other software modules or to human consumers. The key concept is that SOA architecture provides independent services: the service has no fore knowledge of the calling application and the implementation of the service is a black box to the calling application. A service -oriented architecture may be implemented with various technolo gies including web services, messaging, RESTful APIs, etc. Services are usually implemented as APIs (application programming Application n Service n Service 1 Application 1 Application 2 Process Orchestration Manager Enterprise Service Bus Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 271 interfaces) that are available to be called by application systems (or human consumers). A well- defined API registry describes what options are available, parameters that need to be provided, and resulting information that is provided. Data services, which may include the addition, deletion, update, and retrieval of data, are specified in a catalog of available services. To achieve the enterprise goals of scalability (supporting integrations between all applications in the enterprise without using unreasonable amounts of resources to do so) and reuse (having services that are leveraged by all requestors of data of a type), a strong governance model must be established around the design and registration of services and APIs. Prior to developing new data services, it is necessary to ensure that no service already exists that could provide the requested data. In addition, new services need to be designed to meet broad requirements so that they will not be limited to the immediate need but can be reused. 1.3.7.6 Complex Event Processing (CEP) Event processing is a method of tracking and analyzing (processing) streams of information (data) about things that happen (events), and deriving a conclusion from them. Complex event processing (CEP) combines data from multiple sources to identify meaningful events (such as opportunities or threats) to predict behavior or activity and automatically trigger real -time response, such as suggesting a product for a consumer to purchase. Rules are set to guide the event processing and routing. Organizations can use complex event processing to predict behavior or activity and automatically trigger real - time response. Events such as sales leads, web clicks, orders, or customer service calls may happen across the various layers of an organization. Alternatively, they may include news items, text messages, social media posts, stock market feeds, traffic reports, weather reports, or other kinds of data. An event may also be defined as a change of state, when a measurement exceeds a predefined threshold of time, temperature, or other value. CEP presents some data challenges. In many cases, the rate at which events occur makes it impractical to retrieve the additional data necessary to interpret the event as it occurs. Efficient processing typically mandates pre-positioning some data in the CE P engine’s memory. Supporting complex event processing requires an environment that can integrate vast amounts of data of various types. Because of the volume and variety of data usually involved in creating predictions, complex event processing is often tied to Big Data. It often requires use of technologies that support ultra -low latency requirements such as processing real- time streaming data and in -memory databases. (See Chapter 14.) 1.3.7.7 Data Federation and Virtualization When data exists in disparate data stores, it can be brought together in ways other than physical integration. Data Federation provides access to a combination of individual data stores, regardless of structure. Data Virtualization enables distributed databases, as well as multiple heterogeneous data stores, to be accessed and viewed as a single database. (See Chapter 6.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "272 • DMBOK2 1.3.7.8 Data -as-a-Service (DaaS) Software -as-a-service (SaaS) is a delivery and licensing model. An application is licensed to provide services, but the software and data are located at a data center controlled by the software vendor rather than in the data center of the licensing organization. There are similar concepts for providing various tiers of computing infrastructure -as-a-service (IT -as-a-service, platform -as-a-service, database -as-a-service). One definition of Data- as-a-Service (DaaS) is data licensed from a vendor and provided on demand, rather than stored and maintained in the data center of the licensing organization. A common example includes information on the securities sold through a stock exchange and associated prices (current and historical). Although Data -as-a-Service certainly lends itself to vendors that sell data to stakeholders within an industry, the ‘service’ concept is also used within an organization to provide enterprise data or data services to various functions and operational systems. Service organizations provide a catalog of services available, service levels, and pricing schedules. 1.3.7.9 Cloud -based Integration Cloud -based integration (also known as integration platform -as-a-service or IPaaS) is a form of systems integration delivered as a cloud service that addresses data, process, service -oriented architecture (SOA), and application integration use cases. Prior to the emergence of cloud computing, integration could be categorized as either internal or business to business (B2B). Internal integration requirements are serviced through an on -premises middleware platform, and typically use a service bus (ESB) to manage exchange of data between systems. Business- to-business integration is serviced through EDI (electronic data interchange) gateways or value -added netwo rks (VAN) or market places. The advent of SaaS applications created a new kind of demand for integrating data located outside of an organization’s data center, met through cloud -based integration. Since their emergence, many such services have also developed the capability to integrate on -premises applications as well as function as EDI gateways. Cloud -based integration solutions are usually run as SaaS applications in the data centers of the vendors and not the organizations that own the data being integrated. Cloud -based integration involves interacting with the SaaS application data to be integrated using SOA interaction services. (See Chapter 6. ) 1.3.8 Data Exchange Standards Data Exchange Standards are formal rules for the structure of data elements. ISO (International Standards Organization) has developed data exchange standards, as have many industries. A data exchange specification is a common model used by an organization or data exchange group that standardizes the format in which data will be shared. An exchange pattern defines a structure for data transformations needed by any system or organization exchanging data. Data needs to be mapped to the exchange specification. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 273 Although developing and agreeing on a shared message format is a major undertaking, having an agreed upon exchange format or data layout between systems can significantly simplify data interoperability in an enterprise, lowering the cost of support and ena bling better understanding of the data. The National Information Exchange Model (NIEM) was developed to exchange documents and transactions across government organizations in the United States. The intention is that the sender and receiver of information share a common, unambiguous understanding of the meaning of that information. Conformance to NIEM ens ures that a basic set of information is well understood and carries the same consistent meaning across various communities, thus allowing interoperability. NIEM uses Extensible Markup Language (XML) for schema definitions and element representation, which allows the structure and meaning of data to be defined through simple, but carefully defined XML syntax rules. 2. Data Integration Activities Data Integration and Interoperability involves getting data where it is needed, when it is needed, and in the form in which it is needed. Data integration activities follow a development lifecycle. They begin with planning and move through design, development, testing, and implementation. Once implemented, integrated systems must be managed, monitored and enhanced. 2.1 Plan and Analyze 2.1.1 Define Data Integration and Lifecycle Requirements Defining data integration requirements involves understanding the organization’s business objectives, as well as the data required and the technology initiatives proposed to meet those objectives. It is also necessary to gather any relevant laws or regulat ions regarding the data to be used. Some activities may need to be restricted due to the data contents, and knowing up front will prevent issues later. Requirements must also account for organizational policy on data retention and other parts of the data lifecycle. Often requirements for data retention will differ by data domain and type. Data integration and lifecycle requirements are usually defined by business analysts, data stewards, and architects in various functions, including IT, who have a desire to get data in a certain place, in a certain format, and integrated with other data. The requirements will determine the type of DII interaction model, which then determines the technology and services necessary to fulfill the requirements. The process of defining requirements creates and uncovers valuable Metadata. This Metadata should be managed throughout the data lifecycle, from discovery through operations. The more complete and accurate an organization’s Metadata, the better its ability to manage the risks and costs of data integration. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "274 • DMBOK2 2.1.2 Perform Data Discovery Data discovery should be performed prior to design. The goal of data discovery is to identify potential sources of data for the data integration effort. Discovery will identify where data might be acquired and where it might be integrated. The process combines a technic al search, using tools that scan the Metadata and/or actual contents on an organization’s data sets, with subject matter expertise (i.e., interviewing people who work with the data of interest). Discovery also includes high -level assessment of Data Quality , to determine whether the data is fit for the purposes of the integration initiative. This assessment requires not only reviewing existing documentation, interviewing subject matter experts, but also verifying information gathered against the actual data through data profiling or other analysis. (See Section 2.1.4. ) In almost all cases, there will be discrepancies between what is believed about a data set and what is actually found to be true. Data discovery produces or adds to an inventory of organizational data. This inventory should be maintained in a Metadata repository. Ensure this inventory is maintained as a standard part of integration efforts: add or remove data stores, document structu re changes. Most organizations have a need to integrate data from their internal systems. However, data integration solutions may also involve the acquisition of data from outside the organization. There is a vast and ever growing amount of valuable information availa ble for free, or from data vendors. Data from external sources can be extremely valuable when integrated with data from within an organization. However, acquiring and integrating external data takes planning. 2.1.3 Document Data Lineage The process of data discovery will also uncover information about how data flows through an organization. This information can be used to document high -level data lineage: how the data under analysis is acquired or created by the organization, where it moves and is changed w ithin the organization, and how the data is used by the organization for analytics, decision -making, or event triggering. Detailed lineage can include the rules according to which data is changed, and the frequency of changes. Analysis of lineage may identify updates required to documentation of systems in use. Custom -coded ETL and other legacy data manipulation objects should be documented to ensure that the organization can analyze the impact of any changes in the data flow. The analysis process may also identify opportunities for improvements in the existing data flow. For example, finding that code can be upgraded to a simple call to a function in a tool, or can be discarded as no longer relevant. Sometimes an old tool is performing a transformation that is undone later in the process. Finding and removing these inefficiencies can greatly help with the project’s success and with an organization’s overall ability to use its data. 2.1.4 Profile Data Understanding data content and structure is essential to successful integration of data. Data profiling contributes to this end. Actual data structure and contents always differ from what is assumed. Sometimes Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 275 differences are small; other times they are large enough to derail an integration effort. Profiling can help integration teams discover these differences and use that knowledge to make better decisions about sourcing and design. If data profiling is skipped, then information that should influence design will not be discovered until testing or operations. Basic profiling involves analysis of: • Data format as defined in the data structures and inferred from the actual data • Data population, including the levels of null, blank, or defaulted data • Data values and how closely they correspond to a defined set of valid values • Patterns and relationships internal to the data set, such as related fields and cardinality rules • Relationships to other data sets More extensive profiling of the potential source and target data sets is required to understand how well the data meets the requirements of the particular data integration initiative. Profile both the sources and targets to understand how to transform the data to match requirements. One goal of profiling is to assess the quality of data. Assessing the fitness of the data for a particular use requires documenting business rules and measuring how well the data meets those business rules. Assessing accuracy requires comparing to a definitive set of data that has been determined to be correct. Such data sets are not always available, so measuring accuracy may not be possible, especially as part of a profiling effort. As with high -level data discovery, data profiling includes verifying assumptions about the data against the actual data. Capture results of data profiling in a Metadata repository for use on later projects and use what is learned from the process to improv e the accuracy of existing Metadata (Olson, 2003). (See Chapter 13. ) The requirement to profile data must be balanced with an organization’s security and privacy regulations. (See Chapter 7.) 2.1.5 Collect Business Rules Business rules are a critical subset of requirements. A business rule is a statement that defines or constrains an aspect of business processing. Business rules are intended to assert business structure or to control or influence the behavior of the business. Business rules fall into one of four categories: definitions of business terms, facts relating terms to each other, constraints or action assertions, and derivations. Use business rules to support Data Integration and Interoperability at various points, to: • Assess data in potential source and target data sets • Direct the flow of data in the organization • Monitor the organization’s operational data • Direct when to automatically trigger events and alerts For Master Data Management, business rules include match rules, merge rules, survivorship rules, and trust rules. For data archiving, data warehousing, and other situations where a data store is in use, the business rules also include data retention rules. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "276 • DMBOK2 Gathering business rules is also called rules harvesting or business rule mining. The business analyst or data steward can extract the rules from existing documentation (like use cases, specifications, or system code), or they may also organize workshops a nd interviews with subject matter experts (SMEs), or both. 2.2 Design Data Integration Solutions 2.2.1 Design Data Integration Architecture Data integration solutions should be specified at both the enterprise level and t he individual solution level (s ee Chapter 4 ). By establishing enterprise standards, the organization saves time in implementing individual solutions, because assessments and negotiations have been performed in advance of need. An enterprise approach saves money in the cost of licenses through group discounts and in the costs of operating a consistent and less complex set of solutions. Operational resources that support and back up one another can be part of a shared pool. Design a solution to meet the requirements, reusing as many of the existing Data Integration and Interoperability components as is feasible. A solution architecture indicates the techniques and technologies that will be used. It will include an inventory of the involved data structures (both persistent and transitive, existing and required), an indication of the orchestration and frequency of data flow, regulatory and security concerns and remediation, and operating concerns around backup and recovery, availability, and data archive and retention. 2.2.1.1 Select Interaction Model Determine which interaction model or combination will fulfill the requirements – hub-and-spoke, point -to- point, or publish -subscribe. If the requirements match an existing interaction pattern already implemented, re - use the existing system as much as possible, to reduce development effort s. 2.2.1.2 Design Data Services or Exchange Patterns Create or re -use existing integration flows to move the data. These data services should be companions to existing similar data services, but be careful to not create multiple almost- identical services, as troubleshooting and support increasingly become difficult if services proliferate. If an existing data flow can be modified to support multiple needs, it may be worthwhile to make that change instead of creating a new service. Any data exchange specification design should start with industry standards, or other exchange patterns already existing. When possible, make any changes to existing patterns generic enough to be useful to other systems; having specific exchange patterns that only relate to one exchange has th e same issues as point -to-point connections. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 277 2.2.2 Model Data Hubs, Interfaces, Messages, and Data Services Data structures needed in Data Integration and Interoperability include those in which data persists, such as Master Data Management hubs, data warehouses and marts, and operational data stores, and those that are transient and used only for moving or transforming data, such as interfaces, message layouts, and canonical models. Both types should be modeled. (See Chapter 5.) 2.2.3 Map Data Sources to Targets Almost all data integration solutions include transforming data from source to target structures. Mapping sources to targets involves specifying the rules for transforming data from one location and format to another. For each attribute mapped, a mapping specification • Indicates the technical format of the source and target • Specifies transformations required for all intermediate staging points between source and target • Describes how each attribute in a final or intermediate target data store will be populated • Describes whether data values need to be transformed; for example, by looking up the source value in a table that indicates the appropriate target value • Describes what calculations are required Transformation may be performed on a batch schedule, or triggered by the occurrence of a real -time event. It may be accomplished through physical persistence of the target format or through virtual presentation of the data in the target format. 2.2.4 Design Data Orchestration The flow of data in a data integration solution must be designed and documented. Data orchestration is the pattern of data flows from start to finish, including intermediate steps, required to complete the transformation and/or transaction. Batch data integration orchestration will indicate the frequency of the data movement and transformation. Batch data integration is usually coded into a scheduler that triggers the start at a certain time, periodicity, or when an event occurs. The schedule may include multiple steps with dependencies. Real -time data integration orchestration is usually triggered by an event, such as new or updated data. Real - time data integration orchestration is usually more complex and implemented across multiple tools. It may not be linear in nature. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "278 • DMBOK2 2.3 Develop Data Integration Solutions 2.3.1 Develop Data Services Develop services to access, transform, and deliver data as specified, matching the interaction model selected. Tools or vendor suites are most frequently used to implement data integration solutions, such as data transformation, Master Data Management, data warehousing, e tc. Using consistent tools or standard vendor suites across the organization for these various purposes can simplify operational support and lower operating costs by enabling shared support solutions. 2.3.2 Develop Data Flows Integration or ETL data flows will usually be developed within tools specialized to manage those flows in a proprietary way. Batch data flows will be developed in a scheduler (usually the enterprise standard scheduler) that will manage the order, frequency, and dependency of executing the data integration pieces that have been developed. Interoperability requirements may include developing mappings or coordination points between data stores. Some organizations use an ESB to subscribe to data that is created or changed in the organization and other applications to publish changes to data. The enterprise service bus will poll the applications constantly to see if they have any data to publish and deliver to them new or changed data for which they have subscribed. Developing real -time data integration flows involves monitoring for events that should trigger the execution of services to acquire, transform, or publish data. This is usually implemented within one or multiple proprietary technologies and is best implemented with a solution that can manage the op eration across technologies. 2.3.3 Develop Data Migration Approach Data needs to be moved when new applications are implemented or when applications are retired or merged. This process involves transformation of data to the format of the receiving application. Almost all application development projects involve some data migration, even if all that is involved is the population of Reference Data. Migration is not quite a one -time process, as it needs to be executed for testing phases as well as final implementation. Data migration projects are frequently under -estimated or under -designed, because programmers are told to simply move the data; they do not engage in the analysis and design activities required for data integration. When data is migrated without proper analysis, it often looks different from the data that came in through the normal processing. Or the migrated data may not work with the application as anticipated. Profiling data of core operational applications will usually highlight data that has been migra ted from one or more generations of previous operational systems and does not meet the standards of the data that enters the data set through the current application code. (See Chapter 6.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 279 2.3.4 Develop a Publication Approach Systems where critical data is created or maintained need to make that data available to other systems in the organization. New or changed data should be pushed by data producing applications to other systems (especially data hubs and enterprise data buses) either at the time of data change (event -driven) or on a periodic schedule. Best practice is to define common message definitions (canonical model) for the various types of data in the organization and let data consumers (either applications or individuals) who have appropriate access authority subscribe to receive notification of any changes to data of interest. 2.3.5 Develop Complex Event Processing Flows Developing complex event processing solutions requires: • Preparation of the historical data about an individual, organization, product, or market and pre - population of the predictive models • Processing the real -time data stream to fully populate the predictive model and identify meaningful events (opportunities or threats) • Executing the triggered action in response to the prediction Preparation and pre -processing of the historical data needed in the predictive model may be performed in nightly batch processes or in near real- time. Usually some of the predictive model can be populated in advance of the triggering event, such as identifying what products are usually bought together in preparation of suggesting an additional item for purchase. Some processing flows trigger a response to every event in the real -time stream, such as adding an item to a shopping cart; other processing flows attempt to identify particularly meaningful events that trigger action, such as a suspected fraudulent charge attempt on a credit card. The response to the identification of a meaningful event may be as simple as a warning being sent out or as complex as the automatic deployment of armed forces. 2.4 Implement and Monitor Activate the data services that have been developed and tested. Real -time data processing requires real -time monitoring for issues. Establish parameters that indicate potential problems with processing, as well as direct notification of issues. Automated as well as human monitoring for issues should be established, especially as the complexity and risk of the triggered responses rises. There are cases, for example, where issues with automated financial securities trading algorithms have triggered actions that have affected entire markets or bankrupted organizations. Data interaction capabilities must be monitored and serviced at the same service level as the most demanding target application or data consumer. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "280 • DMBOK2 2.4.1 Maintain DII Metadata As previously noted (see Section 2.1), an organization will create and uncover valuable Metadata during the process of developing DII solutions. This Metadata should be managed and maintained to ensure proper understanding of the data in the system, and to prevent the need to rediscover it for future solutions. Reliable Metadata improves an organization’s ability to manage risks, reduce costs, an d obtain more value from its data. Document the data structures of all systems involved in data integration as source, target, or staging. Include business definitions and technical definitions (structure, format, size), as well as the transformation of data between the persistent data stores . Whether data integration Metadata is stored in documents or a Metadata repository, it should not be changed without a review and approval process from both business and technical stakeholders. Most ETL tool vendors package their Metadata repositories with additional functionality that enables governance and stewardship oversight. If the Metadata repository is utilized as an operational tool, then it may even include operational Metadata about wh en data was copied and transformed between systems. Of particular importance for DII solutions is the SOA registry, which provides controlled access to an evolving catalog of information about the available services for accessing and using the data and functionality in an application. 3. Tools 3.1 Data Transformation Engine/ETL Tool A data transformation engine (or ETL tool) is the primary tool in the data integration toolbox, central to every enterprise data integration program. These tools usually support the operation as well as the design of the data transformation activities. Extremely sophisticated tools exist to develop and perform ETL, whether batch or real -time, physically or virtually. For single use point -to-point solutions, data integration processing is frequently implemented through custom coding. Enterprise level solu tions usually require the use of tools to perform this processing in a standard way across the organization. Basic considerations in selecting a data transformation engine should include whether it is necessary to handle batch as well as real -time functionality, and whether unstructured as well as structured data needs to be accommodated, as the most mature tools exist for batch -oriented processing of structured data only. 3.2 Data Virtualization Server Data transformation engines usually perform extract, transform, and load physically on data; however, data virtualization servers perform data extract, transform, and integrate virtually. Data virtualization servers can Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 281 combine structured and unstructured data. A data warehouse is frequently an input to a data virtualization server, but a data virtualization server does not replace the data warehouse in the enterprise information architecture. 3.3 Enterprise Service Bus An enterprise service bus (ESB) refers to both a software architecture model and a type of message- oriented middleware used to implement near real -time messaging between heterogeneous data stores, applications, and servers that reside within the same organization. Most internal data integration solutions that need to execute more frequently than daily use this architecture and this technology. Most commonly, an ESB is used in asynchronous format to enable the free flow of data. An ESB can also be used synchronously in certain situations. The enterprise service bus implements incoming and outgoing message queues on each of the systems participating in message interchange with an adapter or agent installed in each environment. The central processor for the ESB is usually implemented on a ser ver separate from the other participating systems. The processor keeps track of which systems have subscribed interest in what kinds of messages. The central processor continuously polls each participating system for outgoing messages and deposits incoming messages into the message queue for subscribed types of messages and messages that have been directly addressed to that system. This model is called ‘near real- time’ because the data can take up to a couple of minutes to get from sending system to receiving system. This is a loosely coupled model and the system sending data will not wait for confirmation of receipt and update from the receiving system before continuin g processing. 3.4 Business Rules Engine Many data integration solutions are dependent on business rules. An important form of Metadata, these rules can be used in basic integration and in solutions that incorporate complex event processing to enable an organization to respond to events in near real -time. A business rules engine that allows non-technical use rs to manage business rules implemented by software is a very valuable tool that will enable evolution of the solution at a lower cost, because a business rules engine can support changes to predictive models without technical code changes. For example, mo dels that predict what a customer might want to purchase may be defined as business rules rather than code changes. 3.5 Data and Process Modeling Tools Data modeling tools should be used to design not only the target but also the intermediate data structures needed in data integration solutions. The structure of the messages or streams of data that pass between systems and organizations, and are not usually persisted, should nevertheless be modeled. The flow of data between systems and organizations should also be designed, as should complex event processes. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "282 • DMBOK2 3.6 Data Profiling Tool Data profiling involves statistical analysis of data set contents to understand format, completeness, consistency, validity, and structure of the data. All data integration and interoperability development should include detailed assessment of potential data sources and targets to determine whether the actual data meets the needs of the proposed solution. Since most integration projects involve a significant amount of data, the most efficient means of conducting this analysis is to use a data profiling tool. (See Section 2.1.4 and Chapter 13. ) 3.7 Metadata Repository A Metadata repository contains information about the data in an organization, including data structure, content, and the business rules for managing the data. During data integration projects, one or more Metadata repositories may be used to document the technical structure an d business meaning of the data being sourced, transformed, and targeted. Usually the rules regarding data transformation, lineage, and processing used by the data integration tools are also stored in a Metadata repository as are the instructions for scheduled processes such as triggers and frequency. Every tool usually has its own Metadata repository. Suites of tools from the same vendor may share a Metadata repository. One Metadata repository may be designated as a central point for consolidating data from the various operational tools. (See Chapter 12. ) 4. Techniques Several of the important techniques for designing data integration solutions are described in the Essential Concepts in this chapter. The basic goals are to keep the applications coupled loosely, limit the number of interfaces developed and requiring manag ement by using a hub -and-spoke approach, and to create standard (or canonical) interfaces. 5. Implementation Guidelines 5.1 Readiness Assessment / Risk Assessment All organizations have some form of DII already in place – so the readiness/risk assessment should be around enterprise integration tool implementation, or enhancing capabilities to allow interoperability. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 283 Implementing enterprise data integration solutions is usually cost -justified based on implementation between many systems. Design an enterprise data integration solution to support the movement of data between many applications and organizations, and not just the first one to be implemented. Many organizations spend their time reworking existing solutions instead of bringing additional value. Focus on implementing data integration solutions where none or limited integration currently exists, rather than replacing working data integration solutions with a common enterprise solution across the organization. Certain data projects can justify a data integration solution focused only on a particular application, such as a data warehouse or Master Data Management hub. In those cases, any additional use of the data integration solution adds value to the investment, because the first system use already achieved the justification. Application support teams prefer to manage data integration solutions locally. They will perceive that the cost of doing so is lower than leveraging an enterprise solution. The software vendors that support such teams will also prefer that they leverage th e data integration tools that they sell. Therefore, it is necessary to sponsor the implementation of an enterprise data integration program from a level that has sufficient authority over solution design and technology purchase, such as from IT enterprise architecture. In addition, it may be necessary to encourage application systems to participate through positive incentives, such as funding the data integration technology centrally, and through negative incentives, such as refusing to approve the implemen tation of new alternative data integration technologies. Development projects that implement new data integration technology frequently become focused on the technology and lose focus on the business goals. It is necessary to make sure that data integration solution implementation retain s focus on the business goals and requirements, including making sure that some participants in every project are business - or application -oriented, and not just data integration tool experts. 5.2 Organization and Cultural Change Organizations must determine whether responsibility for managing data integration implementations is centralized or whether it resides with decentralized application teams. Local teams understand the data in their applications. Central teams can build deep knowledge of tools and technologies. Many organizations develop a Center of Excellence specializing in the design and deployment of the enterprise data integration solutions. Local and central teams collaborate to develop solutions connecting an application into an enterprise data integration solution. The local team should take primary responsibility for managing the solution and resolving any problems, escalating to the Center of Excellence, if necessary. Data integration solutions are frequently perceived as purely technical; however, to successfully deliver value, they must be developed based on deep business knowledge. The data analysis and modeling activities should be performed by business -oriented res ources. Development of a canonical message model, or consistent standard for how data is shared in the organization, requires a large resource commitment that should involve business modeling resources as well as technical resources. Review all data transformation mapping design and changes with business subject matter experts in each involved system. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "284 • DMBOK2 6. DII Governance Decisions about the design of data messages, data models, and data transformation rules have a direct impact on an organization’s ability to use its data. These decisions must be business- driven. While there are many technical considerations in implementin g business rules, a purely technical approach to DII can lead to errors in the data mappings and transformations as data flows into, through , and out of an organization. Business stakeholders are responsible for defining rules for how data should be modeled and transformed. Business stakeholders should approve changes to any of these business rules. Rules should be captured as Metadata and consolidated for cross -enterprise analysis. Identifying and verifying the predictive models and defining what actions should be automatically triggered by the predictions are also business functions. Without trust that the integration or interoperable design will perform as promised, in a secure, reliable way, there can be no effective business value. In DII, the landscape of governance controls to support trust can be complex and detailed. One approac h is to determine what events trigger governance reviews (exceptions or critical events). Map each trigger to reviews that engage with governance bodies. Event triggers may be part of the System Development Life Cycle (SDLC) at Stage Gates when moving from one phase to another or as part of User Stories. For example, architecture design compliance checklists may include such questions as: If possible, are you using the ESB and tools? Was there a search for reusable services? Controls may come from governance -driven management routines, such as mandated reviews of models, auditing of Metadata, gating of deliverables, and required approvals for changes to transformation rules. In Service Level Agreements, and in Business Continuity/Disaster Recovery plans, real -time operational data integration solutions must be included in the same backup and recovery tier as the most critical system to which they provide data. Policies need to be established to ensure that the organization benefits from an enterprise approach to DII. For example, policies can be put in place to ensure that SOA principles are followed, that new services are created only after a review of existing services, and that all data flowing between systems goes through the enterprise service bus. 6.1 Data Sharing Agreements Prior to the development of interfaces or the provision of data electronically, develop a data sharing agreement or memorandum of understanding (MOU) which stipulates the responsibilities and acceptable use of data to be exchanged, approved by the business data stewards of the data in question. The data sharing agreements should specify anticipated use and access to the data, restrictions on use, as well as expected service levels, including required system up times and response times. These agreements are especially critical for regulated industries, or when personal or secure information is involved. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA INTEGRATION AND INTEROPERABILITY • 285 6.2 DII and Data Lineage Data lineage is useful to the development of DII solutions. It is also often required for data consumers to use data, but it is becoming even more important as data is integrated between organizations. Governance is required to ensure that knowledge of data origins and movement is documented. Data sharing agreements may stipulate limitations to the uses of data and in order to abide by these, it is necessary to know where data moves and persists. There are emergin g compliance standards (for example, Solvency II regulation in Europe) that require organizations be able to describe where their data originated and how it has been changed as it has moved through various systems. In addition, data lineage information is required when making changes to data flows. This information must be managed as a critical part of solution Metadata. Forward and backward data lineage (i.e., where did data get used and where did it come from) is critical as part of the impact analysis needed when making changes to data structures, data flows, or data processing. 6.3 Data Integration Metrics To measure the scale and benefits from implementing Data Integration solutions, include metrics on availability, volume, speed, cost, and usage: • Data Availability o Availability of data requested • Data Volumes and Speed o Volumes of data transported and transformed o Volumes of data analyzed o Speed of transmission o Latency between data update and availability o Latency between event and triggered action o Time to availability of new data sources • Solution Costs and Complexity o Cost of developing and managing solutions o Ease of acquiring new data o Complexity of solutions and operations o Number of systems using data integration solutions 7. Works Cited / Recommended Aiken, P. and Allen, D. M. XML in Data Management . Morgan Kaufmann, 2004. Print. Bahga, Arshdeep, and Vijay Madisetti. Cloud Computing: A Hands -On Approach . CreateSpace Independent Publishing Platform, 2013. Print. Bobak, Angelo R. Connecting the Data: Data Integration Techniques for Building an Operational Data Store (ODS) . Technics Publications, LLC, 2012. Print. Brackett, Michael. Data Resource Integration: Understanding and Resolving a Disparate Data Resource . Technics Publications, LLC, 2012. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "286 • DMBOK2 Carstensen, Jared, Bernard Golden, and JP Morgenthal. Cloud Computing - Assessing the Risks. IT Governance Publishing, 2012. Print. Di Martino, Beniamino, Giuseppina Cretella, and Antonio Esposito. Cloud Portability and Interoperability: Issues and Current Trend . Springer, 2015. Print. SpringerBriefs in Computer Science. Doan, AnHai, Alon Halevy, and Zachary Ives. Principles of Data Integration . Morgan Kaufmann, 2012. Print. Erl, Thomas, Ricardo Puttini, and Zaigham Mahmood. Cloud Computing: Concepts, Technology and Architecture. Prentice Hall, 2013. Print. The Prentice Hall Service Technology Ser. from Thomas Erl. Ferguson, M. Maximizing the Business Value of Data Virtualization . Enterprise Data World, 2012. Web. http://bit.ly/2sVAsui . Giordano, Anthony David. Data Integration Blueprint and Modeling: Techniques for a Scalable and Sustainable Architecture . IBM Press, 2011. Print. Haley, Beard. Cloud Computing Best Practices for Managing and Measuring Processes for On -demand Computing, Applications and Data Centers in the Cloud with SLAs. Emereo Publishing, 2008. Print. Hohpe, Gregor and Bobby Woolf. Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions . Addison - Wesley Professional, 2003. Print. Inmon, W. Building the Data Warehouse. 4th ed. Wiley, 2005. Print. Inmon, W., Claudia Imhoff, and Ryan Sousa. The Corporate Information Factory . 2nd ed. Wiley 2001, Print. Jamsa, Kris. Cloud Computing: SaaS, PaaS, IaaS, Virtualization, Business Models, Mobile, Security and More . Jones and Bartlett Learning, 2012. Print. Kavis, Michael J. Architecting the Cloud: Design Decisions for Cloud Computing Service Models (SaaS, PaaS, and IaaS) . Wiley, 2014. Print. Wiley CIO. Kimball, Ralph and Margy Ross. The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling . 2nd ed. Wiley, 2002. Print. Linthicum, David S. Cloud Computing and SOA Convergence in Your Enterprise: A Step -by-Step Guide . Addison -Wesley Professional, 2009. Print. Linthicum, David S. Enterprise Application Integration . Addison -Wesley Professional, 1999. Print. Linthicum, David S. Next Generation Application Integration: From Simple Information to Web Services . Addison -Wesley Professional, 2003. Print. Loshin, David. Master Data Management. Morgan Kaufmann, 2009. Print. Majkic, Zoran. Big Data Integration Theory: Theory and Methods of Database Mappings, Programming Languages, and Semantics . Springer, 2014. Print. Texts in Computer Science. Mather, Tim, Subra Kumaraswamy, and Shahed Latif. Cloud Security and Privacy: An Enterprise Perspective on Risks and Compliance. O'Reilly Media, 2009. Print. Theory in Practice. Reese, George. Cloud Application Architectures: Building Applications and Infrastructure in the Cloud . O'Reilly Media, 2009. Print. Theory in Practice (O'Reilly). Reeve, April. Managing Data in Motion: Data Integration Best Practice Techniques and Technologies . Morgan Kaufmann, 2013. Print. The Morgan Kaufmann Series on Business Intelligence. Rhoton, John. Cloud Computing Explained: Implementation Handbook for Enterprises . Recursive Press, 2009. Print. Sarkar, Pushpak. Data as a Service: A Framework for Providing Reusable Enterprise Data Services . Wiley -IEEE Computer Society Pr, 2015. Print. Sears, Jonathan. Data Integration 200 Success Secrets - 200 Most Asked Questions On Data Integration - What You Need to Know. Emereo Publishing, 2014. Kindle. Sherman, Rick. Business Intelligence Guidebook: From Data Integration to Analytics . Morgan Kaufmann, 2014. Print. U.S. Department of Commerce. Guidelines on Security and Privacy in Public Cloud Computing . CreateSpace Independent Publishing Platform, 2014. Print. Van der Lans, Rick. Data Virtualization for Business Intelligence Systems: Revolutionizing Data Integration for Data Warehouses . Morgan Kaufmann, 2012. Print. The Morgan Kaufmann Series on Business Intelligence. Zhao, Liang, Sherif Sakr, Anna Liu, and Athman Bouguettaya. Cloud Data Management . Springer; 2014. Print . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "287 CHAPTER 9 Document and Content Management 1. Introduction ocument and Content Management entails controlling the capture, storage, access, and use of data and information stored predominantly outside relational databases.44 Its focus is on maintaining the integrity of and enabling access to documents and other unstructured or semi -structured information , which makes it roughly equivalent to data operations management for relational databases. However, it also has strategic drivers. In many organizations, unstructured data has a direct relationship to structured data. Management decisions about such content should be applied consistently. In addition, as are other types of 44 The types of unstructured data have evolved since the early 2000s, as the capacity to capture and store digital information has grown. The concept of unstructured data continues to refer to data that is not pre -defined through a data model, whether relational or otherwise. Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International D Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "288 • DMBOK2 data, documents , and unstructured content are expected to be secure and of high quality. Ensuring security and quality requires governance, reliable architecture, and well -managed Metadata. Figure 71 Context Diagram: Documents and Content Definition : Controlling the capture, storage, access, and use of data and information stored predominantly outside relational databases. Goals : 1. T o comply with legal obligations and customer expectations regarding Records management. 2. T o ensure effective and efficient storage, retrieval, and use of Documents and Content. 3. T o ensure integration capabilities between structured and unstructured Content. Activities : 1.Plan for Lifecycle Management (P) 1. Plan for Records Management 2. Develop a content strategy 3. Create Content Handling Policies 4. Define Content Information Architecture 2.Manage the Lifecycle (O) 1. Capture Records and Content (O) 2. Manage Versioning and Control 3. Backup and Recovery 4. Manage Retention and Disposal 5. Audit Document and Records 3.Publish and Deliver Content (O) 1. Provide Access, Search, and Retrieval 2. Deliver Through Acceptable Channels Inputs : •Business strategy •IT strategy •Legal retention requirements •T ext file •Electronic format file •Printed paper file •Social media streamDeliverables : •Content and Records Management Strategy •Policy and procedure •Content Repository •Managed record in many media formats •Audit trail and log •E-discovery approaches Suppliers : •Legal team •Business team •IT team •External partyConsumers : •Business user •IT user •Government regulatory agency •Audit team •External customerParticipants : •Data steward •Data management professional •Records management staff •Content management staff •Web development staff •Librarians T echniques : •Metadata tagging •Data markup and exchange format •Data mapping •Storyboarding •InfographicsT ools : •Office productivity software •Enterprise content management system •Controlled vocabulary / meta -data tool •Knowledge management wiki •Visual media tool •Social media •E-discovery technologyMetrics : •Compliance audit metric •Return on investment •Usage metric •Record management KPI •E-discovery KPI •ECM program metric •ECM operational metric (P) Planning, (C) Control, (D) Development, (O) OperationsDocument and Content Management T echnical DriversBusiness Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 289 1.1 Business Drivers The primary business drivers for document and content management include regulatory compliance, the ability to respond to litigation and e -discovery requests, and business continuity requirements. Good records management can also help organizations become more efficient. Well- organized, searchable websites that result from effective management of ontologies and other structures that facilitate searching help improve customer and employee satisfaction. Laws and regulations require that organizations maintain records of certain kinds of activities. Most organizations also have policies, standards, and best practices for record keeping. Records include both paper documents and electronically stored information (ESI). Good records management is necessary for business continuity. It also enables an organization to respond in the case of litigation. E-discovery is the process of finding electronic records that might serve as evidence in a legal action. As the technology for creating, storing, and using data has developed, the volume of ESI has increased exponentially. Some of this data will undoubtedly end up in litigation or regulatory requests. The ability of an organization to respond to an e- discovery request depends on how proactively it has managed records such as email, chats, websites, and electronic documents, as well as raw application data and Metadata. Big Data has become a driver for m ore efficient e -discovery, records retention, and strong information governance. Gaining efficiencies is a driver for improving document management. Technological advances in document management are helping organizations streamline processes, manage workflow, eliminate repetitive manual tasks, and enable collaboration. These technologies have the additional benefits of enabling people to locate, acce ss, and share documents more quickly. They can also prevent documents from being lost. This is very important for e -discovery. Money is also saved by freeing up file cabinet space and reducing document handling costs. 1.2 Goals and Principles The goals of implementing best practices around Document and Content Management include: • Ensuring effective and efficient retrieval and use of data and information in unstructured formats • Ensuring integration capabilities between structured and unstructured data • Complying with legal obligations and customer expectations Management of Documents and Content follows these guiding principles: • Everyone in an organization has a role to play in protecting the organization’s future. Everyone must create, use, retrieve, and dispose of records in accordance with the established policies and procedures. • Experts in the handling of records and content should be fully engaged in policy and planning. Regulatory and best practices can vary significantly based on industry sector and legal jurisdiction. Even if records management professionals are not available to the organization, everyone can be trained to understand the challenges, best practices, and issues. Once trained, business stewards and others can Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "290 • DMBOK2 collaborate on an effective approach to records management. In 2009, ARMA International , a not-for- profit professional association for managing records and information, published a set of Generally Acceptable Recordkeeping Principles® (GARP)45 that describes how business records should be maintained. It also provides a recordkeeping and information governance framework with associated metrics. The first sentence of each principle is stated below. Further explanation can be found on the ARMA website. • Principle of Accountability: An organization shall assign a senior executive to appropriate individuals, adopt policies and processes to guide staff, and ensure program auditability. • Principle of Integrity: An information governance function shall be constructed so the records and information generated or managed by or for the organization have a reasonable and suitable guarantee of authenticity and reliability. • Principle of Protection : An information governance function shall be constructed to ensure a reasonable level of protection to information that is personal or that otherwise requires protection. • Principle of Compliance : An information governance function shall be constructed to comply with applicable laws and other binding authorities, as well as the organization’s policies. • Principle of Availability : An organization shall maintain its information in a manner that ensures timely, efficient, and accurate retrieval of its information. • Principle of Retention : An organization shall retain its information for an appropriate time, taking into account all operational, legal, regulatory, and fiscal requirements, and those of all relevant binding authorities. • Principle of Disposition : An organization shall provide secure and appropriate disposition of information in accordance with its policies, and, applicable laws, regulations and other binding authorities. • Principle of Transparency : An organization shall document its policies, processes and activities, including its information governance function , in a manner that is available to and understood by staff and appropriate interested parties. 1.3 Essential Concepts 1.3.1 Content A document is to content what a bucket is to water: a container. Content refers to the data and information inside the file, document, or website. Content is often managed based on the concepts represented by the documents, as well as the type or status of the documents. Content also has a lifecycle. In its completed form, 45 ARMA International, ARMA Generally Accepted Recordkeeping Principles®, http://bit.ly/2tNF1E4 . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 291 some content becomes a matter of record for an organization. Official records are treated differently from other content. 1.3.1.1 Content Management Content management includes the processes, techniques, and technologies for organizing, categorizing, and structuring information resources so that they can be stored, published, and reused in multiple ways. The lifecycle of content can be active, with daily changes through controlled processes for creation and modification; or it can be more static with only minor, occasional changes. Content may be managed formally (strictly stored, managed, audited, retained or disposed of) or inf ormally through ad hoc updates. Content management is particularly important in websites and portals, but the techniques of indexing based on keywords and organizing based on taxonomies can be applied across technology platforms. When the scope of content management includes the entire enterprise, it is referred to as Enterprise Content Management (ECM). 1.3.1.2 Content Metadata Metadata is essential to managing unstructured data, both what is traditionally thought of as content and documents and what we now understand as ‘Big Data’. Without Metadata, it is not possible to inventory and organize content. Metadata for unstructured data con tent is based on: • Format : Often the format of the data dictates the method to access the data (such as electronic index for electronic unstructured data). • Search -ability : Whether search tools already exist for use with related unstructured data. • Self-documentation: Whether the Metadata is self -documenting (as in filesystems). In this case, development is minimal, as the existing tool is simply adopted. • Existing p atterns : Whether existing methods and patterns can be adopted or adapted (as in library catalogs). • Content s ubjects : The things people are likely to be looking for. • Requirements : Need for thoroughness and detail in retrieval (as in the pharmaceutical or nuclear industry 46). Therefore, detailed Metadata at the content level might be necessary, and a tool capable of content tagging might be necessary. Generally, the maintenance of Metadata for unstructured data becomes the maintenance of a cross -reference between various local patterns and the official set of enterprise Metadata. Records managers and Metadata professionals recognize long -term embedded m ethods exist throughout the organization for documents, records, and other content that must be retained for many years, but that these methods are often costly to re - 46 These industries are responsible for supplying evidence of how certain kinds of materials are handled. Pharmacy manufacturers, for example, must keep detailed records of how a compound came to be and was then tested and handled, before being allowed to be used by people. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "292 • DMBOK2 organize. In some organizations, a centralized team maintains cross-reference patterns between records management indexes, taxonomies, and even variant thesauri. 1.3.1.3 Content Model ing Content modeling is the process of converting logical content concepts into content types, attributes, and data types with relationships. An attribute describes something specific and distinguishable about the content to which it relates. A data type restricts the type of data the attribute may hold, enabling validation and processing. Metadata management and data modeling techniques are used in the development of a content model. There are two levels of content modeling. The first is at the information product level, which creates an actual deliverable like a website. The second is at the component level, which further details the elements that make up the information product model . The level of detail in the model depends on the granularity desired for reuse and structure. Content models support the content strategy by guiding content creation and promoting reuse. They support adaptive content, which is format- free and device-independent. The models become the specifications for the content implemented in such structures suc h as XML schema definition (XSDs), forms, or stylesheets. 1.3.1.4 Content Delivery Methods Content needs to be modular, structured, reusable, and device- and platform - independent. Delivery methods include web pages, print, and mobile apps as well as eBooks with interactive video and audio. Converting content into XML early in the workflow supports reu se across different media channels. Content delivery systems are ‘push’, ‘pull’, or interactive. • Push : In a push delivery system, users choose the type of content delivered to them on a pre - determined schedule. Syndication involves one party creating the content published in many places. Really Simple Syndication (RSS) is an example of a push content delivery mechanism. It distributes content (i.e., a feed) to syndicate news and other web content upon request. • Pull: In a pull delivery system, users pull the content through the Internet. An example of a pull system is when shoppers visit online retail stores. • Interactive : Interactive content delivery methods, such as third -party electronic point of sale (EPOS) apps or customer facing websites (e.g., for enrollment), need to exchange high volumes of real -time data between enterprise applications. Options for sharing data between applications include Enterprise Application Integration (EAI), Changed Data Capture, Data Integration and EII. (See Chapter 8.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 293 1.3.2 Controlled Vocabularies A controlled vocabulary is a defined list of explicitly allowed terms used to index, categorize, tag, sort, and retrieve content through browsing and searching. A controlled vocabulary is necessary to systematically organize documents, records, and content. Vocabularies range in complexity from simple lists or pick lists, to the synonym rings or authority lists, to taxonomies, and, the most complex, thesauri and ontologies. An example of a controlled vocabulary is the Dublin Core , used to catalog publications. Defined policies control over who adds terms to the vocabulary (e.g., a taxonomist or indexer, or librarian). Librarians are particularly trained in the theory and development of controlled vocabularies. Users of the list may only apply terms from the list for its scoped subject area. (See Chapter 10.) Ideally, controlled vocabularies should be aligned with the entity names and definitions in an enterprise conceptual data model. A bottom up approach to collecting terms and concepts is to compile them in a folksonomy , which is a collection of terms and concepts obtained through social tagging. Controlled vocabularies constitute a type of Reference Data. Like other Reference Data, their values and definitions need to be managed for completeness and currency. They can also be thought of as Metadata, as they help explain and support the use of othe r data. They are described in this chapter because Document and Content Management are primary use cases for controlled vocabularies. 1.3.2.1 Vocabulary Management Because vocabularies evolve over time, they require management. ANSI/NISO Z39.19 -2005 is an American standard, which provides guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, describes vocabulary management as a way to “to improv e the effectiveness of information storage and retrieval systems, web navigation systems, and other environments that seek to both identify and locate desired content via some sort of description using language. The primary purpose of vocabulary control is to achieve consistency in the description of content objects and to facilitate retrieval.” 47 Vocabulary management is the function of defining, sourcing, importing, and maintaining any given vocabulary. Key questions to enable vocabulary management focus on uses, consumers, standards, and maintenance: • What information concepts will this vocabulary support? • Who is the audience for this vocabulary? What processes do they support? What roles do they play? • Why is the vocabulary necessary? Will it support an application, content management, or analytics? • What decision -making body is responsible for designating preferred terms? • What existing vocabularies do different groups use to classify this information? Where are they located? How were they created? Who are their subject matter experts? Are there any security or privacy concerns for any of them? • Is there an existing standard that can fulfill this need? Are there concerns of using an external standard vs. internal? How frequently is the standard updated and what is the degree of change of 47 http://bit.ly/2sTaI2h . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "294 • DMBOK2 each update? Are standards accessible in an easy to import / maintain format, in a cost -efficient manner? The results of this assessment will enable data integration. They will also help to establish internal standards, including associated preferred vocabulary through term and term relationship management functions. If this kind of assessment is not done, preferred vocabularies would still be defined in an organization, except they would be done in silos, project by project, lead to a higher cost of integration and higher c hances of Data Quality issues. (S ee Chapter 13.) 1.3.2.2 Vocabulary Views and Micro -controlled Vocabulary A vocabulary view is a subset of a controlled vocabulary, covering a limited range of topics within the domain of the controlled vocabulary. Vocabulary views are necessary when the goal is to use a standard vocabulary containing a large number of terms, but not all terms are relevant to some consumers of the information. For example, a view that only contains terms relevant to a Marketing Business Unit would not contain terms relevant only to Finance. Vocabulary views increase information’s usability by limiting the content to what is appropriate to the users. Construct a vocabulary view of preferred vocabulary terms manually, or through business rules that act on preferred vocabulary term data or Metad ata. Define rules for which terms are included in each vocabulary view. A micro -controlled vocabulary is a vocabulary view containing highly specialized terms not present in the general vocabulary. An example of a micro -controlled vocabulary is a medical dictionary with subsets for medical disciplines. Such terms should map to the hierarchical structure o f the broad controlled vocabulary. A micro - controlled vocabulary is internally consistent with respect to relationships among terms. Micro -controlled vocabularies are necessary when the goal is to take advantage of a standard vocabulary, but the content is not sufficient and there is a need to manage additions/extensions for a specific group of information consumers. Building a micro -controlled vocabulary starts with the same steps as a vocabulary view, but it also includes addition or association of additional preferred terms that are differentiated from the pre - existing preferred terms by indicating a different source. 1.3.2.3 Term and Pick Lists Lists of terms are just that: lists. They do not describe relationships between the terms. Pick lists, web pull - down lists, and lists of menu choices in information systems use term lists. They provide little or no guidance to the user, but they help to control ambiguity by reducing the domain of values. Pick lists are often buried in applications. Content management software can help transform pick lists and controlled vocabularies into pick lists searchable from the home page. These pick lists are managed as faceted taxonomies inside the software. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 295 1.3.2.4 Term Management The standard ANSI/NISO Z39.19 -2005 defines a term as “One or more words designating a concept.”48 Like vocabularies, individual terms also require management. Term Management includes specifying how terms are initially defined and classified and how this information is maintained once it starts being used in different systems. Terms should be managed through a governance processes. Stewards may need to arbitrate to ensure stakeholder feedback is accounted for before terms are changed. Z39.19 defines a preferred term as one of two or more synonyms or lexical variants selected as a term for inclusion in a controlled vocabulary. Term management includes establishing relationships between terms within a controlled vocabulary . There are three types of relationships: • Equivalent term relationship : A relationship between or among terms in a controlled vocabulary that leads to one or more terms to use instead of the term from which the cross -reference is made. This is the most commonly used term mapping in IT functions, indicating a term or value fr om one system or vocabulary is the same as another, so integration technologies can perform their mapping and standardization. • Hierarchical relationship : A relationship between or among terms in a controlled vocabulary that depicts broader (general) to narrower (specific) or whole -part relationships. • Related term relationship: A term that is associatively but not hierarchically linked to another term in a controlled vocabulary. 1.3.2.5 Synonym Rings and Authority Lists A synonym ring is a set of terms with roughly equivalent meaning. A synonym ring allows users who search on one of the terms to access content related to any of the terms. The manual development of synonym rings is for retrieval, not for indexing. They offer synonym con trol, and treat synonyms and near synonymous terms equally. Usage occurs where the indexing environment has an uncontrolled vocabulary or where there is no indexing. Search engines and different Metadata registries have synonym rings (See Chapter 13.) They can be difficult to implement on user interfaces. An authority list is a controlled vocabulary of descriptive terms designed to facilitate retrieval of information within a specific domain or scope. Term treatment is not equal as it is within a synonym ring; instead, one term is preferred and the others are variants. An a uthority file cross -references synonyms and variants for each term to guide the user from a non -preferred to a preferred term. The list may or may not contain definitions of these terms. Authority lists should have designated managers. They may have structure. An example is the US Library of Congress’ Subject Headings. (See Section 1.3.2.1.) 48 http://bit.ly/2sTaI2h . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "296 • DMBOK2 1.3.2.6 Taxonomies Taxonomy is an umbrella term referring to any classification or controlled vocabulary. The best -known example of taxonomy is the classification system for all living things developed by the Swedish biologist Linnaeus. In content management, a taxonomy is a naming structure containing a controlled vocabulary used for outlining topics and enabling navigation and search systems. Taxonomies help reduce ambiguity and control synonyms. A hierarchical taxonomy may contain different types of parent/child relationships useful for both indexers and searchers. Such taxonomies are used to create drill -down type interfaces. Taxonomies can have different structures: • A flat taxonomy has no relationship s among the set of controlled categories. All the categories are equal. This is similar to a list; for example, a list of countries. • A hierarchical taxonomy is a tree structure where nodes are related by a rule. A hierarchy has at least two levels and is bi -directional. Moving up the hierarchy expands the category; moving down refines the category. An example is geography, from continent down to street address . • A polyhierarchy is a tree -like structure with more than one node relation rule. Child nodes may have multiple parents. Those parents may also share grandparents. As such, the traversal paths can be complicated and care must be taken to avoid potential invalid traversals: up the tree from a node that relates to the parent, but not to one of the grandparents. Complicated polyhierarchy structures may be better served with a facet taxonomy instead. • A facet taxonomy looks like a star where each node is associated with the center node. Facets are attributes of the object in the center. An example is Metadata, where each attribute (creator, title, access rights, keywords, version, etc.) is a facet of a content object. • A network taxonomy uses both hierarchical and facet structures. Any two nodes in network taxonomy establish linkages based on their associations. An example is a recommender engine (…if you liked that, you might also like this…). Another example is a thesaurus. With the amount of data being generated, even with the best -defined taxonomies require automated flagging, correction, and routing rules. If taxonomies are not maintained, they will be underutilized or will produce incorrect results. This creates the risk that entities and staff governed by applicable regulations will be out of compliance. For example, in a financial taxonomy, the preferred term may be ‘Postemployment’. Content may come from systems that classify it as ‘Post -Employment’, ‘Post Employment’, or even Post Retirement. To support such cases, appropriate synonym ring and related term relationships should be defined (US GAAP, 2008). Organizations develop their own taxonomies to formalize collective thinking about topics specific to their work. Taxonomies are particularly important for presenting and finding information on websites, as many search engines rely on exact word matches and can only find items tagged or using the same words in the same way. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 297 1.3.2.7 Classification Schemes and Tagging Classification schemes are codes that represent controlled vocabulary . These schemes are often hierarchical and may have words associated with them, such as the Dewey Decimal System and the US Library of Congress Classification (main classes and subclasses). A number based taxonomy, the Dewey Decimal System is also a multi -lingual expression for subject coding, since numbers can be ‘decoded’ into any language. Folksonomies are classification schemes for online content terms and names obtained through social tagging. Individual users and groups use them to annotate and categorize digital content. They typically do not have hierarchical structures or preferred terms. Folksono mies are not usually considered authoritative or applied to document indexing because experts do not compile them. However, because they directly reflect the vocabulary of users, they offer the potential to enhance information retrieval. Folksonomy terms c an be linked to structured controlled vocabularies. 1.3.2.8 Thesauri A thesaurus is type of controlled vocabulary used for content retrieval. It combines characteristics of synonym lists and taxonomies. A thesaurus provides information about each term and its relationship to other terms. Relationships are either hierarchical (parent/child or broad/narrower), associative (‘see also’) or equivalent (synonym or used/used from). Synonyms must be acceptably equivalent in all context scenarios. A thesaurus may also include definitions, citations, etc. Thesauri can be used to organize unstructured content, uncover relationships between content from different media, improve website navigation, and optimize search. When a user inputs a term, a system can use a non - exposed thesaurus (one not directly available to the user) to automatically direct the search to a similar term. Alternatively, the system can suggest related terms with which a user could continue the search. Standards that provide guidance on creating thesauri include ISO 25964 and ANSI/NISO Z39.19. 10.2.2.1.5 Ontologies. 1.3.2.9 Ontology An ontology is a type of taxonomy that represents a set of concepts and their relationships within a domain. Ontologies provide the primary knowledge representation in the Semantic Web, and are used in the exchange of information between Semantic Web applications. 49 Ontology languages such as Resource Description Framework Schema (RDFS) are used to develop ontologies by encoding the knowledge about specific domains. They may include reasoning rules to support processing of that knowledge. OWL (Web Ontology Language), an extension to RDFS, is a formal syntax for defining ontologies. 49 Semantic Web , also known as Linked Data Web or Web 3.0, an enhancement of the current Web where meaning ( i.e., semantics) is machine -process -able. Having a machine (computer) understand more makes it easier to find, share , and combine data / information more easily . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "298 • DMBOK2 Ontologies describe classes (concepts), individuals (instances), attributes, relations, and events. An ontology can be a collection of taxonomies and thesauri of common vocabulary for knowledge representation and exchange of information. Ontologies often r elate to a taxonomic hierarchy of classes and definitions with the subsumption relation, such as decomposing intelligent behavior into many simpler behavior modules and then layers. There are two key differences between a taxonomy (like a data model) and an ontology: • A taxonomy provides data content classifications for a given concept area. A data model specifically calls out the entity to which an attribute belongs and the valid values for that attribute. In an ontology, though, entity, attribute, and content concepts can be completely mixed. Differences are identified through Metadata or other relationships. • In a taxonomy or data model, what is defined is what is known – and nothing else. This is referred to as a closed -world assumption. In an ontology, possible relationships are inferred based on the nature of existing relationships, so something that is not explicitly declared can be true. This is referred to as the ope n-world assumption. While taxonomy management evolved under the Library Sciences, today the art and science of taxonomy and ontology management fall under the semantics management space. (S ee Chapter 10. ) Because the process of modeling ontologies is somewhat subjective, it is important to avoid common pitfalls that cause ambiguity and confusion: • Failure to distinguish between an instance- of relationship and a subclass-of relationship • Modeling events as relations • Lack of clarity and uniqueness of terms • Modeling roles as classes • Failure to reuse • Mixing semantics of modeling language and concepts • Use of a web -based, platform -independent tool (e.g., OOPS!) for ontology validation helps with diagnosis and repair of pitfalls 1.3.3 Documents and Records Documents are electronic or paper objects that contain instructions for tasks, requirements for how and when to perform a task or function, and logs of task execution and decisions. Documents can communicate and share information and knowledge. Examples of documents include procedures, protocols, methods, and specifications. Only a subset of documents will be designated as records. Records provide evidence that actions were taken and decisions were made in keeping with procedures; they can serve as evidence of the organization’s business activities and regulatory compliance. People usually create records, but instruments and monitoring equi pment could also provide data to generate records automatically. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 299 1.3.3.1 Document Management Document management encompasses the processes, techniques, and technologies for controlling and organizing documents and records throughout their lifecycle. It includes storage, inventory, and control, for both electronic and paper documents. More than 90% of the documents c reated today are electronic. While paperless documents are becoming more widely used, the world is still full of historical paper documents. In general, document management concerns files, with little attention to file content. The information content within a file may guide how to manage that file, but document management treats the file as a single entity. Both market and regulatory pressures put focus on records retention schedules, location, transport, and destruction. For example, some data about individuals cannot cross international boundaries. Regulations and statutes, such as the U.S. Sarbanes- Oxley Act and E -Discovery Amendments to the Federal Rules of Civil Procedure and Canada’s Bill 198, are now concerns of corporate compliance officers who push for standardization of records management practices within their organizations. Managing the lifecycle of documents and records includes: • Inventory : Identification of existing and newly created documents / records. • Policy : Creation, approval, and enforcement of documents / records policies, including a document / records retention policy. • Classification of documents / records. • Storage : Short - and long -term storage of physical and electronic documents / records. • Retrieval and Circulation : Allowing access to and circulation of documents / records in accordance with policies, security and control standards, and legal requirements. • Preservation and Disposal : Archiving and destroying documents / records according to organizational needs, statutes, and regulations. Data management professionals are stakeholders in decisions about document classification and retention. They must support consistency between the base structured data and specific unstructured data. For example, if finished output reports are deemed appro priate historic documentation, the structured data in an OLTP or warehousing environment may be relieved of storing the report’s base data. Documents are often developed within a hierarchy with some documents more detailed than others are. Figure 72, based on text from ISO 9000 Introduction and Support Package: Guidance on the Documentation Requirements of ISO 9001, Clause 4.2, depicts a documentation -centric paradigm, appropriate for government or the military. ISO 9001 describes the minimal components of a basic quality management system. Commercial entities may have a different document hierarchies or flows to support business practices. 1.3.3.2 Records Management Document management includes records management . Managing records has special requirements.50 Records management includes the full lifecycle: from record creation or receipt through processing, distribution, 50 The ISO 15489 standard defines records management as “The field of management responsible for the efficient and systematic control of the creation, receipt, maintenance, use and disposition of records, including the processes for capturing and maintaining evidence of and information about business activities and transactions in the form of records.” http://bit.ly/2sVG8EW . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "300 • DMBOK2 organization, and retrieval, to disposition. Records can be physical (e.g., documents, memos, contracts, reports or microfiche); electronic (e.g., email content, attachments, and instant messaging); content on a website; documents on all types of media and hardware; and data captured in databases of all kinds. Hybrid records, such as aperture cards (paper record with a microfiche window imbedded with details or supporting material), combine formats. A Vital Record is type a record required to resume an organization’s operations the event of a disaster . Trustworthy records are important not only for record keeping but also for regulatory compliance. Having signatures on the record contributes to a record’s integrity. Other integrity actions include verification of the event (i.e., witnessing in real time) and double -checking the information after the event. Well -prepared records have characteristics such as: • Content : Content must be accurate, complete , and truthful. • Context : Descriptive information (Metadata) about the record’s creator, date of creation, or relationship to other records should be collected, structured and maintained with the record at the time of record creation. • Timeliness : A record should be created promptly after the event, action or decision occurs. • Permanency : Once they are designated as records, records cannot be changed for the legal length of their existence. • Structure : The appearance and arrangement of a record’s content should be clear. They should be recorded on the correct forms or templates. Content should be legible, terminology should be used consistently. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 301 Figure 72 Document Hierarchy based on ISO 9001- 4.2 Many records exist in both electronic and paper formats. Records Management requires the organization to know which copy (electronic or paper) is the official ‘copy of record’ to meet record keeping obligations. Once the copy of record is determined, the other copy can be safely destroyed. 1.3.3.3 Digital Asset Management Digital Asset Management (DAM) is process similar to document management that focuses on the storage, tracking and use of rich media documents like video, logos, photographs, etc. 1.3.4 Data Map A Data Map is an inventory of all ESI data sources, applications, and IT environments that includes the owners of the applications, custodians, relevant geographical locations, and data types. Policies and Standards Processes and Procedures Government Laws and Regulations What is the law? Who enforces the legislation? When? How? Where? What regulatory rules and details are necessary to implement laws? What is to happen? Where? Why important? Who is responsible? What needs to be done? How to do it Work Instructions What to do Who will do the task? Specific detailed steps for one task Linked to a procedure Other Documents that Express Evidence of Compliance (Records) May include completed files, forms, tags, labels Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "302 • DMBOK2 1.3.5 E-discovery Discovery is a legal term that refers to pre -trial phase of a lawsuit where both parties request information from each other to find facts for the case and to see how strong the arguments are on either side. The US Federal Rules of Civil Procedure (FRCP) have governed the discovery of evidence in lawsuits and other civil cases since 1938. For decades, paper -based discovery rules were applied to e- discovery. In 2006, amendments to the FRCP accommodated the discovery practice and requirements of ESI in the litigation process. Other global regulations have requirements specific to the ability of an organization to produce electronic evidence. Examples include the UK Bribery Act, Dodd -Frank Act, Foreign Account Tax Compliance Act (FATCA), Foreign Corrupt Pract ices Act, EU Data Protection Regulations and Rules, global anti- trust regulations, sector - specific regulations, and local court procedural rules. Electronic documents usually have Metadata (which may not be available for paper documents) that plays an important part in evidence. Legal requirements come from the key legal processes such as e-discovery, as well as data and records retention practices, the legal hold notification (LHN) process, and legally defensible disposition practices. LHN includes identifying information that may be requested in a legal proceeding, locking that data or document down to prevent editing or deletion, and then notifying all parties in an organization that the data or document in question is subject to a legal hold. Figure 73 depicts a high -level Electronic Discovery Reference Model developed by EDRM , a standards and guidelines organization for e- discovery. This framework provides an approach to e -discovery that is handy for people involved in identifying how and where the relevant internal data is stored, what retention policies apply, what data is n ot accessible, and what tools are available to assist in the identification process. Figure 73 Electronic Discovery Reference Model51 The EDRM model assumes that data or information governance is in place. The model includes eight e - discovery phases that can be iterative. As e -discovery progresses, the volume of discoverable data and information is greatly reduced as their relevance is greatly increased. The first phase, Identification, has two sub -phases: Early Case Assessment and Early Data Assessment (not depicted in the diagram). In Early Case Assessment, the legal case itself is assessed for pertinent information, 51 EDRM (edrm.net). Content posted at EDRM.net is licensed under a Creative Commons Attribution 3.0 Unported License. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 303 called descriptive information or Metadata (e.g., keywords, date ranges, etc.). In Early Data Assessment, the types and location of data relevant to the case is assessed. Data assessment should identify policies related to the retention or destruction of relevant data so that ESI can be preserved. Interviews should be held with records management personnel, data custodians or data owners, and information technology personnel to obtain pertinent information. In addition, the involved personnel need to unders tand the case background, legal hold, and their role in the litigation. The next phases in the model are the Preservation and Collection . Preservation ensures that the data that has been identified as potentially relevant is placed in a legal hold so it is not destroyed. Collection includes the acquisition and transfer of identified data from the company to their legal counsel in a legally defensible manner. During the Processing phase data is de -duplicated, searched, and analyzed to determine which data items will move forward to the Review phase. In the Review phase, documents are identified to be presented in response to the request. Review also identifies privileged documents that will be withheld. Much of the selection depends on Metadata associated with the documents. Processing takes place after the Review phase because it addresses content analysis to understand the circumstances, facts and potential ev idence in litigation or investigation and to enhance the search and review processes. Processing and Review depend on analysis, but Analysis is called out as a separate phase with a focus on content. The goal of content analysis is to understand the circumstances, facts, and potential evidence in litigation or investigation, in order to formulate a strategy in response to the legal situation. In the Production phase, data and information are turned over to opposing counsel, based on agreed -to specifications. Original sources of information may be files, spreadsheets, email, databases, drawings, photographs, data from proprietary applications, w ebsite data, voicemail, and much more. The ESI can be collected, processed and output to a variety of formats. Native production retains the original format of the files. Near -native production alters the original format through extraction and conversion. ESI can be produced in an image, or near paper, format. Fielded data is Metadata and other information extracted from native files when ESI is processed and produced in a text -delimited file or XML load file. The lineage of the materials provided during the Production phase is important, because no one wants to be accused of altering data or information provided. Displaying the ESI at depositions, hearings, and trials is part of the Presentation phase. The ESI exhibits can be presented in paper, near paper, near -native and native formats to support or refute elements of the case. They may be used to elicit further information, validate existing facts or positions, or persuade an audience. 1.3.6 Information Architecture Information Architecture is the process of creating structure for a body of information or content. It includes the following components: • Controlled vocabularies • Taxonomies and ontologies • Navigation maps • Metadata maps Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "304 • DMBOK2 • Search functionality specifications • Use cases • User flows The information architecture and the content strategy together describe the ‘what’ – what content will be managed in a system. The design phases describe ‘how’ the content management strategy will be implemented. For a document or content management system, the information architecture identifies the links and relationships between documents and content, specifies document requirements and attributes, and defines the structure of content in a document or content management system. Information architecture is central to developing effective websites. A storyboard provides a blueprint for a web project. It serves as an outline of the design approach, defines the elements that need to go on each web page, and shows the navigation and information flow of how the pages are to work together. This enables development of the navigational models, menus, and other components necessary for the management and use of the site. 1.3.7 Search Engine A search engine is software that searches for information based on terms and retrieves websites that have those terms within their content. One example is Google. Search functionality requires several components: search engine software proper, spider software that roams the Web and stores the Uniform Resource Locators (URLs) of the content it finds, indexing of the encountered keywords and text, and rules for ranking. 1.3.8 Semantic Model Semantic modeling is a type of knowledge modeling that describes a network of concepts (ideas or topics of concern) and their relationships. Incorporated into information systems, semantic models enable users to ask questions of the information in a non -technical way. For example, a semantic model can map database tables and views to concepts that are meaningful to business users. Semantic models contain semantic objects and bindings. Semantic objects are things represented in the model. They can have attributes with cardinality and domains, and identifiers. Their structures can be simple, composite, compound, hybrid, association, parent / subtype, or archetype / version. Bindings represent associations or association classes in UML. These models help to identify patterns and trends and to discover relationships between pieces of information that might otherwise appear disparate. In doing so, they help enable integration of data across different knowledge domains or subject areas. Ontologies and controlled vocabularies are critical to semantic modeling. Data integration uses ontologies in several different ways. A single ontology could be the reference model. If there are multiple data sources, then each individual data source is modeled using an ontology and later mapped to the other ontologies. The hybrid approach uses multiple ontologies that integrate with a common overall vocabulary. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 305 1.3.9 Semantic Search Semantic searching focuses on meaning and context rather than predetermined keywords. A semantic search engine can use artificial intelligence to identify query matches based on words and their context. Such a search engine can analyze by location, intent, word variations, synonyms, and concept matching. Requirements for semantic search involve figuring out what users want which means thinking like the users. If users want search engines to work like natural language, most likely they will want web content to behave this way. The challenge for marketing or ganizations is to incorporated associations and keywords that are relevant to their users as well as their brands. Web content optimized for semantics incorporates natural key words, rather than depending on rigid keyword insertion. Types of semantic keywords include: Core keywords that contain variations; thematic keywords for conceptually related terms; and stem keyw ords that anticipate what people might ask. Content can be further optimized through content relevancy and ‘shareworthiness’, and sharing content through social media integration. Users of Business Intelligence (BI) and analytics tools often have semantic search requirements. The BI tools need to be flexible so that business users can find the information they need for analysis, reports and dashboards. Users of Big Data have a simil ar need to find common meaning in data from disparate formats. 1.3.10 Unstructured Data It is estimated that as much as 80% of all stored data is maintained outside of relational databases. This unstructured data does not have a data model that enables users to understand its content or how it is organized; it is not tagged or structured into rows and columns. The term unstructured is somewhat misleading, as there often is structure in documents, graphics, and other formats, for instance, chapters or headers. Some refer to data stored outside relational databases as non -tabular or semi -structured data. No single term adequately describes the vast volume and diverse format of electronic information that is created and stored in today’s world. Unstructured data is found in various electronic formats: word processing documents, electronic mail, social media, chats, flat files, spreadsheets, XML files, transactional messages, reports, graphics, digital images, microfiche, video recordings, and aud io recordings. An enormous amount of unstructured data also exists in paper files. The fundamental principles of data management apply to both structured and unstructured data. Unstructured data is a valuable corporate asset. Storage, integrity, security, content quality, access, and effective use guide the management of unstructured dat a. Unstructured data requires Data Governance , architecture, security Metadata, and Data Quality . Unstructured and semi -structured data have become more important to data warehousing and Business Intelligence. Data warehouses and their data models may include structured indexes to help users find and analyze unstructured data. Some databases include the capacity to handle URLs to unst ructured data that perform as hyperlinks when retrieved from the database table. Unstructured data in data lakes is described in Chapter 14. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "306 • DMBOK2 1.3.11 Workflow Content development should be managed through a workflow that ensures content is created on schedule and receives proper approvals. Workflow components can include the creation, processing, routing, rules, administration, security, electronic signature, deadline, escalation (if problems occur), reporting and de livery. It should be automated through the use of a content management system (CMS) or a standalone system, rather than manual processes. A CMS has the added benefit of providing version control. When content is checked into a CMS, it will be timestamped, assigned a version number, and tagged with the name of the person who made the updates. The workflow needs to be repeatable, ideally containing process steps common across a variety of content. A set of workflows and templates may be necessary if there are significant differences between content types. Alignment of the stakeholders and distribution points (including technology) is important. Deadlines need to be refined to improve workflows, otherwise you can quickly find your work flows are out of date or there is confusion over which stakeholder is responsible for which piece. 2. Activities 2.1 Plan for Lifecycle Management The practice of document management involves planning for a document’s lifecycle , from its creation or receipt, through its distribution, storage, retrieval, archiving and potential destruction. Planning includes developing classification / indexing systems and taxonomies that enable storage and retrieval of documents. Importantly, lifecycle planning requires creating policy specifically for records. First, identify the organizational unit responsible for managing the documents and records. That unit coordinates the access and distribution internally and externally, and integrates best practices and process flows with other departments throughout the o rganization. It also develops an overall document management plan that includes a business continuity plan for vital documents and records. The unit ensures it follows retention policies aligned with company standards and government regulations. It ensures that records required for long -term needs are properly archived and that others are properly destroyed at the end of their lifecycle in accordance with organizational requirements, statutes, and regulations. 2.1.1 Plan for Records Management Records management starts with a clear definition of what constitutes a record. The team that defines records for a functional area should include SMEs from that area along with people who understand the systems that enable management of the records. Managing electronic records requires decisions about where to store current, active records and how to archive older records. Despite the widespread use of electronic media, paper records are not going away in the near Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 307 term. A records management approach should account for paper records and unstructured data as well as structured electronic records. 2.1.2 Develop a Content Strategy Planning for content management should directly support the organization’s approach to providing relevant and useful content in an efficient and comprehensive manner. A plan should account for content drivers (the reasons content is needed), content creation and delivery. Content requirements should drive technology decisions, such as the selection of a content management system. A content strategy should start with an inventory of current state and a gap assessment. The strategy defines how content will be prioritized, organized, and accessed. Assessment often reveals ways to streamline production, workflow, and approval processes for content creation. A unified content strategy emphasizes designing modular content components for reusability rather than creating standalone content. Enabling people to find different types of content through Metadata categorization and search engine optimization (SEO) is critical to any content strategy. Provide recommendations on content creation, publication, and governance. Policies, standards, and guidelines that apply to content and its lifecycle are useful to sustain and evolve an organization’s content strategy. 2.1.3 Create Content Handling Policies Policies codify requirements by describing principles, direction, and guidelines for action. They help employees understand and comply with the requirements for document and records management. Most document management programs have policies related to: • Scope and compliance with audits • Identification and protection of vital records • Purpose and schedule for retaining records (a.k.a retention schedule) • How to respond to information hold orders (special protection orders); these are requirements for retaining information for a lawsuit, even if retention schedules have expired • Requirements for onsite and offsite storage of records • Use and maintenance of hard drive and shared network drives • Email management, addressed from content management perspective • Proper destruction methods for records (e.g., with pre -approved vendors and receipt of destruction certificates) 2.1.3.1 Social Media Policies In addition to these standard topics, many organizations are developing policies to respond to new media . For example, an organization has to define if social media content posted on Facebook, Twitter, LinkedIn, chat Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "308 • DMBOK2 rooms, blogs, wikis, or online forums constitutes a record, especially if employees post in the course of conducting business using organizational accounts. 2.1.3.2 Device Access Policies Since the pendulum is swinging towards user driven IT with BYOD (bring -your -own -devices ), BYOA (bring - your -own -apps), and WYOD (wear -your -own -devices ), the content and records management functions need to work with these scenarios in order to ensure compliance, security and privacy. Policies should distinguish between informal content (e.g., Dropbox or Evernote) and formal content (e.g., contracts and agreements), in order to put controls on formal content. Policies can also provide guidance on informal content. 2.1.3.3 Handling Sensitive Data Organizations are legally required to protect privacy by identifying and protecting sensitive data. Data Security and/or Data Governance usually establish the confidentiality schemes and identify what assets are confidential or restricted. The people who produce or assemble content must apply these classifications. Documents, web pages, and other content components must be are marked as sensitive based on policies and legal requirements. Once marked, confidential data is either masked or deleted where appropriate. (See Chapter 7.) 2.1.3.4 Responding to Litigation Organizations should prepare for the possibility of litigation requests through proactive e -discovery. (Hope for the best; prepare for the worst.) They should create and manage an inventory of their data sources and the risks associated with each. By identifying data sources that may have relevant information, they can respond in a timely manner to a litigation hold notice and prevent data loss. The appropriate technologies should be deployed to automate e -discovery processes. 2.1.4 Define Content Information Architecture Many information systems such as the semantic web, search engines, web social mining, records compliance and risk management, geographic information systems (GIS), and Business Intelligence applications contain structured and unstructured data, documents, text, images, etc. Users have to submit their needs in a form understandable by the system retrieval mechanism to obtain information from these systems. Likewise, the inventory of documents and structured and unstructured data needs to be described / indexed in a format that allows the retrieval mechanism to identify the relevant matched data and information quickly. User queries may be imperfect in that they retrieve both relevant and irrelevant information, or do not retrieve all the relevant information. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 309 Searches use either content- based indexing or Metadata. Indexing designs look at decision options for key aspects or attributes of indexes based on needs and preferences of users. They also look at the vocabulary management and the syntax for combining ind ividual terms into headings or search statements. Data management professionals may get involved with controlled vocabularies and ter ms in handling Reference Data (see Section 1.3.2.1 ) and Metadata for unstructured data and content. (See Chapter 12. ) They should ensure that there is coordination with efforts to build controlled vocabularies, indexes, classification schemes for information retrieval, and data modeling and Metadata efforts executed as part of data management projects and applications. 2.2 Manage the Lifecycle 2.2.1 Capture Records and Content Capturing content is the first step to managing it. Electronic content is often already in a format to be stored in electronic repositories. To reduce the risk of losing or damaging records, paper content needs to be scanned and then uploaded to the corporate system, indexed, and stored in the repository. Use electronic signatures if possible. When content is captured, it should be tagged (indexed) with appropriate Metadata, such as (at minimum) a document or image identifier, the data and time of capture, the title and author(s). Metadata is necessary for retrieval of the information, as well a s for understanding the context of the content. Automated workflows and recognition technologies can help with the capture and ingestion process, providing audit trails. Some social media platforms offer the capability of capturing records. Saving the social media content in a repository makes it available for review, meta tagging and classification, and management as records. Web crawlers can capture versions of websites. Web capture tools, application -programming interfaces (APIs), and RSS feeds can capture content or social media export tools. Social media records can also be captured manually or via predefined, automated workflows. 2.2.2 Manage Versioning and Control ANSI Standard 859 has three levels of control of data, based on the criticality of the data and the perceived harm that would occur if data were corrupted or otherwise unavailable: formal, revision, and custody: • Formal control requires formal change initiation, thorough evaluation for impact, decision by a change authority, and full status accounting of implementation and validation to stakeholders • Revision control is less formal, notifying stakeholders and incrementing versions when a change is required • Custody control is the least formal, merely requiring safe storage and a means of retrieval Table 15 shows a sample list of data assets and possible control levels. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "310 • DMBOK2 ANSI 859 recommends taking into account the following criteria when determining which control level applies to a data asset: • Cost of providing and updating the asset • Project impact, if changes will have significant cost or schedule consequences • Other consequences of change to the enterprise or project • Need to reuse the asset or earlier versions of the asset • Maintenance of a history of change (when required by the enterprise or the project) Table 15 Levels of Control for Documents per ANSI- 859 Data Asset Formal Revision Custody Action item lists x Agendas x Audit findings x x Budgets x DD 250s x Final Proposal x Financial data and reports x x x Human Resources data x Meeting minutes x Meeting notices and attendance lists x x Project plans (including data management and configuration management plans) x Proposal (in process) x Schedules x Statements of Work x Trade studies x Training material x x Working papers x 2.2.3 Backup and Recovery The document / record management system needs to be included in the organization’s overall corporate backup and recovery activities , including business continuity and disaster recovery planning. A vital records program provides the organization with access to the records necessary to conduct its business during a disaster and to resume normal business afterward. Vital records must be identified, and plans for their protection and recovery must be developed and maintained. A records manager should be involved in risk mitigation and business continuity planning, to ensure these activities account for the security for vital records. Disasters could include power outages, human error, network and hardware failure, software malfunction, malicious attack, as well as natural disasters. A Business Continuity Plan (or Disaster Recovery Plan) contains written policies, procedures, and information designed to mitigate the impact of threats to an organization’s data, including documents, and to recover them as quickly as possible, with minimum disruption, in the event of a disaster. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 311 2.2.4 Manage Retention and Disposal Effective document / records management requires clear policies and procedures, especially regarding retention and disposal of records. A retention and disposition policy will define the timeframes during which documents for operational, legal, fiscal, or historical value must be maintained. It defines when inactive documents can be transferred to a secondary storage facility, such as off -site storage. The policy specifies the processes for compliance and the methods and schedules for the disposition of documents. Legal and regulatory requirements must be considered when setting up retention schedules. Records managers or information asset owners provide oversight to ensure that teams account for privacy and data protection requirements and take actions to prevent identit y theft. Document retention presents software considerations. Access to electronic records may require specific versions of software and operating systems. Technological changes as simple as the installation of new software can make documents unreadable or inaccessible. Non -value -added information should be removed from the organization’s holdings and disposed of to avoid wasting physical and electronic space, as well as the cost associated with its maintenance. There is also risk associated with retaining records past their legally required timeframes. This information remains discoverable for litigation. Still, many organizations do not prioritize the removal of non -value -added information because: • Policies are not adequate • One person’s non- valued -added information is another’s valued information • Inability to foresee future possible needs for current non -value -added physi cal and / or electronic records • There is n o buy -in for Records Management • Inability to decide which records to delete • Perceived cost of making a decision and removing physical and electronic records • Electronic space is cheap. Buying more space when required is easier than archiving and removal processes 2.2.5 Audit Documents / Records Document / records management requires periodic auditing to ensure that the right information is getting to the right people at the right time for decision -making or performing operational activities. Table 16 contains examples of audit measures. An audit usually involves the following steps: • Defining organizational drivers and identifying the stakeholders that comprise the ‘why’ of document / records management • Gathering data on the process (the ‘how’), once it is determined what to examine / measure and what tools to use (such as standards, benchmarks, interview surveys) • Reporting the outcomes • Developing an action plan of next steps and timeframes Table 16 Sample Audit Measures Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "312 • DMBOK2 Document / Records Management Component Sample Audit Measure Inventory Each location in the inventory is uniquely identified. Storage Storage areas for physical documents / records have adequate space to accommodate growth. Reliability and Accuracy Spot checks are executed to confirm that the documents / records are an adequate reflection of what has been created or received. Classification and Indexing Schemes Metadata and document file plans are well described. Access and Retrieval End users find and retrieve critical information easily. Retention Processes The retention schedule is structured in a logical way either by department, functional or major organizational functions. Disposition Methods Documents / records are disposed of as recommended. Security and Confidentiality Breaches of document / record confidentiality and loss of documents / records are recorded as security incidents and managed appropriately. Organizational understanding of documents / records management Appropriate training is provided to stakeholders and staff as to the roles and responsibilities related to document / records management. 2.3 Publish and Deliver Content 2.3.1 Provide Access, Search, and Retrieval Once the content has been described by Metadata / key word tagging and classified within the appropriate information content architecture, it is available for retrieval and use. Portal technology that maintains profiles of users can help them find unstructured data. Search engines can return content based on keywords. Some organizations have professionals retrieve information through internal search tools. 2.3.2 Deliver Through Acceptable Channels There is a shift in delivery expectations as the content users now want to consume or use content on a device of their choosing. Many organizations are still creating content in something like MS Word and moving it into HTML, or delivering content for a given platform, a certain screen resolution, or a given size on the screen. If another delivery channel is desired, this content has to be prepared for that channel (e.g., print). There is the potential that any changed content may need to be brought back into the original format. When structured data from databases is formatted into HTML, it becomes difficult to recover the original structured data, as separating the data from the formatting is not always straightforward. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 313 3. Tools 3.1 Enterprise Content Management Systems An ECM may consist of a platform of core components or a set of applications that can be integrated wholly or used separately. These components, discussed below, can be in- house or outside the enterprise in the cloud. Reports can be delivered through a number of tools, including printers, email, websites, portals, and messaging, as well as through a document management system interface. Depending on the tool, users can search by drill-down, view, download / check -in and out, and print reports on demand. The ability to add, change, or delete reports organized in folders facilitates report management. Report retention can be set for automatic purge or archival to other media, such as disk, CD -ROM, COLD (Computer Output to Laser Disk), etc. Reports can also be retained in cloud storage. As noted, retaining content in unreadable, outdated formats presents risk to the organization. (See Chapters 6 and 8, and Section 3.1.8 .) The boundaries between document management and content management are blurring as business processes and roles intertwine, and vendors try to widen the markets for their products. 3.1.1 Document Management A document management system is an application used to track and store electronic documents and electronic images of paper documents. Document library systems, electronic mail systems, and image management systems are specialized document management systems. Document management systems commonly provide storage, versioning, security, Metadata Management, content indexing, and retrieval capabilities. Extended capabilities of some systems can include Metadata views of documents. Documents are created within a document management system, or captured via scanners or OCR software. These electronic documents must be indexed via keywords or text during the capture process so that the documents can be found. Metadata, such as the creator’s name, and the dates the document was created, revised, stored, is typically stored for each document. Documents can be categorized for retrieval using a unique document identifier or by specifying partial search terms involving the document identifier and / or parts of the expected Metadata. Metadata can be extracted from the document automatically o r added by the user. Bibliographic records of documents are descriptive structured data, typically in Machine -Readable Cataloging (MARC) standard format that are stored in library databases locally and made available through shared catalogs worldwide, as privacy and permissions allow. Some systems have advanced capabilities , such as compound document support and content replication. Word processing software creates the compound document and integrates non- text elements such as spreadsheets, videos, audio and other multimedia types. In addition, a compound document can be an o rganized collection of user interfaces to form a single, integrated view. Document storage includes functions to manage documents. A document repository enables check -in and check- out features, versioning, collaboration, comparison, archiving, status state(s), migration from one Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "314 • DMBOK2 storage media to another, and disposition. It may offer some access to and version management of documents external to its own repository (e.g., in a file share or cloud environment). Some document management systems have a module that may support different types of workflows, such as: • Manual workflows that indicate where the user sends the document • Rules -based workflow, where rules are created that dictate the flow of the document within an organization • Dynamic rules that allow for different workflows based on content Document management systems have a rights management module where the administrator grants access based on document type and user credentials. Organizations may determine that certain types of documents require additional security or control procedures. Se curity restrictions, including privacy and confidentiality restrictions, apply during the document’s creation and management, as well as during delivery. An electronic signature ensures the identity of the document sender and the authenticity of the message, among other things. Some systems focus more on control and security of data and information, than on its access, use, or retrieval, particularly in the intelligence, military, and scientific research sectors. Highly competitive or highly regulated industries, such as the pharmaceutical and financial sectors, also implement extensive security and control measures. 3.1.1.1 Digital Asset Management Since the functionality needed is similar, many document management systems include digital asset management . This is the management of digital assets such as audio, video, music, and digital photographs. Tasks involve cataloging, storage, and retrieval of digital assets. 3.1.1.2 Image Processing An image processing system captures, transforms, and manages images of paper and electronic documents. The capturing capability uses technologies such as scanning, optical and intelligence character recognition, or form processing. Users can index or enter Metadata into the system and save the digitized image in a repository. Recognition technologies include optical character recognition (OCR), which is the mechanical or electronic conversion of scanned (digitized) printed or handwritten text into a form that can be recognized by computer software. Intelligent character recogni tion (ICR) is a more advanced OCR system that can deal with printed and cursive handwriting. Both are important for converting large amounts of forms or unstructured data to a CMS format . Forms processing is the capture of printed forms via scanning or recognition technologies. Forms submitted through a website can be captured as long as the system recognizes the layout, structure, logic, and contents. Besides document images, other digitized images such as digital photographs, infographics, spatial or non - spatial data images may be stored in repositories. Some ECM systems are able to ingest diverse types of digitized Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 315 documents and images such as COLD information, .wav and .wmv (audio) files, XML and healthcare HL7 messages into an integrated repository. Images are often created by using computer software or cameras rather than on paper. Binary file formats include vector and raster (bitmap) types as well as MS Word .DOC format. Vector images use mathematical formulas rather than individual colored blocks, and are very good for creating graphics that frequently require resizing. File formats include .EPS, .AI or .PDF. Raster images use a fixed number of colored pixels to form a complete image, and cannot be resized easily without compromising their resolution. Examples of raster files include .JPEG, .GIF, .PNG, or .TIFF. 3.1.1.3 Records Management System A records management system may offer capabilities such as automation of retention and disposition, e - discovery support, and long -term archiving to comply with legal and regulatory requirements. It should support a vital records program to retain critical business records. This type of system may be integrated with a documents management system. 3.1.2 Content Management System A content management system is used to collect, organize, index, and retrieve content, storing it either as components or whole documents, while maintaining links between components. A CMS may also provide controls for revising content within documents. While a document management system may provide content management functionality over the documents under its control, a content management system is essentially independent of where and how the documents are stored. Content management systems manage content through its lifecycle. For example, a web content management system controls website content through authoring, collaboration, and management tools based on core repository. It may contain user -friendly content cre ation, workflow and change management, and deployment functions to handle intranet, Internet, and extranet applications. Delivery functions may include responsive design and adaptive capabilities to support a range of client devices. Additional components may include search, document composition, e -signature, content analytics, and mobile applications. 3.1.3 Content and Document Workflow Workflow tools support business processes, route content and documents, assign work tasks, track status, and create audit trails. A workflow provides for review and approval of content before it is published. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "316 • DMBOK2 3.2 Collaboration Tools Team collaboration tools enable the collection, storage, workflow, and management of documents pertinent to team activities. Social networking enables individual s and teams to share documents and content inside the team and to reach out to an external group for input using blogs, wikis, RSS, and tagging. 3.3 Controlled Vocabulary and Metadata Tools Tools that help develop or manage controlled vocabularies and Metadata range from office productivity software, Metadata repositories, and BI tools, to document and content management systems. For example: • Data models used as guides to the data in an organization • Document management systems and office productivity software • Metadata repositories, glossaries, or directories • Taxonomies and cross- reference schemes between taxonomies • Indexes to collections (e.g., particular product, market , or installation), filesystems, opinion polls, archives, locations, or offsite holdings • Search engines • BI tools that incorporate unstructured data • Enterprise and departmental thesauri • Published reports libraries, contents and bibliographies, and catalogs 3.4 Standard Markup and Exchange Formats Computer applications cannot process unstructured data / content directly. Standard markup and exchange formats facilitate the sharing of data across information systems and the Internet. 3.4.1 XML Extensible Markup Language (XML) provides a language for representing both structured and unstructured data and information. XML uses Metadata to describe the content, structure, and business rules of any document or database. XML requires translating the structure of the data into a document structure for data exchange. XML tags data elements to identify the meaning of the data. Simple nesting and references provide the relationships between data elements. XML namespaces provide a method to avoid a name conflict when two different documents use the same element names. Older methods of markup include HTML and SGML, to name a few. The need for XML-capable content management has grown for several reasons: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 317 • XML provides the capability of integrating structured data into relational databases with unstructured data. Unstructured data can be stored in a relational DBMS BLOB (binary large object) or in XML files. • XML can integrate structured data with unstructured data in documents, reports, email, images, graphics, audio, and video files. Data modeling should take into account the generation of unstructured reports from structured data, and include them in creating error- correction workflows, backup, recovery, and archiving. • XML also can build enterprise or corporate portals, (Business- to-Business [B2B], Business- to- Customer [B2C]), which provide users with a single access point to a variety of content. • XML provides identification and labeling of unstructured data / content so that computer applications can understand and process them. In this way, structured data appends to unstructured content. An Extensible Markup Interface (XMI) specification consists of rules for generating the XML document containing the actual Metadata and thus is a ‘structure’ for XML. 3.4.2 JSON JSON (JavaScript Object Notation) is an open, lightweight standard format for data interchange. Its text format is language -independent and easy to parse, but uses conventions from the C -family of languages. JSON has two structures: a collection of unordered n ame / value pairs known as objects and an ordered list of values realized as an array. It is emerging as the preferred format in web -centric, NoSQL databases. An alternative to XML, JSON is used to transmit data between a server and web application. JSON is a similar but more compact way of representing, transmitting, and interpreting data than XML. Either XML or JSON content can be returned when using REST tech nology. 3.4.3 RDF and Related W3C Specifications Resource Description Framework (RDF) , a common framework used to describe information about any Web resource, is a standard model for data interchange on the Web. The RDF resources are saved in a triplestore, which is a database used to store and retrieve semantic queries using SPARQL. RDF makes statements about a resource in the form of subject (resource)- predicate (property name) -object (property value) expressions or triples. Usually the subject -predicate -object is each described by a URI (Uniform Resource Identifier), but the subject an d object could be blank nodes and the object could be a literal (null values and null strings are not supported). A URI names the relationship between resources as well as two ends of the link or triple. The most common form of URI is a URL (uniform resour ce locator). This allows structured and semi -structured data to be shared across applications. The Semantic Web needs access to both data and relationships between data sets. The collection of interrelated data sets is also known as Linked Data. URIs provide a generic way to identify any entity that exists. HTML provides a means to structure and link documents on the Web. RDF provides a generic, graph -based data model to link data that describes things. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "318 • DMBOK2 RDF uses XML as its encoding syntax. It views Metadata as data (e.g., author, date of creation, etc.). The described resources of RDF allow for the association of semantic meanings to resources. RDFS (RDF Schema) provides a data modeling vocabulary for RDF data and is an extension of the basic RDF vocabulary. SKOS (Simple Knowledge Organization System) is a RDF vocabulary (i.e., an application of the RDF data model to capture data depicted as a hierarchy of concepts). Any type of classification, taxonomy, or thesaurus can be represented in SKOS. OWL (W3C Web Ontology Language) is a vocabulary extension of RDF. It is a semantic markup language for publishing and sharing OWL documents (ontologies) on the Web. It is used when information contained in documents needs to be processed by applications rathe r than humans. Both RDF and OWL are Semantic Web standards that provide a framework for sharing and reuse of data, as well as enabling data integration and interoperability, on the Web. RDF can help with the ‘variety’ characteristic of Big Data. If the data is accessible using the RDF triples model, data from different sources can be mixed and the SPARQL query language used to find connections and patterns without predefining a schema. As W3C describes it, “RDF has features that facilitate data merging even if the underlying schemas differ, and it specifically supports the evolution of schemas over time without requiring all the data consumers to be changed.” 52 It can integrate disparate data from many sources and formats and then either reduce or replace the data sets (known as data fusion) through semantic alignment. (See Chapter 14.) 3.4.4 Schema.org Labeling content with semantic markup (e.g., as defined by the open source Schema.org) makes it easier for semantic search engines to index content and for web crawlers to match content with a search query. Schema.org provides a collection of shared vocabularies or schemas for on -page markup so that the major search engines can understand them. It focuses on the meaning of the words on web pages as well as terms and keywords. Snippets are the text that appears under every search result. Rich snippets are the detailed information on specific searches (e.g., gold star ratings under the link). To create rich snippets, the content on the web pages needs to be formatted properly with structured data like Microdata (a set of tags introduced with HTML5) and shared vocabularies from Schema.org. The Schema.org vocabulary collection can also be used for structured data interoperability (e.g., with JSON). 3.5 E-discovery Technology E-discovery often involves review of large volumes of documents. E -discovery technologies offer many capabilities and techniques such as early case assessment, collection, identification, preservation, processing, optical character recognition (OCR), culling, similar ity analysis, and email thread analysis. Technology -assisted review (TAR) is a workflow or process where a team can review selected documents and mark them relevant 52 W3C, “ Resource Description Framework (RDF),” http://bit.ly/1k9btZQ . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 319 or not. These decisions are become input for the predictive coding engine that reviews and sorts remaining documents according to relevancy. Support for information governance may be a feature as well. 4. Techniques 4.1 Litigation Response Playbook E-discovery starts at the beginning of a lawsuit. However, an organization can plan for litigation response through the development of a playbook containing objectives, metrics , and responsibilities before a major discovery project begins. The playbook defines the target environment for e- discovery and assesses if gaps exist between current and target environments. It documents business processes for the lifecycle of e- discovery activities and identifies roles and responsibilities of the e -discovery team. A playbook can also enable an organization to identify risks and proactively prevent situations that might result in litigation. To compile a playbook, • Establish an inventory of policies and procedures for specific departments (Legal, Records Management, IT). • Draft policies for topics, such as litigation holds, document retention, archiving, and backups. • Evaluate IT tool capabilities such as e -discovery indexing, search and collection, data segregation and protection tools as well as the unstructured ESI sources / systems. • Identify and analyze pertinent legal issues. • Develop a communication and training plan to train employees on what is expected. • Identify materials that may be prepared in advance for tailoring to a legal case. • Analyze vendor services in case outside services are required. • Develop processes on how to handle a notification and keep the playbook current. 4.2 Litigation Response Data Map E-discovery often has a limited timeframe (e.g., 90 days). Providing attorneys with a data map of the IT and ESI environment available can enable an organization to respond more effectively. A data map is a catalog of information systems. It describes the systems and their uses, the information they contain, retention policies, and other characteri stics. Catalogs often identify systems of record, originating applications, archives, disaster recovery copies, or backups, and media used for each. A data map should be comprehensive, containing all systems. Since email is often an object of scrutiny in litigation, the map should also describe how email is stored, processed, and consumed. Mapping business processes to the list of the systems and documenting user roles and privileges will enable assessment and documentation of information flows. The process of creating the data map will demonstrate the value of creating Metadata as part of the document management process. Metadata is critical for searching. It also gives ESI documents context and enables cases, transcripts, undertakings, etc. to be associated with supporting documents. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "320 • DMBOK2 An e- discovery data map should indicate which records are readily accessible and which are not. There are different e-discovery rules for these two categories. The inaccessible data needs to be identified and the reasons why it is inaccessible need to be documented. To respond appropriately to litigation, an organization should have an inventory of records in offsite storage, including external cloud storage. Often, systems inventories already exist. For example, they may be maintained by Data Architecture, Metadata Management or IT Asset Management. The legal and / or records management functions should determine whether these can be extended for e- discovery p urposes. 5. Implementation Guidelines Implementing ECM is a long -term effort that can be perceived as expensive. As with any enterprise -wide effort, it requires buy- in from a wide range of stakeholders, and funding support from an executive committee for funding. With a large project, there is a risk that it will fall victim to budget cuts, business swings, management changes or inertia. To minimize risks, ensure that the content, not the technology, drives decisions for ECM implementation. Configure the workflow around the organizational needs to show value. 5.1 Readiness Assessment / Risk Assessment The purpose of an ECM readiness assessment is to identify areas where content management improvement is needed and to determine how well adapted the organization is to changing its processes to meet these needs. A Data Management Maturity Assessment model can help in this process. ( See Chapter 15.) Some ECM critical success factors are similar to those in IT projects (e.g., executive support, involvement of users, user training, change management, corporate culture, and communication). Specific ECM critical success factors include content audit and classification for existing content, appropriate information architecture, support of the content lifecycle, definitions of appropriate Metadata tags, and the ability to customize functions in an ECM solution. Because ECM solutions involve technical and pro cess complexity, the organization needs to ensure that it has the appropriate resources to support the process. Risks can arise with ECM implementations due to project size, complexity in integrating with other software applications, process and organizational issues, and the effort required to migrate content. Lack of training for core team members and internal sta ff can lead to uneven use. Other risks include failure to put policies, processes, and procedures in place or lack of communication with stakeholders. 5.1.1 Records Management Maturity ARMA’s Generally Accepted Recordkeeping Principles® (See s ection 1.2) can guide an organization’s assessment of it s policies and practices for Records Management . Along with GARP, ARMA International has an Information Governance Maturity Model that can help assess an organization’s recordkeeping program and Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 321 practices.53 This Maturity Model describes the characteristics of the information governance and recordkeeping environment at five levels of maturity for each of the eight GARP principles: • Level 1 Sub -Standard : Information governance and recordkeeping concerns are not addressed or just minimally • Level 2 In Development : Developing recognition that information governance and recordkeeping can have an impact on the organization • Level 3 Essential: Minimum requirements that must be addressed to meet the legal and regulatory requirements • Level 4 Proactive: A proactive information governance function has been established with a focus on continuous improvement • Level 5 Transformational : Information governance is integrated into the corporate infrastructure and business processes Several standards can be applied for technical assessments of records management systems and applications. For example, • DoD 5015.2 Electronic Records Management Software Applications Design Criteria Standard • ISO 16175, Principles and Functional Requirements for Records in Electronic Office Environments • The Model Requirements for the Management of Electronic Records (MoReq2) • The Records Management Services (RMS) specification from the Object Management Group (OMG) Gaps and risks identified in records management readiness assessments should be analyzed their potential impact on the organization. Businesses are subject to laws that require maintenance and secure destruction of records. If an organization does not inve ntory its records, it is already at risk since it cannot know if records have been stolen or destroyed. An organization can spend a lot of time and money trying to find records if it lacks a functional record retention program. Lack of adherence to legal a nd regulatory requirements can lead to costly fines. Failure to identify and protect vital records can put a company out of business. 5.1.2 E-discovery Assessment A readiness assessment should examine and identify improvement opportunities for the litigation response program. A mature program will specify clear roles and responsibilities, preservation protocols, data collection methodologies, and disclosure processes. Both the program and resulting processes should be documented, defensible, and auditable. The program needs to understand the organization’s information lifecycle and develop an ESI data map for data sources (see Section 2.1.3.4 ). Since data preservation is a critical legal requirement, data retention policies should be proactively reviewed and assessed in anticipation of litigation. There should be a plan to work with IT to quickly implement litigation holds as required. The risks of not having defined a proactive litigation response should be assessed and quantified. Sometimes organizations re spond only if there is anticipated litigation, and then there is a scramble to find relevant documents and information to review. Most likely, this type of organization either over specifies the amount of data to be kept (i.e., everything) or does not have data 53 ARMA International, Information Governance Maturity Model , http://bit.ly/2sPWGOe . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "322 • DMBOK2 deletion policies in place. Not having a retention schedule for data and information can lead to legal liabilities if older unpurged records are required for e -discovery, but not available. 5.2 Organization and Cultural Change People can be a greater challenge than the technology. There may be issues in adapting the management practices in daily activities and getting people to use ECM. In that some cases, ECM can lead to more tasks; for example, scanning paper documents and defining required Metadata. Often organizations manage information, including records, departmentally, creating information silos that hinder the sharing and proper management of data. A holistic enterprise approach to content and records management can eliminate users’ perception that they need to store copies of the content. The ideal solution is a single repository, centrally and securely managed, with clear ly defined policies and processes enforced across the enterprise. Training and communication about the processes, policies, and tools are critical to the success of a records management or ECM program. Privacy, data protection, confidentiality, intellectual property, encryption, ethical use, and identity are the important issues that document and content management professionals must deal with in cooperation with other employees, management, and regulato rs. A centralized organization often deals with processes to improve access to information, control the growth of materials taking up office space, reduce operating costs, minimize litigation risks, safeguard vital information, and support better decision -making. Both content and records management need to be elevated organizationally, and not seen as low- level or low - priority functions. In heavily regulated industries, the Records and Information Management (RIM) function needs to be closely aligned with the corporate legal function along with the e- discovery function. If the organization has objectives to improve operational efficiency by managing information better, then RIM should be aligned with marketing or an operational support group. If the organization sees RIM as part of IT, the RIM function should report directly to the CIO or CDO. Often the RIM function is found in ECM program or Enterprise Information Management (EIM) program. 6. Documents and Content Governance 6.1 Information Governance Frameworks Documents, records, and other unstructured content represent risk to an organization. Managing this risk and getting value from this information both require governance. Drivers include: • Legal and regulatory compliance • Defensible disposition of records • Proactive preparation for e -discovery • Security of sensitive information • Management of risk areas such as email and Big Data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 323 Principles of successful Information Governance function s are emerging. One set of principles is the ARMA GARP® principles (see Section 1.2). Other principles include: • Assign executive sponsorship for accountability • Educate employees on information governance responsibilities • Classify information under the correct record code or taxonomy category • Ensure authenticity and integrity of information • Determine that the official record is electronic unless specified differently • Develop policies for alignment of business systems and third -parties to information governance standards • Store, manage, make accessible, monitor, and audit approved enterprise repositories and systems for records and content • Secure confidential or personally identifiable information • Control unnecessary growth of information • Dispose information when it reaches the end of its lifecycle • Comply with requests for information (e.g., discovery, subpoena, etc.) • Improve continuously Figure 74 Information Governance Reference Model54 54 EDRM (edrm.net). Content posted at EDRM.net is licensed under a Creative Commons Attribution 3.0 Unported License. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "324 • DMBOK2 The Information Governance Reference Model (IGRM) (Figure 74) shows the relationship of Information Governance to other organizational functions. The outer ring includes the stakeholders who put policies, standards, processes, tools and infrastructure in place to manage information. The center shows a lifecycle diagram with each lifecycle component within the color or colors of the stakeholder(s) who executes that component. The IGRM complements ARMA’s GARP ®. Sponsorship by someone close to or within the ‘C’ suite is a critical requirement for the formation and sustainability of the Information Governance function . A cross -functional senior level Information Council or Steering Committee is established that meets on a regular basis. The Council is responsible for an enterprise Information Governance strategy, operating procedures, guidance on technology and standards, communications and training, monitoring, and fund ing. Information Governance policies are written for the stakeholder areas, and then ideally technology is applied for enforcement. 6.2 Proliferation of Information Generally, unstructured data grows much faster than structured data. This adds to the challenge of governance. Unstructured data is not necessarily attached to a business function or department. Its ownership can be difficult to ascertain. It can also be difficult to classify the content of unstructured data, since the business purpose of the content cannot always be inferred from the system. Unmanaged unstructured data, without required Metadata, represents risk. It can be misinterpreted and, if content is not known, it may be mishandled or present privacy concerns. (See Chapter 14.) 6.3 Govern for Quality Content Managing unstructured data requires effective partnership between data stewards and other data management professionals and records managers. For example, business data stewards can help define web portals, enterprise taxonomies, search engine indexes, and content management issues. Document and content governance focuses on policies related to retention, electronic signatures, reporting formats, and report distribution. Policies will imply or state expectations about quality. Accurate, complete, and up -to-date information will aid in making decisions. High quality information improves competitive advantage and increases organizational effectiveness. Defining quality content requires understanding the context of its production and use. • Producers : Who creates the content and why do they create it? • Consumers: Who uses the information and for what purposes? • Timing : When is the information needed? How frequently does it need to be updated or accessed? • Format : Do consumers need the content in a specific format for to meet their goals? Are there unacceptable formats? • Delivery : How will information be delivered? How will consumers access the information? How will security be enforced to prevent inappropriate access to electronic content? Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 325 6.4 Metrics Key Performance Indicators (KPIs) are both quantitative and qualitative measures used to review organizational performance against its goals. KPIs can be developed at the strategic and operational levels. Some KPIs may be appropriate for both levels, especially if they measure lifecycle functions or risks. 6.4.1 Records Management At the strategic level, KPIs can be developed within such areas of records management compliance with regulatory requirements (e.g., time taken to meet requirements), and / or governance (e.g., compliance with policies). At the operational level, KPIs can be developed within such areas of records management resources (e.g., operational and capital costs), training (e.g., number of classes given, number of employees trained , and at what level), delivery of daily records management services and operations (e.g., percentage meeting user SLAs), and / or integration of records management functio ns with other business systems (e.g., percentage of integration). Criteria to measure success of a records management system implementation can include the following percentages: • Percentage of total documents and email per user identified as corporate records • Percentage of the identified corporate records declared as such and put under records control • Percentage of total stored records that have the proper retention rules applied These percentages can then be compared to determine best practice percentages. Sometimes, measuring records management implementation success is a simple budgetary matter. A financial determination examines at what point the implementation of an electronic records management system becomes less expensive than acquiring more room to s tore paper records. ARMA’s GARP principle categories and maturity model can guide the definition of KPIs. ARMA’s Information Governance Assessment software platform can identify information related compliance risks and develop metrics for governance function maturity in areas such as e -records and e -discovery (e.g., litigation holds). 6.4.2 E-discovery One common KPI of e- discovery is cost reduction. Another KPI is efficiency gained in collecting information ahead of time rather reactively (e.g., average time in days to turn around e -discovery requests). How quickly an organization can implement a legal hold notification process (LH N) is third type of KPI. Measurement of e-discovery is critical to a better rate of litigation wins. The EDRM model can guide development of KPIs based on what is required by each phase. ERDM also publishes a Metrics Model for e - discovery metrics. 55 The primary elements of Volume, Time, and Cost are in the center surrounded by the 55 EDRM Metrics Model, http://bit.ly/2rURq7R. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "326 • DMBOK2 seven aspects of e-discovery work (Activities, Custodians, Systems, Media, Status, Format, and QA) which affect the outcome of the center elements. 6.4.3 ECM KPIs should be developed to measure both tangible and intangible benefits of ECM . Tangible benefits include increased productivity, cost reduction, improved information quality, and improved compliance. Intangible benefits include improved collaboration, and simplification of job routines and workflow. As ECM is being established, KPIs will focus on program and operational metrics. Program metrics include number of ECM projects, adoption, and user satisfaction levels. Operational metrics include the typical system type KPIs, such as the amount of downtime, number of users, etc. Specific ECM metrics such as storage utilization (e.g., comparison of amount used with ECM implementation vs. amount used before ECM) and search retrieval performance can also be used as KPIs. Information retrieval is measured by precision and recall. Prec ision is the proportion of the retrieved documents that are actually relevant. Recall is a proportion of all relevant documents that are actually retrieved. Over time, KPIs related to the value of business solutions can be developed. • Financial KPIs can include the cost of ECM system, reduced costs related to physical storage, and percentage decrease in operational costs. • Customer KPIs can include percentage incidents resolved at first contact and number of customer complaints. • KPIs representing more effective and productive internal business processes can include percentage of paperwork reduced, percentage of error reduction using workflow and process automation. • Training KPIs can include number of training sessions for management and non- management. • Risk mitigation KPIs can include reduction of discovery costs, and number of audit trails tracking e - discovery requests. 7. Works Cited / Recommended Boiko, Bob. Content Management Bible. 2nd ed. Wiley, 2004. Print. Diamond, David. Metadata for Content Management: Designing taxonomy, metadata, policy and workflow to make digital content systems better for users. CreateSpace, 2016. Print. Hedden, Heather. The Accidental Taxonomist. Information Today, Inc., 2010. Print. Lambe, Patrick . Organising Knowledge: Taxonomies, Knowledge and Organisational Effectiveness . Chandos Publishing, 2007. Print. Chandos Knowledge Management. Liu, Bing. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data . 2nd ed. Springer, 2011. Print. Data -Centric Systems and Applications. Nichols, Kevin. Enterprise Content Strategy: A Project Guide . XML Press, 2015. Print. Read, Judith and Mary Lea Ginn. Records Management. 9th ed. Cengage Learning, 2015. Print. Advanced Office Systems and Procedures. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DOCUMENT AND CONTENT MANAGEMENT • 327 Rockley, Ann and Charles Cooper. Managing Enterprise Content: A Unified Content Strategy . 2nd ed. New Riders, 2012. Print. Voices That Matter. Smallwood, Robert F. Information Governance: Concepts, Strategies, and Best Practices . Wiley, 2014. Print. Wiley CIO. US GAAP Financial Statement Taxonomy Project. XBRL US GAAP Taxonomies . v1.0 Technical Guide Document Number: SECOFM - USGAAPT -TechnicalGuide. Version 1.0. April 28, 2008 http://bit.ly/2rRauZt. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "329 CHAPTER 1 0 Reference and Master Data 1. Introduction n any organization, certain data is required across business areas, processes, and systems. The overall organization and its customers benefit if this data is shared and all business units can access the same customer lists, geographic location codes, business unit lists, delivery options, part lists, accounting cost center codes, governmental tax codes, and other data used to run the business. People using data generally assume a level of consistency exists across the organization, until they see disparate data. In most organizations , systems and data evolve more organically than data management professionals would like. Particularly in large organizations, various projects and initiatives, mergers and acquisitions, and other business activities result in multiple systems executing es sentially the same functions, isolated from each other. These conditions inevitably lead to inconsistencies in data structure and data values between systems. This variability increases costs and risks. Both can be reduced through the management of Master Data and Reference Data. Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International I Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "330 • DMBOK2 Figure 75 Context Diagram: Reference and Master Data Definition : Managing reconciled and integrated data through stewardship and semantic consistency in support of enterprise -wide needs to share its data assets. Goals : 1. Ensuring the organization has complete, consistent, current, authoritative Master and Reference Data across organizational processes. 2. Enabling Master and Reference Data to be shared across enterprise functions and applications. 3. Lowering the cost and reducing the complexity of data usage and integration through standards, common data models, and integration patterns. Activities : 1.Define Drivers and Requirements (P) 2.Evaluate and Assess Data Sources (P) 3.Define Architectural Approach (D) 4.Model Data (D) 5.Define Stewardship and Maintenance Processes (C) 6.Establish Governance Policies (C) 7.Implement Data Sharing / Integration Services (D,O) 1. Acquire Data Sources for Sharing 2. Publish Reference and Master DataInputs : •Candidate Data Stores and Values •Cross Functional Requirements •Industry Standards •Metadata •Purchased Data and/or Open Data and Code Sets •Business RulesDeliverables : •Master and Reference Data Requirements •Data Models and Integration Patterns •Reliable Reference and Master Data •Reusable Data Services •Data Exception Reports Suppliers : •Subject Matter Experts •Data Stewards •Application Developers •Data Providers •Business Analysts •Infrastructure Systems AnalystsConsumers : •Subject Matter Experts •Data Integrators •Application Users •Application Developers •Solution ArchitectsParticipants : •Data Analysts •Data Modelers •Data Stewards •Data Integrators •Data Architects •Data Quality Analysts T echniques : •Conditions -of-use agreements •Business key cross references •Processing Log analysisT ools : •Data Integration T ools •Data Remediation T ools •Operational Data Stores •Data Sharing Hubs •Data Modeling T ools •Metadata Repositories •MDM Application PlatformsMetrics : •Data Quality and Compliance •Data Change Activity •Data Ingestion and Consumption •Data Sharing Availability •Data Steward Coverage •Data Sharing Volume and Usage (P) Planning, (C) Control, (D) Development, (O) OperationsReference and Master Data Business Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 331 1.1 Business Drivers The most common drivers for initiating a Master Data Management program are: • Meeting organizational data requirements : Multiple areas within an organization need access to the same data sets, with the confidence that the data sets are complete, current, and con sistent. Master Data often form the basis of these data sets (e.g., determining whether an analysis includes all customers depends on having a consistently applied definition of a customer) . • Managing Data Quality : Data inconsistencies, quality issues, and gaps, lead to incorrect decisions or lost opportunities; Master Data Management reduces these risks by enabling a consistent representation of the entities critical to the organization. • Managing the costs of data integration : The cost of integrating new data sources into an already complex environment are higher in the absence of Master Data, which reduces variation in how critical entities are defined and identified. • Reducing risk : Master Data can enable simplification of data sharing architecture to reduce costs and risk associated with a complex environment. The drivers for managing Reference Data are similar. Centrally managed Reference Data enables organizations to: • Meet data requirements for multiple initiatives and reduce the risks and costs of data integration through use of consistent Reference Data • Manage the quality of Reference Data While data -driven organizational initiatives focus on transactional data (increasing sales or market share, reducing costs, demonstrating compliance), the ability to leverage such transactional data is highly dependent on the availability and quality of Reference and Master Data. Improving the availability and quality of Reference and Master Data has a dramatic impact on overall quality of the data and business confidence in data. These processes have additional benefits to an organization, including simpli fication of IT landscape, improved efficiency and productivity, and with these, the potential to improve the customer experience. 1.2 Goals and Principles The goals of a Reference and Master Data Management program include: • Ensuring the organization has complete, consistent, current, authoritative Master and Reference Data across organizational processes • Enabling Master and Reference Data to be shared across enterprise functions and applications • Lowering the cost and reducing the complexity of data usage and integration through standards, common data models, and integration patterns Reference and Master Data Management follow these guiding principles: • Shared Data: Reference and Master Data must be managed so that they are shareable across the organization. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "332 • DMBOK2 • Ownership : Reference and Master Data belong to the organization, not to a particular application or department. Because they are widely shared, they require a high level of stewardship. • Quality : Reference and Master Data Management require ongoing Data Quality monitoring and governance. • Stewardship : Business Data Stewards are accountable for controlling and ensuring the quality of Reference Data. • Controlled Change : o At a given point of time, Master Data values should represent the organization’s best understanding of what is accurate and current. Matching rules that change values should be applied with caution and oversight. Any identifier merged or split should be re versible. o Changes to Reference Data values should follow a defined process; changes should be approved and communicated before they are implemented. • Authority : Master Data values should be replicated only from the system of record. A system of reference may be required to enable sharing of Master Data across an organization. 1.3 Essential Concepts 1.3.1 Differences Between Master and Reference Data Different types of data play different roles within an organization. They also have different management requirements. A distinction is often made between T ransaction and Master Data, as well as between Master Data and Reference Data. Malcolm Chisholm has proposed a six -layer taxonomy of data that includes Metadata, Reference Data, enterprise structure data, transaction structure data, transaction activity data, and transaction audit data (Chisholm, 2008; Talburt and Zhou, 2015). Within this taxonomy, he defines Master Data as an aggregation of Reference Data, enterprise structure data, and transaction structure data: • Reference Data , for example, code and description tables, is data that is used solely to characterize other data in an organization, or solely to relate data in a database to information beyond the boundaries of the organization. • Enterprise Structure Data , for example, a chart of accounts, enables reporting of business activity by business responsibility. • Transaction Structure Data , for example customer identifiers, describes the things must be present for a transaction to occur: products, customers, vendors. Chisholm’s definition distinguishes Master Data from transaction activity data that records details about transactions, and from transaction audit data that describes the state of transactions, as well as from Metadata, which describes other data (Chisholm, 2008). I n this respect, Chisholm’s definition is similar to the DAMA Dictionary’s definition: Master Data is “the data that provides the context for business activity data in the form of common and abstract concepts that relate to the activity. It includes the details (definitions and identifiers) of internal and external objects involved in business transactions, such as customers, products, employees, vendors, and controlled domains (code values)” (DAMA, 2009). Many people understand Master Data to include both transaction structure data and enterprise structure data. David Loshin’s definition of Master Data aligns largely with these types. He describes Master Data objects as Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 333 core business objects used in different applications across an organization, along with their associated Metadata, attributes, definitions, roles, connections, and taxonomies. Master Data objects represent those ‘things’ that matter most to an organization – those that are logged in transactions, reported on, measured, analyzed (Loshin, 2008). Master Data requires identifying and / or developing a trusted version of truth for each instance of conceptual entities such as product, place, account, person, or o rganization and maintaining the currency of that version. The primary challenge with Master Data is entity resolution (also called identity management), the process of discerning and managing associations between data from different systems and processes. The entity instances represented by Master Data rows will be represented differently across systems. Master Data Management works to resolve these differences in order to consistently identify individual entity instances (i.e., specific customers, products , etc.) in different contexts. This process must also be managed over time, so that the identifiers for these Master Data entity instances remain consistent.56 Reference Data and Master Data share conceptually similar purposes. Both provide context critical to the creation and use of transactional data. (Reference Data also provides context for Master Data.) They enable data to be meaningfully understood. Importantly, both are shared resources that should be managed at the enterprise level. Having multiple instances of the same Reference Data is inefficient and inevitably leads to inconsistency between them. Inconsistency leads to ambiguity, and ambiguity introduces risk to an organization. A successful Reference Data or Master Data Management program involves the full range of data management functions ( Data Governance , Data Quality, Metadata Management, Data Integration , etc.). Reference Data also has characteristics that distinguish it from other kinds of Master Data (e.g., enterprise and transactional structure data). It is less volatile. Reference Data sets are generally less complex and smaller than either T ransactional or Master Data sets. They have fewer columns and fewer rows. The challenges of entity resolution are not part of Reference Data Management. The focus of data management differs between Reference and Master Data: • Master Data Management (MDM) entails control over Master Data values and identifiers that enable consistent use, across systems, of the most accurate and timely data about essential business entities. The goals of MDM include ensuring availability of accurate, current values while reducing risks associated with ambiguous identifiers (those identified with more than one instance of an entity and those that refer to more than one entity). • Reference Data Management (RDM) entails control over defined domain values and their definitions. The goal of RDM is to ensure the organization has access to a complete set of accurate and current values for each concept represented. One challenge of Reference Data Management is that of ownership or responsibility for definition and maintenance. Some Reference Data originates outside of the organizations that use it. Some crosses internal organizational boundaries and may not be owned by a single department. Other Reference Data may be created and maintained within a department but have potential value elsewhere in an organization. Determining responsibility for obtaining data and managing updates is part of RDM. Lack of accountability introduces risk, as differences in Reference Data may cause misunderstanding of data context (as when two business units have 56 John Talburt and Yinle Zhou (2015) describe the two step process in ER: first, determine whether two records refer to the sam e entity, then merge and reconcile data in the records in order to create a master record. They refer to Entity Identity Informati on Management (EIIM) as the process of ensuring that “an entity under management in the MDM system is consistently labeled with the same unique identifier from process to process.” Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "334 • DMBOK2 different values to classify the same concept). Because Master and Reference Data provide context for transactions, they shape the Transaction data entering an organization during operations (for example, in CRM and ERP systems). They also frame analysis p erformed on Transaction Data. 1.3.2 Reference Data As noted, Reference Data is any data used to characterize or classify other data, or to relate data to information external to an organization (Chisholm, 2001). The most basic Reference Data consists of codes and descriptions, but some Reference Data can be more complex and incor porate mappings and hierarchies. Reference Data exists in virtually every data store. Classifications and categories may include statuses or types (e.g., Order Status: New, In Progress, Closed, Cancelled). External information may include geographic or standards information (e.g., Country Code: DE, US, TR). Reference Data may be stored in different ways to meet the different needs. For example, data integration (e.g., data mappings for standardization or Data Quality checks), or other application functionality (e.g., synonym rings to enable search and discovery). It may also have device specific user interface considerations (e.g., multiple languages). Common storage techniques use: • Code tables in relational databases, linked via foreign keys to other tables to maintain referential integrity functions within the database management system • Reference Data Management systems that maintain business entities, allowed, future -state, or deprecated values, and term mapping rules to support broader application and data integration use • Object attribute specific Metadata to specify permissible values with a focus on API or user interface access Reference Data Management entails control and maintenance of defined domain values, definitions, and the relationships within and across domain values. The goal of Reference Data Management is to ensure values are consistent and current across different functions and that the data is accessible to the organization. Like other data, Reference Data requires Metadata. An important Metadata attribute for Reference Data include s its source. For example, the governing body for industry standard Reference Data. 1.3.2.1 Reference Data Structure Depending on the granularity and complexity of what the Reference Data represents, it may be structured as a simple list, a cross- reference, or a taxonomy. The ability to use and maintain Reference Data should be accounted for when structuring it within a database or a Reference Data Management system. 1.3.2.1.1 Lists The simplest form of Reference Data pairs a code value with a description in a list, such as in Table 17. The code value is the primary identifier, the short form reference value that appears in other contexts. The description states what the code represents. The description may be displayed in place of the code on screens, pages, drop-down lists, and report s. Note that in this example, the code value for the United Kingdom is GB Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 335 according to international standards, and not UK, even though UK is a common short form used in many forms of communication. Balance between standards compliance and usability when defining Reference Data requirements. Table 17 Simple Reference List Code Value Description US United States of America GB United Kingdom (Great Britain) Depending on the content and complexity of the Reference Data, additional attributes may be required to define the meaning of the code. Definitions provide information that the label alone does not provide. Definitions rarely appear on reports or drop -down lists. However, they do appear in places like Help functions for applications, which guide the appropriate use of codes in context. Lists, like any Reference Data, must meet the requirements of data consumers, including requirements for the appropriate level of detail. If a list of values is intended to support data classification by casual users, a highly detailed list will likely cau se Data Quality issues and adoption challenges. Similarly, a list of values that is too generic would prevent knowledge workers from capturing sufficient level of detail. To accommodate such cases, it is better to maintain distinct lists that are related vs. attempting to have a single list that is the standard for all user communities. Table 18 provides an example related to status codes for help desk tickets. Without the information provided by the definition, ticket status would be ambiguous to anyone unfamiliar with the system. This differentiation is especially necessary for classifications driving performance metrics or other Business Intelligence analytics. Table 18 Simple Reference List Expanded Code Description Definition 1 New Indicates a newly created ticket without an assigned resour ce 2 Assigned Indicates a ticket that has a named resource assigned 3 Work In Progress Indicates the assigned resource started working on the ticket 4 Resolved Indicates request is assumed to be fulf illed per the assigned resource 5 Cancelled Indicates request was canceled based on requester interaction 6 Pending Indicates request cannot proceed without additional information 7 Fulfilled Indicates request was fulfille d and verified by the requester 1.3.2.1.2 Cross -Reference Lists Different applications may use different code sets to represent the same concept. These code sets may be at different granularities or the same granularity with different values. Cross -reference data sets translate between codes values. Table 19 presents a US State Code cross- reference (an example of multiple representations at the same level of grain). The US Postal Service State Codes are two character alpha codes. FIPS uses a numeric to express the same concept. The ISO State Code also includes a reference to the country. Table 19 Cross -Reference List USPS State Code ISO State Code FIPS Numeric State Code State Abbreviation State Name Formal State Name CA US-CA 06 Calif. California State of California KY US-KY 21 Ky. Kentucky Commonwealth of Kentucky WI US-WI 55 Wis. Wisconsin State of Wisconsin Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "336 • DMBOK2 Language requirements may affect Reference Data structure. Multi -language lists are a specific instance of a cross -reference list. While code lists provide a standard, machine -readable format, language -specific glossaries provide usable content. Table 20 provides an example from the ISO 3166 standard. There are different ways to handle multi -language lists depending on how many languages and character sets are involved. Lists do not need to be normalized to be effective. The denormalized structure makes i t somewhat easier to comprehend the relationships. Table 20 Multi -Language Reference List ISO 3166- 1 Alpha 2 Country Code English Name Local Name Local Name Local Alphabet French Name … CN China Zhong Guo 中国 /中國 Chine 1.3.2.1.3 Taxonomies Taxonomic Reference Data structures capture information at different levels of specificity. For example, a US ZIP Code may be a meaningful category itself, and it exists within a town, a county, and a state. These relationships can be expressed within reference table and multiple levels of analysis could be done using ZIP code as a driver. Taxonomies enable content classification and multi- faceted navigation to support Business Intelligence. Taxonomic Reference Data can be stored in a recursive relationship. Taxonomy management t ools also maintain hierarchical information. Table 21 and Table 22 show examples of two common hierarchical taxonomies. In both cases, the hierarchy includes a code, description, and a reference to a parent code that classifies the individual codes. For example, in Table 21, Floral plants ( 10161600) is a parent code to Roses, Poinsettias, and Orchids. In Table 22, Retail Trade (440000) is the parent to Food and Beverage Stores (445000), which is the parent to Specialty Food Stores (445200). Table 21 UNSPSC (Universal Standard Products and Services Classification)57 Code Value Description Parent Code 10161600 Floral plants 10160000 10161601 Rose plants 10161600 10161602 Poinsettias plants 10161600 10161603 Orchid plants 10161600 10161700 Cut flowers 10160000 10161705 Cut roses 10161700 Table 22 NAICS (North America Industry Classification System)58 Code Value Description Parent Code 440000 Retail Trade 440000 445000 Food and Beverage Stores 440000 445200 Specialty Food Stores 445000 445210 Meat Markets 445200 445220 Fish and Seafood Markets 445200 445290 Other Specialty Food Stores 445200 445291 Baked Goods Stores 445290 445292 Confectionary and Nut Stores 445290 57 http://bit.ly/2sAMU06 . 58 http://bit.ly/1mWACqg. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 337 1.3.2.1.4 Ontologies Some organizations include ontologies used to manage website content as part of Reference Data . They fit this category in that they are used to characterize other data or to relate organizational data to information beyond the boundaries of the organization. Ontologies can also be understood as a form of Metadata. Ontologies and other complex taxonomies need to be managed in ways similar to how Reference Data is managed. Values need to be complete, current, and clearly defined. Best practices for maintaining ontologies are similar to those for Reference Data Management. One of the primary use cases for ontologies is content management. They are described in more detail in Chapter 9. 1.3.2.2 Proprietary or Internal Reference Data Many organizations create Reference Data to support internal processes and applications. O ften, this proprietary reference data grows organically over time. Part of RDM includes managing these data sets and, ideally, creating consistency between them, where this consistency serves the organization. For example, if different business units use different terms to describe the s tatus of an account, then it is difficult for anyone in the organization to determine the overall number of clients it serves at a point in time. In helping manage internal R eference Data sets, Data Stewards must balance between the need to have common words for the same information and the need for flexibility where processes differ from one another. 1.3.2.3 Industry Reference Data Industry Reference Data is a broad term to describe data sets that are created and maintained by industry associations or government bodies rather than by individual organizations, in order to provide a common standard for codifying important concepts. This codification leads to a common way to understand data and is a prerequisite for data sharing and interoperability. For example, the International Classification of Diseases (ICD) codes provide a common way to classify health conditions (diagnoses) and treatments (procedures) a nd thus to have a consistent approach to delivering health care and understanding outcomes. If every doctor and hospital creates their own code set for diseases, it would be virtually impossible to understand trends and patterns. Industry Reference Data is produced and maintained external to the organizations that use it, but it is required to understand transactions within those organizations. It may be needed to support specific Data Quality Management efforts (e.g., third party business directories), business calculations ( e.g., foreign exchange rates), or business data augmentation ( e.g., marketing data). These data sets vary widely, depending on the industry and the individual code set. (See Chapter 10.) 1.3.2.4 Geographic or Geo -statistical Data Geographic or geo -statistical reference enables classification or analysis based on geography. For example, census bureau reports describe population density and demographic shifts that support market planning and research. Weather history mapped to strict geographic classification can support inventory management and promotional planning. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "338 • DMBOK2 1.3.2.5 Computational Reference Data Many business activities rely on access to common, consistent calculations. For example, foreign exchange calculations rely on managed, time stamped exchange value tables. Computational Reference Data differs from other types because of the frequency with which it chang es. Many organizations purchase this kind of data from third parties who ensure that it is complete and accurate. Attempting to maintain this data internally is likely to be fraught with latency issues. 1.3.2.6 Standard Reference Data Set Metadata Reference Data , like other data, can change over time. Given its prevalence within any organization, it is important to maintain key Metadata about Reference Data sets to ensure their lineage and currency are understood and maintained. Table 23 provides examples of this Metadata. Table 23 Critical Reference Data Metadata Attributes Reference Data Set Key Information Description Formal Name Official, especially if external name of the Reference Data set (e.g., ISO 3166- 1991 Country Code List) Internal Name Name associated with the data set within the organization (e.g., Country Codes – ISO) Data Provider The party that provides and maintains the Reference Data set. This can be external (ISO), internal (a specific department), or external – extended (obtained from an external party but then extended and modified internally). Data Provider Data Set Source Description of where data provider’s data sets can be obtained. This is likely a Universal Resource Identifier (URI) within or outside of the enterprise network. Data Provider Latest Version Number If available and maintained, this describes the latest version of the external data provider’s data set where information may be added or deprecated from the version in the organization Data Provider Latest Version Date If available and maintained, this describes when the standard list was last updated Internal Version Number Version number of the current Reference Data set or version number of the last update that was applied against the data set Internal Version Reconciliation Date Date when data set was last updated based on the external source Internal Version Last Update Date Date data set was last changed. This does not mean reconciliation with an external version. 1.3.3 Master Data Master Data is data about the business entities (e.g., employees, customers, products, financial structures, assets, and locations) that provide context for business transactions and analysis. An entity is a real world object (person, organization, place, or thing). Entities are represented by entity instances, in the form of data / records. Master Data should represent the authoritative, most accurate data available about key business entities. When managed well, Master Data values are trusted and can be used with co nfidence. Business rules typically dictate the format and allowable ranges of Master Data values. Common organizational Master Data includes data about: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 339 • Parties , made up of individuals and organizations, and their roles, such as customers, citizens, patients, vendors, suppliers, agents, business partners, competitors, employees, or students • Products and Services , both internal and external • Financial structures , such as contracts, general ledger accounts, cost centers, or profit centers • Locations , such as addresses and GPS coordinates 1.3.3.1 System of Record, System of Reference When there are potentially different versions of ‘the truth’, it is necessary to distinguish between them. In order to do so, one must know where data originates or is accessed, and which data has been prepared for particular uses. A System of Record is an authoritative system where data is created/captured, and/or maintained through a defined set of rules and expectations (e.g., an ERP system may be the System of Record for sell -to customers). A System of Reference is an authoritative system where data consumers can obtain reliable data to support transactions and analysis, even if the information did not originate in the system of reference. MDM applications, Data Sharing Hubs, and Data Warehouses often serve as sy stems of reference. 1.3.3.2 Trusted Source, Golden Record A Trusted Source is recognized as the ‘best version of the truth’ based on a combination of automated rules and manual stewardship of data content. A trusted source may also be referred to as a Single View, 360° View. Any MDM system should be managed so that it is a trusted source. Within a trusted source, records that represent the most accurate data about entity instances can be referred to as Golden Records . The term Golden Record can be misleading. Tech Ta rget defines a Golden Record as “the ‘single version of the truth’, where ‘truth’ is understood to mean the reference to which data users can turn when they want to ensure that they have the correct version of a piece of information. The golden record encompasses all the data in every system of record (SOR) within a particular organization. ” 59 However, the two parts of this definition bring the concept into question, as data in different systems may not align into ‘a single version of the truth’. Within any Master Data effort, the merging/resolution of data from multiple sources into a ‘Golden Record’ does not mean that it is always a 100% complete and 100% accurate representation of all the entities within the organization (especially in organizations that have multiple SOR’s supplying data to the Master Data environment). Promising that data is ‘Golden’ when it is not can undermine the confidence of data consumers. This is why some prefer the term Trusted Source to refer to the “best version we have” of the Master Data. Doing so puts the emphasis on how data is defined and managed to get to a best version. It also helps different data consumers see the component pieces of the ‘single version’ that are important to them. Finance and Actuarial areas often have a different perspective of ‘single version’ of Customer than does the Marketing area. The Trusted Source provides multiple perspectives of business entities as identified and defined by Data Stewards. 59 http://bit.ly/2rRJI3b . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "340 • DMBOK2 1.3.3.3 Master Data Management As described in the chapter introduction, Master Data Management entails control over Master Data values and identifiers that enable consistent use, across systems, of the most accurate and timely data about essential business entities. The goals include ensuring availability of accurate, current values while reducing t he risk of ambiguous identifiers. Gartner defines Master Data Management as “a technology -enabled discipline in which business and IT work together to ensure the uniformity, accuracy, stewardship, semantic consistency, and accountability of the enterprise’s official shared Master Data assets. Master Data is the consistent and uniform set of identifiers and extended attributes that describes the core entities of the enterprise including customers, prospects, citizens, suppliers, sites, hierarchies, and chart of accounts.” 60 Gartner’s definition stresses that MDM is a discipline made up of people, processes, and technology. It is not a specific application solution. Unfortunately, the acronym MDM (Master Data Management) is often used to refer to systems or products used to manage Master Data. 61 MDM applications can facilitate the methods, and sometimes quite effectively, but using an MDM application does not guarantee that Master Data is being managed to meet the organizational needs. Assessing an organization’s MDM requirements includes identifying: • Which roles, organizations, places, and things are referenced repeatedly • What data is used to describe people, organizations, places, and things • How the data is defined and structured, including the granularity of the data • Where the data is created/sourced, stored, made available, and accessed • How the data changes as it moves through systems within the organization • Who uses the data and for what purposes • What criteria are used to understand the quality and reliability of the data and its sources Master Data Management is challenging. It illustrates a fundamental challenge with data: People choose different ways to represent similar concepts and reconciliation between these representations is not always straightforward; as importantly, information changes over time and systematically accounting for these changes takes planning, data knowledge, and technical skills. In short, it takes work. Any organization that has recognized the need for MDM probably already has a complex system landscape, with multiple ways of capturing and storing references to real world entities. Because of both organic growth over time or from mergers and acquisitions, the systems that provided input to the MDM process may have different definitions of the entities themselves and very likely have different standards for Data Quality. Because of this complexity, it is best to approach Master Data Management one data doma in at a time. Start small, with a handful of attributes, and build out over time. Planning for Master Data Management includes several basic steps. Within a domain: • Identify candidate sources that will provide a comprehensive view of the Master Data entities 60 http://gtnr.it/2rQOT33 . 61 Note that, throughout the DAMA -DMBOK, MDM refers to the overall process of managing Master Data, rather than to just the tools used to manage this data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 341 • Develop rules for accurately matching and merging entity instances • Establish an approach to identify and restore inappropriately matched and merged data • Establish an approach to distribute trusted data to systems across the enterprise Executing the process, though, is not as simple as these steps imply, as MDM is a lifecycle management process. Activities critical to the lifecycle include: • Establishing the context of Master Data entities, including definitions of associated attributes and the conditions of their use. This process requires governance. • Identifying multiple instances of the same entity represented within and across data sources; building and maintaining identifiers and cross -references to enable information integration. • Reconciling and consolidating data across sources to provide a master record or the best version of the truth. Consolidated records provide a merged view of information across systems and seek to address attribute naming and data value inconsistencies. • Identifying improperly matched or merged instances and ensuring they are resolved and correctly associated with identifiers. • Provisioning of access to trusted data across applications, either through direct reads, data services, or by replication feeds to transactional, warehousing , or analytical data stores. • Enforcing the use of Master Data values within the organization. This process also requires governance and change management to en sure a shared enterprise perspective. 1.3.3.4 Master Data Management Key Processing Steps Key processing steps for MDM are illustrated in Figure 76. They include data model management; data acquisition; data validation, standardization, and enrichment; entity resolution; and stewardship and sharing. In a comprehensive MDM environment, the logical data model will be physically instantiated in multiple platforms. It guides the implementation of the MDM solution, providing the basis of data integration services. It should guide how applications are configured to take advantage of data reconciliation and Data Quality verification capabilities. Figure 76 Key Processing Steps for MDM Data Model ManagementData Validation, Standardization, and EnrichmentData Sharing & StewardshipData AcquisitionEntity Resolution Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "342 • DMBOK2 1.3.3.4.1 Data Model Management Master Data work brings t o light the importance of clear and consistent logical data definitions. The model should help the organization overcome ‘system speak’. Terms and definitions used within a source system may make sense within the confines of that system but they do not always make sense at an enterprise level. For Master Data, terms and definitions used at an enterprise level should be in context of the business conducted across the organization and not necessarily dependent on the source system contributing data values. For attributes that make up Master Data, the granularity of the definition and associated data values must also make sense across the organization. Source systems may present the identical attribute name but the data values are in completely different contexts at the enterprise level. Similarly, source systems may present differently named attributes that at the enterprise level coalesce to a single attribute and the data values are in the proper context. Sometimes , multiple attribute s are presented from a single source and their respective data values are used to derive a single data value for an attribute defined at the enterprise level. 1.3.3.4.2 Data Acquisition Even within a given source, data representing the same entity instance can look different, as illustrated in Table 24, where there are inconsistencies in how names, addresses, and telephone numbers are presented. This example will be referenced again later in the chapter. Table 24 Source Data as Received by the MDM System Source ID Name Address Telephone 123 John Smith 123 Main, Dataland, SQ 98765 234 J. Smith 123 Main, Dataland, DA 2345678900 345 Jane Smith 123 Main, Dataland, DA 234-567-8900 Planning for, evaluating, and incorporating new data sources into the Master Data Management solution must be a reliable, repeatable process. Data acquisition activities involve: • Receiving and responding to new data source acquisition requests • Performing rapid, ad -hoc, match , and high-level Data Quality assessments using data cleansing and data profiling tools • Assessing and communicating complexity of data integration to the requesters to help them with their cost-benefit analysis • Piloting acquisition of data and its impact on match rules • Finalizing Data Quality metrics for the new data source • Determining who will be responsible for monitoring and maintaining the quality of a new source’s data • Completing integration into the overall data management environment 1.3.3.4.3 Data Validation, Standardization, and Enrichment To enable entity resolution, data must be made as consistent as possible. This entails, at a minimum, reducing variation in format and reconciling values. Consistent input data reduces the chance o f errors in associating records. Preparation processes include: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 343 • Validation : Identifying data prove -ably erroneous or likely incorrect or defaulted (for example, removal of clearly fake email addresses) • Standardization: Ensuring data content conforms to standard Reference Data values (e.g., country codes), formats (e.g., telephone numbers) , or fields (e.g., addresses) • Enrichment : Adding attributes that can improve entity resolution services (e.g., Dunn and Bradstreet DUNS Number and Ultimate DUNS Number for relating company records, Acxiom or Experian Consumer IDs for individual records) Table 25 illustrates the results of the cleansing and standardization process on the example from Table 24. Addresses with different formats are now recognizably the same. Phone numbers include standard formatting. Table 25 Standardized and Enriched Input Data Source ID Name Address (Cleansed) Telephone (Cleansed) 123 John Smith 123 Main, Dataland, SQ 98765 234 J. Smith 123 Main, Dataland, SQ 98765 +1 234 567 8900 345 Jane Smith 123 Main, Dataland, SQ 98765 +1 234 567 8900 1.3.3.4.4 Entity Resolution and Identifier Management Entity resolution is the process of determining whether two references to real world objects refer to the same object or to different objects (Talburt, 2011). Entity resolution is a decision -making process. Models for executing the process differ based on the approach they take to determining the similarity between two references. While resolution always takes place between pairs of references, the process can be systematically extended to include large data sets. Entity resolution is critical to MDM, as the process of matc hing and merging records enables the construction of the Master Data set. Entity resolution includes a set of activities (reference extraction, reference preparation, reference resolution, identity management, relationship analysis) that enable the identity of entity instances and the relationship between entity instances to be managed over time. Within the process of reference resolution, two references may be identified as representing the same entity through the process of determining equivalency. These references can then be linked through a value (a global identifier) that i ndicates that they are equivalent (Talburt, 2011). 1.3.3.4.4.1 Matching Matching , or candidate identification , is the process of identifying how different records may relate to a single entity. The risks with this process are: • False positives : Two references that do not represent the same entity are linked with a single identifier. This results in one identifier that refers to more than one real -world entity instance. • False negatives : Two references represent the same entity but they are not linked with a single identifier. This results in multiple identifiers that refer to the same real -world entity when each instance is expected to have one -and-only -one identifier. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "344 • DMBOK2 Both situations are addressed through a process called similarity analysis or matching , in which the degree of similarity between any two records is scored, often based on weighted approximate matching between corresponding attribute values. If the score is above a specified threshold, the two records are considered to represent the same en tity (a match). Through similarity analysis, slight variations in data can be recognized and data values can be consolidated. Two basic approaches, which can be used toge ther, are deterministic and probabilistic: • Deterministic algorithms, like parsing and standardization, rely on defined patterns and rules for assigning weights and scores for determining similarity. Deterministic algorithms are predictable in that the patterns matched and the rules applied will always yield the same results. This type of matching works out -of-the-box with relatively good performance, but it is only as good as the situations anticipated by the people who developed the rules. • Probabilistic algorithms rely on statistical techniques for assessing the probability that any pair of records represents the same entity. This relies on the ability to take data samples for training purposes by looking at the expected results for a subset of the recor ds and tuning the matcher to self- adjust based on statistical analysis. These matchers are not reliant on rules, so the results may be nondeterministic. However, because the probabilities can be refined based on experience, probabilistic matchers are able to improve their matching precision as more data is analyzed. 1.3.3.4.4.2 Identity Resolution Some matches occur with great confidence, based on exact data matches across multiple fields. Other matches are suggested with less confidence due to conflicting values. For example: • If two records share the same last name, first name, birth date, and social security number, but the street address differs, is it safe to assume they refer to the same person who has changed their mailing address? • If two records share the same social security number, street address, and first name, but the last name differs, is it safe to assume they refer to the same person who has changed their last name? Would the likelihood be increased or decreased based on gen der and age? • How do these examples change if the social security number is unknown for one record? What other identifiers are useful to determine the likelihood of a match? How much confidence is required for the organization to assert a match? Table 26 illustrates the conclusion of the process for the sample records in Table 24 and Table 25. Here , the second two entity instances (Source ID 234 and 345) are determined to represent the same person (Jane Smith), while the first one (Source ID 123) is identified as representing a different person (John Smith). Table 26 Candidate Identification and Identity Resolution Source ID Name Address (Cleansed) Telephone (Cleansed) Candidate ID Party ID 123 John Smith 123 Main, Dataland, SQ 98765 XYZ 1 234 J. Smith 123 Main, Dataland, SQ 98765 +1 234 567 8900 XYZ, ABC 2 345 Jane Smith 123 Main, Dataland, SQ 98765 +1 234 567 8900 ABC 2 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 345 Despite the best efforts, match decisions sometimes prove to be incorrect. It is essential to maintain the history of matches so that matches can be undone when discovered to be incorrect. Match rate metrics enable organizations to monitor the impact and effectiveness of their matching inference rules. Reprocessing of match rules can help identify better match candidates as new information is received by the entity resolution process. 1.3.3.4.4.3 Matching Workflows / Reconciliation Types Match rules for different scenarios require different workflows : • Duplicate identification match rules focus on a specific set of data elements that uniquely identify an entity and identify merge opportunities without taking automatic action. Business Data Stewards can review these occurrences and decide to take action on a case-by -case basis. • Match -link rules identify and cross -reference records that appear to relate to a master record without updating the content of the cross -referenced record. Match -link rules are easier to implement and much easier to reverse. • Match -merge rules match records and merge the data from these records into a single, unified, reconciled, and comprehensive record. If the rules apply across data sources, create a single , unique , and comprehensive record in each data store. Minimally, use trusted data from one data store to supplement data in other data stores, replacing missing values or values thought to be inaccurate. Match -merge rules are complex, and seek to provide the unified, reconciled version of information across multiple records and data sources. The complexity is due to the need to identify which field from which source can be trusted based on a series of rules. The introduction of each new source can change these rules over time. The challenges with match- merge rules include the operational complexity of reconciling the data and the cost of reversing the operation if there is a false merge. Match -link is a simpler operation, as it acts on the cross- reference registry and not the individual attributes of the merged Master Data record, even though it may be more difficult to present comprehensive information from multiple records. Periodically re -evaluate match -merge and match -link rules because confidence levels change over time. Many data matching engines provide statistical correlations of data values to help establish confidence levels. (See Chapter 13. ) 1.3.3.4.4.4 Master Data ID Management Managing Master Data involves managing identifiers. There are two types of identifiers that need to be managed across data sources in an MDM environment: Global IDs and Cross-Reference (x -Ref) information. A Global ID is the MDM solution -assigned and - maintained unique identifier attached to reconciled records. Its purpose is to uniquely identify the entity instance. In the example in Table 26, when multiple records were determined to represent the same entity instance, the value ‘ABC’ was assigned to both as a candidate ID. The records were resolved to the single Party ID of ‘2’. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "346 • DMBOK2 Global IDs should be generated by only one authorized solution, regardless of which technology is performing Master Data integration activities, to avoid any risk of duplicate values. Global IDs can be numbers or GUIDs (Global Unique Identifiers), as long as uniqueness can be maintained. The key complexity that needs to be handled for Global ID generation is to how to maintain the right global ID (to perform appropriate downstream data updates) due to an unmerge-remerge. X-Ref Management is management of th e relationship between source IDs and the Global ID. X -Ref management should include capabilities to maintain history of such mappings to support match rate metrics, and to expose lookup services to enable data integration. 1.3.3.4.4.5 Affiliation Management Affiliation Management is establishing and maintaining relationships between Master Data records of entities that have real -world relationships. Examples include ownership affiliations (e.g., Company X is a subsidiary of Company Y, a parent -child relationship) or other associat ions (e.g., Person XYZ works at Company X). Data Architecture design of an MDM solution must resolve whether to leverage parent -child relationships, affiliation relationships, or both for a given entity. • Affiliation relationships provide the greatest flexibility through programming logic. The relationships type can be used to expose such data in a parent -child hierarchy. Many downstream solutions, such as reporting or account navigation tools would want to see a hierarchical view of the information. • Parent -Child relationships require less programming logic as the navigation structure is implied. However, if the relationship changes and there isn’t an available affiliation structure, this may influence the quality of the data and Business Intelligence dimensions. 1.3.3.4.5 Data Sharing and Stewardship Although much of the work of Master Data Management can be automated through tools that enable processing of large numbers of records, it still requires stewardship to resolve situations where data is incorrectly matched. Ideally, lessons learned from the stewardship process can be used to improve matching algorithms and reduce instances of manual work. ( See Chapter s 3 and 8.) 1.3.3.5 Party Master Data Party Master Data includes data about individuals, organizations, and the roles they play in business relationships. In the commercial environment, parties include customers, employees, vendors, partners, and competitors. In the public sector, parties are usually citizens. Law enforcement focuses on suspects, witnesses, and victims. Not-for- profit organizations focus on members and donors. While in healthcare, the focus is on patien ts and providers; in education, it is on students and faculty. Customer Relationship Management (CRM) systems manage Master Data about customers. The goal of CRM is to provide complete and accurate information about each and every customer. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 347 An essential aspect of CRM is identifying duplicate, redundant, or conflicting data from different systems and determining whether the data represents one or more than one customer. CRM must be able to resolve conflicting values, reconcile differences, and accurately represent current knowledge of the customer. This process requires robust rules as well as knowledge of the structure, granularity, lineage, and quality of data sources. Specialized MDM systems perform similar functions for individuals, organizations and their roles, employees, and vendors. Regardless of industry or focus, managing business party Master Data poses unique challenges: • The complexity of roles and relationships played by individuals and organizations • Difficulties in unique identification • The number of data sources and the differences between them • The multiple mobile and social communications channels • The importance of the data • The expectations of how customers want to be engaged Master Data is particularly challenging for parties playing multiple roles across an organization (e.g., an employee who is also a customer) and utilizing differing points of contact or engagement methods (e.g., interaction via mobile device application that is tied to a social media site ). 1.3.3.6 Financial Master Data Financial Master Data includes data about business units, cost centers, profit centers, general ledger accounts, budgets, projections, and projects. Typically, an Enterprise Resource Planning (ERP) system serves as the central hub for financial Master Data (chart of accounts), with project details and transactions created and maintained in one or more spoke applications. This is especially common in organizations with distributed back -office functions. Financial Master Data solutions not only create, maintain, and share information; many can also simulate how changes to existing financial data may affect the organization’s bottom line. Financial Master Data simulations are often part of Business Intelligence reporting, analysis, and planning modules, as well as more straightforward budgeting and projecting. Through these applications, versions of financial structures can be modeled to understand potential financial impacts. Once a decision is made, the ag reed upon structural changes can be disseminated to all appropriate systems. 1.3.3.7 Legal Master Data Legal Master Data includes data about contracts, regulations, and other legal matters. Legal Master Data allows analysis of contracts for different entities providing the same products or services, to enable better negotiation or to combine contracts into Master Agreements . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "348 • DMBOK2 1.3.3.8 Product Master Data Product Master Data can focus on an organization’s internal products and services or on industry -wide (including competitor) products and services. Different types of product Master Data solutions support different business functions. • Product Lifecycle Management (PLM ) focuses on managing the lifecycle of a product or service from conception, through development, manufacturing, sale / delivery, service, and disposal. Organizations implement PLM systems to reduce time to market. In industries with long product developmen t cycles (as much as 8 to 12 years in the pharmaceutical industry), PLM systems enable organizations to track cross -process cost and legal agreements as product concepts evolve from ideas to potential products under different names and potentially differen t licensing agreements. • Product Data Management (PDM ) supports engineering and manufacturing functions by capturing and enabling secure sharing of product information such as design documents (e.g., CAD drawings), recipes (manufacturing instructions), standard operating procedures, and bills of materials. PD M functionality can be enabled through specialized systems or ERP applications. • Product data in Enterprise Resource Planning (ERP) systems focuses on SKUs to support order entry down to inventory level, where individual units can be identified through a variety of techniques. • Product data in Manufacturing Execution Systems (MES ) focus on raw inventory, semi -finished goods, and finished goods, where finished goods tie to products that can be stored and ordered through the ERP system. This data is also important across the supply chain and logistics systems. • Product data in a Customer Relationship Management (CRM) system that supports marketing, sales, and support interactions can include product family and brands, sales rep association, and customer territory management, as well as marketing campaigns. Many product masters closely tie to Reference Data Management systems. 1.3.3.9 Location Master Data Location Master Data provides the ability to track and share geographic information and to create hierarchical relationships or territories based on geographic information. The distinction between reference and Master Data blurs for location data. Here is the difference: • Location Reference Data typically includes geopolitical data, such as countries, states or provinces, counties, cities or towns, postal codes, and geographic positioning coordinates, such as latitude, longitude, and altitude. This data rarely changes, and changes are handled by external organizations. Location Reference Data may also include geographic regions and sales territories as defined by the organization. • Location Master Data includes business party addresses and business party location, as well as facility addresses for locations owned by the organization. As organizations grow or contract, these addresses change more frequently than other Location Reference Data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 349 Different industries require specialized earth science data (geographic data about seismic faults, flood plains, soil, annual rainfall, and severe weather risk areas) and related sociological data (population, ethnicity, income, and terrorism risk), usually supplied from external sources. 1.3.3.10 Industry Master Data – Reference Directories Reference Directories are authoritative listings of Master Data entities (companies, people, products, etc.) that organizations can purchase and use as the basis of their transactions. While reference directories are created by external organizations, a managed and reconciled version of the information is maintained in the organization’s own systems. Examples of licensed reference directories include Dun and Bradstreet’s (D &B) Company Directory of worldwide Company Headquarters, Subsidiaries, and Branch locations, and the American Medical Association’s Prescriber Database. Reference directories enable Master Data use by: • Providing a starting point for matching and linking new records. For example, in an environment with five data sources, each source can be compared against the directory (5 comparison points) vs. against each other (10 comparison points) . • Providing additional data elements that may not be as easily available at the time of record creation (e.g., for a physician, this may include medical license status; for a company, this may include a six digit NAICS industry classification). As an organization’s records match and reconcile with the reference directories, the trusted record will deviate from the reference directory with traceability to other source records, contributing attributes, and transformation rules. 1.3.4 Data Sharing Architecture There are several basic architectural approaches to reference and Master Data integration . Each Master Data subject area will likely have its own system of record. For example, the human resource system usually serves as the system of record for employee data. A CRM system might serve as the system of record for customer data, while an ERP system might serve as the system of record for financial and product data. The data sharing hub architecture model shown in Figure 77 represents a hub -and-spoke architecture for Master Data. The Master Data hub can handle interactions with spoke items such as source systems, business applications, and data stores while minimizing the number of integration points. A local data hub can ex tend and scale the Master Data hub. ( See Chapter 8 .) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "350 • DMBOK2 Figure 77 Master Data Sharing Architecture Example Each of the three basic approaches to implementing a Master Data hub environment has pros and cons: • A Registry is an index that points to Master Data in the various systems of record. The systems of record manage Master Data local to their applications. Access to Master Data comes from the master index. A registry is relatively easy to implement because it require s few changes in the systems of record. But often, complex queries are required to assemble Master Data from multiple systems. Moreover, multiple business rules need to be implemented to address semantic differences across systems in multiple places. • In a Transaction Hub , applications interface with the hub to access and update Master Data. The Master Data exists within the Transaction Hub and not within any other applications. The Transaction Hub is the system of record for Master Data. Transaction Hubs enable better governance and provide a consistent source of Master Data. However, it is costly to remove the functionality to update Master Data from existing systems of record. Business rules are implemented in a single system: the Hub. • A Consolidated approach is a hybrid of Registry and Transaction Hub. The systems of record manage Master Data local to their applications. Master Data is consolidated within a common repository and made available from a data- sharing hub, the system of reference for Mast er Data. This eliminates the need to access directly from the systems of record. The Consolidated approach provides an enterprise view with limited impact on systems of record. However, it entails replication of data and there will be latency between the hub and the systems of record. ODS DW (local) MDS App App App App App App DM External Partners External Partners LZ LDS (optional) DM DW (enterprise)MDS LDS ODS DW App LZ DM Cloud EnvironmentMaster Data Hub EnvironmentB2B Environment Master Data Sharing Hub Local Data Sharing Hub Operational Data Store Data Warehouse Application Solution Landing Zone Data Mart Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 351 2. Activities As emphasized in Section 1.3.1 , Master Data and Reference Data share certain characteristics (they are shared resources that provide context and meaning for other data and should be managed at the enterprise level), but they also differ in important ways (Reference Data sets are smaller, less volat ile, do not require matching, merging, and linking, etc.). The activities section will first describe activities associated with MDM, and then describe those related to Reference Data. 2.1 MDM Activities 2.1.1 Define MDM Drivers and Requirements Each organization has different MDM drivers and obstacles, influenced by the number and type of systems, their age, the business processes they support, and how data is used for both transactions and analytics. Drivers often include opportunities to improve customer service and/or operational efficiency, as well as to reduce risks related to privacy and compliance. Obstacles include differences in data meaning and structure between systems. These are often tied to cultural barriers – some business units may not want to incur the costs of changing their processes, even if change is presented as good for the enterprise as a whole. It is relatively easy to define requirements for Master Data within an application. It is more difficult to define standard requirements across applications. Most organizations will want to approach one Master Data subject area, or even one entity, at a ti me. Prioritize Master Data efforts based on cost / benefit of proposed improvements and on the relative complexity of the Master Data subject area. Start with the simplest category in order to learn from the process. 2.1.2 Evaluate and Assess Data Sources Data in existing applications forms the basis of a Master Data Management effort. It is important to understand the structure and content of this data and the processes through which it is collected or created. One outcome from an MDM effort can be improvement s in Metadata generated through the effort to assess the quality of existing data. One goal of assessment is to understand how complete data is with respect to the attributes that comprise Master Data. This process includes clarifying the definitions and g ranularity of those attributes. Semantic issues will arise at some point when defining and describing attributes. The Data Stewards will need to collaborate with the business areas on reconciliation and agreement on attribute naming and enterprise level definitions. (See Chapters 3 and 13.) The other part of assessing sources is to understand the quality of the data. Data Quality problems will complicate a Master Data project, so the assessment process should include addressing root causes of data issues. Never assume that data will be of high quality – it is safer to assume that is it not of high quality. Always assess its qualit y and suitability for a Master Data environment. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "352 • DMBOK2 The biggest challenge, as noted, will be disparity between sources. Data may be of high quality within any given source, but still not fit together with data from other sources, due to structural differences and differences in the values by which similar attributes are represented. Master Data initiatives provide the opportunity to define and implement standards in applications in which data is created or collected. For some Master Data entities, such as client, customer, or vendor, it is possible to purchase standardized data (such as Reference Directories) to enable the MDM effort. Several vendors have services that will supply cleansed data related to individual pe ople or business entities or professions (e.g., health care professionals), that can be compared to an organization’s internal data to improve contact information, addresses, and names (see Chapter 10) . In addition to assessing the quality of existing data , it also necessary to understand the technology that supports the collection of inputs to an MDM effort. Existing technology will influence the architectural approach to MDM. 2.1.3 Define Architectural Approach The architectural approach to MDM depends on business strategy, the platforms of existing data sources, and the data itself, particularly its lineage and volatility, and the implications of high or low latency. Architecture must account for data consumptio n and sharing models. Tooling for maintenance depends on both business requirements and architecture options. Tooling helps define and is dependent on the approach to stewardship and maintenance. The number of source systems to be integrated into the Master Data solution and the platforms of those systems need to be accounted for when determining the approach to integration. The size and geographic spread of an organization will also influence the integration approach. Small organizations may effectively utilize a transaction hub whereas a global organization with multiple systems is more likely to utilize a registry. An organization with ‘siloed’ business units and various source systems may decide that a consolidated approach is the correct path to follow. Business domain experts, Data Architects, and Enterprise Architects should provide perspective on approach. The data sharing hub architecture is particularly useful when there is no clear system of record for Master Data. In this case, multiple systems supply data. New data or updates from one system can be reconciled with data already supplied by another system . The data -sharing hub becomes the source of Master Data content for data warehouses or marts, reducing the complexity of extracts and the processing time for data transformation, remediation, and reconciliation. Of course, data warehouses must reflect changes made to the data -sharing hub for historical purposes, while the data -sharing hub itself may need to reflect just the current state. 2.1.4 Model Master Data Master Data Management is a data integration process . To achieve consistent results and to manage the integration of new sources as an organization expands, it is necessary to model the data within subject areas. A logical or canonical model can be defined over the subject areas within the data -sharing hub. This would allow establishment of enterprise level definitions of subject area entities and attributes. ( See Chapter s 5 and 8.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 353 2.1.5 Define Stewardship and Maintenance Processes Technical solutions can do remarkable work matching, merging, and managing identifiers for master records. However, the process also requires stewardship , not only to address records that fall out of the process, but also to remediate and improve the processes that cause them to fall out in the first place. MDM projects should account for resources required to support the ongoing quality of Master Data. Th ere is a need to analyze records, provide feedback to source systems, and provide input that can be used to tune and improve the algorithms that drive the MDM solution. 2.1.6 Establish Governance Policies to Enforce Use of Master Data The initial launch of a Master Data effort is challenging and takes a lot of focus. The real benefits (operational efficiency, higher quality, better customer service) come once people and systems start using the Master Data. The overall effort has to include a roadmap for systems to adopt master values and identifiers as input to processes. Establish unidirectional closed loops between systems to maintain consistency of values across systems. 2.2 Reference Data Activities 2.2.1 Define Drivers and Requirements The primary drivers for Reference Data Management are operational efficiency and higher Data Quality . Managing Reference Data centrally is more cost effective than having multiple business units maintain their own data sets. It also reduces the risk of inconsistency between systems. That said, some Reference Data sets are more important than others; comp lex Reference Data sets take more work to set up and maintain than do simple ones. The most important Reference Data sets should drive requirements for a Reference Data Management system. Once such a system is in place, new Reference Data sets can be set u p as part of projects. Existing Reference Data sets should be maintained based on a published schedule. 2.2.2 Assess Data Sources Most industry standard Reference Data sets can be obtained from the organizations that create and maintain them. Some organizations supply such data free of charge. Others charge a fee. Intermediaries also package and sell Reference Data, often with value- added features. Depending on the number and type of Reference Data sets needed by an organization, it may be better to purchase from a vendor, especially if that vendor will guarantee the delivery of updates on a set schedule and will perform basic quality control on the data. Most organizations also rely on Reference Data that is internally created and maintained. Determining the source for internal or local reference data is often more challenging than doing so for industry standard Reference Data. As is the case with Master Data, internal sources for Reference Data must be identified, Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "354 • DMBOK2 compared, and assessed. Owners of existing data must understand the benefits of central management and agree to support processes to steward the data for the good of the enterprise. 2.2.3 Define Architectural Approach Before purchasing or building a tool to manage Reference Data, it is critical to account for requirements and for the challenges posed by the Reference Data to be managed. For example, the volatility of data (most Reference Data is relatively static, but some is quite volatile), the frequency of updates, and the consumption models. Determine whether it is required to keep historical data on changes to the values or the definitions of the values. If the organization will purchase data from a vendor, account for the delivery and integration method. The architectural approach needs to recognize that, invariably, some Reference Data will need to be updated manually. Ensure that the interface for updates is straightforward and can be configured to enforce basic data entry rules, such as ensuring parent/ child relationships are maintained in Reference Data that includes hierarchies. The RDM tool should enable Stewards to make ad hoc updates without the need for technical support and should include workflows to ensure approvals and notifications are automat ed. Data Stewards should schedule known updates to align with the publication of new codes. Data consumers should be informed of all changes. In cases where Reference Data drives programming logic, the potential impact of changes should be assessed and acc ounted for before the changes are introduced. 2.2.4 Model Reference Data Sets Many people think of Reference Data as simply codes and descriptions. However, much Reference Data is more complicated than that. For example, a ZIP Code data set will usually include information on state and county, as well as other geo -political attributes. For purposes of enabling long -term usage and establishing accurate Metadata, as well as for the maintenance process itself, it is valuable to create data models of Reference Data sets. Models help data consumers understand the relationships within the Reference Data set and they can be used to establish Data Quality rules. 2.2.5 Define Stewardship and Maintenance Processes Reference Data requires stewardship to ensure that values are complete and current and that definitions are clear and understandable. In some cases, stewards will be directly responsible for hands- on maintenance of Reference Data; in other cases, they may facilitate the process. For example, if several different business units require Reference Data to support the same concept, a steward may facilitate discussions that define common values in a crosswalk. As part of the stewardship process, it is helpful to capture basic Metadata about each Reference Data set. This could include: steward name, originating organization, expected frequency of updates, schedule for updates, processes using the Reference Data, whether historical versions of the data need to be retained, and more (see Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 355 Section 1.3.2.6). Documenting what processes use Reference Data will enable more effective communication regarding changes to the data. Many Reference Data Management tools include workflows to manage review and approval of changes to Reference Data. These workflows themselves depend on identifying who within an organization is responsible for Reference Data content. 2.2.6 Establish Reference Data Governance Policies An organization only gets value from a centrally- managed Reference Data repository if people actually use the data from that repository. It is important to have policies in place that govern the quality and mandate the use of Reference Data from that repository, whether directly through publication from that repository or indirectly from a system of reference that is populated with data from the central repository. 3. Tools and Techniques MDM requires tooling specifically designed to enable identity management. Master Data Management can be implemented through data integration tools, data remediation tools, operational data stores (ODS), data sharing hubs (DSH) or specialized MDM applications. Several vendors offer solutions that can cover one or mo re Master Data subject areas. Other vendors promote use of their data integration software products and implementation services to create custom Master Data solutions. Packaged solutions for product, account and party as well as packaged Data Quality check services can jumpstart large programs. Incorporation of such services can enable organizations to use best- of-breed solutions, while integrating them to their overall business architecture to meet specific needs. 4. Implementation Guidelines Master and Reference Data Management are forms of data integration. The implementation principles that apply to data integration and interoperability apply to MDM and RDM. ( See Chapter 8 .) MDM and RDM capabilities cannot be implemented overnight. Solutions require specialized business and technical knowledge. Organizations should expect to implement Reference and Master Data solutions incrementally through a series of projects defined in an implementation roadmap, prioritized based on business needs and guided by an overall architecture. Note that MDM programs will fail without proper governance. Data Governance professionals must understand the challenges of MDM and RDM and assess the organization’s maturity and abili ty to meet them. (See Chapter 15.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "356 • DMBOK2 4.1 Adhere to Master Data Architecture Establishing and following proper reference architecture is critical to managing and sharing Master Data across an organization. The integration approach should take into account the organizational structure of the business, the number of distinct systems of record, the Data Governance implementation, the importance of access and latency of data values, and the number of consuming systems and applications. 4.2 Monitor Data Movement Data integration processes for Master and Reference Data should be designed to ensure timely extraction and distribution of data across the organization. As data flows within a Reference or Master Data sharing environment , data flow should be monitored in order to: • Show how data is shared and used across the organization • Identify data lineage from / to administrative systems and applications • Assist root cause analysis of issues • Show effectiveness of data ingestion and consumption integration techniques • Denote latency of data values from source systems through consumption • Determine validity of business rules and transformations executed within integration components 4.3 Manage Reference Data Change Since Reference Data is a shared resource, it cannot be changed arbitrarily. The key to successful Reference Data Management is organizational willingness to relinquish local control of shared data. To sustain this support, provide channels to receive and respond to requests for changes to Reference Data. The Data Governance Council should ensure that policies and procedures are implemented to handle changes to data within reference and Master Data environments. Changes to Reference Data will need to be managed. Minor changes may affect a few rows of data. For example, when the Soviet Union broke into independent states, the term Soviet Union was deprecated and new codes were added. In the healthcare industry, procedure and diagnosis codes are updated annually to account for refinement of existing codes, obsoleting of codes, and the introduction of new codes. Major revisions to Reference Data impact data structure. For example, ICD -10 Diagnostic Codes are structured in ways very different from ICD -9. ICD10 has a different format. There are different values for the same concepts. More importantly, ICD -10 has additional principles of organization. ICD10 codes have a different granularity and are much more specific, so more information is conveyed in a single code. Consequently, there are many more of them (as of 2015, there were 68,000 ICD- 10 codes, compared with 13,000 ICD -9s). 62 The mandated use of ICD -10 codes in the US in 2015 required significant planning. Healthcare companies needed to make system changes as well as adjustments to impacted reporting to account for the new standard. 62 http://bit.ly/1SSpds9 (accessed 8/13/16). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 357 Types of changes include: • Row level changes to external Reference Data sets • Structural changes to external Reference Data sets • Row level changes to internal Reference Data sets • Structural changes to internal Reference Data sets • Creation of new Reference Data sets Changes can be planned / scheduled or ad hoc. Planned changes, such as monthly or annual updates to industry standard codes, require less governance than ad hoc updates. The process to request new Reference Data sets should account for potential uses beyond those of the original requestor. Change requests should follow a defined process, as illustrated in Figure 78. When requests are received, stakeholders should be notified so that impacts can be assessed. If changes need approval, discussions should be held to get that approval. Changes should be communicated. Figure 78 Reference Data Change Request Process 4.4 Data Sharing Agreements Sharing and using Reference and Master Data across an organization requires collaboration between multiple parties internal to the organization and sometimes with parties external to it. To assure proper access and use, establish sharing agreements that stipulate what data can be shared and under what conditions. Having these agreements in place will help when issues arise regarding the availability of data within or quality of data brought into the data -sharing environment. This effort should be driven by the Data Governance function. It may involve Data Architects, Data Providers, Data Stewards, Application Developers, Business Analysts as well as Compliance / Privacy Officers and Security Officers. Those responsible for the data -sharing environment have an obligation to downstream data consumers to provide high quality data. To fulfill this responsibility, they are dependent on upstream systems. SLA’s and metrics should be established to measure the availability and quality of shared data. Processes should be put in place to address the root causes of issues with Data Quality or availability. A standard approach to communications should be put in place to keep all affected parties informed about the e xistence of issues and the status of remediation efforts. (See Chapter 8.) Update and Inform (If applicable)Decide and CommunicateIdentify ImpactIdentify StakeholderReceive Change Request Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "358 • DMBOK2 5. Organization and Cultural Change Reference and Master Data Management require people to relinquish control of some of their data and processes in order to create shared resources. It is not always easy to do this. While data management professionals can see that locally managed data is ri sky, the people who manage it locally need to get their work done and they may perceive MDM or RDM efforts as adding complication to their processes. Fortunately, most people recognize that these efforts make fundamental sense. It is better to have one accurate and complete view of a single customer than to have multiple partial views. Improving the availability and quality of reference and Master Data will undoubtedly require changes to procedures and traditional practices. Solutions should be scoped and implemented based on current organizational readiness and future needs tied to the organization’s mission and vision. Perhaps the most challenging cultural change is central to governance: Determining which individuals are accountable for which decisions – business Data Stewards, Architects, Managers, and Executives – and which decisions data stewardship teams, program steering committees , and the Data Governance Council should make collaboratively. 6. Reference and Master Data Governance Because they are shared resources, Reference and Master Data require governance and stewardship. Not all data inconsistencies can be resolved through automation. Some require that people talk to each other. Without governance, R eference and Master Data solutions will just be additional data integration utilities, unable to deliver their full potential. Governance processes will determine: • The data sources to be integrated • The Data Quality rules to be enforced • The conditions of use rules to be followed • The activities to be monitored and the frequency of monitoring • The priority and response levels of data stewardships efforts • How information is to be represented to meet stakeholder needs • Standard approval gates, expectations in RDM and MDM deployment Governance processes also bring compliance and legal stakeholders together with information consumers to ensure organizational risks are mitigated through definition and incorporation of privacy, security, and retention policies. As an ongoing process, Data Governance must have the ability to review, receive , and consider new requirements and changes to existing rules, while making principles, rules, and guidelines available to those using Reference and Master Data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "REFERENCE AND MASTER DATA • 359 6.1 Metrics Certain metrics can be tied to Reference and Master Data Quality and the processes that support these efforts: • Data Quality and compliance : DQ dashboards can describe the quality of R eference and Master Data. These metrics should denote the confidence (as a percentage) of a subject area entity or associated attribute and its fit-for- purpose for use across the organization. • Data change activity : Auditing the lineage of trusted data is imperative to improving Data Quality in a data- sharing environment. Metrics should denote the rate of change of data values. These metrics will provide insight to the systems supplying data to the sharing environment, and can be used to tune algorithms in MDM processes. • Data ingestion and consumption: Data is supplied by upstream systems and used by downstream systems and processes. These metrics should denote and track what systems are contributing data and what business areas are subscribing data from the sharing environment. • Service Level Agreements : SLAs should be established and communicated to contributors and subscribers to ensure usage and adoption of the data -sharing environment. The level of adherence to SLAs can provide insight into both support processes and the technical and data problems t hat might slow down the MDM application. • Data Steward coverage : These metrics should note the name or group responsible for data content, and how often the coverage is evaluated. They can be used to identify gaps in support. • Total Cost of Ownership : There are multiple factors of this metric and different ways to represent it. From a solution view, costs can include environment infrastructure, software licenses, support staff, consulting fees, training, etc. Effectiveness of this metric is largely base d on its consistent application across the organization. • Data sharing volume and usage : Data ingestion and consumption volumes need to be tracked to determine the effectiveness of the data- sharing environment. These metrics should denote the volume and velocity of data defined, ingested, and subscribed to and from the data -sharing environment. 7. Works Cited / Recommended Abbas, June. Structures for Organizing Knowledge: Exploring Taxonomies, Ontologies, and Other Schema . Neal -Schuman Publishers, 2010. Print. Abernethy, Kenneth and J. Thomas Allen. Exploring the Digital Domain: An Introduction to Computers and Information Fluency . 2nd ed., 2004. Print. Allen Mark and Dalton Cervo. Multi -Domain Master Data Management: Advanced MDM and Data Governance in Practice . Morgan Kaufmann, 2015. Print. Bean, James. XML for Data Architects: Designing for Reuse and Integration . Morgan Kaufmann, 2003. Print. The Morgan Kaufmann Series in Data Management Systems. Berson, Alex and Larry Dubov. Master Data Management and Customer Data Integration for a Global Enterprise . McGraw -Hill, 2007. Print. Brackett, Michael. Data Sharing Using a Common Data Architecture. Wiley, 1994. Print. Wiley Professional Computing. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "360 • DMBOK2 Cassell, Kay Ann and Uma Hiremath. Reference and Information Services: An Introduction . 3d ed. ALA Neal -Schuman, 2012. Print. Cervo, Dalton and Mark Allen. Master Data Management in Practice: Achieving True Customer MDM. Wiley, 2011. Print. Chisholm, Malcolm. “What is Master Data?” BeyeNetwork, February 6, 2008. http://bit.ly/2spTYOA Web . Chisholm, Malcolm. Managing Reference Data in Enterprise Databases: Binding Corporate Data to the Wider World . Morgan Kaufmann, 2000. Print. The Morgan Kaufmann Series in Data Management Systems. Dreibelbis, Allen, et al. Enterprise Master Data Management: An SOA Approach to Managing Core Information. IBM Press, 2008. Print. Dyche, Jill and Evan Levy. Customer Data Integration: Reaching a Single Version of the Truth . John Wiley and Sons, 2006. Print. Effingham, Nikk. An Introduction to Ontology . Polity, 2013. Print. Finkelstein, Clive. Enterprise Architecture for Integration: Rapid Delivery Methods and Techniques . Artech House Print on Demand, 2006. Print. Artech House Mobile Communications Library. Forte, Eric J., et al. Fundamentals of Government Information: Mining, Finding, Evaluating, and Using Government Resources . Neal - Schuman Publishers, 2011. Print. Hadzic, Fedja, Henry Tan, Tharam S. Dillon. Mining of Data with Complex Structures . Springer, 2013. Print. Studies in Computational Intelligence. Lambe, Patrick. Organising Knowledge: Taxonomies, Knowledge and Organisational Effectiveness . Chandos Publishing, 2007. Print. Chandos Knowledge Management. Loshin, David. Enterprise Knowledge Management: The Data Quality Approach . Morgan Kaufmann, 2001. Print. The Morgan Kaufmann Series in Data Management Systems. Loshin, David. Master Data Management. Morgan Kaufmann, 2008. Print. The MK/OMG Press. Menzies, Tim, et al. Sharing Data and Models in Software Engineering . Morgan Kaufmann, 2014. Print. Millett, Scott and Nick Tune. Patterns, Principles, and Practices of Domain- Driven Design . Wrox, 2015. Print. Stewart, Darin L. Building Enterprise Taxonomies . Mokita Press, 2011. Print. Talburt, John and Yinle Zhou. Entity Information Management Lifecycle for Big Data . Morgan Kauffman, 2015. Print. Talburt, John. Entity Resolution and Information Quality . Morgan Kaufmann, 2011. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "361 CHAPTER 1 1 Data Warehousing and Business Intelligence 1. Introduction he concept of the Data Warehouse emerged in the 1980s as technology enabled organizations to integrate data from a range of sources into a common data model. Integrated data promised to provide insight into operational processes and open up new possibilities for leveraging data to make d ecisions and create organizational value. As importantly, data warehouses were seen as a means to reduce the proliferation of decision support systems (DSS) , most of which drew on the same core enterprise data. The concept of an enterprise warehouse promised a way to reduce data redundancy, improve the consistency of information, and enable an enterprise to use its data to make better decisions. Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International T Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "362 • DMBOK2 Figure 79 Context Diagram: DW/BI Data warehouses began to be built in earnest in the 1990s. Since then (and especially with the co -evolution of Business Intelligence as a primary driver of business decision- making), data warehouses have become Participants : •Sponsors & Product Owner •Architects and Analysts •DW/BI Specialists (BI Platform, Data Storage) •Project Manager •Change ManagerDefinition : Planning, implementation, and managing an integrated data system to support knowledge workers engaged in reporting, query, and analysis. Goals : 1. T o build and maintain the data system with the technical and business requirements needed to deliver integrated data that supports operational functions, compliance, and business intelligence. 2. T o create insights to support and enable effective business analysis and decision making. Activities : 1.Understand Requirements (P) 2.Define and Maintain the DW and BI Architecture (P) 3.Develop the Data Warehouse and Data Marts (D) 4.Populate the Data Warehouse (D) 5.Implement the Business Intelligence Portfolio (D) 6.Maintain Data Products (O)Inputs : •Business Requirements •Scalability, Operational, Infrastructure & Support Requirements •Data Quality, Security and Access Requirements •IT Strategy •Related IT Policies & Standards •Internal Data Feeds •Master and Reference Data •Industry and External DataDeliverables : •DW and BI Architecture •Data Products •Population Process •Governance Activities •Data Lineage •Training and Adoption Plan •Release Plan •Production Support Process •Load T uning Activities •BI Activity Monitoring •Reporting Strategy Suppliers : •Business Executive •Governance Body •Enterprise Architecture •Data Producers •Information Consumers •Subject Matter ExpertsConsumers : •Information Consumers •Customers •Partners •Managers and Executives •Subject Matter Experts T echniques : •Prototypes to Drive Requirements •Self-Service BI •Queryable Audit DataT ools : •Metadata Repositories •Data Integration T ools •Business Intelligence T oolsMetrics : •Usage Metrics •Subject Area Coverage Percentages •Response/Performance Metrics (P) Planning, (C) Control, (D) Development, (O) OperationsData Warehousing and Business Intelligence T echnical DriversBusiness Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 363 ‘mainstream’. Most enterprises have data warehouses and warehousing is the recognized core of enterprise data management.63 Even though well established, the data warehouse continues to evolve. As new forms of data are created with increasing velocity, new concepts, such as data lakes, are emerging that will influence the future of the data warehouse. See Chapter s 8 and 15. 1.1 Business Drivers The primary driver for data warehousing is to support operational functions, compliance requirements, and Business Intelligence (BI) activities (though not all BI activities depend on warehouse data). Increasingly organizations are being asked to provide data as evidence that they have complied with regulatory requirements. Because they contain historical data, warehouses are often the means to respond to such requests. Nevertheless, Business Intelligence support continues to be the primary reason for a war ehouse. BI promises insight about the organization, its customers, and its products. An organization that acts on knowledge gained from BI can improve operational efficiency and competitive advantage. As more data has become available at a greater velocity, BI has evolved from retrospective assessment to predictive analytics. 1.2 Goals and Principles Organizations implement data warehouses in order to: • Support Business Intelligence activity • Enable effective business analysis and decision- making • Find ways to innovate based on insights from their data The implementation of a Data Warehouse should follow these guiding principles: • Focus on business goals: Make sure DW serves organizational priorities and solves business problems. • Start with the end in mind : Let the business priority and scope of end -data- delivery in the BI space drive the creation of the DW content. • Think and design globally; act and build locally : Let end -vision guide the architecture, but build and deliver incrementally, through focused projects or sprints that enable more immediate return on investment. • Summarize and optimize last, not first : Build on the atomic data. Aggregate and summarize to meet requirements and ensure performance, not to replace the detail. • Promote transparency and self -service : The more context (Metadata of all kinds) provided, the better able data consumers will be to get value out of the data. Keep stakeholders informed about the data and the processes by which it is integrated. • Build Metadata with the warehouse : Critical to DW success is the ability to explain the data. For example, being able to answer basic questions like “Why is this sum X?” “How was that computed?” 63 http://bit.ly/2sVPIYr . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "364 • DMBOK2 and “Where did the data come from?” Metadata should be captured as part of the development cycle and managed as part of ongoing operations. • Collaborate : Collaborate with other data initiatives, especially those for Data Governance, Data Quality, and Metadata. • One size does not fit all : Use the right tools and products for each group of data consumers. 1.3 Essential Concepts 1.3.1 Business Intelligence The term Business Intelligence (BI) has two meanings. First, it refers to a type of data analysis aimed at understanding organizational activities and opportunities. Results of such analysis are used to improve organizational success. When people say that data holds the key to competit ive advantage, they are articulating the promise inherent in Business Intelligence activity: that if an organization asks the right questions of its own data, it can gain insights about its products, services, and customers that enable it to make better decisions about how to fulfill its strategic objectives. Secondly, Business Intelligence refers to a set of technologies that support this kind of data analysis. An evolution of decisions support tools, BI tools enable querying, data mining, statistical analysis, reporting, scenario modeling, data visualization, and dashboarding. They are used for everything from budgeting to advanced analytics. 1.3.2 Data Warehouse A Data Warehouse (DW) is a combination of two primary components: An integrated decision support database and the related software programs used to collect, cleanse, transform, and store data from a variety of operational and external sources. To support historical, analytical, and BI requirements, a data warehouse may also include dependent data marts, which are subset copies of data from the warehouse. In its broadest context, a data warehouse includes any data stores or extracts used to support the deli very of data for BI purposes. An Enterprise Data Warehouse (EDW) is a centralized data warehouse designed to service the BI needs of the entire organization. An EDW adheres to an enterprise data model to ensure consistency of decision support activities across the enterprise. 1.3.3 Data Warehousing Data Warehousing describes the operational extract, cleansing, transformation, control, and load processes that maintain the data in a data warehouse. The data warehousing process focuses on enabling an integrated and historical business context on operational data by enforcing business rules and maintaining appropriate business data relationships. Data warehousing also includes processes that interact with Metadata repositories. Traditionally, data warehousing focuses on structured data: elements in defined fields, whether in files or tables, as documented in data models. With recent advances in technology, the BI and DW space now embraces semi - Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 365 structured and unstructured data. Semi -structured data, defined as electronic elements organized as semantic entities with no required attribute affinity, predates XML but not HTML; an EDI transfer could serve as an example. Unstructured data refers to data that is not predefined through a data model. Because unstructured data exists in a range of formats and encompasses items such as e -mail, free format text, business documents, videos, photos, and web pages to name a few, defining a feasible storage const ruct that sustains analytic workloads within warehousing governance has been a challenge yet to be overcome. 1.3.4 Approaches to Data Warehousing Much of the conversation about what constitutes a data warehouse has been driven by two influential thought leaders – Bill Inmon and Ralph Kimball – who have different approaches to modeling and developing warehouses. Inmon defines a data warehouse as “a subject -oriented, integrated, time -variant and non -volatile collection of data in support of management’s decision -making process .”64 A normalized relational model is used to store and manage data. Kimball defines a warehouse as “ a copy of transaction data specifically structured for query and analysis.” Kimball’s approach calls for a dimensional model. (See Chapter 5.) While Inmon and Kimball advocate different approaches to building warehouses, their definitions recognize similar core ideas: • Warehouse s store data from other systems • The act of storage includes organizing the data in ways that increase its value • Warehouses make data acc essible and usable for analysis • Organizations build warehouses because they need to make reliable, integrated data available to authorized stakeholders • Warehouse data serves many purposes, from support of workflow to operational mana gement to predictive analytics 1.3.5 Corporate Information Factory (Inmon) Bill Inmon’s Corporate Information Factory (CIF) is one of the two primary patterns for data warehousing. The component parts of Inmon’s definition of a data warehouse , “a subject oriented, integrated, time variant, and nonvolatile collection of summary and detailed historical data,” describe the concepts that support the CIF and point to the differences between warehouses and operational systems. • Subject -oriented : The data warehouse is organized based on major business entities, rather than focusing on a functional or application. • Integrated : Data in the warehouse is unified and cohesive. The same key structures, encoding and decoding of structures, data definitions, naming conventions are applied consistently throughout the warehouse. Because data is integrated, Warehouse data is not simply a copy of operational data. Instead, the warehouse becomes a system of record for the data. 64 http://bit.ly/1FtgeIL, last accessed 2/27/2016. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "366 • DMBOK2 • Time variant : The data warehouse stores data as it exists in a set point in time. Records in the DW are like snapshots. Each one reflects the state of the data at a moment of time. This means that querying data based on a specific time period will always produce the s ame result, regardless of when the query is submitted. • Non- volatile: In the DW, records are not normally updated as they are in operational systems. Instead, new data is appended to existing data. A set of records may represent different states of the same transaction. • Aggregate and detail data : The data in the DW includes details of atomic level transactions, as well as summarized data. Operational systems rarely aggregate data. When warehouses were first established, cost and space considerations drove the need to summarize data. Summarized da ta can be persistent (stored in a table) or non- persistent (rendered in a view) in contemporary DW environments. The deciding factor in whether to persist data is usually performance. • Historical : The focus of operational systems is current data. Warehouses contain historical data as well. Often they house vast amounts of it. Inmon, Claudia Imhoff , and Ryan Sousa describe data warehousing in the context of the Corporate Information Factory (CIF). See Figure 80. CIF components include: • Applications : Applications perform operational processes. Detail data from applications is brought into the data warehouse and the operational data stores (ODS) where it can be analyzed. • Staging Area : A database that stands between the operational source databases and the target databases. The data staging area is where the extract, transform, and load effort takes place. It is not used by end users. Most data in the data staging area is transient, al though typically there is some relatively small amount of persistent data. • Integration and transformation: In the integration layer, data from disparate sources is transformed so that it can be integrated into the standard corporate representation / model in the DW and ODS. • Operational Data Store (ODS) : An ODS is integrated database of operational data. It may be sourced directly from applications or from other databases. ODS’s generally contain current or near term data (30-90 days), while a DW contains historical data as well (often several years of d ata). Data in ODS’s is volatile, while warehouse data is stable. Not all organizations use ODS’s. They evolved as to meet the need for low latency data. An ODS may serve as the primary source for a data warehouse; it may also be used to audit a data warehouse. • Data marts : Data marts provide data prepared for analysis. This data is often a sub -set of warehouse data designed to support particular kinds of analysis or a specific group of data consumers. For example, marts can aggregate data to support faster analysis. Dimensional modeling (using denormalization techniques) is often used to design user -oriented data marts. • Operational Data Mart (OpDM) : An OpDM is a data mart focused on tactical decision support. It is sourced directly from an ODS, rather than from a DW. It shares characteristics of the ODS: it contains current or near- term data. Its contents are volatile. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 367 • Data Warehouse : The DW provides a single integration point for corporate data to support management decision -making, and strategic analysis and planning. The data flows into a DW from the application systems and ODS, and flows out to the data marts, usually in one direc tion only. Data that needs correction is rejected, corrected at its source, and ideally re -fed through the system. • Operational reports : Reports are output from the data stores. • Reference, Master, and external data : In addition to transactional data from applications, the CIF also includes data required to understand transactions, such as reference and Master Data. Access to common data simplifies integration in the DW. While applications consume current master and Reference Data, the DW also requires historical values and the timeframes during which they were valid (see Chapter 10). Figure 80 depicts movement within the CIF, from data collection and creation via applications (on the left) to the creation of information via marts and analysis (on the right). Movement from left to right includes other changes. For example, • The purpose shifts from execution of operational functions to analysis • End users of systems move from front line workers to decision- makers • System usage moves from fixed operations to ad hoc uses • Response time requirements are relaxed (strategic decisions take more time than do daily operations) • Much more data is involved in each operation, query, or process The data in DW and marts di ffers from that in applications: • Data is organized by subject rather than function • Data is integrated data rather than ‘siloed’ • Data is time -variant vs. current -valued only • Data has higher latency in DW than in applications • Significantly more historical data is available in DW than in applications Figure 80 The Corporate Information Factory Reference Data DM DM DM DW Op DM ODS App App App App Analysis Historical Reference Data Operational Reports (per App)Operational Reports (integrated) Integration & Transformation Raw Detailed Data Exploratory Analysis Operational AnalysisApplicationsData Marts Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "368 • DMBOK2 1.3.6 Dimensional DW (Kimball) Kimball’s Dimensional Data Warehouse is the other primary pattern for DW development. Kimball defines a data warehouse simply as “a copy of transaction data specifically structured for query and analysis” (Kimball, 2002). The ‘copy’ is not exact, however. Warehouse data is stored in a dimens ional data model. The dimensional model is designed to enable data consumers to understand and use the data, while also enabling query performance. 65 It is not normalized in the way an entity relationship model is. Often referred to as Star Schema , dimensional models are comprised of facts , which contain quantitative data about business processes (e.g., sales numbers), and dimensions, which store descriptive attributes related to fact data and allow data consumers to answer questions about the facts (e.g., how many units of product X were sold this quarter?) A fact table joins with many dimension tables, and when viewed as a diagram, appears as a star. (See Chapter 5 .) Multiple fact tables will share the common, or conformed, dimensions via a ‘bus’, similar to a bus in a computer. 66 Multiple data marts can be integrated at an enterprise level by plugging into the bus of conformed dimensions. The DW bus matrix shows the intersection of business processes that generate fact data and data subject areas that represent dimensions. Opportunities for conformed dimensions exist where multiple processes use the same data. Table 27 is a sample bus matrix. In this example, the business processes for Sales, Inventory, and Orders all require Date and Product data. Sales and Inventory both require Store data, while Inventory and Orders require Vendor data. Date, Product, Store , and Vendor are all candidates for conformed dimensions. In contrast, Warehouse is not shared; it is used only by Inventory. Table 27 DW-Bus Matrix Example Subject Areas Business Processes Date Product Store Vendor Warehouse Sales X X X Inventory X X X X X Orders X X X Conformed Dimension Candidate Yes Yes Yes Yes No The enterprise DW bus matrix can be used to represent the long -term data content requirements for the DW/BI system, independent of technology. This tool enables an organization to scope manageable development efforts. Each implementation builds an increment of the overall architecture. At some point, enough dimensional schemas exist to make good on the promise of an integrated enterprise data warehouse environment. Figure 81 represents Kimball’s Data Warehouse Chess Pieces view of DW/BI architecture. Note that Kimball’s Data Warehouse is more expansive than Inmon’s. The DW encompasses all components in the data staging and data presentation areas. 65 http://bit.ly/1udtNC8 . 66 The term bus came from Kimball’s electrical engineering background, where a bus was something providing common power to a number of electrical components. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 369 • Operational source systems : Operational / transactional applications of the Enterprise. These create the data that is integrated into the ODS and DW. This component is equivalent to the application systems in the CIF diagram. • Data staging area: Kimball’s staging includes the set of processes needed to integrate and transform data for presentation. It can be compared to a combination of CIF’s integration, transformation, and DW components. Kimball’s focus is on efficient end -delivery of the anal ytical data, a scope smaller than Inmon’s corporate management of data. Kimball’s enterprise DW can fit into the architecture of the data staging area. • Data presentation area : Similar to the Data Marts in the CIF. The key architectural difference being an integrating paradigm of a ‘DW Bus,’ such as shared or conformed dimensions unifying the multiple data marts. • Data access tools : Kimball’s approach focuses on end users’ data requirements. These needs drive the adoption of appropriate data access tools. Figure 81 Kimball's Data Warehouse Chess Pieces67 1.3.7 DW Architecture Components The data warehouse environment includes a collection of architectural components that need to be organized to meet the needs of the enterprise. Figure 82 depicts the architectural components of the DW/BI and Big Data Environment discussed in this section. The evolution of Big Data has changed the DW/BI landscape by adding another path through which data may be brought into an enterprise. 67 Adapted from Kimball and Ross (2002). Used with permission . Operational Source SystemsData Staging AreaData Presentation AreaData Access T ools Extract Extract Extract ExtractLoad Load Load LoadData Mart #1 Data Mart #2 Data Mart #NAccess Access Access AccessSERVICES : Clean Combine Standardize Conform Dimensions REPORT WRITERS ANALYTICAL APPLICATIONS MODELS: Forecasting Scoring Data MiningAD-HOC QUERIES DATA STORE: Flat Files Relational Tables XML Datasets PROCESSING : Sorting SequencingNO QUERIESDW BUS Conformed Dimensions Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "370 • DMBOK2 Figure 82 also depicts aspects of the data lifecycle. Data moves from source systems into a staging area where it may be cleansed and enriched as it is integrated and stored in the DW and/or an ODS. From the DW , it may be accessed via marts or cubes and used for various kinds of reporting. Big Data goes through a similar process but with a significant difference: while most warehouses integrate data before landing it in tables, Big Data solutions ingest data befo re integrating it. Big Data BI may include predictive analytics and data mining, as well as more traditional forms of reporting. (See Chapter 14.) 1.3.7.1 Source Systems Source Systems, on the left side of Figure 82, include the operational systems and external data to be brought into the DW/BI environment. These typically include operational systems such as CRM, Accounting, and Human Resources applications, as well as operational systems that differ based on industr y. Data from vendors and external sources may also be included, as may DaaS, web content, and any Big Data computation results. 1.3.7.2 Data Integration Data integration covers Extract, Transform, and L oad (ETL), data virtualization, and other techniques of getting data into a common form and location. In a SOA environment, the data services layers are part of this component. In Figure 82, all the arrows represent data integration processes. (See Chapter 8.) Figure 82 Conceptual DW/BI and Big Data Architecture © DATALEADERS.ORG Report Interact Compare Evaluate Predict Learn Data Visualization DaaS Big Data Results MDMReference & Master Data Conformed DimensionsData Warehouse Data Mastering Data Quality Intervention Enrichment & AugmentationSources BI Conceptual DW/BI and Big Data Architecture Application Operational Reporting Big Data Data & T ext Mining Unstructured Analytics Predictive Analytics Machine Learning Operational Reporting & Analytics Geospatial and Demographic Analytics Performance Management CubesODS Data Mart Dependent Data Stores Evaluate Model Explore Integrate Ingest Data LakeEmail Multimedia Sensors IoT Social Network Web DaaS DW Operational Systems Central Warehouse Subject -Oriented Non-Volatile Time-Variant Atomic Historical Data Staging Area Clean Integrate Enrich Standardize Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 371 1.3.7.3 Data Storage Areas The warehouse has a set of storage areas: • Staging area: A staging area is an intermediate data store between an original data source and the centralized data repository. Data is staged so that it can be transformed, integrated, and prepped for loading to the warehouse. • Reference and Master Data conformed dimensions : Reference and Master Data may be stored in separate repositories. The data warehouse feeds new Master Data and is fed by conformed dimension contents from the separate repositories. • Central Warehouse : Once transformed and prepped, the DW data usually persists in the central or atomic layer. This layer maintains all historical atomic data as well as the latest instance of the batch run. The data structure of this area is developed and influenced based on performance needs and use patterns. Several design elements are brought to bear: o The relationship between the business key and surrogate keys for performance o Creation of indices and foreign keys to support dimensions o Change Data C apture (CDC) techniques that are used to detect, maintain, and store history • Operational Data Store (ODS) : The ODS is a version of a central persisted store that supports lower latencies and , therefore, operational use. Since the ODS contains a time window of data and not the history, it can be refreshed much more quickly than a warehouse. Sometimes real -time streams are snapshotted at predefined intervals into the ODS to enable integrated reporting and analysis. Over time, with the increasing frequency of updates driven by business needs, and growing technology and techniques to integrate real -time data into the DW, many installations have merged their ODS into the existing DW or Data Mart architecture. • Data marts : A data mart is a type of data store often used to support presentation layers of the data warehouse environment. It is also used for presenting a departmental or functional sub -set of the DW for integrated reporting, query, and analysis of historical information. The data mart is oriented to a specific subject area, a single department, or a single business process. It can also form the basis of a virtualized warehouse where the combined marts comprise the resulting warehouse entity. Data integration processes will re fresh, update, or expand the contents of the various marts from the persistence layer. • Cubes : Three classic implementation approaches support Online Analytical Processing (OLAP). Their names relate to underlying database type s, such as Relational, Multi -dimensional, and Hybrid. 1.3.8 Types of Load Processing Data warehousing involves two main types of data integration processes : historical loads and ongoing updates. Historical data is usually loaded only once, or a few times while working out data issues, and then never again. Ongoing updates are consistently scheduled and executed to keep the data in the warehouse up -to-date. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "372 • DMBOK2 1.3.8.1 Historical Data One advantage of a data warehouse is that it can capture detailed history of the data it stores. There ar e different methods to capture this detail. An organization that wants to capture history should design based on requirements. Being able to reproduce point -in-time snapshots requires a different approach than simply presenting current state. The Inmon data warehouse suggests that all data is stored in a single data warehouse layer. This layer will store cleansed, standardized, and governed atomic level data. A common integration and transformation layer facilitates reuse across the delivery implementations. An enterprise data model is required for success. Once validated, this single store is available to different data consumers via a star structured data mart. The Kimball data warehouse suggests that the data warehouse is composed of a combination of departmental data marts containing cleansed, standardized, and governed data. The data marts will store the history at the atomic level. Conformed dimensions and conformed facts will deliver enterprise level information. Another approach, the Data Vault , also cleanses and standardizes as part of the staging process. History is stored in a normalized atomic structure, dimensional surrogate, primary and alternate keys are defined. Ensuring that the business and surrogate key relationship remains intact bec omes the secondary role of the vault – this is the data mart history. Facts are persisted here as atomic structures. The vault is then available to a variety of data consumers via data marts. By retaining the history inside the vault, reloading facts is po ssible when later increments introduce grain changes. It is possible to virtualize the presentation layer, facilitating agile incremental delivery and collaborative development with the business community. A final materialization process can implement a mo re traditional star data mart for production end user consumption. 1.3.8.2 Batch Change Data Capture Data Warehouses are often loaded daily and serviced by a nightly batch window. The load process can accommodate a variety of change detection, as each source system may require differing change capture techniques. Database log techniques are likely candidates for in -house developed applications as vendor purchased applications are unlikely to tolerate modification with triggers or additional overhead. Time stamped or log table loads are the most common. Full loads occur when dealing with legacy systems built without native time stamping capabilities (yes, there are applications without databases) or when certain batch recovery conditions apply. Table 28 summarizes the difference between change data capture techniques, including their relative complexity and speed. The overlap column identifies whether there may be data duplication between source system changes and the target environment. When Overlap is ‘Yes’ , this change data may already be present. When the Delete indicator is set to ‘Yes’, the Change Data Method will track any deletes that have occurred in the source system – useful for expiring dimensions no longer in use. When Deletes are not tracked by the source system, additional efforts are required to determine when they occur. (See Chapter 8.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 373 Table 28 CDC Technique Comparison Method Source System Requirement Complexit y Fact Load Dimensio n Load Overlap Delete s Time stamped Delta Load Changes in the source system are stamped with the system date and time. Low Fast Fast Yes No Log Table Delta Load Source system changes are captured and stored in log tables Medium Nomina l Nominal Yes Yes Database Transactio n Log Database captures changes in the transaction log High Nomina l Nominal No Yes Message Delta Source system changes are published as [near] real-time messages Extreme Slow Slow No Yes Full Load No change indicator, tables extracted in full and compared to identify change Simple Slow Nominal Yes Yes 1.3.8.3 Near -real-time and Real -time With the onset of Operational BI (or Operational Analytics) pushing for lower latency and more integration of real-time or near -real-time data into the data warehouse, new architectural approaches emerged to deal with the inclusion of volatile data. For example, a common application of operational BI is the automated banking machine data provisioning. When making a banking transaction, historica l balances and new balances resulting from immediate banking actions need to be presented to the banking customer real -time. Two key design concepts that are required for provisioning data in near -real-time are isolation of change and alternatives to batch processing. The impact of the changes from new volatile data must be isolated from the bulk of the historical, non -volatile DW data. Typical architectural approaches for isolation include a combination of building partitions and using union queri es for the different partitions. Alternatives to batch processing handle the increasingly shorter latency requirements for data availability in the DW. There are three main types: trickle feeds, messaging, and streaming, which differ by where data is accumulated while waiting to be processed. (See Chapter 8.) • Trickle feeds (Source accumulation) : Rather than run on a nightly schedule, trickle feeds execute batch loads on a more frequent schedule (e.g., hourly, every 5 minutes) or when a threshold is reached (e.g., 300 transactions, 1G of data). This allows some processing to happen during the day, but not as intensely as with a dedicated nightly batch process. Care is needed to ensure that if a trickle feed batch takes longer to complete than the time between feeds, the next feed is delayed so that the data is still loaded in proper order. • Messaging (Bus accumulation): Message interaction in real -time or near -real-time is useful when extremely small packets of data (messages, events, or transactions) are published to a bus as they occur. Target systems subscribe to the bus, and incrementally process the packets into the warehouse as needed. Source systems and target systems are independent of each other. Data -as-a-Service (DaaS) frequently uses this method. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "374 • DMBOK2 • Streaming (Target accumulation) : Rather than wait on a source -based schedule or threshold, a target system collects data as it is received into a buffer area or queue, and processes it in order. The result interaction or some aggregate may later appear as an additional feed to the warehou se. 2. Activities 2.1 Understand Requirements Developing a data warehouse is different from developing an operational system. Operational systems depend on precise, specific requirements. Data warehouses bring together data that will be used in a range of different ways. Moreover, usage will evolve over time as users analyze and explore data. Take time in the initial phases to ask questions related to capabilities and sources of data to support these capabilities. This time to design pays off in reduced rework costs later when the data processing is being tested using the actual data sources. In gathering requirements for DW/BI projects, begin with business goals and strategy. Identify and scope the business areas, then identify and interview the appropriate business people. Ask what they do and why. Capture specific questions they are asking n ow, and those they want to ask of the data. Document how they distinguish between and categorize important aspects of the information. Where possible, define and capture key performance metrics and calculations. These can uncover business rules that provide the foundation for automation of Data Quality expectations. Catalog requirements and prioritize them into those necessary for production go -live and adoption of the warehouse and those that can wait. Look for items that are simple and valuable to jump -start the productivity of the initial project release. A DW/BI project requirements write -up should frame the whole context of the business areas and / or processes that are in scope. 2.2 Define and Maintain the DW/BI Architecture The DW/BI architecture should describe where data comes from, where it goes, when it goes, why and how it goes into a warehouse. The ‘how’ includes the hardware and software detail and the organizing framework to bring all the activities together. Technical requirements should include performance, availability, and timing needs. ( See Chapter s 4 and 8.) 2.2.1 Define DW/BI Technical Architecture The best DW/BI architectures will design a mechanism to connect back to transactional level and operational level reports in an atomic DW. This mechanism will protect the DW from having to carry every transactional detail. An example is providing a viewing mechanism for key operational reports or forms based on a transactional key, such as Invoice Number. Customers will always want all the detail available, but some of the Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 375 operational data, such as long description fields, has value only in the context of the original report, and does not provide analytic value. A conceptual architecture is a starting point. Many activities are necessary to correctly align the non -functional requirements to the business needs. Prototyping can quickly prove or disprove key points before making expensive commitments to technologies or architectures. In addition, empowering the business community with knowledge and adoption programs championed through a sanctioned change management team will assist in transition and ongoing operational success. A natural extension to this transformation process is the maintenance, or at least validation, with the enterprise data model. Since the focus is on which data structures are in use by which organizational areas, check the physical deployment against the l ogical model. Make any updates if omissions or errors arise. 2.2.2 Define DW/BI Management Processes Address production management with a coordinated and integrated maintenance process, delivering regular releases to the business community. It is crucial to establish a standard release plan (see Section 2.6 ). Ideally, the warehouse project team should manage each update to the deployed data product as a software release that provisions additional functionality. Establishing a schedule for releases allows for an annual demand and resource plan and standard de livery schedule. Use the internal release to tweak this standardized schedule, the resource expectations and estimate sheets derived for it. Establishing a functioning release process ensures that management understands this to be a data product - centric proactive process and not an installed product addressed through reactive issue resolution. It is critical to work pro- actively and collaboratively in a cross -functional team to continuously grow and enhancement features – reactive support systems reduce adoption. 2.3 Develop the Data Warehouse and Data Marts Typically, DW/BI projects have three concurrent development tracks: • Data : The data necessary to support the analysis the business wants to do. This track involves identifying the best sources for the data and designing rules for how the data is remediated, transformed, integrated, stored, and made available for use by the appl ications. This step also includes deciding how to handle data that doesn’t fit expectations. • Technology : The back -end systems and processes supporting the data storage and movement. Integration with the existing enterprise is fundamental, as the warehouse is not an island unto itself. Enterprise Architectures, specifically Technology and Application special ties, usually manage this track. • Business Intelligence tools : The suite of applications necessary for data consumers to gain meaningful insight from deployed data products. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "376 • DMBOK2 2.3.1 Map Sources to Targets Source -to-target mapping establishes transformation rules for entities and data elements from individual sources to a target system. Such mapping also documents lineage for each data element available in the BI environment back to its respective source(s). The most difficult part of any mapping effort is determining valid links or equivalencies between data elements in multiple systems. Consider the effort to consolidate data into a DW from multiple billing or order management systems. Chances are that tables and fields that contain equivalent data do not have the same names or structures. A solid taxonomy is necessary to map data elements in different systems to a consistent structure in the DW. Most often, this taxonomy is the logical data model. The mapping process must also address whether data in different structures is to be appended, changed in place, or inserted. 2.3.2 Remediate and Transform Data Data remediation or cleansing activities enforce standards and correct and enhance the domain values of individual data elements. Remediation is particularly necessary for initial loads where significant history is involved. To reduce the complexity of the target system, source systems should be made responsible for data remediation and correction. Develop strategies for rows of data that are loaded but found to be incorrect. A policy for deleting old records may cause some havoc with related tables and surrogate keys, expiring a row and loading the new data as a completely new row may be a better op tion. An optimistic load strategy may include creating dimension entries to accommodate fact data. Such a process must account for how to update and expire such entries. Pessimistic load strategies should include a recycle area for fact data that cannot be associated with corresponding dimension keys. These entries require appropriate notification, alerting and reporting to ensure they are tracked, and reloaded later. Fact jobs should consider first loading recycled entries, then processing newly arrived content. Data transformation focuses on activities that implement business rules within a technical system. Data transformation is essential to data integration. Defining the correct rules by which to integrate data often requires direct involvement from Data Stewards and other SMEs. Rules should be documented so that they can be governed. Data integration tools perform these tasks. ( See Chapter 8.) 2.4 Populate the Data Warehouse The largest part of the work in any DW/BI effort is the preparation and processing of the data. The design decisions and principles for what data detail the DW contains are a key design priority for DW/BI architecture. Publishing clear rules for what data will be available via only operational reporting (such as in non- DW) is critical to the success of DW/BI efforts. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 377 The key factors to consider when defining a population approach are required latency, availability of sources, batch windows or upload intervals, target databases, dimensional aspects, and timeframe consistency of the data warehouse and data mart. The appr oach must also address Data Quality processing, time to perform transformations, and late- arriving dimensions and data rejects. Another aspect to defining a population approach centers around change data capture process – detecting changes in the source system, integrating those changes together, and aligning changes across time. Several databases now provision log capture functionality that data integration tools can operate on directly, so the database tells the user what has changed. Scripting processes can be written or generated where this function is not available. Several techniques are available to the design and build teams for integration and latency alignment across heterogeneous feeds. The first increment paves the way for additional capability development and onboarding new business units. Many new technologies, processes , and skills are necessary, as well as careful planning and attention to detail. Downstream increments are to build on top of this foundational element, so more investments are recommended to sustain high quality data, technical architecture, and transitioning to production. Create processes to facilitate and automate timely identification of data errors with end user workflow integration. 2.5 Implement the Business Intelligence Portfolio Implementing the BI Portfolio is about identifying the right tools for the right user communities within or across business units. Find similarities through alignment of common business processes, performance analysis, management styles, and requirements. 2.5.1 Group Users According to Needs In defining the target user groups, there is a spectrum of BI needs . First, know the user groups and then match the tool to the user groups in the company. On one end of the spectrum are IT developers concerned with extracting data, who focus on advanced functionality. On the other end, information consumers may want fast access to previously developed and executed reports. These consumers may want some degree of interactivity , such as drill, filter, sort, or may only want to see a static report. Users may move from one class to another as their skills increase or as they perform different functions. A supply chain manager, for example, may want to view a static report on financials but a highly interactive report for analyzing inventory. A financial analyst and a line manager responsible for expenses may be power users when analyzing total expenses, but are satisfied with a static report of one phone bill. Executives and managers will use a combination of fixed reports, dashboards, and scorecards. Managers and power users tend to want to drill into these reports slice and dice the data to identify the root causes of problems. External customers may use any of these tools as part of their experience. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "378 • DMBOK2 2.5.2 Match Tools to User Requirements The marketplace offers an impressive range of reporting and analytics tools. Major BI vendors offer classic pixel- perfect report capabilities that were once the domain of application reports. Many application vendors offer embedded analytics with standard content fetched from pre- populated cubes or aggregate tables. Virtualization has blurred the lines between on- premises data sources and external purchased or open data, and in some cases provisions user -controlled report -centric integration on demand. In other words, it is prudent for companies to use common infrastructure and delivery mechanisms. These include the web, email, and applications for the delivery of all kinds of information and reports, of which DW/BI is a subset. Many vendors are now combining related BI Tools, through mergers and acquisitions or net new development, and offering BI Suites. Suites are the primary option at the Enterprise Architecture level but given that most organizations have already purchased in dividual tools, or embraced open source tools, questions around replacement versus co -existence are likely to surface. Remember that every BI tool comes with a price, requiring system resources, support, training, and architectural integration. 2.6 Maintain Data Products An implemented warehouse and its customer -facing BI tools is a data product. Enhancements (extensions, augmentations, or modifications) to an existing DW platform should be implemented incrementally. Maintaining the scope for an increment, and executing a critical path for key work items, can be a challenge in a dynamic work environment. Set priorities with business partners and focus work on mandatory enhancements. 2.6.1 Release Management Release Management is critical to an incremental development processes that grows new capabilities, enhances the production deployment, and ensures provision of regular maintenance across the deployed assets. This process will keep the warehouse up -to-date, clean, and operating at its best. However, this process requires the same alignment between IT and Business as between the Data Warehouse model and the BI capabilities. It is a continual improvement effort. Figure 83 illustrates an example release process, based on a quarterly schedule. Over the year, there are three business- driven releases and one technology -based release (to address requirements internal to the warehouse). The process should enable incremental development of the warehouse and management of the backlog of requirements. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 379 Figure 83 Release Process Example 2.6.2 Manage Data Product Development Lifecycle While data consumers are using the existing DW, the DW team is preparing for the next iteration, with the understanding that not all items will go to production. Align the iterations to releases with a backorder work list prioritized by the business units. Each iteration will extend an existing increment or add new functionality by onboarding a business unit. Releases will align functionality to the business unit, whereas the iteration will align the functionality to the configuration itself managed by the product manager. Those items that business believes ready and feasible for further investigation can be reviewed, adjusted if necessary, and then promoted to a pilot or sandbox environment, where business users investigate new approaches, experiment with new techniques, or develop new models or learning algorithms. This area may Business Release + 1 Incremental Delivery Quarterly Timeframe Requirements Frozen Work Prioritization (MoSCoW)Business Release +2 Incremental Delivery Quarterly Timeframe Requirements Frozen Work Prioritization (MoSCoW)Business Release +3 Incremental Delivery Quarterly Timeframe Requirements Frozen Work Prioritization (MoSCoW) Work Prioritization De-ScopeInternal Release 0 Quarterly Timeframe 0.1 Harvest Deliverables 0.2 Update Estimates0.3 Lessons Learned0.4 Knowledge Management0.5 Software / Hardware Refresh0.6 Training / Education0.7 Address Work Arounds Implementation Prioritization Limitations (Work Arounds) Known DefectsRelease +4,5,6 Plan => From Work Intake triaged against MoSCoW List from Release 1,2,30.1 Method Deliverables Update0.2 Work Effort Calculator Update0.3 Best Practice Update0.4 Awareness Update0.5 Software / Hardware Capacity Horizon Update0.6 Resource Certification Update 0.7 Tactical to Strategic Alignment Update4 thRelease is an Internal Delivery•3 quarterly releases to business units each providing incremental capabilities •Work scope managed with MoSCoW list •Time managed with TimeBoxes BICCMUST CouldShould Won’t Work Around Defect MitigateMitigate PublishReview BICC BICC Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "380 • DMBOK2 see less governance and supervision than other business- facing areas but some form of sandbox prioritizing is necessary. Akin to the traditional quality assurance or testing environment, scrutinize items in the pilot area for fit into the production world. How well pilot items perform determines their next steps. Take care not to promote blindly and without regard to downstr eam Data Quality or governance issues. The lifespan in production is just an existential measure: it must be of the highest practical quality to be in production. Items passing the pilot and deemed production -ready by both business and IT representatives can be promoted to production as new data products. This completes one iteration. Items not passing pilot can be rejected entirely or returned to development for fine-tuning. Perhaps additional support from the DW team is needed at this time to advance the item in the next promotion iteration. 2.6.3 Monitor and Tune Load Processes Monitor load processing across the system for bottlenecks and dependencies. Employ database tuning techniques where and when needed, including partitioning, tuned backup, and recovery strategies. Archiving is a difficult subject in data warehousing. Users often consider the data warehouse as an active archive due to the long histories that are built and are unwilling, particularly if the On Line Analytical Processing (OLAP) sources have dropped records, to see the data warehouse engage in archiving. ( See Chapter 6.) 2.6.4 Monitor and Tune BI Activity and Performance A best practice for BI monitoring and tuning is to define and display a set of customer -facing satisfaction metrics. Average query response time and the number of users per day, week, or month are examples of useful metrics. In addition to the statistical measures available from the systems, it is useful to survey DW/BI customers regularly. Regular review of usage statistics and patterns is essential. Reports providing frequency and resource usage of data, queries, and reports allow prudent enhancement. Tuning BI activity is analogous to the principle of profiling applications in order to kno w where the bottlenecks are and where to apply optimization efforts. The creation of indexes and aggregations is most effective when done according to usage patterns and statistics. Tremendous performance gains can come from simple solutions such as posting the completed daily results to a report that runs hundreds or thousands of times a day. Transparency and visibility are the key principles that should drive DW/BI monitoring. The more one can expose the details of the DW/BI activities, the more data consumers can see and understand what is going on (and have confidence in the BI), and less di rect end -customer support will be required. Providing a dashboard that exposes the high- level status of data delivery activities, with drill -down capability, is a best practice that allows an on-demand -pull of information by both support personnel and cust omers. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 381 The addition of Data Quality measures will enhance the value of this dashboard where performance is more than just speed and timing. Use heat maps to visualize workload on infrastructure, data throughput, and compliance to operating agreement levels. 3. Tools Choosing the initial set of tools can be a long process. It includes attempting to satisfy near -term requirements, non-functional specifications, and the as -yet to be created next generation requirements. Decision criteria tool sets, process implementation tools, and professional services offerings can facilitate and expedite this activity. It is critical to evaluate not only the conventional build or buy positions, but also the rent option provisioned as Software -as-a-Service. Renting SaaS tools and the associated expertise is weighed against the cost of building from scratch or deploying purchased products from vendors. Consider ongoing upgrade and potential replacement costs as well. Alignment to a set OLA (Operational Level Agreement) can bridge forecasted costs, and provide input into setting compelling fees and penalties for term violations. 3.1 Metadata Repositories Large organizations often find themselves with many tools from different vendors, each deployed potentially at differing versions. Key to this effort is the ability to stitch Metadata together from a variety of sources. Automating and integrating population of this repository can be achieved with a variety of techniques. (See Chapter 13.) 3.1.1 Data Dictionary / Glossary A data dictionary is necessary to support the use of a DW. The dictionary describes data in business terms and includes other information needed to use the data (e.g., data types, details of structure, and security restrictions). Often , the content for the data dictionary comes directly from the logical data model. Plan for high quality Metadata by ensuring modelers take a disciplined approach to managing definitions as part of the modeling process. In some organizations, business users actively participate in the development of the data dictionary by supplying, defining, and then stewarding corrections to definitions of subject area data elements. Embrace this activity through a collaboration tool, monitor activities through a Center of Excellence, and ensure that content created through this activity is retained in the logical model. Ensuring agreement between the business -facing content and the technical -facing physical data model will reduce the risk of downstream errors and rework. (See Chapter 13.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "382 • DMBOK2 3.1.2 Data and Data Model Lineage Many data integration tools offer lineage analysis that considers both the developed population code and the physical data model and database. Some offer web interfaces to monitor and update definitions and other Metadata. Documented data lineage serves many purposes: • Investigation of the root causes of data issues • Impact analysis for system changes or data issues • Ability to determine the reliability of data, based on its origin Look to implement an integrated impact and lineage tool that can understand all the moving parts involved in the load process, as well as end user reporting and analytics. Impact analysis reports will outline which components are affected by a potential ch ange, expediting and streamlining estimating and maintenance tasks. Many key business processes, relationships, and terminologies are captured and explained during development of the data model. The logical data model holds much of this information, which is often lost or ignored during development or production deployment . It is critical to ensure that this information is not discarded and that the logical and physical models are updated after deployment and are in sync. 3.2 Data Integration Tools Data integration tools are used to populate a data warehouse. In addition to doing the work of integrating data, they enable scheduling of jobs in ways that account for complex data delivery from multiple sources. In selecting a tool, also account for these features that enable management of the system: • Process audit, control, restart, and scheduling • The ability to selectively extract data elements at execution time and pass that extract to a downstream system for audit purposes • Controlling which operations can or cannot execute and restarting a failed or aborted run (see Chapter 8) A variety of data integration tools also offer integration capabilities with the BI portfolio, supporting import and export of workflow messages, email, or even semantic layers. Workflow integration can drive Data Quality defect identification, resolution, and escalation processes. Messaging through email or alert processing driven from email is a common practice especially for mobile devices. In addition, the ability to provision a data target as a semantic layer can be a data virtualization candidate for agile implementations. 3.3 Business Intelligence Tools Types The maturity of the BI market, and a wide range of available BI tools, makes it rare for companies to build their own BI tools. 68 The purpose of this section is to introduce the types of tools available in the BI marketplace, and provide an overview of their chief characteristics with information to help match the tools to the 68 The material in this section is primarily from “The Business Intelligence Market” by Cindi Howson, BIScorecard® , http://bit.ly/2tNirv5 ; used by permission, with minor changes and additions. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 383 appropriate customer -level capabilities. BI tools are evolving quickly, enabling a transition from IT-led, standardized reporting to self -service, business- driven data exploration.69 • Operational reporting is the application of BI tools to analyze business trends, both short -term (month-over -month) and longer -term (year -over -year). Operational reporting can also help discover trends and patterns. Use Tactical BI to support short -term business decisions. • Business performance management (BPM) includes the formal assessment of metrics aligned with organizational goals. This assessment usually happens at the executive level. Use Strategic BI to support long -term corporate goals and objectives. • Descriptive, self -service analytics provides BI to the front lines of the business, where analytical capabilities guide operational decisions. Operational analytics couples BI applications with operational functions and processes, to guide decisions in near -real-time. The requirement for lo w latency (near real -time data capture and data delivery) will drive the architectural approach to operational analytics solutions. Service -oriented Architecture (SOA) and Big Data become necessary to support operational analytics fully (see Chapters 8 and 15). 3.3.1 Operational Reporting Operational Reporting involves business users generating reports directly from transactional systems, operational applications, or a data warehouse. This is typically an application functionality. Often business areas will start to use a DW for operational reporting, especially if DW/BI governance is poor, or the DW contains additional data that enhances the operational, transaction data. Often the reports will appear as ad -hoc queries, when in fact they are simple reports or are used to initiate workflow. From a data management perspective, the key is to understand if the data necessary for this reporting exists within the application itself, or if it requires data enhancements from the DW or operational data store. Data exploration and reporting tools, sometimes called ad -hoc query tools, enable users to author their own reports or create outputs for use by others. They are less concerned with the precise layout because they are not trying to generate an invoice or the like. However, they do want to include charts and tables quickly and intuitively. Often the reports created by business users become standard reports, not exclusively used for ad hoc business questions. The needs within business operations reporting are often different from the needs within business query and reporting. With business query and reporting, the data source is usually a data warehouse or data mart (though not always). While IT develops production reports, power users and ad hoc business users develop their own reports with business query tools. Use reports generated with business query tools individually, departmentally, or enterprise -wide. 69 Dataversity refers to this trend as the “democratization of data technologies.” See Ghosh, Paramita. “A Comparative Study of Business Intelligence and Analytics Market Trends.” Dataversity. January 17, 2017. http://bit.ly/2sTgXTJ (accessed 2017- 01-22). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "384 • DMBOK2 Production reporting crosses the DW/BI boundary and often queries transactional systems to produce operational items such as invoices or bank statements. The developers of production reports tend to be IT personnel. Traditional BI tools cover some data visualization methods such as tables, pie charts, line charts, area charts, bar charts, histograms, turnkey box (candlestick) as examples fairly well. Data visualizations can be delivered in a static format, such as a p ublished report, or a more interactive online format; and some support end -user interaction where drilling or filtering capabilities facilitate analysis of data within the visualization. Others allow the visualization to be changed by the user on demand. (See Chapter 14.) 3.3.2 Business Performance Management Performance management is a set of integrated organizational processes and applications designed to optimize execution of business strategy; applications include budgeting, planning, and financial consolidation. There have been a number of major acquisitions in this segment, as ERP vendors and BI vendors see great growth opportunities here and believe BI and Performance Management are converging. How frequently customers buy BI and performance management from the same vendor depends on product capabilities. Broadly speaking, Performance Management technology enables processes to help meet organizational goals. Measurement and a feedback loop with positive reinforcement are key elements. Within the BI space, this has taken the form of many strategic enterprise applications, suc h as budgeting, forecasting, or resource planning. Another specialization has formed in this area: creating scorecards driven by dashboards for user interaction. Dashboards, like those found in automobiles, provide the necessary summary o r aggregate information to the end user with most recent updates (Eckerson, 2005). 3.3.3 Operational Analytic Applications Henry Morris of IDC coined the term Analytic Applications in the 1990s, clarifying how they are different from general OLAP and BI tools (Morris, 1999). Analytic applications include the logic and processes to extract data from well -known source systems, such as vendor ERP systems, a data model for the data mart, and pre -built reports and dashboards. They provide businesses with a pre -built solution to optimize a functional area (people management, for example) or industry vertical (retail analytics, for example). Different types of analytic applications include customer, financial, supply chain, manufacturing, and human resource applications. 3.3.3.1 Multi -dimensional Analysis – OLAP Online Analytical Processing (OLAP) refers to an approach to providing fast performance for multi -dimensional analytic queries. The term OLAP originated, in part, to make a clear distinction from OLTP, Online Transactional Processing. The typical output of OLAP queries is in a matrix format. The dimensions form the rows and columns of the matrix, and the factors, or measures, are the values inside the matrix. Conceptually, this illustrates as a cube. Multi- dimensional analysis with cubes is particularly useful where there are well - known ways analysts want to look at summaries of data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 385 A traditional application is financial analysis, where analysts want to repeatedly traverse known hierarchies to analyze data; for example, date (such as Year, Quarter, Month, Week, Day), organization (such as Region, Country, Business Unit, Department), a nd product hierarchy (such as Product Category, Product Line, Product). Many tools today embed OLAP cubes into their software footprint and some even seamlessly automate and integrate the definition and population process. This means that any user in any b usiness process can slice and dice their data. Align this capability with the power users in the subject area communities and deliver it along a self -service channel empowering these selected users to analyze their data their way. Typically, OLAP tools have both a server component and an end user client -facing component installed on the desktop, or available on the web. Some desktop components are accessible from within a spreadsheet appearing as an embedded menu or function item. T he architecture selected (ROLAP, MOLAP, HOLAP) will guide the development efforts but common to all will be definition of cube structure, aggregate needs, Metadata augmentation and analysis of data sparsity. Structuring the cube to provision desired functional requirements may require splitting larger dimensions into separate cubes to accommodate storage, population, or calculation requirements. Use levels of aggregation to ensure calculation and retrieval of desired formulas occurs within agreed upon response times. End user augmentation of hierarchies enable fulfillment the aggregation, calculation, or population requirements. In addition, sparsity of cube data may require addition or removal of aggregate str uctures or refine materialization needs in the warehouse data layer provisioning it. Provisioning role -based security or multi- language text within the cube may require extra dimensions, additional functions, calculations, or sometimes creating separate cube structures. Striking a balance between end user flexibility, performance, and serv er workloads means some negotiating is to be expected. The negotiation typically occurs during the loading processes and may require hierarchy changes, aggregate structure changes or additional warehouse materialized data objects. Strike the right balance among cube count, server workload, and delivered flexibility, so that the refresh occurs in a timely manner, and cubes provide reliable and consistent queries without high storage or server utilization costs. The value of On Line Analytical Processing (OLAP) Tools and cubes is reduction of the chance of confusion and erroneous interpretation, by aligning the data content with the analyst’s mental model. The analyst can navigate through the database and screen for a particular subset of the data, changing the data’s orientation a nd defining analytical calculations. Slice -and-dice is the user -initiated process of navigation by calling for page displays interactively, through the specification of slices via rotations and drill down / up. Common OLAP operations include slice and dice , drill down, drill up, roll up, and pivot. • Slice : A slice is a subset of a multi -dimensional array corresponding to a single value for one or more members of the dimensions not in the subset. • Dice : The dice operation is a slice on more than two dimensions of a data cube, or more than two consecutive slices. • Drill down / up : Drilling down or up is a specific analytical technique whereby the user navigates among levels of data, ranging from the most summarized (up) to the most detailed (down). • Roll -up: A roll- up involves computing all of the data relationships for one or more dimensions. To do this, define a computational relationship or formula. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "386 • DMBOK2 • Pivot : A pivot changes the dimensional orientation of a report or page display. Three classic implementation approaches support Online Analytical Processing. • Relational Online Analytical Processing (ROLAP) : ROLAP supports OLAP by using techniques that implement multi -dimensionality in the two -dimensional tables of relational database management systems (RDBMS). Star schema joins are a common database design technique used in ROLAP environments. • Multi -dimensional Online Analytical Processing (MOLAP ): MOLAP supports OLAP by using proprietary and specialized multi- dimensional database technology. • Hybrid Online Analytical Processing (HOLAP) : This is simply a combination of ROLAP and MOLAP. HOLAP implementations allow part of the data to be stored in MOLAP form and another part of the data to be stored in ROLAP. Implementations vary on the control a designer has to vary the mix of partitioning. 4. Techniques 4.1 Prototypes to Drive Requirements Quickly prioritize requirements before the implementation activities begin by creating a demonstration set of data and applying discovery steps in a joint prototype effort. Advances in data virtualization technologies can alleviate some of the traditional implementation pains through collaborative prototyping techniques. Profiling the data contributes to prototyping and helps reduces risk associated with unexpected data. The DW is often the first place where the pain of poor quality data in source systems or data entry functions becomes apparent. Profiling also discloses d ifferences between sources that may present obstacles to data integration. Data may be of high quality within its sources, but because sources differ , the data integration process becomes more complicated. Evaluation of the state of the source data leads to more accurate up -front estimates for feasibility and scope of effort. The evaluation is also important for setting appropriate expectations. Plan to collaborate with the Data Quality and Data Governance team(s) and to draw on the expertise of other SMEs to understan d data discrepancies and risks. (S ee Chapter s 11 and 13.) 4.2 Self-Service BI Self-service is a fundamental delivery channel within the BI portfolio. This typically funnels user activity within a governed portal where, depending on the privileges of the user, a variety of functionality is provided ranging from messaging, alerts, viewing scheduled production reports, interacting with analytic reports, developing ad hoc reporting and of course dash boarding and score carding. Reports can be pushed to the portal on standard Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 387 schedules, to be retrieved by the users at their leisure. Users can also pull data by executing reports from within the portal. These portals share content across organizational boundaries. Extending the collaboration tool outwards toward the user communit y can also provide self- service tips and tricks, an integrated communique on load status, overall performance, and release progress as well as dialog forums. Mediate forum content through the support channel and then facilitate with user group sessions through the maintenance channel. Visualization and statistical analysis tooling allows for rapid data exploration and discovery. Some tools allow for business- centric construction of dashboard like objects that can be rapidly shared, reviewed, and revitalized. Once the domain of IT and de velopers only, many data shaping, calculation, and visualization techniques can now be employed by the business community. This offers a degree of workload distribution and integration efforts can be feasibly prototyped through business channels and then m aterialized and optimized by IT. 4.3 Queryable Data In order to maintain lineage, all structures and processes should have the capability to create and store audit information at a grain useful for tracking and reporting. Allowing users to query this audit data enables the users to verify for themselves the condition and arrival of the data, which improves user confidence. Audit information also allows for more detailed trouble- shooting when data issues arise. 5. Implementation Guidelines A stable architecture that can scale to meet future requirements is paramount to the success of a data warehouse. A production support team capable of dealing with the daily loading, analysis , and end user feedback is mandatory. In addition, to sustain success, ensure that the warehouse and the business unit teams are aligned. 5.1 Readiness Assessment / Risk Assessment There may be a gap between when an organization embraces a new venture, and when it has the ability to sustain that venture. Successful projects start with a Prer equisite Checklist. All IT projects should have business support, be aligned with strategy, and have a defined architectural approach. In addition, a DW should: • Define data sensitivity and security constraints • Perform tool selection • Secure resources • Create an ingestion process to evaluate and receive source data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "388 • DMBOK2 Identify and inventory sensitive or restricted data elements in the warehouse. This data will need to be masked or obfuscated to prevent access by unauthorized personnel. Additional constraints may apply when considering outsourcing for implementation or maintenance activities. Account for security constrains before selecting tools and assigning resources. Ensure Data Governance processes for review and approval have been followed. DW/BI projects risk refocus or total cancellation due to these overarching factors. 5.2 Release Roadmap Because they require a large development effort, warehouses are built incrementally. Whatever method chosen to implement, be it waterfall, iterative or agile, it should account for the desired end state. That is why a roadmap is a valuable planning tool. The method combined with the maintenance processes can be both flexible and adaptive to balance the pressures of individual project delivery with overall goals of re -usable data and infrastructure. An incremental approach leveraging the DW bus matrix as a communication and marketing tool is suggested. Use business- determined priorities tethered by exposure metrics to determine how much rigor and overhead to apply to each increment; a small single -sourced delivery may afford rule relaxation especially when limited exposure is felt should those issues be realized by the organization. Each increment will modify existing capabilities or add brand new capabilities typically aligned with a newly onboarded b usiness unit. Apply a consistent needs and abilities process to determine the next business unit to onboard. Maintain a back -order or work item list to identify outstanding capabilities and the business -facing priorities. Determine any technical dependenci es that require delivery in a different order. Then package this work into a software release. Each release can be delivered at an agreed -upon pace: quarterly, monthly, weekly, or even faster when appropriate. Manage the releases with the business partners by assembling a roadmap: a listing of releases by date by capabilities. 5.3 Configuration Management Configuration management aligns with the release roadmap and provides the necessary back office stitching and scripts to automate development, testing, and transportation to production. It also brands the model by the release at the database level, and ties the codebase to that b rand in an automated manner so that manually coded, generated programs and semantic layer content is harmonized across the environment and is versioned controlled. 5.4 Organization and Cultural Change Starting with and keeping a consistent business focus throughout the DW/BI lifecycle is essential to success. Looking at the value chain of the enterprise is a good way to understand the business context. The specific business processes in a company’s value chain provide a natural business- oriented context in which to frame areas of analysis. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 389 Most importantly, align projects behind real business needs and assess the necessary business support, considering these critical success factors: • Business sponsorship : Is there appropriate executive sponsorship, i.e., an identified and engaged steering committee and commensurate funding? DW/BI projects require strong executive sponsorship. • Business goals and scope : Is there a clearly identified business need, purpose, and scope for the effort? • Business resources: Is there a commitment by business management to the availability and engagement of the appropriate business subject matter experts? The lack of commitment is a common point of failure and a good enough reason to halt a DW/BI project until commitment is confirmed. • Business readiness: Is the business partner prepared for a long -term incremental delivery? Have they committed themselves to establishing centers of excellence to sustain the product in future releases? How broad is the average knowledge or skill gap within the target community and can that be crossed within a single increment? • Vision alignment: How well does the IT Strategy support the Business Vision? It is vital to ensure that desired functional requirements correspond to business capabilities that are or can be sustained in the immediate IT roadmap. Any significant departures or material gaps in capability alignment can stall or stop a DW/BI program. 5.4.1 Dedicated Team Many organizations have a dedicated team to manage the ongoing operations of the production environment. (See Chapter 6). A separate set of hands operating the delivered data product is beneficial to workload optimization as this group has repeating tasks on a calendar cycle and may be further used for any escalation items whereas the maintenance channel will see workload sp ikes aligned to specific deliverables. A front office support group interacts with the maintenance team to foster inter -department relationships and ensure critical activities are addressed in upcoming releases. It notifies the team of any deficiencies to be addressed. A back office support tea m in operations will ensure that production configuration has executed as required. They will escalate alerts and report on throughput status. 6. DW/BI Governance Industries that are highly regulated and need compliance -centric reporting will benefit greatly from a well - governed data warehouse . Critical to ongoing support and vital to release planning is ensuring that governance activities are completed and addressed during the implementation. More and more organizations are extending their Software Development Lifecycle with specific deliverab les aimed at addressing governance needs. Warehouse governance processes should be aligned with risk management. They should be business- driven, Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "390 • DMBOK2 since different kinds of businesses have different needs (e.g., marketing and advertising companies will use their data differently from financial institutions). Governance processes should mitigate risk, not curtail execution. The most critical functions are those that govern the business -operated discovery or refinement area, and those that ensure pristine quality within the warehouse itself. As the refinement area leads all initiative boundaries, handshaking and well running p rocedures are necessary to instantiate, operate, transfer, and discard the data in these areas. Data archival and time horizons are key elements in boundary agreements as they help avoid sprawl. Monitoring of these environments and schedules to determine longevity terms are included in user group sessions as well as management meetings. Loading data into the warehouse means assigning time, resources, and programming efforts to see remediated, credible, high quality data arrive to the end user community, in a timely manner of course. Consider one -off or limited -use events as part of the lifecycle, and perhaps curtail them within the pilot area itself, or within a user -controlled ‘sandbox’ area. Real- time analysis processes can feed time -aligned aggregate results back into the data ware house through an automated process. Policy is defined for the procedures enacted upon the real -time environment, and governance applies to the brokerage of results into the warehouse for organizational consumption. Apply data discrimination to known or cataloged items managed through a risk exposure mitigation matrix. Those items with a deemed high exposure, and low mitigation or difficult early detection, warrant governance functions to curtail the associated risk. Depending on the sensitivity of the data being examined, a separate workspace for selected local personnel may also be required. A thorough review with corporate security and legal personnel during policy formation creates a final safety net. 6.1 Enabling Business Acceptance A key success factor is business acceptance of data , including the data being understandable, having verifiable quality, and having a demonstrable lineage. Sign- off by the Business on the data should be part of the User Acceptance Testing. Perform structured random testing of the data in the BI tool against data in the source systems over the initial load, and after a few update load cycles, to meet sign -off criteria. Meeting these requirements is paramount for every DW/BI implementation. Consider, up -front, a few critically important architectural sub -comp onents, along with their supporting activities: • Conceptual Data Model: What information is core to the organization? What are the key business concepts and how are they related to each other? • Data Quality feedback loop : How are data issues identified and remediated? How are owners of the systems in which issues originate informed about problems and held accountable for fixing them? What is the remediation process for issues that are caused by the DW data integration pro cesses? • End- to-end Metadata : How does the architecture support the integrated end -to-end flow of Metadata? In particular, is access to meaning and context designed into the architecture? How do data consumers answer basic questions like \"What does this report mean?\" or \"What does th is metric mean?\" Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 391 • End- to-end verifiable data lineage : Are the items exposed to business users traceable to the source systems in an automated, maintained manner? Is a system of record identified for all data? 6.2 Customer / User Satisfaction Perceptions of the quality of data will drive customer satisfaction but satisfaction is dependent on other factors as well, such as data consumers’ understanding of the data and the operations team’s responsiveness to identified issues. Collecting, underst anding, and acting on customer feedback can be facilitated through regularly scheduled meetings with user representatives. Such interaction can also help the warehouse team share information about the release roadmap and understand how data consumers are u sing the warehouse. 6.3 Service Level Agreements Business and technical expectations for the environments should be specified in Service Level Agreements (SLAs). Often , the response time, data retention, and availability requirements differ greatly between classes of business needs and their respective supporting systems (e.g., ODS versus DW versus data mart). 6.4 Reporting Strategy Ensure that a reporting strategy exists within and across the BI Portfolio. A reporting strategy includes standards, processes, guidelines, best practices, and procedures. It will ensure users have clear, accurate, and timely information. The reporting strategy must address • Security access to ensure that only entitled users will gain access to sensitive data elements • Access mechanisms to describe how users want to interact, report, examine , or view their data • User community type and appropriate tool to consume it with • Nature of the reports summary, detailed, exception as well as frequency, timing, distribution , and storage formats • Potential use of visualization capabilities to provision graphical output • Trade -offs between timeliness and performance Standard reports should be evaluated periodically to ensure they are still providing value, as just executing reports incurs cost in storage and processing. Implementation and maintenance processes and management activities are critical. Aligning the appropriate reporting tools to the business community is a critical success factor. Depending on the size and nature of the organization, there are probably many different reporting tools used in a variety of processes. Ensure that the audienc e is capable of ma king the best use of the reporting tools; users that are more sophisticated will have increasingly complex demands. Maintain a decision matrix based on these demands to determine upgrades or future tool selection. Data source governance monitoring and control are also vital. Ensure that appropriate levels of data are provisioned securely for authorized personnel, and that subscription data is accessible according to agreed - upon levels. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "392 • DMBOK2 A Center of Excellence can provide training, start- up sets, design best practices, data source tips and tricks and other point solutions or artifacts to help empower business users towards a self- service model. In addition to knowledge management, this center can provide timely communications across the developer, designer, analyst and subscribing user communities. 6.5 Metrics 6.5.1 Usage Metrics DW usage metrics typically include the number of registered users, as well as connected users or concurrent connected users. These metrics show how many people within the organization are using the data warehouse. How many user accounts are licensed for each tool is a gre at start, especially for the auditors. However, how many actually connect with that tool is a better measurement, and how many queries (or query equivalents) are dispatched by a user community per timeframe is an even better technical measurement, especial ly for capacity planning. Allow for multiple analysis metrics such as audit users, generated user query capacity, and consuming users. 6.5.2 Subject Area Coverage Percentages Subject area coverage percentages measure how much of the warehouse (from a data topology perspective) is being accessed by each department. They also highlight which data is shared across departments, and which is not, but could be. Mapping operational source(s) to targets is another natural extension, which enforces and validates the lineage and Metadata already collected, and can provide penetration analysis for which source systems are in analytical use by which departments. This c an help focus tuning efforts on those high impact analytic queries by mitigating any changes to heavily used sourced objects. 6.5.3 Response and Performance Metrics Most query tools measure response time. Retrieve response or performance metrics from tools. This data will inform metrics about the number and type of users. Harvest load times for each data product in raw format from the population processes. These should also be expressed as a percentage of expected support: so a mart that is expected to be refreshed daily and loaded in a four- hour window is 100% supported wh en it loads in four hours. Apply this process to any extracts generated for downstream processing too. Most tools will retain, in a log or repository, query records, data refresh, and data extract times for the objects provided to the users. Divide this data into scheduled and executed objects, and express as raw counts both attempted and succeeded. Highly popular objects or queries performing poorly are likely in -need of attention Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA WAREHOUSING AND BUSINESS INTELLIGENCE • 393 before satisfaction metrics suffer. This can guide defect analysis, maintenance planning, as well as capacity planning if a group of objects is failing regularly. Remediation may vary depending on the tool, but sometimes creating or dropping one index can result in great improvements. (See Chapter 6.) A natural follow on for this is the validation and adjustment of service levels. Adjust items that have consistently failed in the next release, or in the absence of necessary funding, the support level must be reduced. 7. Works Cited / Recommended Adamson, Christopher. Mastering Data Warehouse Aggregates: Solutions for Star Schema Performance . John Wiley and Sons, 2006. Print. Adelman, Sid and Larissa T. Moss. Data Warehouse Project Management. Addison -Wesley Professional, 2000. Print. Adelman, Sid, Larissa Moss and Majid Abai. Data Strategy . Addison -Wesley Professional, 2005. Print. Adelman, Sid, et al. Impossible Data Warehouse Situations: Solutions from the Experts . Addison -Wesley, 2002. Print. Aggarwal, Charu. Data Mining: The Textbook . Springer, 2015. Print. Biere, Mike. Business Intelligence for the Enterprise . IBM Press, 2003. Print. Biere, Mike. The New Era of Enterprise Business Intelligence: Using Analytics to Achieve a Global Competitive Advantage . IBM Press, 2010. Print. IBM Press. Brown, Meta S. Data Mining for Dummies . For Dummies, 2014. Print. For Dummies. Chorianopoulos, Antonios. Effective CRM using Predictive Analytics . Wiley, 2016. Print. Delmater, Rhonda and Monte Hancock Jr. Data Mining Explained; A Manager's Guide to Customer -Centric Business Intelligence . Digital Press, 2001. Print. Dyche, Jill. E -Data: Turning Data Into Information With Data Warehousing . Addison - Wesley, 2000. Print. Eckerson, Wayne W. Performance Dashboards: Measuring, Monitoring, and Managing Your Business . Wiley, 2005. Print . Han, Jiawei, Micheline Kamber and Jian Pei. Data Mining: Concepts and Techniques . 3rd ed. Morgan Kaufmann, 2011. Print. The Morgan Kaufmann Ser in Data Management Systems. Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction . 2nd ed. Springer, 2011. Print. Springer Series in Statistics. Hill, Thomas, and Paul Lewicki. Statistics: Methods and Applications . Statsoft, Inc., 2005. Print. Howson, Cindi. Successful Business Intelligence: Unlock the Value of BI and Big Data . 2nd ed. Mcgraw -Hill Osborne Media, 2013. Print. Imhoff, Claudia, Lisa Loftis, and Jonathan G. Geiger. Building the Customer -Centric Enterprise: Data Warehousing Techniques for Supporting Customer Relationship Management. John Wiley and Sons, 2001. Print. Imhoff, Claudia, Nicholas Galemmo, and Jonathan G. Geiger. Mastering Data Warehouse Design: Relational and Dimensional Techniques . John Wiley and Sons, 2003. Print. Inmon, W. H., Claudia Imhoff, and Ryan Sousa. The Corporate Information Factory . 2nd ed. John Wiley and Sons, 2000. Print. Inmon, W.H., and Krish Krishnan. Building the Unstructured Data Warehouse. Technics Publications, LLC., 2011. Print. Josey, Andrew. TOGAF Version 9.1 Enterprise Edition: An Introduction . The Open Group, 2011. Kindle. Open Group White Paper. Kaplan, Robert S and David P. Norton. The Balanced Scorecard: Translating Strategy into Action . Harvard Business Review Press, 1996. Kindle. Kimball, Ralph, and Margy Ross. The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling. 3d ed. Wiley, 2013. Print. Kimball, Ralph, et al. The Data Warehouse Lifecycle Toolkit. 2nd ed. Wiley, 2008. Print. Kimball, Ralph. The Data Warehouse ETL Toolkit: Practical Techniques for Extracting, Cleaning, Conforming, and Delivering Data . Amazon Digital Services, Inc., 2007. Kindle. Linoff, Gordon S. and Michael J. A. Berry. Data Mining Techniques: For Marketing, Sales, and Customer Relationship Management. 3rd ed. Wiley, 2011. Print. Linstedt, Dan. The Official Data Vault Standards Document (Version 1.0) (Data Warehouse Architecture) . Amazon Digital Services, Inc., 2012. Kindle. Loukides, Mike. What Is Data Science? O'Reilly Media, 2012. Kindle. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "394 • DMBOK2 Lublinsky, Boris, Kevin T. Smith, and Alexey Yakubovich. Professional Hadoop Solutions . Wrox, 2013. Print. Malik, Shadan. Enterprise Dashboards: Design and Best Practices for IT. Wiley, 2005. Print. Morris, Henry. “Analytic Applications and Business Performance Management.” DM Review Magazine, March, 1999. http://bit.ly/2rRrP4x . Moss, Larissa T., and Shaku Atre. Business Intelligence Roadmap: The Complete Project Lifecycle for Decision -Support Applications . Addison -Wesley Professional, 2003. Print. Ponniah, Paulraj. Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals . Wiley -Interscience, 2001. Print. Provost, Foster and Tom Fawcett. Data Science for Business: What you need to know about data mining and data- analytic thinking. O'Reilly Media, 2013. Print. Reeves, Laura L. A Manager’s Guide to Data Warehousing . Wiley, 2009. Print. Russell, Matthew A. Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More . 2nd ed. O'Reilly Media, 2013. Print. Silverston, Len, and Paul Agnew. The Data Model Resource Book Volume 3: Universal Patterns for Data Modeling . Wiley, 2008. Print. Simon, Alan. Modern Enterprise Business Intelligence and Data Management: A Roadmap for IT Directors, Managers, and Architects . Morgan Kaufmann, 2014. Print. Thomsen, Erik. OLAP Solutions: Building Multidimensional Information Systems . 2nd ed. Wiley, 2002. Print. Vitt, Elizabeth, Michael Luckevich and Stacia Misner. Business Intelligence. Microsoft Press, 2008. Print. Developer Reference . WAGmob. Big Data and Hadoop . WAGmob, 2013. Kindle. Wremble, Robert and Christian Koncilia. Data Warehouses and Olap: Concepts, Architectures and Solutions . IGI Global, 2006. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "395 CHAPTER 1 2 Metadata Management 1. Introduction he most common definition of Metadata , “data about data,” is misleadingly simple. The kind of information that can be classified as Metadata is wide -ranging. Metadata includes information about technical and business processes, data rules and constraints, and logical and physical data structures. It describes the data itself (e.g., databases, data elements, data models), the concepts the data represents (e.g., business processes, application systems, software code, technology infrastructure), and the connections (relationships) between the data and concepts. Metadata helps an organization understand its data, its systems, and its workflows. It e nables Data Quality assessment and is integral to the management of databases and other applications. It contributes to the ability to process, maintain, integrate, secure, audit, and govern other data. Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International T Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "396 • DMBOK2 Figure 84 Context Diagram: Metadata Definition : Planning, Implementation, and control activities that contributes to the ability to process, maintain, integrate, secure, audit and govern other data Goals : 1. Provide organizational understanding of business terms and usage including technical lineage. 2. Collect and integrate metadata from diverse sources. 3. Provide a standard way to access metadata and enable known level of trust in data exchange. 4. Ensure metadata quality, consistency, currency, and security. Activities : 1.Define Metadata Strategy (P) 2.Understand Metadata Requirements (P) 1. Business User Requirements 2. T echnical User Requirements 3.Define Metadata Architecture (P) 1. Create Metamodel (D) 2. Apply Metadata Standards (C) 3. Manage Metadata Stores (C) 4.Create and Maintain Metadata (O) 1. Integrate Metadata (O) 2. Distribute and Deliver Metadata (O) 5.Query, Report and Analyze Metadata (O) Inputs : •Business and T echnical Metadata •Operational Requirements •Metadata Policy and Standards •Data Architecture •Metadata Issues •Process Metadata •Operational Metadata •Data Governance MetadataDeliverables : •Metadata Strategy •Metadata Standards •Metadata Architecture •Metamodel •Unified Metadata •Metadata Stores •Data Lineage •Impact Analysis •Metadata Control Process Suppliers : •Business Data Stewards •Data Managers •Data Governance Bodies •Data Modelers •Database AdministratorsConsumers : •Application Developers •Data Analyst •Data Integrators •Business Users •Knowledge Workers •Customers & Collaborators •Data Scientists •Data Journalists •Data CuratorsParticipants : •Data Stewards •Project Managers •Data Architects •Business Analysts •System Analysts T echniques : •Data Lineage and Impact Analysis •Metadata for Big Data IngestT ools : •Metadata Repository Management T ools •Metadata Repositories in other T ools •Integration T oolsMetrics : •Metadata Coverage Scorecard •Metadata Quality Scorecard •Metadata Repository Activities (P) Planning, (C) Control, (D) Development, (O) OperationsMetadata Management Business Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 397 To understand Metadata’s vital role in data management, imagine a large library, with hundreds of thousands of books and magazines, but no card catalog. Without a card catalog, readers might not even know how to start looking for a specific book or even a specific topic. The card catalog not only provides the necessary information (which books and materials the library owns and where they are shelved) it also enables patrons to find materials using different starting points (subject area, author, or title). Without the catalog, finding a specific book would be difficult if not impossible. An organization without Metadata is like a library without a card catalog. Metadata is essential to data management as well as data usage (see multiple references to Metadata throughout the DAMA -DMBOK). All large organizations produce and use a lot of data. Across an organization, different individuals will have different levels of data knowledge, but no individual will know everything about the data. This information must be documented or the organization risks losing valuable knowledge about itself. Metadata provides the primary means of capturing and managing organizational knowledge about data. However, Metadata management is not only a knowledge management challenge; it is also a risk management necessity. Metadata is necessary to ensure an organization can identify private or sensitive data and that it can manage the data lifecycle for its own benefit and in order to meet compliance requirements and minimize risk exposure. Without reliable Metadata, an organization does not know what data it has, what the data represents, where it originates, how it moves through systems, who has access to it, or what it means for the data to be of high quality. Without Metadata, an organization cannot manage its data as an asset. Indeed, without Metadata, an organization may not be able to manage its data at all. As technology has evolved, the speed at which data is generated has also increased. Technical Metadata has become integral to the way in which data is moved and integrated. ISO’s Metadata Registry Standard, ISO/IEC 11179, is intended to enable Metadata -driven exchange of data in a heterogeneous environment, based on exact definitions of data. Metadata present in XML and other formats enables use of the data. Other types of Metadata tagging allow data to be exchanged while retaining signifiers of ownership, security requirements, etc. (See Chapter 8.) Like other data, Metadata requires management. As the capacity of organizations to collect and store data increases, the role of Metadata in data management grows in importance. To be data -driven, an organization must be Metadata- driven. 1.1 Business Drivers Data cannot be managed without Metadata. In addition, Metadata itself must be managed. Reliable, well - managed Metadata helps: • Increase confidence in data by providing context and enabling the measurement of Data Quality • Increase the value of strategic information (e.g., Master Data) by enabling multiple uses • Improve operational efficiency by identifying redundant data and processes • Prevent the use of out- of-date or incorrect data • Reduce data -oriented research time • Improve communication between data consumers and IT professionals • Create accurate impact analysis , thus reducing the risk of project failure • Improve time -to-market by reducing system development life -cycle time Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "398 • DMBOK2 • Reduce training costs and lower the impact of staff turnover through thorough documentation of data context, history, and origin • Support regulatory compliance Metadata assists in representing information consistently, streamlining workflow capabilities, and protecting sensitive information, particularly when regulatory compliance is required. Organizations get more value out of their data assets if their data is of high quality. Quality data depends on governance. Because it explains the data and processes that enable organizations to function, Metadata is critical to Data Governance . If Metadata is a guide to the data in an organization, then it must be well managed. Poorly managed Metadata leads to: • Redundant data and data management processes • Replicated and redundant dictionaries, repositories, and other Metadata storage • Inconsistent definitions of data elements and risks associated with data misuse • Competing and conflicting sources and versions of Metadata which reduce the confidence of data consumers • Doubt about the reliability of Metadata and data Well -executed Metadata management enables a consistent understanding of data resources and more efficient cross -organizational development. 1.2 Goals and Principles The goals of Metadata management include: • Document and manage organizational knowledge of data- related business terminology in order to ensure people understand data content and can use data consistently • Collect and integrate Metadata from diverse sources to ensure people understand similarities and differences between data from different parts of the organization • Ensure Metadata quality, consistency, currency, and security • Provide standard ways to make Metadata accessible to Metadata consumers (people, systems, and processes) • Establish or enforce the use of technical Metadata standards to enable data exchange The implementation of a successful Metadata solution follows these guiding principles: • Organizational commitment: Secure organizational commitment (senior management support and funding) to Metadata management as part of an overall strategy to manage data as an enterprise asset. • Strategy : Develop a Metadata strategy that accounts for how Metadata will be created, maintained, integrated, and accessed. The strategy should drive requirements, which should be defined before evaluating, purchasing, and installing Metadata management products. The Metadata strategy must align with business priorities. • Enterprise perspective : Take an enterprise perspective to ensure future extensibility, but implement through iterative and incremental delivery to bring value. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 399 • Socialization: Communicate the necessity of Metadata and the purpose of each type of Metadata; socialization of the value of Metadata will encourage business use and, as importantly, the contribution of business expertise. • Access : Ensure staff members know how to access and use Metadata. • Quality : Recognize that Metadata is often produced through existing processes (data modeling, SDLC, business process definition) and hold process owners accountable for the quality of Metadata. • Audit : Set, enforce, and audit standards for Metadata to simplify integration and enable use. • Improvement : Create a feedback mechanism so that consumers can inform the Metadata Management team of Metadata that is incorrect or out -of-date. 1.3 Essential Concepts 1.3.1 Metadata vs. Data As stated in the chapter introduction, Metadata is a kind of data, and it should be managed as such. One question that some organizations face is where to draw the line between data that is not Metadata and data that is Metadata. Conceptually, this line is related to the level of abstraction represented by the data. For example, in reporting on the release of the US National Security Administration’s surveillance of the phone usage of people in the US, phone numbers and times of calls were routinely referred to as ‘Metadata’, implying that the ‘real’ data comprised only the content of the phone conversations. Common sense recognizes that telephone numbers and duration of phone calls are also just plain data. 70 A rule of thumb might be that one person’s Metadata is another’s data. Even something that seems like Metadata (e.g., a list of column names) may be just plain data – if, for instance, this data was the input for an analysis aimed at understanding data con tent across different organizations. To manage their Metadata, organizations should not worry about the philosophical distinctions. Instead , they should define Metadata requirements focused on what they need Metadata for (to create new data, understand existing data, enable movement between systems, access data, and share data) and source data to meet these requirements. 1.3.2 Types of Metadata Metadata is often categorized into three types: business, technical , and operational. These categories enable people to understand the range of information that falls under the overall umbrella of Metadata, as well as the functions through which Metadata is produced. That said, the categories could also lead to confusion, especially if people get caught up in questions about which category a set of Metadata belongs or who is supposed to use it. It is best to think of these categories in relation to where Metadata originates rather than how it is used. In 70 Cole, David. “We kill people based on metadata.” New York Review of Books. 10 May 2014. http://bit.ly/2sV1ulS . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "400 • DMBOK2 relation to usage, the distinctions between Metadata types are not strict. Technical and operational staff use ‘business’ Metadata and vice versa. Outside of information technology, for example, in library or information science, Metadata is described using a different set of categories: • Descriptive Metadata (e.g., title, author, and subject) describes a resource and enables identification and retrieval. • Structural Metadata describes relationships within and among resources and their component parts (e.g., number of pages, number of chapters). • Administrative Metadata (e.g., version numbers, archive dates) is used to manage resources over their lifecycle. These categories can help with informing the process of defining Metadata requirements. 1.3.2.1 Business Metadata Business Metadata focuses largely on the content and condition of the data and includes details related to Data Governance. Business Metadata includes the non- technical names and definitions of concepts, subject areas, entities, and attributes; attribute data types and other attribute properties; range descriptions; calculations; algorithms and business rules; valid domain values and their definitions. Examples of Business Metadata include: • Definitions and descriptions of data sets, tables, and columns • Business rules, transformation rules, calculations, and derivations • Data models • Data Quality rules and measurement results • Schedules by which data is updated • Data provenance and data lineage • Data standards • Designations of the system of record for data elements • Valid value constraints • Stakeholder contact information (e.g., data owners, data stewards) • Security/privacy level of data • Known issues with data • Data usage notes 1.3.2.2 Technical Metadata Technical Metadata provides information about the technical details of data, the systems that store data, and the processes that move it within and between systems. Examples of Technical Metadata include: • Physical database table and column names • Column properties • Database object properties Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 401 • Access permissions • Data CRUD (create, replace, update , and delete) rules • Physical data models, including data table names, keys, and indexes • Documented relationships between the data models and the physical assets • ETL job details • File format schema definitions • Source -to-target mapping documentation • Data lineage documentation, including upstream and downstream change impact information • Program and application names and descriptions • Content update cycle job schedules and dependencies • Recovery and backup rules • Data access rights, groups, roles 1.3.2.3 Operational Metadata Operational Metadata describes details of the processing and accessing of data. For example: • Logs of job execution for batch programs • History of extracts and results • Schedule anomalies • Results of audit, balance, and control measurements • Error Logs • Reports and query access patterns, frequency, and execution time • Patches and Version maintenance plan and execution, current patching level • Backup, retention, date created, disaster recovery provisions • SLA requirements and provisions • Volumetric and usage patterns • Data archiving and retention rules, related archives • Purge criteria • Data sharing rules and agreements • Technical roles and responsibilities, contacts 1.3.3 ISO / IEC 11179 Metadata Registry Standard ISO’s Metadata Registry Standard , ISO/IEC 11179, provides a framework for defining a Metadata registry. It is designed to enable Metadata -driven data exchange based on exact definitions of data, beginning with data elements. The standard is structured in several parts: • Part 1: Framework for the Generation and Standardization of Data Elements • Part 3: Basic Attributes of Data Elements • Part 4: Rules and Guidelines for the Formulation of Data Definitions • Part 5: Naming and Identification Principles for Data Elements • Part 6: Registration of Data Elements Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "402 • DMBOK2 1.3.4 Metadata for Unstructured Data By its nature, all data has some structure, though not all of it is formally structured in the familiar rows, columns, and records of relational databases. Any data that is not in a database or data file, including documents or other media, is considered unstructured data. (See Chapter s 9 and 14.) Metadata is as essential to the management of unstructured data as it is to the management of structured data – perhaps even more so. Think again about the card catalog analogy from the chapter i ntroduction. Books and magazines in a library are good examples of unstructured data. The primary use of the Metadata in a card catalog is to find the materials one is looking for, whatever their format. Metadata for unstructured data includes descriptive Metadata, such as catalog information and thesauri keywords; structural Metadata, such as tags, field structures, and format; administrative Metadata, such as sources, update schedules, access rights, and navigation information; bibliographic Metadata, such as library catalog entries; record keeping Metadata, such as retention policies; and preservation Metadata, such as storage, archival conditio n, and rules for conservation. (S ee Chapter 9.) While most assertions about Metadata for unstructured data are connected to traditional content management concerns, new practices are emerging around managing unstructured data in data lakes. Organizations wanting to take advantage of data lakes, using Big Data platforms such as Hadoop, are finding that they must catalog ingested data in order to enable later access. Most put in place processes to collect Metadata as part of data ingestion. A minimum set of Metadata attributes needs to be collected about each object ingested in the data lake (e.g., name, format, source, version, date received, etc.). This produces a catalog of data lake contents. 1.3.5 Sources of Metadata As should be clear from the types of Metadata, Metadata can be collected from many different sources. Moreover, if Metadata from applications and databases has been well -managed, it can simply be harvested and integrated. However, most organizations do not manage Metadata well at the application leve l because Metadata is often created as a by- product of application processing rather than as an end product (i.e., it is not created with consumption in mind). As with other forms of data, there is a lot of work in preparing Metadata before it can be integ rated. The majority of operational Metadata is generated as data is processed. The key to using this Metadata is to collect it in a usable form, and to ensure that those responsible for interpreting it have the tools they need to do so. Keep in mind that interpreting data in places like error logs itself requires Metadata that describes the logs. Similarly, a large portion of technical Metadata can be harvested from database objects. It is possible to reverse engineer knowledge about data from existing systems and to harvest business Metadata from existing data dictionaries, models, and process documentation (Loshin, 2001; Aiken, 1995), but there are risks in doing so. The biggest risk is not knowing how much care was taken to develop and refine the definitions in the first place. If definitions are underdeveloped or ambiguous, then they will not provide data consumers with the information they need to understand the data they are using. It is better to be intentional about developing definitions than to simply accept existing ones. Development of definitions takes time and the right skill set (e.g., writing and facilitation skills). This is why the development of business Metadata require s stewardship. (See Chapter 3. ) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 403 Much of the technical Metadata required to manage databases and the business Metadata required to use data can be collected and developed as part of project work. For example, the process of modeling data requires discussions on the meaning of data elements and the relation between them. Knowledge shared during such discussions should be captured and groomed for use in Data Dictionaries, Business Glossaries, and other repositories. The data models themselves include important details about the physical char acteristics of data. Time should be allocated to ensure that project artifacts contain high quality Metadata that aligns with enterprise standards. Well -defined business Metadata is reusable from project -to-project and can drive a consistent understanding of how business concepts are represented in different data sets. As part of developing Metadata intentionally so that it can be reused, an organizat ion can also plan for the integration of Metadata. For example, it can develop an inventory of systems, and all Metadata related to particular system can be tagged with the same system identifier. Creating Metadata for its own sake rarely works well. Most organizations will not fund this type of effort and, even when they do, they are unlikely to put in place processes for maintenance. In this respect, as in others, Metadata is like other data: It should be created as the product of a well -defined process, using tools that will support its overall quality. Stewards and other data management professionals should ensure that there are processes in place to maintain Metadata related to these processes. For example, if an organization harvests critical Metadata from its data models, it should ensure that there is a change management process in place to keep models current. To give a sense of the breadth of Metadata in any organization, a range of sources is outlined here, in alphabetical rather than priority order. 1.3.5.1 Application Metadata Repositories A Metadata repository refers to the physical tables in which the Metadata is stored. Often these are built into modeling tools, BI tools, and other applications. As an organization matures, it will want to integrate Metadata from repositories in these applications to enable dat a consumers to look across the breadth of information. 1.3.5.2 Business Glossary The purpose of a business glossary is to document and store an organization’s business concepts and terminology, definitions, and the relatio nships between those terms. In many organizations, the business glossary is merely a spreadsheet. However, as organizations mature, they often purchase or build glossaries that contain robust information and the capability to manage it over time. As with all data -oriented systems, bu siness glossaries should be architected to account for hardware, software, database s, processes, and human resources with differing roles and responsibilities. The business glossary application is structured to meet the functional requirements of the three core audiences: • Business users: Data analysts, research analysts, management, and executive staff use the business glossary to understand terminology and data. • Data Stewards : Data Steward s use the business glossary to manage the lifecycle of terms and definitions and to enhance enterprise knowledge by associating data assets with glossary terms; for Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "404 • DMBOK2 example, linking terms to business metrics, reports, Data Quality analysis, or technology components. Data stewards raise terminology and usage issues and help resolve differences across the organization. • Technical users : Technical users use the business glossary to make architecture, systems design, and development decisions, and to conduct impact analysis. The business glossary should capture business terms attributes such as: • Term name, definition, acronym or abbreviation, and any synonyms • Business unit and or application responsible for managing the data associated with the terminology • Name of the person identifying the term, and date updated • Categorization or taxonomy association for the term (business functional association) • Conflicting definitions that need resolution, nature of the problem, action timeline • Common misunderstandings in terms • Algorithms supporting definitions • Lineage • Official or authoritative source for the data supporting the term Every business glossary implementation should have a basic set of reports to support the governance processes. It is recommended that organizations do not ‘print the glossary’ because glossary content is not static. Data stewards are generally responsible for glossary development, use, operations, and reporting. Reporting includes, tracking for new terms and definitions that have not been reviewed yet, those in a pending status, and those that are missing definitions or other attributes. (S ee Section 6.4 .) Ease of use and functionality can vary widely. The simpler and easier business glossary search, the more likely the glossary content will be used. However, the most important characteristic of a glossary is that it contains robust content. 1.3.5.3 Business Intelligence (BI) Tools Business Intelligence tools produce various types of Metadata relevant to the Business Intelligence design including overview information, classes, objects, derived and calculated items, filters, reports, report fields, report layout, reports users, report distribution frequency, and report distribution channels. 1.3.5.4 Configuration Management Tools Configuration management tools or databases (CMDB) provide the capability to manage and maintain Metadata specifically related to the IT assets, the relationships among them, and contractual details of the asset. Each asset in the CMDB database is referred to as a configuration item (CI). Standard Metadata is collected and managed for each CI type. Many organizations integrate the CMDB with the change management processes to identify the related assets or applications impacted by a change to a specific asset. Repositories provide mechan isms to link the assets in the Metadata repository to the actual physical implementation details in CMDB to give a complete picture of the data and the platforms. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 405 1.3.5.5 Data Dictionaries A data dictionary defines the structure and contents of data sets, often for a single database, application, or warehouse. The dictionary can be used to manage the names, descriptions, structure, characteristics, storage requirements, default values, relationships, uniquene ss, and other attributes of every data element in a model. It should also contain table or file definitions. Data dictionaries are embedded in database tools for the creation, operation, and manipulation of data contained in them. To make this Metadata ava ilable to data consumers, it must be extracted from the database or modeling tools. Data dictionaries can also describe in business terminology what data elements are available to the community, provisioned under what security restrictions, and applied in which business process. Time can be saved when defining, publishing, and maintaining a semantic layer for reporting and analysis by leveraging the content directly from the logical model. However, as noted earlier, existing definitions should be used with caution, especially in an organization with a low level of maturity around Metadata management. Many key business processes, relationships, and terminologies are explained during the development of the data model. This information, captured in the logical data model, is often lost when physical structures are deployed to production. A data dictionary can help ensure that this information is not lost entirely to the organization and that the logical and physical models are kept in agreement after production deployment. 1.3.5.6 Data Integration Tools Many data integration tools are used for executables to move data from one system to another or between various modules within the same system. Many of these tools generate transient files, which might contain copies or derived copies of the data. These tools are capable of loading data from various sources and then operating on the loaded data, through grouping, remediation, re -formatting, joining, filtering, or other operations, and then generating output data, which is distributed to the target locations. They document the lineage as data as it moves between systems. Any successful Metadata solution should be able to use the lineage Metadata as it is moves through the integration tools and expose it as a holistic lineage from the actual sources to the final destinations. Data integration tools provide application interfaces (API) to allow external Metadata repositories to extract the lineage information and the transient files Metadata. Once the Metadata repository collects the information, some tools can generate a holist ic lineage diagram for any data element. Data integration tools also provide Metadata about the execution of the various data integration jobs, including last successful run, duration, and job status. Some Metadata repositories can extract the data integration runtime statistics and Metadata and expose it alongside the data elements. (See Chapters 6 and 8 .) 1.3.5. 7 Database Management and System Catalogs Database catalogs are an important source of Metadata. They describe the content of databases, along with sizing information, software versions, deployment status, network uptime, infrastructure uptime, availability, and many other operational Metadata attributes. The most common form of database is relational. Relational databases manage the data as a set of tables and columns, where a table contains one or more columns, indexes, constraints, views, and procedures. A Metadata solution should be able to connect to the vario us databases and Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "406 • DMBOK2 data sets and read all of the Metadata exposed by the database. Some of the Metadata repository tools can integrate the exposed Metadata from the system management tools to provide a more holistic picture about the captured physical assets. 1.3.5.8 Data Mapping Management Tools Mapping management tools are used during the analysis and design phase of a project to transform requirements into mapping specifications, which can then be consumed directly by a data integration tool or used by the developers to generate data integration code. Mapping documenta tion is also often held in Excel documents across the enterprise. Vendors are now considering centralized repositories for the mapping specifications with capabilities to perform version control and change analysis between versions. Many mapping tools integrate with data integration tools to automate the generation of the data integration programs and most can exchange data with other Metadata and Reference Data repositories. (See Chapter 8.) 1.3.5.9 Data Quality Tools Data Quality tools assess the quality of data through validation rules. Most of these tools provide the capability to exchange the quality scores and profiles patterns with other Metadata repositories, enabling the Metadata repository to attach the quality scores to the relevant physical assets. 1.3.5.10 Directories and Catalogs While data dictionaries and glossaries contain detailed information about terminology, tables, and fields, a directory or catalog contains information about systems, sources, and locations of data within an organization. A directory of Metadata is particularly useful to developers and data super users, such as data stewardship teams and data analysts, to understand the scope of data in the enterprise, whether to research issues or to find information about sourcing new applications. 1.3.5.11 Event Messaging Tools Event messaging tools move data between diverse systems. To do so, they require a lot of Metadata. They also generate Metadata that describes this movement. These tools include graphic interfaces through which they manage the logic of data movement. They can export the interface ’s implementation details, movement logic, and processing statistics to other Metadata repositories. 1.3.5.12 Modeling Tools and Repositories Data modeling tools are used to build various types of data models: conceptual, logical, and physical. These tools produce Metadata relevant to the design of the application or system model, like subject areas, logical entities, logical attributes, entity, and attribute relationships, super types and subtypes, tables, columns, Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 407 indexes, primary and foreign keys, integrity constraints, and other types of attribution from the models. Metadata repositories can ingest the models created by these tools and integrate the imported Metadata into the repository. Modeling tools are often the source of data dictionary content. 1.3.5.13 Reference Data Repositories Reference Data documents the business values and descriptions of the various types of enumerated data (domains) and their contextual use in a system. Tools used to manage Reference Data are also capable of managing relationships between the various codified values within the same or across domains. These suites of tools normally provide capabilities to send the collected Reference Data to a Metadata repository, which in turn will provide mechanisms to associate the Reference Data to the business glossary and to the locat ions where it is physically implemented , like columns or fields. 1.3.5.14 Service Registries A service registry manages and stores the technical information about services and service end -points from a service -oriented architecture (SOA) perspective. For example, definitions, interfaces, operations, input and output parameters, policies, versions, and sample usage scenarios. Some of the most important Metadata related to services includes service version, location of service, data center, availability, deployment date, service port, IP address, stats port, connection timeout, and connection retry timeout. Service registries can be interrogated to satisfy various needs , like displaying a list of all available services, services with a specific version, obsolete services, or details about a specific service. Services can also be reviewed for potential re -use. The information contained in these repositories provides important facts on what data exists and how it moves between various systems or applications. Metadata in service repositories can be extracted and incorporated with Metadata collected from other tools to provide a complete picture of how data is moving between the various systems. 1.3.5.15 Other Metadata Stores Other Metadata stores include specialized lists such as event registries, source lists or interfaces, code sets, lexicons, spatial and temporal schema, spatial reference, and distribution of digital geographic data sets, repositories of repositories, and business rules. 1.3.6 Types of Metadata Architecture Like other forms of data, Metadata has a lifecycle. Conceptually, all Metadata management solutions include architectural layers that correspond to points in the Metadata lifecycle: • Metadata creation and sourcing • Metadata storage in one or more repositories • Metadata integration • Metadata delivery Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "408 • DMBOK2 • Metadata usage • Metadata control and management Different architectural approaches can be used to source, store, integrate, maintain, and make Metadata accessible to consumers. 1.3.6.1 Centralized Metadata Architecture A centralized architecture consists of a single Metadata repository that contains copies of Metadata from the various sources. Organizations with limited IT resources or those seeking to automate as much as possible, may choose to avoid this architecture option. Organizations seeking a high degree of consistency within the common Metadata repository can benefit from a centralized architecture. Advantages of a centralized repository include: • High availability, since it is independent of the source systems • Quick Metadata retrieval, since the repository and the query reside together • Resolved database structures not affected by the proprietary nature of third party or commercial systems • Extracted Metadata may be transformed, customized, or enhanced with additional Metadata that may not reside in the source system, improving quality Some limitations of the centralized approach include: • Complex processes are necessary to ensure that changes in source Metadata are quickly replicated into the repository • Maintenance of a centralized repository can be costly • Extraction could require custom modules or middleware • Validation and maintenance of customized code can increase the demands on both internal IT staff and the software vendors Figure 85 shows how Metadata is collected in a standalone Metadata repository with its own internal Metadata store. Figure 85 Centralized Metadata Architecture Metadata Portal ENTERPRISE METADATA REPOSITORY BI Tools Modeling ToolsETL Tools Services RepositoryDBMS ToolsReference DataData Quality ToolsMessaging ToolsConfigura- tion Tools Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 409 The internal store is populated through a scheduled import (arrows) of the Metadata from the various tools. In turn, the centralized repository exposes a portal for the end users to submit their queries. The Metadata portal passes the request to the centralized Metadata repository. The centralized repository will fulfill the request from the collected Metadata. In this type of implementation, the capability to pass the request from the user to various tools directly is not supported. Global search across th e Metadata collected from the various tool is possible due to the collection of various Metadata in the centralized repository. 1.3.6.2 Distributed Metadata Architecture A completely distributed architecture maintains a single access point. The Metadata retrieval engine responds to user requests by retrieving data from source systems in real time; there is no persistent repository. In this architecture, the Metadata management environment maintains the necessary source system catalogs and lookup information needed to process user queries and searches effectively. A common object request broker or similar middleware protocol accesses these source systems. Advantages of distributed Metadata A rchitecture include: • Metadata is always as current and valid as possible because it is retrieved from its source • Queries are distributed, possibly improving response and process time • Metadata requests from proprietary systems are limited to query processing rather than requiring a detailed understanding of proprietary data structures, therefore minimizing the implementation and maintenance effort required • Development of automated Metadata query processing is likely simpler, requiring minimal manual intervention • Batch processing is reduced, with no Metadata replication or synchronization processes Distributed architectures also have limitations: • No ability to support user -defined or manually inserted Metadata entries since there is no repository in which to place these additions • Standardization of presenting Metadata from various systems • Query capabilities are directly affected by the availability of the participating source systems • The quality of Metadata depends solely on the participating source systems Figure 86 Distributed Metadata Architecture Figure 86 illustrates a distributed Metadata A rchitecture. There is no centralized Metadata repository store and the portal passes the users’ requests to the appropriate tool to execute. As there is no centralized store for the Metadata Portal BI Tools Modeling ToolsETL Tools Services RepositoryDBMS ToolsReference DataData Quality ToolsMessaging ToolsConfigura- tion Tools Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "410 • DMBOK2 Metadata to be collected from the various tools, every request has to be delegated down to the sources; hence, no capability exist for a global search across the various Metadata sources. 1.3.6.3 Hybrid Metadata Architecture A hybrid architecture combines characteristics of centralized and distributed architectures. Metadata still moves directly from the source systems into a centralized repository. However, the repository design only accounts for the user -added Metadata, the critical standardized items, and the additions from manual sources. The architecture benefits from the near -real-time retrieval of Metadata from its source and enhanced Metadata to meet user needs most effectively, when needed. The hybrid approach lowers the effort for manual IT intervention and custom- coded access functio nality to proprietary systems. The Metadata is as current and valid as possible at the time of use, based on user priorities and requirements. Hybrid architecture does not improve system availability. The availability of the source systems is a limitation, because the distributed nature of the back -end systems handles processing of queries. Additional overhead is required to link those initial results with the Metadata augmentation in the central reposi tory before presenting the result set to the end user. Many organizations can benefit from a hybrid architecture, including those that have rapidly- changing operational Metadata, those that need consistent, uniform Metadata, and those that experience substantial growth in Metadata and Metadata sources. Organiz ations with more static Metadata and smaller Metadata growth profiles may not see the maximum potential from this architecture alternative. 1.3.6.4 Bi-Directional Metadata Architecture Another advanced architectural approach is bi -directional Metadata A rchitecture , which allows Metadata to change in any part of the architecture (source, data integration, user interface) , and then feedback is coordinated from the repository (broker) into its original source. Various challenges are apparent in this approach. The design forces the Metadata repository to contain the latest version of the Metadata source and forces it to manage changes to the source, as well. Changes must be trapped systematically, and then resolved. Additional sets of process interfaces to tie the repository back to the Metadata source(s) must be built and maintained. Figure 87 illustrates how common Metadata from different sources is collected in a centralized Metadata store. Users submit their queries to the Metadata portal, which passes the request to a centralized repository. The centralized repository will try to fulfill the user request from the common Metadata collected initially from the various sources. As the request becomes more specific or the user needs more detailed Metadata then the centralized repository will delegate down to the specific source to research the sp ecific details. Global search across the various tools is available due to the common Metadata collected in the centralized repository. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 411 Figure 87 Bi-Directional Metadata Architecture 2. Activities 2.1 Define Metadata Strategy A Metadata strategy describes how an organization intends to manage its Metadata and how it will move from current state to future state practices. A Metadata strategy should provide a framework for development teams to improve Metadata management. Developing Metadata requirements will help clarify the drivers of the strategy and identify potential obstacles to enacting it. The strategy includes defining the organization’s future state enterprise Metadata A rchitecture and the implementation phases required to meet strategic objectives. Steps include: • Initiate Metadata strategy planning : The goal of initiation and planning is to enable the Metadata strategy team to define its short- and long -term goals. Planning includes drafting a charter, scope, and objectives aligned with overall governance efforts and establishing a communications plan to support the effort. Key stakeholders should be in volved in planning. • Conduct key stakeholder interviews : Interviews with business and technical stakeholder provide a foundation of knowledge for the Metadata strategy. • Assess existing Metadata sources and information architecture : Assessment determines the relative degree of difficulty in solving the Metadata and systems issues identified in the interviews and documentation review. During this stage, conduct detailed interviews of key IT staff and review documentation of the system architectures, data models, etc. Metadata Portal ENTERPRISE METADATA REPOSITORY BI Tools Modeling ToolsETL Tools Services RepositoryDBMS ToolsReference DataData Quality ToolsMessaging ToolsConfigura- tion ToolsBI MetadataModeling MetadataETL MetadataServices MetadataDBMS MetadataReference MetadataData Quality MetadataMessaging MetadataConfigura- tion Metadata Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "412 • DMBOK2 • Develop future Metadata Architecture : Refine and confirm the future vision, and develop the long - term target architecture for the managed Metadata environment in this stage. This phase must account for strategic components, such as organization structure, alignment with Data Governance and stewardship, managed Metadata A rchitecture, Metadata delivery architecture, technical architecture, and security architecture. • Develop a phased implementation plan : Validate, integrate, and prioritize findings from the interviews and data analyses. Document the Metadata strategy and define a phased implementation approach to move from the existing to the future managed Metadata environment. The strategy will evolve over time, as Metadata requirements, the architecture, and the lifecycle of Metadata are better understood. 2.2 Understand Metadata Requirements Metadata requirements start with content: What Metadata is needed and at what level. For example, physical and logical names need to be captured for both columns and tables. Metadata content is wide -ranging and requirements will come from both business and technical data consu mers. (See S ection 1.3.2.) There are also many functionality -focused requirements associated with a comprehensive Metadata solution: • Volatility : How frequently Metadata attributes and sets will be updated • Synchronization: Timing of updates in relation to source changes • History : Whether historical versions of Metadata need to be retained • Access rights : Who can access Metadata and how they access, along with specific user interface functionality for access • Structure : How Metadata will be modeled for storage • Integration : The degree of integration of Metadata from different sources; rules for integration • Maintenance : Processes and rules for updating Metadata (logging and referring for approval) • Management : Roles and responsibilities for managing Metadata • Quality : Metadata quality requirements • Security: Some Metadata cannot be exposed because it will reveal the existence of highly protected data 2.3 Define Metadata Architecture A Metadata Management system must be capable of extracting Metadata from many sources. Design the architecture to be capable of scanning the various Metadata sources and periodically updating the repository. The system must support the manual updates of Metadata, requests, searches, and lookups of Metadata by various user groups. A managed Metadata environment should isolate the end user from the various and disparate Metadata sources. The architecture should provide a single access point for the Metadata repository. The access point must supply all related Metadata resources transparently to the user. Users sho uld be able to access Metadata without being Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 413 aware of the differing environments of the data sources. In analytics and Big Data solutions, the interface may have largely user -defined functions (UDF) to draw on various data sets, and the Metadata exposure to the end user is inherent to those customiza tions. With less reliance on UDF in solutions, end users will be gathering, inspecting, and using data sets more directly and various supporting Metadata is usually more exposed. Design of the architecture depends on the specific requirements of the organization. Three technical architectural approaches to building a common Metadata repository mimic the approaches to designing data warehouses: centralized, distributed, and hybrid ( see S ection 1.3.6). These approaches all take into account implementation of the repository, and how the update mechanisms operate. 2.3.1 Create MetaModel Create a data model for the Metadata repository , or metamodel, as one of the first design steps after the Metadata strategy is complete and the business requirements are understood. Different levels of metamodel may be developed as needed; a high -level conceptual model, that explains the relationships between systems, and a lower level metamodel that details the attributions, to describe the elements and processes of a model. In addition to being a planning tool and a means of articulating requirements, the metamodel is in itself a valuable source of Metadata. Figure 88 depicts a sample Metadata repository metamodel. The boxes represent the high - level major entities, which contain the data. Figure 88 Example Metadata Repository Metamodel 2.3.2 Apply Metadata Standards The Metadata solution should adhere to the agreed -upon internal and external standards as identified in the Metadata strategy. Metadata should be monitored for compliance by governance activities. Organization internal Metadata standards include naming conventions, cu stom attributions, security, visibility, and processing documentation. Organization external Metadata standards include the data exchange formats and application -programming interfaces design. SystemBusiness Glossary Data Model Data Store ApplicationGlossary T erms Entity File/Table Attribute Field/Column Code SetsCodified Domain Code Value Business ValueArchitecture Business Metadata Logical Data Physical Data T echnical Metadata Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "414 • DMBOK2 2.3.3 Manage Metadata Stores Implement control activities to manage the Metadata environment. Control of repositories is control of Metadata movement and repository updates performed by the Metadata specialist. These activities are administrative in nature and involve monitoring and responding to reports, warnings, job logs, and resolving various issues in the implemented repository environment. Many control activities are standard for data operations and interface maintenance. Control activities should have Data Governance oversight. Control activities include: • Job scheduling and monitoring • Load statistical analysis • Backup, recovery, archive, purging • Configuration modifications • Performance tuning • Query statistics analysis • Query and report generation • Security management Quality control activities include: • Quality assurance, quality control • Frequency of data update – matching sets to timeframes • Missing Metadata reports • Aging Metadata report Metadata management activities include: • Loading, scanning, importing , and tagging assets • Source mapping and movement • Versioning • User interface management • Linking data sets Metadata maintenance – for NOSQL provisioning • Linking data to internal data acquisition – custom links and job Metadata • Licensing for external data sources and feeds • Data enhancement Metadata, e.g., Link to GIS And training, including: • Education and training of users and data stewards • Management metrics generation and analysis • Training on the control activities and query and reporting Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 415 2.4 Create and Maintain Metadata As described in Section 1.3.5, Metadata is created through a range of processes and stored in many places within an organization. To be of high quality, Metadata should be managed as a product. Good Metadata is not created by accident. It requires planning. (See Chapter 13. ) Several general principles of Metadata management describe the means to manage Metadata for quality: • Accountability : Recognize that Metadata is often produced through existing processes (data modeling, SDLC, business process definition) and hold process owners accountable for the quality of Metadata. • Standards : Set, enforce, and audit standards for Metadata to simplify integration and enable use. • Improvement : Create a feedback mechanism so that consumers can inform the Metadata Management team of Metadata that is incorrect or out -of-date. Like other data, Metadata can be profiled and inspected for quality. Its maintenance should be scheduled or completed as an auditable part of project work. 2.4.1 Integrate Metadata Integration processes gather and consolidate Metadata from across the enterprise, including Metadata from data acquired outside the enterprise. The Metadata repository should integrate extracted technical Metadata with relevant business, processes, and stewardship Metadata. Metadata can be extracted using adapters, scanners, bridge applications, or by directly accessing the Metadata in a source data store. Adapters are availabl e with many third party vendor software tools, as well as from Metadata integration tools. In some cases, adapters will be developed using the tool API’s. Challenges arise in integration that will require governance. Integrating internal data sets, external data such as government statistics, and data sourced from non -electronic forms, such as white papers, articles in magazines, or reports, can raise numero us questions on quality and semantics. Accomplish repository scanning in two distinct approaches. • Proprietary interface : In a single -step scan and load process, a scanner collects the Metadata from a source system, then directly calls the format -specific loader component to load the Metadata into the repository. In this process, there is no format- specific file output and the collection and loading of Metadata occurs in a single step. • Semi -proprietary interface : In a two -step process, a scanner collects the Metadata from a source system and outputs it into a format- specific data file. The scanner only produces a data file that the receiving repository needs to be able to read and load appropriately. The interface is a more open architecture, as the file is readable by many methods. A scanning process uses and produces several types of files during the process. • Control file : Containing the source structure of the data model Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "416 • DMBOK2 • Reuse file: Containing the rules for managing reuse of process loads • Log files : Produced during each phase of the process, one for each scan or extract and one for each load cycle • Temporary and backup files : Use during the process or for traceability Use a non- persistent Metadata staging area to store temporary and backup files. The staging area supports rollback and recovery processes and provides an interim audit trail to assist repository managers when investigating Metadata source or quality issues . The staging area may take the form of a directory of files or a database. Data Integration tools used for data warehousing and Business Intelligence applications are often used effectively in Metadata integration processes. (See Chapter 8. ) 2.4.2 Distribute and Deliver Metadata Metadata is delivered to data consumers and to applications or tools that require Metadata feeds. Delivery mechanisms include: • Metadata intranet websites for browse, search, query, reporting, and analysis • Reports, glossaries, and other documents • Data warehouses, data marts, and BI (Business Intelligence) tools • Modeling and software development tools • Messaging and transactions • Web services and Application Programming Interfaces (APIs) • External organization interface solutions (e.g., supply chain solutions) The Metadata solution often links to a Business Intelligence solution, so that both the scope and the currency of Metadata synchronize with the BI content. A link provides a means of integration into the delivery of BI to the end user. Similarly, some CRM (Customer Relationship Management) or other ERP (Enterprise Resource Planning) solutions may require Metadata integration at the application delivery layer. Metadata is exchanged with external organizations using files (flat, XML, or JSON structured) or through web services. 2.5 Query, Report, and Analyze Metadata Metadata guides the use of data assets. Use Metadata in Business Intelligence (reporting and analysis), business decisions (operational, tactical, strategic), and in business semantics (what they say, what they mean – business lingo). A Metadata repository must have a front -end application that supports the search- and-retrieval functionality required for all this guidance and management of data assets. The interface provided to business users may have a different set of functional requirements than that for technical users and developers. Some reports facilitate future development such as change impact analysis, or trouble shoot varying definitions for data warehouse and Business Intelligence projects, such as data lineage reports. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 417 3. Tools The primary tool used to manage Metadata is the Metadata repository. This will include an integration layer and often an interface for manual updates. Tools that produce and use Metadata become sources of Metadata that can be integrated into a Metadata rep ository. 3.1 Metadata Repository Management Tools Metadata Management tools provide capabilities to manage Metadata in a centralized location (repository). The Metadata can be either manually entered or extracted from various other sources through specialized connect ors. Metadata repositories also provide capabilities to exchange Metadata with other systems. Metadata management tools and repositories themselves are also a source of Metadata, especially in a hybrid Metadata architectural model or in large enterprise implementations. Metadata management tools allow for the exchange of the collected Metadata with other Metadata repositories, enabling the collection of various and diverse Metadata from different sources into a centralized repository, or enabling the enriching and standardization of the diverse Metadata as it moves between the repositories. 4. Techniques 4.1 Data Lineage and Impact Analysis A key benefit of discovering and documenting Metadata about the physical assets is to provide information on how data is transformed as it moves between systems. Many Metadata tools carry information about what is happening to the data within their environments and provide capabilities to view the lineage ac ross the span of the systems or applications they interface. The current version of the lineage based on programming code is referred to as ‘As Implemented Lineage’. In contrast, lineage describe in mapping specification documents is referred to as ‘As Designed Lineage’. The limitations of a lineage build are based on the coverage of the Metadata management system. Function - specific Metadata repositories or data visualization tools have information about the data lineage within the scope of the environments they interact with but will not provide visibility to what is happening to the data outside their environments. Metadata management systems import the ‘As Implemented’ lineage from the various tools that can provide this lineage detail and then augment the data lineage with the ‘As Designed’ from the places where the actual implementation details is not extractable. The process of connecting the pieces of the data lineage referred to as stitching. It results in a holistic visualization of the data as it moves from its original locations (official source or system of record) until it lands in its final destination. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "418 • DMBOK2 Figure 89 shows a sample data element lineage. In reading this, the ‘Total Backorder’ business data element, which is physically implemented as column zz_total, depends on 3 other data elements: ‘Units Cost in Cents’ physically implemented as ‘yy_unit_cost’, ‘Tax i n Ship to State’ implemented in ‘yy_tax’ and ‘Back Order Quantity’ implemented in ‘yy_qty’. Although a lineage graphic, such as in Figure 89, describes what is happening to a particular data element, not all business users will understand it. Higher levels of lineage (e.g., ‘System Lineage’) summarize movement at the system or application level. Many visualization tools provide zoom -in / zoom -out capability, to show data element lineage in the context of system lineage. For example, Figure 90 shows a sample system lineage, where at a glance, general data movement is understood and visualized at a system or an application level. Figure 89 Sample Data Element Lineage Flow Diagram Figure 90 Sample System Lineage Flow Diagram *)Restricted Information *)Refreshed Weekly *)Includes Cancelled Orders *)US Orders Only, for Intl see.. *)Steward : John Doe *)Currency is US Dollars Unit Cost in Cents yy_unt_cost Order History zz_ord_tran_histActive Order xx_cur_ordShip T o State yy_state_cd Back Order Quantity yy_qtyTax in Ship T o State yy_taxT otal Backorder zz_total System 1 System 2WarehouseSystem 3 System 4 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 419 As the number of data elements in a system grows, the lineage discovery becomes complex and difficult to manage. In order to successfully achieve the business goals, a strategy for discovering and importing assets into the Metadata repository requires planning and design. Successful lineage discovery needs to account for bo th business and technical focus: • Business focus: Limit the lineage discovery to data elements prioritized by the business. Start from the target locations and trace back to the source systems where the specific data originates. By limiting the scanned assets to those that move, transfer, or update the selected data elements, this approach will enable business data consumers to understand what is happening to the specific data element as it moves through systems. If coupled with Data Quality measurements, lineage can be used to pinpoint where system design adversely impacts the quality of the data. • Technical focus : Start at the source systems and identify all the immediate consumers, then identify all the subsequent consumers of the first set identified and keep repeating these steps until all systems are identified. Technology users benefit more from the system discovery strategy in order to help answer the various questions about the data. This approach will enable technology and business users to answer question about discovering data elements across the enterprise, like “Where is social security number?” or generate impact reports like “What systems are impacted if the width of a specific column is changed?” This strategy can, however, be complex to manage. Many data integration tools offer lineage analysis that considers not only the developed population code but the data model and the physical database as well. Some offer business user facing web interfaces to monitor and update definitions. These begin to look like business glossaries. Documented lineage helps both business and technical people use data. Without it, much time is wasted in investigating anomalies, potential change impacts, or unknown results. Look to implement an integrated impact and lineage tool that can understand all the moving parts involved in the load process as well as end user reporting and analytics. Impact reports outline which components are affected by a potential change expediting and streamlining estimating and maintenance tasks. 4.2 Metadata for Big Data Ingest Many data management professionals are familiar and comfortable with structured data stores, where every item can be clearly identified and tagged. Nowadays, though, much data comes in less structured formats. Some unstructured sources will be internal to the organization, and some will be external. In either case, there is no longer a need to physically bring the data to one place. Through the new technologies, the program will go to the data as opposed to moving the data to the program, reducing the amount of data movement, and speeding up the execution of the process. Nevertheless, successful data management in a data lake depends on managing Metadata. Metadata tags should be applied to data upon ingestion. Metadata then can be used to identify data content available for access in the data lake. Many ingestion engines profile data as it is ingested. Data profiling can identify data domains, relationships, and Data Quality issues. It can also enable tagging. On ingestion, Metadata tags can be added to identify sensitive or private (like Personally Identifiable Information – PI I) data, for example. Data scientists may add confidence, textual identifiers, and codes representing behavior clusters. ( See Chapter 14. ) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "420 • DMBOK2 5. Implementation Guidelines Implement a managed Metadata environment in incremental steps in order to minimize risks to the organization and to facilitate acceptance. Implement Metadata repositories using an open relational database platform. This allows development and implementation of various controls and interfaces tha t may not be anticipated at the start of a repository development project. The repository contents should be generic in design, not merely reflecting the source system database designs. Design contents in alignment with the enterprise subject area experts, and based on a comprehensive Metadata model. Planning should account for integrating Metadata so that data consumers can see across different data sources. The ability to do so will be one of the most valuable capabilities of the repository. It should hou se current, planned, and historical versions of the Metadata. Often, the first implementation is a pilot to prove concepts and learn about managing the Metadata environment. Integration of Metadata projects into the IT development methodology is necessary. There will be variations depending on architecture and types of storage. 5.1 Readiness Assessment / Risk Assessment Having a solid Metadata strategy helps everyone make more effective decisions. First and foremost, people should be aware of the risks of not managing Metadata. Assess the degree to which the lack of high quality Metadata might result in: • Errors in judgment due to incorrect, incomplete , or invalid assumptions or lack of knowledge about the context of the data • Exposure of sensitive data, which may put customers or employees at risk, or impact the credibility of the business and lead to legal expenses • Risk that the small set of SMEs who know the data will leave and take their knowledge with them Risk is reduced when an organization adopts a solid Metadata strategy. Organizational readiness is addressed by a formal assessment of the current maturity in Metadata activities. The assessment should include the critical business data elements, available Metadata glossaries, lineage, data profiling and Data Quality processes, MDM (Master Data Management) maturity, and other aspects. Findings from the assessment, aligned with business priorities, will provide the basis for a strategic approach to improvement of Metadata Management practices. A formal assessment also provides the basis for a business case, sponsorship and funding. The Metadata strategy may be part of an overall Data Governance strategy or it may be the first step in implementing effective Data Governance . A Metadata assessment should be conducted via objective inspection of existing Metadata, along with interviews with key stakeholders. The deliverables from a risk assessment include a strategy and roadmap. 5.2 Organizational and Cultural Change Like other data management efforts, Metadata initiatives often meet with cultural resistance. Moving from an unmanaged to a managed Metadata environment takes work and discipline. It is not easy to do, even if most Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 421 people recognize the value of reliable Metadata. Organizational readiness is a major concern, as are methods for governance and control. Metadata Management is a low priority in many organizations. An essential set of Metadata needs coordination and commitment in an organization. It can be structures of employee identification data, insurance policy numbers, vehicle identification numbers, or product specifications, which if changed, would require major overhauls of many enterprise systems. Look for that good example where control will reap immediate quality benefits for data in the company. Build the argument from concrete business -relevant examples. Implementation of an enterprise Data Governance strategy needs senior management support and engagement. It requires that business and technology staff be able to work closely together in a cross -functional manner. 6. Metadata Governance Organizations should determine their specific requirements for the management of the Metadata lifecycle and establish governance processes to enable those requirements. It is recommended that formal roles and responsibilities be assigned to dedicated resources, especially in large or business critical areas. Metadata governance processes themselves depend on reliable Metadata, so the team charged with managing Metadata can test principles on the Metadata they create and use. 6.1 Process Controls The Data Governance team should be responsible for defining the standards and managing status changes for Metadata – often with workflow or collaboration software – and may be responsible for promotional activities and training development or actual training across the organization. More mature Metadata governance will require business terms and definitions to progress through varying status changes or governance gates; for example, from a candidate term, to approved, to published, and to a final point in the lifecycle of replaced or retired. The governance team may also manage business term associations such as related terms, as well as the categorization of and grouping of the terms. Integration of the Metadata strategy into the SDLC is needed to ensure that changed Metadata is collected when it is changed. This helps ensure Metadata remains current. 6.2 Documentation of Metadata Solutions A master catalog of Metadata will include the sources and targets currently in scope. This is a resource for IT and business users and can be published out to the user community as a guide to ‘what is where’ and to set expectations on what they will find: • Metadata implementation status • Source and the target Metadata store Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "422 • DMBOK2 • Schedule information for updates • Retention and versions kept • Contents • Quality statements or warnings (e.g., missing values) • System of record and other data source statuses (e.g., data contents history coverage, retiring or replacing flags) • Tools, architectures, and people involved • Sensitive information and removal or masking strategy for the source In documents and content management, data maps show similar information. Visualizations of the overall Metadata integration systems landscape are also maintained as part of Metadata documentation. (See Chapter 9.) 6.3 Metadata Standards and Guidelines Metadata standards are essential in the exchange of data with operational trading partners. Companies realize the value of information sharing with customers, suppliers, partners, and regulatory bodies. The need for sharing common Metadata to support the optimal us age of shared information has spawned many sector -based standards. Adopt industry- based and sector -sensitive Metadata standards early in the planning cycle. Use the standards to evaluate Metadata Management technologies. Many leading vendors support multip le standards, and some can assist in customizing industry -based and sector -sensitive standards. Tool vendors provide XML and JSON or REST support to exchange data for their data management products. They use the same strategy to bind their tools together into suites of solutions. Technologies, including data integration, relational and multidimension al databases, requirements management, Business Intelligence reporting, data modeling, and business rules, offer import and export capabilities for data and Metadata using XML. Vendors maintain their proprietary XML schemas and document type definitions (D TD) or more commonly the XML schema definitions (XSD). These are accessed through proprietary interfaces. Custom development is required to integrate these tools into a Metadata management environment. Guidelines include templates and associated examples and training on expected inputs and updates including such rules as ‘do not define a term by using the term’ and completeness statements. Different templates are developed for different types of Metadata, and are driven in part by the Metadata solution selected. Ongoing monitoring of guidelines for effectiveness and necessary updates is a governance responsibility. The ISO standards for Metadata provide guidance for tool developers but are unlikely to be a concern for organizations who implement using commercial tools, since the tools should meet the standards. Regardless, it can be helpful to have a good understanding of these standards and their repercussions. 6.4 Metrics It is difficult to measure the impact of Metadata without first measuring the impact of the lack of Metadata. As part of risk assessment, obtain metrics on the amount of time data consumers spend searching for information, in order to show improvement after the Metadata solution is put in place. The effectiveness of the Metadata Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "METADATA MANAGEMENT • 423 implementation can also be measured in terms of the completeness of the Metadata itself, of the management routines associated with it, and of Metadata usage. Suggested metrics on Metadata environments include: • Metadata repository completeness : Compare ideal coverage of the enterprise Metadata (all artifacts and all instances within scope) to actual coverage. Reference the strategy for scope definitions. • Metadata Management Maturity : Metrics developed to judge the Metadata maturity of the enterprise, based on the Capability Maturity Model (CMM- DMM) approach to maturity assessment. (See Chapter 15.) • Steward representation : Organizational commitment to Metadata as assessed by the appointment of stewards, coverage across the enterprise for stewardship, and documentation of the roles in job descriptions. • Metadata usage : User uptake on the Metadata repository usage can be measured by repository login counts. Reference to Metadata by users in business practice is a more difficult measure to track. Anecdotal measures on qualitative surveys may be required to capture this measure . • Business Glossary activity : Usage, update, resolution of definitions, coverage . • Master Data service data compliance : Sho ws the reuse of data in SOA solutions. Metadata on the data services assists developers in deciding when new development could use an existing service. • Metadata documentation quality : Assess the quality of Metadata documentation through both automatic and manual methods. Automatic methods include performing collision logic on two sources, measuring how much they match, and the trend over time. Another metric would measure the percentage of at tributes that have definitions, trending over time. Manual methods include random or complete survey, based on enterprise definitions of quality. Quality measures indicate the completeness, reliability, currency, etc., of the Metadata in the repository. • Metadata repository availability : Uptime, processing time (batch and query). 7. Works Cited / Recommended Aiken, Peter. Data Reverse Engineering: Slaying the Legacy Dragon . 1995. Foreman, John W. Data Smart: Using Data Science to Transform Information into Insight. Wiley, 2013. Print. Loshin, David. Enterprise Knowledge Management: The Data Quality Approach . Morgan Kaufmann, 2001. Marco, David. Building and Managing the Meta Data Repository: A Full Lifecycle Guide . Wiley, 2000. Print. Milton, Nicholas Ross. Knowledge Acquisition in Practice: A Step -by-step Guide. Springer, 2007. Print. Decision Engineering. Park, Jung- ran, ed. Metadata Best Practices and Guidelines: Current Implementation and Future Trends . Routledge, 2014. Print. Pomerantz, Jeffrey. Metadata . The MIT Press, 2015. Print. The MIT Press Essential Knowledge ser. Schneier, Bruce. Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World . W. W. Norton and Company, 2015. Print. Tannenbaum, Adrienne. Implementing a Corporate Repository: The Models Meet Reality . Wiley, 1994. Print. Wiley Professional Computing. Warden, Pete. Big Data Glossary . O'Reilly Media, 2011. Print. Zeng, Marcia Lei and Jian Qin. Metadata . 2nd ed. ALA Neal -Schuman, 2015. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "424 CHAPTER 1 3 Data Quality Management 1. Introduction Data Quality can be defined as the degree to which dimensions of Data Quality meet the requirements. This implies that requirements should be formulated for each (relevant) dimension. A much shorter definition for quality of data is ‘fit for purpose.’ Data that meets the requirements are of sufficient quality; data that doesn’t meet the requirements are of insufficient quality. To keep it simple, we respectively speak of high and low, or poor quality data. Effective Data Management involves a set of interrelated processes enabling an organization to use its data to achieve strategic goals. An underlying assertion is that the data itself is of high quality. Data Quality Data Warehousing & Business Intelligence Reference & Master DataDocument &Content ManagementData Integration & InteroperabilityData SecurityData Storage & OperationsData Modeling & DesignData Architecture Data Quality MetadataData Governance DAMA -DMBOK2 Data Management Framework Copyright © 2017 by DAMA International Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 425 Management is the planning, implementation , and control of activities that apply quality management techniques to data in order to assure it is fit for consumption and meets the needs of data consumers. High quality data is context driven. This means that the same data may be simultaneously viewed as high quality by some areas of an organization while being viewed as low quality by other areas. Many organizations fail to engage with this question of context, that is, high Data Quality being that which is fit for purpose. If we understand organizations as data manufacturing machines, we can assert (from our experience in manufacturing) that organizations that formally manage the quality of data will be more effective, more efficient and deliver a better experience than thos e that leave Data Quality to chance. However, no organization has perfect business processes, technical processes, or data management practices. In reality, all organizations experience problems related to their Data Quality. Many factors undermine quality data: lack of understanding about the effects on organizational success, leadership that does not value Data Quality, poor planning, ‘siloed’ system design, inconsistent development processes, incomplete documentation, a lack of standards, or a lack of Data Governance . As is the case with Data Governance and with Data Management as a whole, Data Quality Management is a function, not a program or project. This is because projects and even programs have starts, middles, and ends. A Data Quality Function is, or should be, a continuing business as usual set of activities. It will include both projects and programs (to address specific Data Quality improvements) as well as operational work, along with a commitment to communications and training. Most importa ntly, the long -term success of a Data Quality improvement program depends on getting an organization to change its culture and adopt a quality mindset. As stated in The Leader’s Data Manifesto , “fundamental, lasting chang e requires committed leadership and involvement from people at all levels in an organization.” People who use data to do their jobs – which in most organizations is a very large percentage of employees – need to drive change. One of the most critical chang es to focus on is how their organizations manage and improve the quality of their data. 71 Formal Data Quality Management is similar to continuous quality management for other product manufacturing. It includes managing data through its lifecycle by setting standards, building quality into the processes that create, transform, and store data, an d measuring data against standards. Managing data to this level usually requires a Data Quality Function team. The Data Quality Function team is responsible for engaging both business and technical data management professionals and driving the work of applying quality management techniques to data to ensure that data is fit for consumption for a variety of purposes. The team will likely be involved with a series of projects through which they can establish processes and best practices while addressing high priority data issues. 71 For the full text of The Leader’s Data Manife sto, see http://bit.ly/2sQhcy7. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "426 • DMBOK2 Figure 91 Context Diagram : Data Quality Definition: The planning, implementation, and control of activities that apply techniques for collecting and handling data ensuring it addresses the needs of the enterprise and local consumer and is fit for use. Goals : 1. Develop a governed approach to make data fit for purpose based on data consumers’ requirements. 2. Define standards, requirements, and specifications for data quality controls as part of the data lifecycle. 3. Define and implement processes to measure, monitor, and report on data quality levels. 4. Identify and advocate for opportunities to improve the quality of data, through process and system improvements. Activities : 1.Define a Data Quality Framework (P) 2.Define High Quality Data (P) 3.Identify Dimensions and supporting Business Rules (P) 4.Perform an Initial Data Quality Assessment (P) 5.Identify and Prioritize Improvements (P) 6.Define Goals for Data Quality Improvement (P) 7.Develop and Deploy Data Quality Operations (D,O,C) 1. Manage Data Quality Rules 2. Measure and Monitor Data Quality 3. Develop Procedures for Managing Data Issues 4. Establish Data Quality Service Level Agreements 5. Data Quality ResponseInputs : •Data Policies and Standards •Data Quality Expectations •Business Requirements •Business Rules •Data Requirements •Business Metadata •T echnical Metadata •Operational Metadata •Data Sources and Data Stores •Data Lineage •Data Issues •Data Sharing AgreementsDeliverables : •Data Quality Strategy & Framework •Data Quality Improvement Program •Analyses from Data Profiling •Recommendations based on Root Cause Analysis of Issues •Data Quality Procedures •Data Quality Reports •Data Quality Governance Reports •Data Quality Service Level Agreements •Data Quality Policies and Guidelines •Remediated Data •Certified Data Suppliers : •Business Management •Subject Matter Experts •Data Analysts •Data Architects •Data Modelers •System Specialists •Data Stewards •Business Process Analysts •Data VendorsConsumers : •Knowledge Workers •Data Stewards •Data Professionals •IT Operations •Data Governance Bodies •Partner Organizations •Centers of ExcellenceParticipants : •CDO •Data Quality Analysts •Data Stewards •Data Owners •Data Analysts •Subject Matter Experts •Database and Integration Specialists •Data Professionals •DQ Managers •IT Operations T echniques : •Data Quality Metrics •Profiling •Preventive and Corrective Actions •Root Cause Analysis •CorrectionsT ools : •Data Profiling T ools •Business Rule Engine •Data Parsing and Formatting •Data Transformation and Standardization •Data Enrichment •Incident ManagementMetrics : •Return on Investment •Levels of Quality •Data Quality Scorecard •Data Issues Reports •Service Level Conformance •Data Quality Plan Progress (P) Planning, (C) Control, (D) Development, (O) OperationsData Quality Management Business Drivers T echnical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 427 1.1 Business Drivers The business drivers for improving the trust in data through formal Data Quality Management are: • Enhancing the stakeholder experience and organization’s reputation • Increasing the effectiveness of the organization • Reducing risks and costs associated with poor quality data • Improving organizational efficiency and productivity. Data Quality Management achieves these outcomes using a systematic approach by: • Understanding the context driving the need for the data (fitness) • Matching the Data Quality needs to this context (purpose) • Ensuring this quality is achieved repeatably through stable process. The tangible benefits that organizations experience with data of sufficient quality include: • Customers trusting the data to be correct and have confidence in dealing with the organization • Employees answering questions more quickly and consistently • Revenue gains through identifying business opportunities and effective invoicing • Decreased customer service calls and ability to solve the calls when they arrive • Staff spending less time trying to figure out if the data is right and more time using the data to gain insight, make good decisions and serve customers. • The organization meeting complying with regulatory requirements and improving its credit rating. Poor -quality data is risk -laden (see Chapter 1). It can damage an organization’s reputation , resulting in fines, lost revenue, lost customers, and negative media exposure. 1.2 Goals and Principles From a data user’s perspective, “Every day I come to work and my data is ready. I do not have to spend hours (or days or months) correcting it, massaging it, or fixing business processes that have been damaged by low quality data. I understand the data’s purposes and the data meets my needs.” Data Quality Management focuses on these goals: • Developing an approach to make data fit for purpose based on data consumers’ requirements • Defining standards and specifications for Data Quality controls as part of the data lifecycle • Defining and implementing processes to measure, monitor , and report on Data Quality levels • Identify and advocate for opportunities to improve the quality of data through process and system improvements. Data Quality improvement programs should be guided by the following principles: • Criticality : A Data Quality improvement program should focus on the data most critical to the enterprise and its customers. Priorities for improvement should be based on the criticality of the data and on the level of risk if data is not correct. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "428 • DMBOK2 • Standards -driven : All stakeholders in the data lifecycle have Data Quality requirements. To the degree possible, these requirements should be defined in the form of measurable standards and expectations against which the quality of data can be measured. • Objective measurement and transparency : Data Quality levels need to be measured objectively and consistently. Measurements and measurement methodology should be shared with stakeholders since they are the arbiters of quality. • Prevention : The focus of Data Quality improvement program should be on preventing data errors and conditions that reduce the quality of data; it should not be focused on simply correcting records. • Root cause remediation: Improving the quality of data goes beyond correcting errors. Problems with the quality of data should be understood and addressed at their root causes rather than just their symptoms. Because these causes are often related to process or system design, im proving Data Quality often requires changes to processes and the systems that support them. • Embedded in business processes: Business process owners are responsible for the quality of data produced through their processes. They must enforce Data Quality standards in their processes. • Systematically enforced : System owners must systematically enforce Data Quality requirements. • Connected to service levels : Data Quality reporting and issues management should be implemented and incorporated into Service Level Agreements (SLA). 1.3 Essential Concepts 1.3.1 Critical Data Most organizations have a lot of data, not all of which is of equal importance. The first principle of Data Quality Management is to focus improvement efforts on data that is most important to the organization and its customers. Doing so gives the Data Quality improvement program scope and focus and enables it to make a direct, measurable impact on business needs. Critical Data Elements are identified by linking them to the Data Quality business drivers; customer experience, effectiveness, and efficiency. Critical data needs to be managed to a defined Data Quality to meet these objectives. Without a particular data element there will be significant impairment to the success of the organization. This impairment size drives a business case fo r the level of Data Quality Management of a particular data element. This value prioritization approach creates a sliding scale o f value. At a defined value level, data elements will become “critical data elements”. For example, if the data in the customer email address field is incomplete, we will not be able to send product information to our customers via email and we will lose potential sales. We know that for every email we send out, we earn $1.00 in revenue. This simple example links a clear value driver to Data Quality improvement. The obvious question here might be, “can we have a scale of criticality?”. The answer is “yes”. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 429 While specific drivers for criticality will differ by industry, there are common characteristics across organizations. Critical data is often used in: • Regulatory, financial , or management reporting • Business operational needs • Measuring product quality and customer satisfaction • Business strategy, especially efforts at competitive differentiation. Master and Reference Data is usually critical by definition. Data sets or individual data elements can be assessed for criticality based on the processes that consume them, the nature of the reports they appear in, or the financial, regulatory, or reputational risk to the organization if something were to go wrong with the data.72 1.3.2 Data Quality Dimensions A Data Quality dimension is a measurable feature or characteristic of data. The term dimension is used to make the connection to dimensions in the measurement of physical objects (e.g., length, width, height). Data Quality dimensions provide a vocabulary for defining Data Quality requirements. From there, they can be used to define the results of the initial Data Quality Assessment as well as ongoing measurement. In order to measure the quality of data, an organization needs to establish dimensions that are both important to business processes (worth measuring) and measurable. Dimensions provide a basis for measurable rules, which themselves should be directly connected to potential risks in critical processes. Data Quality dimensions are the key concepts that drive the Data Quality principles of “standards driven” and “objective measurement and transparency”. They are so useful, that when we ask, “What is high -Data Quality?”, we can answer with the Data Quality dimension requirements. For example, we know that if the customer email address field is incomplete, then we will lose $1.00 in revenue. After examining what is possible (and the reality of customers changing their email addresses), we determine that we can achieve 98% as a measure of Data Quality. We will measure and improve our processes until we have a valid, complete , and accurate email address for at least 98% of our customers. While some practitioners use different sets of Data Quality dimensions (see section 8 Appendix of this chapter), the table below contains the common ideas. It contains definitions of a set of Data Quality dimensions, about which there is general agreement and describes approaches to measuring them. 72 See Jugulum (2014), Chapters 6 and 7 for an approach to rationalizing critical data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "430 • DMBOK2 Table 29 Common Dimensions of Data Quality Dimension of Quality Description Validity Validity refers to whether data values are consistent with a defined domain of values. A domain of values may consist of: • A data type (e.g. , text with or without special characters, a number, or a date) • A data range (e.g. , a numeric range, date range , or a set of valid text values in a reference table) • A format (e.g. , a phone number must contain country, area codes , and the correct number of digits or a currency must have only two decimal places) • The precision of expected values. For example , a date -stamp must be recorded to a millisecond or a number recorded to no more than two decimal places. • The time frame (certain values may also only be valid for a specific length of time, for example, data that is generated from RFID (radio frequency ID) or some scientific data sets). Completeness Completeness refers to whether all the required data is present. For example, are all the mandatory components of an address populated , including the street number, street name, city , and country? For example, if the customer account is marked as a debt risk, there must be a debt manager appointed. If the customer account is not marked as a debt risk, there must not be a debt manager appointed. Completeness can be measured at the column, record, or data set level. • Are columns/data elements populated to the level expected? (Some columns are mandatory. Optional columns are populated only under specific conditions.) • Are records populated correctly? (Records with different statuses may have different expectations for completeness.) • Does the data set contain all the records expected? Consistency Consistency is ensuring that data values are coded using the same approach, assessment and valuation criteria. Consistency is between two different data items, potentially within a data set and between data sets, and across time. Consistency may be defined between one set of data element values and another data element set within the same record (record- level consistency), between one set of data element values and another data element set in different records (cross -record consist ency), or between one set of data element values and the same data element set within the same record at different points in time (temporal consistency). Consistency can also be used to refer to consistency of format. Take care not to confuse consistency with accuracy or correctnes s. Consistency can be measured using the following ways: • Cross record consistency is between data values in the same column – are all the customer addresses recording the head- office details or do some record service delivery centers? • Between associated data sets, where data sets are expected to have values that are linked. For example, when moving data from data capture systems to data warehousing systems, a consistency quality measure will maintain confidence in the values in the data warehou se. • Across time can be applied to any of the previous cases. For example, has the grading of students been the same over time? If a subject is completed previously, would the same learning performance yield the same grade today? Integrity Integrity refers to the lack of incoherent values and broken relationships in data. Integrity can be measured using the following ways: • Coherence – where one data value implies a limited range in another data value and they are matching. For example, a country has a defined set of states or provinces. If a customer address is in a certain country, they must use a state or province name for the associated set. For example , Alberta is a province of Canada, Pisa is a province of Italy , and Tasmania is a state of Australia. • Parent- child (referential integrity) – where every child must have a parent data value. For example, all the customer address country names must exist on the valid list of countries where the organization is authorized to sell products to. Data sets without integrity are seen as corrupted or have data loss. Data sets without referential integrity have ‘orphans’ – invalid reference keys, or ‘duplicates’ – identical rows which may negatively affect aggregation functions. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 431 Dimension of Quality Description Timeliness Data Timeliness refers to the time that data needs to become accessible to a user after its capture or update. Timeliness is the expectation and actual delay before the data becomes available. For example , An electricity utility needs to maintain its power grid and must have demand data measured and available to a system operator within a few seconds. Otherwise , electricity generation may get out of alignment with demand , causing an energy excess or shortage. As another example, a statistical agency of government produces the Gross Domestic Product (GDP) report two months after the end of the quarter. In order to meet this timeliness requirement, all the data collections that make up the GDP will have their own specific timeliness requirements. Currency Currency is the date that the data was last updated relative to now and the likelihood that it is still correct. Different data sets will have different currency expectations from relatively static data to highly volatile data. Static data remains current for a long period. Volatile data remains current for a relatively short period. For example, country codes are re latively static, remaining current for a long period. A business’ bank account is relatively volatile as the balance is constantly changing. Volatile data, like stock prices on financial web pages, will often be shown with an “as -of-time”, so that data consumers understand the risk that the data has changed since it was recorded. During the day, while the markets are open, such data will be updated frequently – it is volatile. Once markets close, the data will remain unchanged but will still be current since the market itself is inactive –it is static. Key terms used in currency include: • Update time – the time stamp of the last update • Volatility measures the rate of change of the date. It may measure across all the data values or measure within a segment. For example, the rate that domestic customers change their address may be different from the rate that corporate customers change their address. • Latency measures the time between when the data was created and when it was made available for use. For example, overnight population of a data warehouse may end up with different data latencies. Data can have a latency of 1 day for data entered into the s ystem early on the prior day, but only a few minutes for data generated just before the load. (See Chapter 8.). Reasonableness Reasonableness asks whether a data pattern meets expectations. For example, whether a distribution of sales in a geographic area makes sense based on what is known about the customers in that area. For example, based on previous customer logins at 5pm, are today’s customer logins to our systems out of the ordinary? Key terms used in reasonableness include: • Fixed values – this is where the measure is fixed, not dependent on previous behavior. The maximum currency value of a transaction may be limited in size by the technical limits of a bank’s systems and other values bigger than this value is “unreasonable”. • Benchmark value – it is possible to measure the average, deviation or any other statistical function and ensure the latest set of values conforms to expected boundaries. For example, there have been between 5,0000 and 10,000 customer address changes per da y, but yesterday there were 2,000,000. On a government data collection, the average size company has 20 employees, but the latest returns had an unusual number with an average of 1,500 employees reported. Uniqueness / Deduplication Uniqueness states that no real -world entity exists more than once within the data set. Asserting uniqueness of the entities within a data set implies that each row relates to each unique entity in the real world, and only that specific entity. Uniqueness can be identified using the following techniques: • Key Structure – where there are duplicate keys within the data set (See Chapter 5). For example, in a data set containing customers, many of which share the same customer number. • Related data – where other data in the data may point to a duplicate. For example , two customers with different customer numbers, but having the same name, date of birth , and office address. • Inquiries or complaints that users cannot access certain data. For example, a customer with multiple services can only access some of their services as the others are recorded under a different customer number (which is a duplicate). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "432 • DMBOK2 Dimension of Quality Description Accuracy Accuracy refers to the degree that data correctly represents ‘real -life’ entities. For example, is the person’s name in our database actually the person’s name in real life? Does the customer actually use that email address? Accuracy is difficult to measure unless an organization can reproduce data collection or manually confirm accuracy of records. Most measures of accuracy use the other dimensions to imply accuracy. Common techniques to improve accuracy include: • Checking consistency with a data source that has been verified as accurate; a government company register (system of record) or a commercial company Information Broker (system of reference). • Ongoing calibration of devices with reality. For example, over time, weather gauges may drift from providing accurate measurements. An annual program of field inspections visits 10% of the devices and recalibrates them to a standard to ensure they are deli vering accurate measurements. • Sampling data values with reality. For example, small number of emails are sent to customers (mainly) to drive sales, but also have the benefit of checking the accuracy of the email address (does it exist, is it used)? 1.3.3 Data Quality Business Rules Business rules describe how organizations should operate internally in order to be successful and compliant with the outside world. Business Rules describe how data should exist in order to be useful and usable within an organization. Data Quality Rules are aligned with dimensions and are used to describe Data Quality requirements. For example , the business rules for managing state/province names could be: • Validity – All state/province names must be a value from the reference table stored in the Metadata repository • Completeness – All addresses must have a state/province if the country is divided into states/provinces. Otherwise, they should not have a state/province value • Integrity – All state/province names must have a value that is linked to the country name of the address • Currency – The reference values will be periodically updated and the current addresses will need to be brought up to date with the current names. This will keep all the state/province names current with the latest reference values. Notes: • Not every dimension is applicable every time • This is very long handed, often business rules will be compressed to cover multiple Data Quality dimensions and multiple data elements at once • Data Quality Rules can be implemented through a number of techniques , including data entry pick lists, reference data lookups , and quality inspection reports • Data Quality Rules are commonly implemented in software or by using document templates for data entry. Some common business rule types (aligned with most common dimensions) are: • Validity : can be implemented through rules such as: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 433 o Format compliance rules : One or more patterns specify values assigned to a data element, such as standards for formatting telephone numbers. o Value domain membership : Specify that a data element’s assigned value is included in those enumerated in a defined data value domain, such as 2 -character United States Postal Codes for a STATE field. o Range conformance : A data element assigned value must be within a defined numeric, lexicographic, or time range, such as greater than 0 and less than 100 for a numeric range. o Mapping conformance : Indicating that the value assigned to a data element must correspond to one selected from a value domain that maps to other equivalent corresponding value domain(s). The STATE data domain again provides a good example, since State values may be represent ed using different value domains (USPS Postal codes, FIPS 2 -digit codes, full names), and these types of rules validate that ‘AL’ and ‘01’ both map to ‘Alabama.’ • Completeness or Value presence and record completeness : Rules defining the conditions under which missing values are acceptable or unacceptable. • Consistency rules : Conditional assertions that refer to maintaining a relationship between two (or more) data elements based on the actual values of those data elements. For example, address validation where postal codes correspond to particular States or Provinces. • Consistency may also drive Definitional conformance rules : Confirm that the same understanding of data definitions is implemented and used properly in processes across the organization. Confirmation includes algorithmic agreement on calculated fields, including any time or local constraints, and rollup and status interdependence rules. • Integrity rules : Rules expressed as assertions to maintain a relationship between two (or more) data elements based on the actual values of those data elements. For example, address validation where postal codes correspond to particular States or Provinces. • Timeliness validation : Rules that indicate the characteristics associated with expectations for accessibility and availability of data. Timeliness rules often break down a process into a series of stages where timeliness is monitored through the cycle to ensure the dependent d ata is also achieving the timeliness requirement. • Uniqueness : Rules that specify which entities must have a unique representation and whether one and only one record exists for each represented real world object. In Data Quality checks, we may have rules that go beyond simple tests to assess the likelihood of a dup licate value. For example, it may search for people living at the same address that are behaving in a cooperative fashion (that leads us to believe they may be the same person). • Reasonableness : May be implemented through aggregating functions applied to sets of data instances (see Section 4.5). Examples of aggregation checks include validating reasonableness by a: o relative measure. For example, today’s record count in a file is assessed by keeping statistics over time to generate an understanding of “normality” which is used to assess reasonableness in this particular instance. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "434 • DMBOK2 o absolute measure. For example, validating the reasonableness of an average amount calculated from a set of transactions by a set of fixed thresholds that the values must fall within. These concepts of relative and absolute measures for determining reasonability can be used on any data types, such as an absolute date range (fixed period in time), or a relative date range (e.g. to today), computed totals, record counts, etc. • Accuracy verification: Compare a data value against a corresponding value in a system of record or other verified source (e.g. marketing data purchased from a vendor) to verify that the values match. 1.3.4 Data Quality Improvement Lifecycle Most approaches to improving Data Quality are based on the techniques of quality improvement in the manufacture of physical products.73 In this paradigm data is understood as the product of a set of processes. At its simplest, a process is defined as a series of steps that turns inputs into outputs. A process that creates data may consist of one- step (data collection) or many steps: data c ollection, integration into a data warehouse, aggregation in a data mart, etc. At any step data can be negatively affected. It can be collected incorrectly, dropped or duplicated between systems, aligned or aggregated incorrectly, etc. Improving Data Quality requires the ability to assess the relationship between inputs and outputs in order to ensure that inputs meet the requirements of the process and that outputs conform to expectations. Since outputs from one process become inputs to other processes, req uirements must be defined along the whole data chain. A general approach to Data Quality improvement, shown in Figure 92, is a version of the Shewhart / Deming cycle. 74 Based on the scientific method, the Shewhart / Deming cycle is a problem -solving model known as ‘plan -do-check -act’. Improvement comes through a defined set of steps. The condition of the data must be measured against standards and, if it does not meet sta ndards, root cause(s) of the discrepancy from standards must be identified and remediated. Root causes may be found in any of the steps of the process, technical or non-technical. Once remediated, data should be monitored to ensure that it continues to meet requirements. Figure 92 The Shewhart Chart with the role of a Data Quality Team indicated 73 See Wang (1998), English (1999), Redman (2001), Loshin (2001), and McGilvray (2008). See Pierce (2004) for an overview of lit erature related to the concept of data as a product. 74 See American Society for Quality: http://bit.ly/1lelyBK Plan -Do-Check -Act was originated by Walter Shewhart and popularized by W. Edwards Deming. 6 Sigma’s Define, Measure, Analyze, Improve, Control (DMAIC) is a variation on this cycle. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 435 For a given data set, a Data Quality Management cycle begins by the Data Quality team identifying the data that does not meet data consumers’ requirements and data issues that are obstacles to the achievement of business objectives. Data needs to be assessed against key dimensions of quality and known business requirements. Root causes of issues will need to be identified so that stakeholders can understand the costs of remediation and the risks of not remediating the issues. This work is often done in conj unction with the relevant business operational teams, Data Stewards and other stakeholders. In the Plan stage, the Data Quality team assesses the scope, impact, and priority of known issues and evaluates alternatives to address them. This plan should be based on a solid foundation of analysis of the root causes of issues. From knowledge of the causes and th e impact of the issues, cost / benefit can be understood, priority can be determined, and a basic plan can be formulated to address them. In the Do stage, the Data Quality team leads efforts to address the root causes of issues and plan for ongoing monitoring of data. For root causes that are based on non -technical processes, the Data Quality team can work with process owners to implement changes. Fo r root causes that require technical changes, the Data Quality team should work with technical teams to ensure that requirements are implemented correctly and that technical changes do not introduce errors. The Check stage involves actively monitoring the quality of data as measured against requirements. This is largely the responsibility of the business operations teams that manage the data on a daily basis. As long as data meets defined thresholds for quality, addit ional actions from the Data Quality team are not required. The processes will be considered under control and meeting business requirements. However, if the data falls below acceptable quality thresholds, then additional action must be taken to bring it up to acceptable levels. The Act stage is for activities to address and resolve emerging Data Quality issues. This activity is usually also performed by the business operational team. The cycle restarts as the Data Quality team identifies new issues. Continuous improvement is achieved by starting a new cycle. New cycles begin as: • Existing measurements fall below thresholds • New data sets come under investigation • New Data Quality requirements emerge for existing data sets • Business rules, standards, or expectations change The DMBOK Data Quality activities are mapped to this diagram in Table 30. Table 30 Data Quality Activities Dimension of Quality Description Plan (Data Quality Team) 1 Define High Quality Data 2 Define a Data Quality Policy 3 Define Scope of Initial Assessment 4 Perform Initial Data Quality Assessment 5 Identify & Prioritize Improvements 6 Define Goals for Data Quality Improvement Do (Data Quality Team) 7 Develop and Deploy Data Quality Operations a. Develop Data Quality Operational Procedures Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "436 • DMBOK2 Dimension of Quality Description Check (Operations) 8. Develop and Deploy Data Quality Operations a. Correct Data Quality Defects b. Measure and Monitor Data Quality c. Report on Data Quality levels and findings Act Correct Data Quality Defects This mapping enables understanding Data Quality using a manufacturing mindset. 1.3.5 Common Causes of Data Quality Issues Many people assume that most Data Quality issues are caused by data entry errors and that adding more technology will solve the problem. A more sophisticated understanding recognizes that gaps in – or poor execution of – business and technical processes cause many more problems than mis- keying. Figure 93 Sources of Data Quality Issues Data Quality issues can emerge at any point in the data lifecycle – from creation to disposal. When investigating root causes, analysts should look for potential culprits like problems with data entry, data processing, system design and manual intervention in automate d processes. Many issues will have multiple causes and contributing factors (especially if people have created ways to work around them). These causes of issues also imply ways to prevent issues: through improvement to interface design, testing of Data Quality Rules as part of processing, a focus on Data Quality within system design and strict controls on manual intervention in automated processes. Common causes of data quality issuesLack of Oversight Data Entry ProcessesData Processing Functions System DesignFixing Previous Issues Poor Data QualityLack of awareness Lack of priority Lack of leadership Lack of management Lack of justification Poor value measurementNo regard for ”downstream” Inconsistent process execution Changes to business processes Stale business rules Changed data structures Poor user engagement User training Poor usability Data Edit Checks List Entry Placement Field OverloadingReferential integrity Uniqueness constraints Processing errors and gaps Timing errors Data overflow and type issues Poor master and reference data managementInadequate recovery functions Chopping and changing reference data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 437 1.3.5.1 I ssues Caused by Lack of Oversight Many Data Quality problems are caused by a lack of organizational commitment to Data Quality in the form of both management and governance. For example, a business function may be solely managed to financial or completion metrics (such as finishing a call in 3 minutes or entering so many forms per day or just finishing the data entry in the next 2 weeks) without corresponding Data Quality metrics. This causes a lack of management focus and priority, as Data Quality is not valued equally with other performance measures. The existence of poor business metrics (e.g. missing data requirements) also demonstrates poor governance as there is no active oversight. Every organization has information and data assets that are of value to its operations. Indeed the operations of every organization depend on the ability to share information. Despite this, few organizations manage these assets with rigor through the full lifecycle. Within most organizations data disparity (differences in data structure, format, and use of values) is a larger problem than just simple errors; it can be a major obstacle to the integration of data. One of the reasons data stewardship focus on defining terms and consolidating the language around data is because that is the starting point for improving the consistency of data. Barriers to effective management of Data Quality include: 75 • Lack of awareness on the part of leadership and staff • Lack of priority • Lack of business governance • Lack of leadership and management • Difficulty in justification of improvements • Inappropriate or ineffective instruments to measure value These barriers have negative effects on customer experience, productivity, morale, organizational effectiveness, revenue, and competitive advantage. They increase the costs of running the organization and introduce risks as well. (See Chapter 16) . 1.3.5.2 Issues Caused by Data Entry Processes At the very start of the data lifecycle, data capture, there are a number of factors that can contribute to poor Data Quality. They include: • Poor user engagement : If the people who are entering the data see little or no value in entering the data (even if they are paid), then Data Quality is likely to suffer. For example, if customers are asked to enter too much personal data to set up an account, they are likely to take less care or even deliberately respond inaccurately. • Training issues: Lack of process knowledge can lead to incorrect data entry even if controls and edits are in place. If data producers are not aware of the impact of incorrect data or if they are incented for 75 Adapted from The Leader’s Data Manifesto . https://dataleaders.org/. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "438 • DMBOK2 speed, rather than accuracy, they are likely to make choices based on drivers other than the quality of the data. • Poor usability : User interfaces that are jumbled, inconsistent, lack clear instruction, use unexpected approaches, or ask for information in an unusual sequence require the user to work harder to enter the data. This is likely to lead to poorer Data Quality through both the inability to find the place to enter the data and omission through frustration. • No data entry edit checks : If a data entry interface does not have edits or controls to prevent incorrect data from being put in the system data processors are likely to take shortcuts, such as skipping non -mandatory fields and failing to update defaulted fields. Key Data Quality dimensions managed in data entry are validity, completeness, consistency, integrity and reasonableness. • Inadequate list entry placement : Even simple features of data entry interfaces, such as the order of values within a drop -down list, can contribute to data entry errors. • Field overloading : Some organizations re- use fields over time for different business purposes rather than making changes to the data model and user interface. This practice results in inconsistent and confusing population of the fields. 1.3.5.3 Issues Caused by Data Processing Functions As data progresses through a value chain, Data Quality issues may be introduced. The factors that cause this include: • No regard for “downstream” processes : If each automated process and user works in a siloed environment without understanding where data comes from and where data goes to, then issues can arise with Data Quality. This may not even be deliberate. Users are often unaware of the use of their data and the issues they are causing. • Inconsistent business process execution : Different user groups may use the data differently and perform business processes differently. Inconsistent execution may be due to training or documentation issues as well as to different requirements in different parts of the business. • Changes to business processes: Business processes change over time. With these changes, new business rules and Data Quality requirements are introduced. Data is likely to be impacted unless changes to business rules are propagated throughout the entire system. Business rule changes ar e not always incorporated into systems in a timely or comprehensive manner. • Stale business rules : Over time, business rules change. They should be periodically reviewed and updated. If there is automated measurement of rules then the technical process for measuring rules should also be updated. If it is not updated, issues may not be identified or fa lse positives will be produced (or both). • Changed data structures : Source systems may change structures without informing downstream consumers (both human and system) or without providing sufficient time to account for the changes. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 439 This can result in invalid values or other conditions that prevent data movement and loading, or in more subtle changes that may not be detected immediately. 1.3.5.4 Issues Caused by System Design Automated processing may have faults that introduce data errors that persist after they are fixed. There may also be choices made to encourage improved performance that reduce the Data Quality. Some of the System Design factors include: • Failure to enforce referential integrity : Referential integrity is necessary to ensure high quality data at an application or system level. If referential integrity is not enforced or if validation is switched off (for example, to improve response times), various Data Quality issues can arise: o Duplicate data that breaks uniqueness rules o Orphan rows, which can be included in some reports and excluded from others, leading to multiple values for the same calculation o Inability to upgrade due to restored or changed referential integrity requirements o Inaccurate data due to missing data being assigned default values • Failure to enforce uniqueness constraints: Multiple copies of data instances within a table or file that is expected to contain unique instances is an example of this failure. If there are insufficient checks for uniqueness of instances, or if the unique constraints are turned off in the database to improve performance, data aggregation results can be overstated. • Processing errors and gaps : If the data mapping or layout is incorrect, or the rules for processing the data are not accurate, the data processed will have Data Quality issues, ranging from incorrect calculations to data being assigned to or linked to improper fields, keys, or rela tionships. • Timing errors : If processing dependencies are not managed or processing continues despite a failure, systems could try to process data of different currency leading to data mismatch and data loss. For example, if a new electricity meter is installed on a house but the meter reading is not reset to the new meter’s initial reading then electricity usage measurement errors will occur. • Data overflows and type issues : If assumptions within the data model are not supported by the actual data then there will be Data Quality issues ranging from data loss due to field types and lengths being exceeded by the actual data, to data being assigned to improper IDs or keys. • Poor reference and Master Data Management : By definition, reference and master data are shared between many business functions. Defects in data can have repercussions on many business operations. In particular, duplicates in the master list of clients are a common challenge. (see chapter 10). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "440 • DMBOK2 1.3.5.5 Issues Caused by Fixing Issues Manual data patches are changes made directly on the data in the database, not through the business rules in the application interfaces or processing. These are scripts or manual commands generally created in a hurry and used to ‘fix’ data in an emergency such as intentional injection of bad data, lapse in security, internal fraud, or external source for business disruption. Like any untested code, they have a high risk of causing further errors through unintended consequences such as changing more data than required, or not propagating the patch to all historical data affected by the original issue. Most patches also change t he data in place rather than preserving the prior state and adding corrected rows. These changes are generally NOT undo -able without a complete restore from backup as there is only the database log to show the changes. Therefore, these shortcuts are strongly discouraged – they are opportunities for security breaches and business disruption longer than a proper correction would cause. All changes should go through a governed change management process. Factors that drive the need for manual data patches on the database include: • Inadequate recovery functions : If data becomes corrupted in the system there should be user functions able to recover the record and fix the errors. Without appropriate recovery functions the users are forced to fix the data using manual data patches. • Changing reference data : Reference data should be changed infrequently. If reference data changes regularly data validity errors will arise in the database that may need to be corrected. If there are no functions to update reference data on the actual data then this will also dr ive manual data patches. 2. Activities 2.1 Define a Data Quality Framework Improving Data Quality requires an approach that accounts for the work that needs to be done and the way people will execute it. Data Quality priorities must align with business strategy. Adopting or developing a framework and methodology will help guide both strategy and tactic s while providing a means to measure progress and impacts. A framework should include methods to: • Understand and prioritize business needs • Identify the data critical to meeting business needs • Define business rules and Data Quality standards based on business requirements • Assess data against expectations • Share findings and get feedback from stakeholders • Prioritize and manage issues Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 441 • Identify and prioritize opportunities for improvement • Measure, monitor , and report on Data Quality • Manage Metadata produced through Data Quality processes • Integrate Data Quality controls into business and technical processes A framework should also account for how to organize for Data Quality and how to leverage Data Quality tools. As noted in the chapter introduction, improving Data Quality requires a Data Quality Function team to engage business and technical staff and define a program of work that addresses critical issues, defines best practices and puts in place operational processes that support ongoing management of Data Quality. Often such a team will be part of the Data Management Organization. Data Quality analysts will need to work closely with Data Stewards on all levels. They should also influence policy, including policy about business processes and systems development. However, such a team will not be able to solve all of an organization’s Data Quality challenges. Data Quality work and a commitment to high quality data need to become embedded in organizational practices. The Data Quality Framework should account for how to extend best practices. (See Chapter 17.) 2.2 Define High Quality Data Not all data is of equal importance. Data Quality Management efforts should focus first on the most important data in the organization: data that, if it were of higher quality, would provide greater value to the organization and its customers. Data can be prioritized based on factors such as regulatory requirements, financial value and direct impact on customers. Often, Data Quality improvement efforts start with Master Data, which is, by definition, among the most important data in any organization. The re sult of the importance analysis is a ranked list of data, which the Data Quality team can use to focus their work efforts. Many people recognize poor quality data when they see it. Few are able to define what they mean by high quality data. Alternatively, they define it in very general terms: “The data has to be right.” “We need accurate data.” High quality data is fit for the purposes of data consumers. Before launching a Data Quality improvement program, it is beneficial to understand business needs, define terms, identify organizational pain points, and start to build consensus about the drivers and priorities for Data Quality improvement. Ask a set of questions to understand current state and assess organizational readiness for Data Quality improvement: • What do stakeholders mean by ‘high quality data’? • What is the impact of low quality data on business operations and strategy? • How will higher quality data enable business strategy? • What priorities drive the need for Data Quality improvement? • What is the tolerance for poor quality data? • What governance is in place to support Data Quality improvement? • What additional governance structures will be needed? Getting a comprehensive picture of the current state of Data Quality in an organization requires approaching the question from different perspectives: • An understanding of business strategy and goals • Interviews with stakeholders to identify pain points, risks , and business drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "442 • DMBOK2 • Direct assessment of data through profiling and other form of analysis • Documentation of data dependencies in business processes • Documentation of technical architecture and systems support for business processes This kind of assessment can reveal a significant number of opportunities. These need to be prioritized based on the potential benefit to the organization. Using input from stakeholders, including Data Stewards and business and technical Subject matter experts, the Data Quality team should define the meaning of Data Quality and propose priorities. 2.3 Identify Dimensions and Supporting Business Rules Having identified the critical data, Data Quality analysts need to identify business rules that describe or imply expectations about the dimensions of Data Quality. Often rules themselves are not explicitly documented. They may need to be reverse engineered through analysis of existing business processes, workflows, regulations, policies, standards, system edits, software code, triggers and procedures, status code assignment and use, and plain old common sense. For example, if a marketing company wants to t arget efforts at people in a specific demographic then potential indexes of Data Quality might be the level and reasonability of population in demographic fields like birth date, age, gender and household income. Most business rules are associated with how data is collected or created, but Data Quality measurement centers around whether data is fit for use. The two (data creation and data use) are related. People want to use data because of what it represents and w hy it was created. For example, to understand an organization’s sales performance during a specific quarter or over time depends on having reliable data about the sales process (number and type of units sold, volume sold to existing customers vs. new custo mers, etc.). It is not possible to know all the ways that data might be used, but it is possible to understand the process and rules by which data was created or collected. Measurements that describe whether data is fit for use should be developed in relation to known uses and measurable rules based on dimensions of Data Quality: completeness, validity, integrity, etc. that provide the basis for meaningful metrics. Dimensions of quality enable analysts to characterize both rules (field X is mandatory and must be populat ed) and findings (e.g., the field is not populated in 3% of the records; the data is only 97% complete). At the field or column level, rules can be straightforward. Completeness rules are a reflection of whether a field is mandatory or optional, and, if optional, the conditions under which it should be populated. Validity rules are dependent on stipulating the domain of valid values and, in some cases, the relationship between fields. For example, a US ZIP Code needs to be valid, in and of itself, and correctly associated with a US State code. Rules should also be defined at the data set level. For example, every customer must have a valid mailing address. Defining Data Quality Rules is challenging because most people are not used to thinking about data in terms of rules. It may be necessary to get at the rules indirectly by asking stakeholders about the input and output requirements of a business process. I t also helps to ask about pain points, what happens when data is missing or incorrect, how they identify issues, how they recognize bad data, etc. Keep in mind that it is not necessary to know all the rules in order to assess data. Discovery and refinement of rules is an ongoing process. One of the best ways to get at rules is to share results of assessments. These results often give stakeholders a new Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 443 perspective on the data from which they can articulate rules that tell them what they need to know about the data. 2.4 Perform an Initial Data Quality Assessment Once the most critical business needs and the data that supports them have been identified, the most important part of the Data Quality Assessment is actually looking at that data, querying it to understand data content and relationships, and comparing actual data to rules and expectations. The first time this is done, analysts will discover many things: undocumented relationships and dependencies within the data, implied rules, redundant data, contradictory data, etc., as well as data that actually does co nform to rules. With the help of data stewards, other subject matter experts and data consumers, Data Quality analysts will need to sort out and prioritize findings. • The goal of an initial Data Quality Assessment is to learn about the data in order to define an actionable plan for improvement. It is usually best to start with a small, focused effort – a basic proof of concept – to demonstrate how the improvement proces s works. Steps include: • Define the goals of the assessment; these will drive the work • Identify the data to be assessed; focus should be on a small data set, even a single data element, or a specific Data Quality problem • Identify uses of the data and the consumers of the data • Identify known risks with the data to be assessed, including the potential impact of data issues on organizational processes • Inspect the data based on known and proposed rules • Document levels of non-conformance and types of issues • Perform additional, in -depth analysis based on initial findings in order to o Quantify findings o Prioritize issues based on business impact o Develop hypotheses about root causes of data issues • Meet with data stewards, subject matter experts , and data consumers to confirm issues and priorities • Use findings as a foundation for planning: o Remediation of issues, ideally at their root causes o Controls and process improvements to prevent issues from recurring o Ongoing controls and reporting 2.5 Identify and Prioritize Potential Improvements Having demonstrated that Data Quality should be improved, the next goal is to do it strategically. Doing so requires identifying and prioritizing potential improvements. Identification may be accomplished by full -scale data profiling of larger data sets to understand the breadth of existing issues. It may also be accomplished by other means such as interviewing stakeholders about the data issues that impact them and following up with analysis of the business impact of those issues. Ultimately, prioritization requires a combination of data analysis and discussion with stakeholders. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "444 • DMBOK2 The steps to perform a full data profiling and analysis are essentially the same as those in performing a small- scale assessment: define goals, understand data uses and risks, measure against rules, document and confirm findings with subject matter experts, use this information to prioritize remediation and improvement efforts. However, there are sometimes technical obstacles to full -scale profiling. The effort will need to be coordinated across a team of analysts and overall results will need to be summar ized and understood if an effective action plan is to be put in place. Large -scale profiling efforts, like those on a smaller scale, should still focus on the most critical data. 2.6 Define Goals for Data Quality Improvement The knowledge obtained through the preliminary assessments forms the basis for specific Data Quality improvement program goals. Improvement can take different forms, from simple remediation (e.g., correction of errors on records) to remediation of root cau ses. Remediation and improvement plans should account for quick hits – issues that can be addressed immediately at low cost – and longer -term strategic changes. The strategic focus of such plans should be to address root causes of issues and to put in place mechanisms to prevent issues in the first place. Be aware that many things can get in the way of improvement efforts: system constraints, age of data, ongoing project work that uses the questionable data, overall complexity of the data landscape, cultural resistance to change. To prevent these constraints from stalling the program, set specific, achievable goals based on consistent quantification of the business value of the improvements to Data Quality. For example, a goal may be to improve the completeness of customer data from 90% to 95% based on process improvements and system edits. Obviously, showing improvement will involve comparing initial measurements and improved results. But the value comes wit h benefits of the improvement: fewer customer complaints, less time spent correcting errors, etc. Measure these things to explain the value of the improvement work. No one cares about levels of field completeness unless there is a business impact. There must be a positive return on investment for improvements to data. When issues are found, determine ROI of fixes based on: • The criticality (importance ranking) of the data affected • Amount of data affected • The age of the data • Number and type of business processes impacted by the issue • Number of customers, clients, vendors, or employees impacted by the issue • Risks associated with the issue • Costs of remediating root causes • Costs of potential workarounds In assessing issues, especially those where root causes are identified and technical changes are required, always seek out opportunities to prevent issues from recurring. Preventing issues generally costs less than correcting them – sometimes orders of mag nitude less. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 445 2.7 Develop and Deploy Data Quality Operations Many Data Quality improvement programs get started through a set of improvement projects identified via results of the Data Quality Assessment. In order to sustain Data Quality, a Data Quality Function should put in place a process that allows the team to manage Data Quality Rules and standards, monitor data’s ongoing conformance with rules, identify and manage Data Quality issues and report on quality levels. In support of these activities, Data Quality analysts and data stewards will also be engaged in activities such as documenting data standards and business rules and establishing Data Quality requirements for vendors. 2.7.1 Manage Data Quality Rules The process of profiling and analyzing data will help an organization discover (or reverse engineer) business and Data Quality Rules. As the Data Quality practice matures , the capture of such rules should be built into the system development and enhancement process. Defining rules upfront will: • Set clear expectations for Data Quality dimensions • Provide requirements for system edits and controls that prevent data issues from being introduced • Provide Data Quality requirements for all relevant dimensions to vendors and other external parties • Create the foundation for ongoing Data Quality measurement and reporting In short, Data Quality Rules and standards are a critical form of Metadata. To be effective, they need to be managed as Metadata. Rules should be: • Documented consistently : Establish standards and templates for documenting rules so that they have a consistent format and meaning. • Defined in terms of Data Quality dimensions : Dimensions of quality help people understand what is being measured. Consistent application of dimensions will help with the measurement and issue management processes. • Tied to business impact : While Data Quality dimensions enable understanding of common problems, they are not a goal in -and-of-themselves. Standards and rules should be connected directly to their impact on organizational success. Measurements that are not tied to business processes should not be taken. • Backed by data analysis : Data Quality analysts should not guess at rules. Rules should be tested against actual data. In many cases, rules will show that there are issues with the data. But analysis can also show that the rules themselves are not complete. • Confirmed by subject matter expert : The goal of the rules is to describe how the data should look. Often, it takes knowledge of organizational processes to confirm that rules correctly describe the data. This knowledge comes when subject matter experts confirm or explain the results of dat a analysis. • Accessible to all data consumers : All data consumers should have access to documented Data Quality Rules. Such access allows them to better understand the data. It also helps to ensure that the Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "446 • DMBOK2 rules are correct and complete. Ensure that consumers have a means to ask questions about and provide feedback on rules. 2.7.2 Measure and Monitor Data Quality • Much of the work of a Data Quality team will focus on measuring and reporting on quality. High -level categories of Data Quality metrics include: • Return on Investment: Statements on cost of improvement efforts vs. the benefits of improved Data Quality • Levels of quality: Measurements of the number and percentage of errors or requirement violations within a data set or across data sets • Data Quality trends: Quality improvement over time (i.e. , a trend) against thresholds and targets, or quality incidents per period • Data issue management metrics: o Counts of issues by dimensions of Data Quality o Issues per business function and their statuses (resolved, outstanding, escalated) o Issue by priority and severity o Time to resolve issues • Data Quality plan rollout: status of the rollout, next stages and an ongoing prioritization process to maintain value in the roadmap. The operational Data Quality Management procedures depend on the ability to measure and monitor the quality of data. There are two equally important reasons to implement operational Data Quality measurements: 1. To inform data consumers about levels of quality 2. To manage risk that change may be introduced through changes to business or technical processes Some measurements serve both purposes. Measurements should be developed based on findings from data assessment and root cause analysis. Measurements intended to inform data consumers will focus on critical data elements and relationships that, if they are not of high quality, will directly impact business processes. Measurements related to managing risk should focus on relationships that have gone wrong in the past and may go wrong in the future. For example, if data is derived based on a set of ETL (extrac t, transform, load) rules and those rules may be impacted by changes to business processes, measurements should be put in place to detect changes to the data. Knowledge of past problems should be applied to manage risk. For example, if numerous data issues are associated with complex derivations, then all derivations should be assessed – even those that have not been associated with data issues. In most cases, i t is worthwhile to put in place measurements that monitor functions similar to those that have had problems. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 447 In Figure 94, the Data Quality threshold is the dotted line in each graph. The Data Quality team progresses each of these trends: • 2, 4, and 5 are ongoing issues (the team is monitoring progress to bring them back under threshold) • 1, 3, 10 are today’s issues (the team is mobilizing efforts to identify and address the trend) • 7, 8, 11 are on today’s watch list (the team is taking an interest but has not yet mobilized to address) • 6 is a long -term intransigent issue (the team has tried a couple of ideas but is still looking for a permanent solution) • 9, 12 are fine (the team are happy with these trends). Figure 94 Illustrations of data defects trends over time Table 31 Data Quality Monitoring Techniques Granularity Common Dimensions Treatment Data Element Validity, Integrity, Consistency, Reasonableness Edit checks in application Data element validation services Specially programmed applications Data Record Consistency, completeness, currency Edit checks in application Data record validation services Specially programmed applications Data set Completeness, Uniqueness, Reasonableness Inspection inserted between processing stages 2.7.3 Develop Procedures for Managing Data Issues In modern enterprises, many different types of incidents are already effectively managed. For example, health and safety issues, property and building issues, and Information Technology issues. Therefore, before developing new processes for managing data issues, look to extend an existing process and toolset. The key elements of an incident management process are: TimeDQ Measurement Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "448 • DMBOK2 • Standardize Data Quality issues and activities : Since the terms used to describe data issues may vary across lines of business, it is valuable to define a standard vocabulary for the concepts used. Doing so will simplify classification and reporting. Standardization also makes it easier to measure the volume of issues and acti vities, identify patterns and interdependencies between systems and participants, and report on the overall impact of Data Quality activities. The classification of an issue may change as the investigation deepens and root causes are exposed. • Provide an assignment process for data issues: The operational procedures direct the analysts to assign Data Quality incidents to individuals for diagnosis and to provide alternatives for resolution. Drive the assignment process within the incident tracking system by suggesting those individuals with specific areas of expertise. • Manage issue escalation procedures : Data Quality issue handling requires a well -defined system of escalation based on the impact, duration, or urgency of an issue. Specify the sequence of escalation within the Data Quality Service Level Agreement. The incident tracking system will implemen t the escalation procedures, which helps expedite efficient handling and resolution of data issues. • Manage Data Quality resolution workflow : The Data Quality SLA specifies objectives for monitoring, control, and resolution - all of which define a collection of operational workflows. The incident tracking system can support workflow management to track progress with issues diagnosis and resolu tion. Even when using a mature, standardized process, the Data Quality team must design and implement detailed operational procedures for the steps within this: • Diagnosing issues: The objective is to review the symptoms of the Data Quality incident, trace the lineage of the data in question, identify the problem and where it originated and pinpoint potential root causes of the problem. The procedure should describe how the Data Qu ality Operations team would: o Review the data issues in the context of the appropriate information processing flows and isolate the location in the process where the flaw is introduced o Evaluate whether there have been any environmental changes that would cause errors entering into the system o Evaluate whether or not there are any other process issues that contributed to the Data Quality incident o Determine whether there are issues with external data that have affected the quality of the data NOTE: The work of root cause analysis requires input from technical and business subject matter experts. While the Data Quality team may lead and facilitate this kind of work effort, success requires cross -functional collaboration. • Formulating options for remediation: Based on the diagnosis, evaluate alternatives for addressing the issue. These may include: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 449 o Addressing non-technical root causes such as lack of training, lack of leadership support, unclear accountability and ownership, etc. o Modification of the systems to eliminate technical root causes o Developing controls to prevent the issue o Introducing additional inspection and monitoring o Directly correcting flawed data o Taking no action based on the cost and impact of correction versus the value of the data correction • Resolving issues: Having identified options for resolving the issue the Data Quality team must confer with the business data owners to determine the best way to resolve the issue. These procedures should detail how the analysts: o Assess the relative costs and merits of the alternatives o Recommend one of the planned alternatives o Provide a plan for developing and implementing the resolution o Implement the resolution 2.7.4 Establish Data Quality Service Level Agreements A Data Quality Service Level Agreement (SLA) specifies an organization’s expectations for response and remediation for Data Quality issues at key boundaries. Key boundaries include: • Entry and exit from the organization: For example, a supplier is providing critical data to an organization for use in their data processing, so a key part of the contract is the Data Quality requirements. • Key separation points within an organization: For example, a research unit within an organization will acquire data from various locations, both internal and external to the organization. Data Quality inspections as scheduled in the SLA help to identify issues to fix, and over time, reduce the number of issues. While enabling the isolation and root cause analysis of data flaws, there is an expectation that the operational procedures will pr ovide a scheme for remediation of root causes within an agreed timeframe. Having Data Quality inspection and monitoring in place increases the likelihood of detection and remediation of a Data Quality issue before a significant business impact can occur. Data Quality control defined SLA includes the same information as Data Quality requirements (see section ???) with the following additions: • Data elements covered by the agreement • Business impacts associated with data flaws • Data Quality dimensions and business rules associated with each data element • Methods for measuring against those expectations • Acceptability threshold for each measurement • Steward(s) to be notified in case the acceptability threshold is not met • Timelines and deadlines for expected resolution or remediation of the issue • Escalation strategy, and possible rewards and penalties Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "450 • DMBOK2 The Data Quality SLA also defines the roles and responsibilities associated with performance of operational Data Quality procedures. Data stewards and the operational Data Quality staff, while upholding the level of Data Quality service, should consider their Data Quality SLA constraints and connect Data Quality to individual performance plans. When issues are not addressed within the specified resolution times an escalation process must exist to communicate non -observance of the level of service up the management and governance chain. The Data Quality SLA establishes the time limits for notification generation, the names of those in that management chain and when escalation needs to occur. Given the set of Data Quality Rules, methods for measuring conformance, the acceptability thresholds defined by the business clients, and the service level agreements, the Data Quality team can monitor compliance of the data to the business expectations, as well as how well the Data Quality team performs on the procedures associated with data errors. SLA reporting can be on a scheduled basis driven by business and operational requirements. Particular focus will be on report trend analysis in cases focused on periodic rewards and penalties if such concepts are built into the SLA framework. 2.7.5 Data Quality Response The work of assessing the quality of data and managing data issues will not benefit the organization unless the information is shared through reporting so that data consumers understand the condition of the data. Reporting should focus around: • Data Quality scorecard which provides a high -level view of the scores associated with various metrics, reported to different levels of the organization within established thresholds • Data Quality trends which show over time how the quality of data is measured and whether trending is up or down • SLA Metrics such as whether operational Data Quality staff diagnose and respond to Data Quality incidents in a timely manner • Data Quality issue management which monitors the status of issues and resolutions • Conformance of the Data Quality team to governance policies • Conformance of IT and business teams to Data Quality policies • Positive effects of improvement projects Reporting should align to metrics in the Data Quality SLA as much as possible so that the team’s goals are aligned with those of its customers. The Data Quality improvement program should also report on the positive effects of improvement projects. It is best to do this in business terms to continually remind the organization of the direct effect that data has on customers. 3. Tools While the focus of Data Quality improvement efforts is often on the prevention of errors, Data Quality can also be improved through some forms of data processing. (See Chapter 8.) Tools should be selected and tool architectures should be set in the planning phase of the enterprise Data Quality improvement program. Tools Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 451 provide a partial rule set starter kit but organizations need to create and input their own context specific rules and actions into any tools. The key functions of tools are: • Profiling: exploring patterns of data values • Business Rule Engines: specialist tools that automate the handing of Data Quality requirements • Data Parsing and formatting: breaking data values into components and reassembling them to identify a correct response • Data Transformation and Standardization: Used to change the values to meet standard requirements • Data Enrichment: Used to augment the data with additional data (maybe thought of Metadata ) • Incident Management: Used to automate data issue management. 3.1 Data Profiling Tools Data profiling tools produce high -level statistics that enable analysts to identify patterns in data and perform initial assessment of quality characteristics. Some tools can be used to perform ongoing monitoring of data. Profiling tools are particularly important for data discovery efforts because they enable assessment of large data sets. Profiling tools augmented with data visualization capabilities will aid in the process of discovery. (See Chapters 5 and 8, and Section 1.3.9.). Data profiling is only the first step in data analysis. It helps identify potential issues. Data Quality team members also need to query data more deeply to answer questions raised by profiling results and find patterns that provide insight into root causes of data issues. For example, querying to discover and quantify other aspects of Data Quality, such as uniqueness and integrity. 3.2 Business Rule Templates and Engines Business Rule Templates are specialist tools that allow analyst s to capture expectations for data standard forms. Templates help bridge the communications gap between business and technical teams. Consistent formulation of rules makes it easier to translate business needs into code, whether that code is embedded in a r ules engine, the data analyzer component of a data-profiling tool, or a data integration tool. A template can have several sections, one for each type of business rule to implement. 3.3 Modelin g and ETL Tools 3.3 Data Parsing and Formatting Data Parsing is the process of analyzing data using pre -determined rules to define its content or value. Data parsing enables the data analyst to define sets of patterns that feed into a rule engine used to distinguish between valid and invalid data values. Matching specific pattern(s) triggers actions. Many Data Quality issues involve situations where variation in data values representing similar concepts introduces ambiguity. The separate components (commonly referred to as ‘tokens’) can be extracted and rearranged into a standard representation to crea te a valid pattern. When an invalid pattern is recognized, the Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "452 • DMBOK2 application may attempt to transform the invalid value into one that meets the rules. Perform standardization by mapping data from some source pattern into a corresponding target representation. Another good example is a customer name since names may be represented in thousands of different forms. A good standardization tool will be able to parse the different components of a customer name, such as given name, middle name, family name, initials, titles, generational designations and then rearrange those components into a standard representation that other data services will be able to manipulate. The human ability to recognize familiar patterns contributes to an ability to characterize variant data values belonging to the same abstract class of values; people recognize different types of telephone numbers because they conform to frequently used pat terns. An analyst describes the format patterns that all represent a data object, such as Person Name, Product Description, and so on. A Data Quality tool parses data values that conform to any of those patterns, and even transforms them into a single, standardized form that will simplify the assessment, similarity analysis, and remediation processes. Pattern- based parsing can automate the recognition and subsequent standardization of meaningful value components. 3.4 Data Transformation and Standardization During normal processing, data rules trigger and transform the data into a format that is readable by the target architecture. However, readable does not always mean acceptable. Rules are created directly within a data integration stream or rely on alterna te technologies embedded in or accessible from within a tool. Data transformation builds on these types of standardization techniques. Organizations should guide rule -based transformations by mapping data values in their original formats and patterns into a target representation. Parsed components of a pattern are subjected to rearrangement, corrections, or any changes as directed by the rules in the knowledge base. In fact, standardization is a special case of transformation, employing rules that capture context, linguistics and idioms recognized as common over time, through repeated analysis by the rules analyst or tool vendor. (See Chapter 3.) 3.5 Data Enrichment Data enrichment or enhancement is the process of adding data elements to a data set to increase its quality, including usability. Some enhancements are gained by integrating data sets internal to an organization. External data can also be purchased to enhance organizational data (see Chapter 10). Examples of data enhancement include: • Time/Date stamps : One way to improve data is to document the time and date that data items are created, modified, or retired, which can help to track historical data events. If issues are detected with the data, timestamps can be very valuable in root cause analysis becau se they enable analysts to isolate the timeframe of the issue. • Audit data : Auditing can document data lineage which is important for historical tracking as well as validation. • Reference vocabularies : Business specific terminology, ontologies and glossaries enhance understanding and control while bringing customized business context. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 453 • Contextual information: Adding context such as location, environment, or access methods and tagging data for review and analysis. • Geographic information: Geographic information can be enhanced through address standardization and geocoding, which includes regional coding, municipality, neighborhood mapping, latitude / longitude pairs, or other kinds of location -based data. • Demographic information : Customer data can be enhanced through demographic information such as age, marital status, gender, or income. Business entity data can be associated with annual revenue, number of employees, size of occupied space etc. • Psychographic information: Data used to segment the target populations by specific behaviors, habits, or preferences, such as product and brand preferences, organization memberships, leisure activities, commuting transportation style, shopping time preferences etc. • Valuation information: Use this kind of enhancement for asset valuation, inventory, and sale. 3.6 Incident Management System Decisions made during the issue management process should be tracked in an Incident Management System. When the data in such a system is managed well it can provide valuable insight into the causes and costs of data issues. Include a description of the issue and the root causes, options for remediation and the decision on how to resolve the issue. The Incident Management System will collect performance data relating to issue resolution, work assignments, volume of issues, frequency of occurrence, as well as the time to respond, diagnose, plan a solution, and resolve issues. These metrics can provide valuable insights into the effectiveness of the current workflow, as well as systems and resource utilization. They are important management data points that can drive continuous operational improvement for Data Quality control. Incident management data also helps data consumers. Decisions based upon remediated data should be made with knowledge that it has been changed, why it has been changed and how it has been changed. That is one reason why it is important to record the methods of modification and the rationale for them. Make this documentation available to data consumers and developers researching code changes. While changes may be obvious to the people who implement them, the history of changes will be lost to future data consumers unless it is documented. 4. Techniques Data Quality techniques are arranged into a standard sequence: • Metrics –effectively measuring the quality of data • Profiling – finding statistical and relationship patterns • Preventative or corrective actions – how to avoid Data Quality problems in the first place • Root cause analysis – getting to the fundamentals of a problem • Corrections – how to tidy up a data issue Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "454 • DMBOK2 4.1 Effective Metrics A critical component of managing Data Quality is developing metrics that inform data consumers about quality characteristics that are important to their uses of data. Many things can be measured, but not all of them are worth the time and effort. In develo ping metrics, Data Quality analysts should account for these characteristics: • Measurability : A Data Quality metric needs to be something that can be counted. For example, address completeness needs to be defined in order to be measured. Expected results should be quantifiable within a discrete range. • Business relevance: While many things are measurable, not all translate into useful metrics. Measurements need to be relevant to data consumers. The value of the metric is limited if it cannot be related to some aspect of business operations or performance. Every Data Quality metric should correlate with the influence of the data on key business expectations. • Acceptability: The Data Quality dimensions frame the business requirements for Data Quality. Quantifying along the identified dimension provides hard evidence of Data Quality levels. Determine whether data meets business expectations based on specified acceptability thresholds. If the score is equal to or exceeds the threshold, the quality of the data meets business expectations. If the score is below the threshold, it does not. • Accountability / Stewardship : Metrics should be understood and approved by key stakeholders (e.g., business owners and Data Stewards). They are notified when the measurement for the metric shows that the quality does not meet expectations. The business data owner is accountable, whil e a data steward takes appropriate corrective action. • Controllability : A metric should reflect a controllable aspect of the business. In other words, if the metric is out of range, it should trigger action to improve the data. If there is no way to respond, then the metric is probably not useful. • Trending : Metrics enable an organization to measure Data Quality improvement over time. Tracking helps Data Quality team members monitor activities within the scope of a Data Quality SLA and data sharing agreement and demonstrate the effectiveness of improvement a ctivities. Once an information process is stable, statistical process control techniques can be applied to detect changes to the predictability of the measurement results and the business and technical processes on which it provides insight. Metric measurement results can be described at two levels: the detail related to the execution of individual rules and overall results aggregated from the rules. Each rule should have a standard, target , or threshold index for comparison. This function most often reflects the percentage of correct data or percentage of exceptions depending on the formula used. For example: 𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉 (𝑟𝑟)= (𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 𝑉𝑉𝑇𝑇 𝑇𝑇𝑇𝑇(𝑟𝑟)−𝑇𝑇 𝑇𝑇𝑇𝑇𝑇𝑇 𝐸𝐸𝑇𝑇 𝑉𝑉𝑇𝑇𝑇𝑇𝑇𝑇𝐸𝐸𝑇𝑇 𝑇𝑇𝑇𝑇𝑉𝑉 (𝑟𝑟)) 𝑇𝑇 𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 𝑉𝑉𝑇𝑇 𝑇𝑇𝑇𝑇(𝑟𝑟) 𝑉𝑉 𝑇𝑇𝐼𝐼𝑉𝑉 𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉 (𝑟𝑟)= 𝑇𝑇 𝑇𝑇𝑇𝑇𝑇𝑇𝐸𝐸𝑇𝑇 𝑉𝑉𝑇𝑇𝑇𝑇𝑇𝑇𝐸𝐸𝑇𝑇 𝑇𝑇𝑇𝑇𝑉𝑉 (𝑟𝑟) 𝑇𝑇 𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇 𝑉𝑉𝑇𝑇 𝑇𝑇𝑇𝑇(𝑟𝑟) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 455 r represents the rule being tested. For example, 10,000 tests of a business rule (r) found 560 exceptions. In this example, the Valid Data Quality result would be 9440/10,000 = 94.4%, and the Invalid Data Quality result would be 560/10,000 = 5.6%. Organizing the metrics and results as shown in Table 30 can help to structure measures, metrics and indicators across the report, reveal possible rollups and enhance communications. The report can be more formalized and linked to projects that will remediate the issues. Filtered reports are useful for data stewards looking for trends and contributions. Table 30 provides examples of rules constructed in this manner. Where applicable, results of rules are expressed in both positive percentages (the portion of the data that conforms to rules and expectations) and negative percentages (the portion of the data that does not conform to the rule). Data Quality Rules provide the foundation for operational management of Data Quality. Rules can be integrated into application services or data services that supplement the data lifecycle, either through Commercial Off The Shelf (COTS) Data Quality tools, rules engines and reporting tools for monitoring and reporting, or custom - developed applications. Table 32 Data Quality Metric Examples Example of completeness Business Rule General Example Business Rule Population of field is mandatory Postal Code must be populated in the address table for 100%. Measure Count the number of records where data is populated, compare to the total number of records Count populated: 700,000 Count not populated: 300,000 Total count: 1,000,000 Metrics (calculation) Divide the obtained number of records where data is populated by the total number of records in the table or database and multiply it by 100 to get to percentage complete Positive measure: 700,000/1,000,000*100 = 70% populated Negative measure: 300,000/1,000,000 *100 = 30% not populated Status indicator Acceptable if the business rule is met. Unacceptable is the business rule is not met. Unacceptable. There are too much unpopulated Postal Codes. Example of uniqueness Business Rule General Example Business Rule There should be only one record per entity instance in a table There should be one and only one current row per postal code on the Postal Codes master list: 0% duplicates. Measure Count the number of duplicate records identified; report on the percentage of records that represent duplicates Count of incomplete transactions: 2000 Count of attempted transactions: 1,000,000 Metrics (calculation) Divide the number of duplicate records by the total number of records in the table or database and multiply it by 100 10,000/1,000,000*100 = 1.0% of postal codes are present on more than one current row Status indicator Acceptable if the business rule is met. Unacceptable is the business rule is not met. Unacceptable. There are too many duplicates. Example of timeliness Business Rule General Example Business Rule Records must arrive within a scheduled timeframe 99% of equity market record should arrive within 5 minutes of being transacted Measure Count the number of records failing to arrive on time from a data service for business transactions to be completed Count of incomplete transactions: 2000 Count of attempted transactions: 1,000,000 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "456 • DMBOK2 Business Rule General Example Metrics (calculation) Divide the number of incomplete transactions by the total number of attempted transactions in a time period and multiply by 100. Positive: (1,000,000 – 2000) / 1,000,000*100 = 99.8% of transaction records arrived within defined timeframe Negative: 2000/1,000,000*100 = 0.20% of transactions did not arrive within defined timeframe Status indicator Acceptable if the business rule is met. Unacceptable is the business rule is not met. Acceptable. More than 99% of records arrive within 5 minutes. Example of validity Business Rule General Example Business Rule If field X = value 1, then field Y must = value 1 - prime Alle shipped orders should be billed Measure Count the number of records where the rule is met Count of records where status for shipping = ‘Shipped’ and status for billing = ‘Billed’: 999,000 Count of total records: 1,000,000 Metrics (calculation) Divide the number of records that meet the condition by the total number of records Positive: 999,000/1,000,000*100 = 99.9% of records conform to the rule Negative: (1,000,000- 999,000) / 1,000,000 *100 = 0.1% do not m eet the rule. Status indicator Acceptable if the business rule is met. Unacceptable if the business rule is not met. Unacceptable. Too many orders are not billed. 4.2 Data Profiling Data Profiling is a form of data analysis used to inspect data and assess quality. Data profiling uses statistical techniques to discover the true structure, content, and quality of a collection of data (Olson, 2003). A profiling engine produces statistics that analysts can use to identify patterns in data content and structure. For example: • Counts of nulls: Identifies nulls exist and allows for inspection of whether they are allowable or not • Max/Min value: Identifies outliers, like negatives • Max/Min length: Identifies outliers or invalids for fields with specific length requirements • Frequency distribution of values for individual columns: Enables assessment of reasonability (e.g., distribution of country codes for transactions, inspection of frequently or infrequently occurring values, as well as the percentage of the records populated with defaulted values) • Data type and format: Identifies level of non -conformance to format requirements, as well as identification of unexpected formats (e.g., number of decimals, embedded spaces, sample values) Profiling also includes cross- column analysis, which can identify overlapping or duplicate columns and expose embedded value dependencies. Inter -table analysis explores overlapping values sets and helps identify foreign key relationships. Most data profiling tools allow for drilling down into the analyzed data for further investigation. Results from the profiling engine must be assessed by an analyst to determine whether data conforms to rules and other requirements. A good analyst can use profiling results to confirm known relationships and uncover hidden characteristics and patterns within and between data sets, including business rules and validity constraints. Profiling is usually used as part of data discovery for projects (especially data integration projects; see Chapter 8) or to assess the current state of data that is targeted for improvement. Results of data profiling Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 457 can be used to identify opportunities to improve the quality of both data and Metadata (Olson, 2003; Maydanchik, 2007). While profiling is an effective way to understand data, it is just a first step to Data Quality improvement. It enables organizations to identify potential problems. Solving problems requires other forms of analysis, including business process analysis, analysis of data lineage , and deeper data analysis that can help isolate root causes of problems from stakeholders along the data chain. When planning large scale profiling, ensure that time is allocated to share results, prioritize problems , and determine w hich issues require in -depth analysis. Create shareable, linkable , and re -usable code modules that execute repeated Data Quality checks and audit processes that developers can get from a library. If the module needs to change, then all the code linked to that module will get updated. Such modules simplify the maintenance process. Well -engineered code blocks can prevent many Data Quality problems. Another benefit is they ensure processes are executed consistently. Where laws or policy mandate reporting of specific quality results, the lineage of results often needs to be described. Quality check modules can provide this. For data that has any questionable quality dimension and that is highly rated, qualify the information in the shared environments with quality notes and confidence ratings. 4.3 Preventive Actions Create shareable, linkable, and re -usable code modules that execute repeated Data Quality checks and audit processes that developers can get from a library. If the module needs to change, then all the code linked to that module will get updated. Such modules simplify the maintenance process. Well -engineered code blocks can prevent many Data Quality problems. As importantly, they ensure processes are executed consistently. Where laws or policy mandate reporting of specific quality results, the lineage of res ults often needs to be described. Quality check modules can provide this. For data that has any questionable quality dimension and that is highly rated, qualify the information in the shared environments with quality notes, and confidence ratings. 4.4 Effective Data Quality Metrics The best way to create high quality data is to prevent poor quality data from entering an organization. Preventive actions stop known errors from occurring. Inspecting data after it is in production will not improve its quality. Approaches include: • Establish data entry controls : Create data entry rules that prevent invalid or inaccurate data from entering a system. • Train data producers : Ensure staff in upstream systems understand the impact of their data on downstream users. Give incentives or base evaluations on data accuracy and completeness, rather than just speed. • Define and enforce rules : Create a ‘data firewall,’ which has a table with all the business Data Quality Rules used to check if the quality of data is good before being used in an application such as a data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "458 • DMBOK2 warehouse. A data firewall can inspect the level of quality of data processed by an application and if the level of quality is below acceptable levels, analysts can be informed about the problem. • Demand high quality data from data suppliers : Examine an external data provider’s processes to check their structures, definitions, data source(s) , and data provenance. Doing so enables assessment of how well their data will integrate and helps prevent the use of non -authoritative data or data acquired without permission from the owner. • Implement Data Governance and Stewardship : Ensure roles and responsibilities are defined that describe and enforce rules of engagement, decision rights , and accountabilities for effective management of data and information assets (McGilvray, 2008). Work with data stewards to revise the process of, and mechanisms for, generating, sending , and receiving data. • Institute formal change control : Ensure all changes to stored data are defined and tested before being implemented. Prevent changes directly to data outside of normal processing by establishing gating processes. 4.5 Root Cause Analysis Data Quality issues should be addressed systemically at their root causes to minimize the costs and risks of corrective actions. ‘Solve the problem where it happens’ is the best practice in Data Quality Management. A root cause of a problem is a factor tha t, if eliminated, would remove the problem itself. Root cause analysis is a process of understanding factors that contribute to problems and the ways they contribute. Its purpose is to identify underlying conditions that, if eliminated, would mean problems would disappear. For example, a data process that runs each month requires a file of customer information as input. Measurement of the data shows that in April, July, October and January, the quality of the data goes down. Inspection of the timing of delivery shows that in March, June, September and December, the file is delivered on the 30th of the month, whereas at other times it is delivered on the 25th. Further analysis shows that the team responsible for delivering the file is also responsible for clo sing quarterly financial processes. These processes take precedence over other work and the files are delivered late during those months, impacting the quality. The root cause of the Data Quality problem turns out to be a process delay caused by a competing priority. It can be addressed by scheduling file delivery and ensuring that resources can deliver within the schedule. Common techniques for root cause analysis include Pareto analysis (the 80/20 rule), fishbone diagram analysis, track and trace, process analysis, and the Five Whys (Sakichi Toyoda ). 4.6 Corrections Corrections are implemented after a problem has occurred and been detected. Where there have been process, system , or user failures, there is also usually a trail of poor -quality data that needs to be repaired. Corrective actions should include avoiding recurrence of the causes of the quality problems, with process, system changes and user awareness. These fixes shoul d use the activities for upgrading the Data Quality Management in section 2. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 459 There is often a trail of poor -quality data though multiple systems and even into other organizations as modern systems often transmit data in close to real time. Data Cleansing or Scrubbing transforms data to make it conform to data standards and domain rules. Cleansing includes detecting and correcting data errors to bring the quality of data to an acceptable level. It costs money and introduces risk to continuously remediate data through cleansing. Ideally, the need for data cleansing should decrease over time as root causes of data issues are resolved. In some situations, correcting on an ongoing basis may be necessary, as re -processing the data in a midstream system is cheaper than any other alternative. Perform data cleansing can be performed in three general ways: • Fully Automated : Automated correction techniques include rule -based standardization, normalization and correction. The modified values are obtained or generated and committed without manual intervention. An example is automated address correction which submits delivery a ddresses to an address standardizer that conforms and corrects delivery addresses using rules, parsing, standardization, and reference tables. Automated correction requires an environment with well-defined standards, commonly accepted rules and known error patterns. The amount of automated correction can be reduced over time if this environment is well -managed and corrected data is shared with upstream systems. • Manually -directed : Use automated tools to remediate and correct data but require manual review before committing the corrections to persistent storage. Apply name and address remediation, identity resolution and pattern -based corrections automatically, and use some scoring mechanism to propose a level of confidence in the correction. Corrections with scores above a particular level of confidence may be committed without review, however, corrections with scores below the level of confidence are presented to the data steward for review and approval. Commit all approved corrections and review those not approved to understand whether to adjust the applied underlying rules. Environments in which sensitive data sets require human oversight (e.g., MDM) are good examples of where manual -directed correction may be suited. • Manual : Sometimes manual correction is the only option in the absence of tools or automation or if it is determined that the change is better handled through human oversight. Manual corrections are best done through an interface with controls and edits, which provide an audit trail for changes. Avoid using the alternative of making corrections and committing the updated records directly in production environments as it is extremely risky. 5. Implementation Guidelines Improving the quality of data within an organization is not an easy task – even when launched from within a Data Governance Function and with the support of senior management. A classic discussion is whether it is better to implement a Data Quality Function top -down or bottom -up. Typically, a hybrid approach works best – top-down for sponsorship, consistency, and resources, but bottom -up to discover what is actually broken and to achieve incremental successes. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "460 • DMBOK2 Improving Data Quality requires changes in how people think about and behave toward data. Cultural change is challenging. It requires planning, training, and reinforcement. (See Chapter 17.) While the specifics will differ from organization to organization, most Data Quality Function implementations need to plan for: • Metrics on the value of high quality data and the cost of poor -quality data: One way to raise organizational awareness of the need for Data Quality Management is through metrics that describe the value of data and the return on investment from improvements. These metrics which differ from Data Quality scores provide the basis for funding improvements and changing the behavior of both staff and management. See Chapter 15. • Operating model for IT/Business interactions: Business people know what the important data is, and what it means. Data Custodians from IT understand where and how the data is stored and so they are well placed to translate definitions of Data Quality into queries or code that identify specific records that do not comply. See Chapter 12. • Changes in how projects are executed: Project oversight must ensure project funding includes steps related to Data Quality (e.g., profiling and assessment, definition of quality expectations, data issue remediation, prevention, and correction, building con trols and measurements). It is prudent to make sure issues are identified early and to build Data Quality expectations upfront in projects. • Changes to business processes: Improving Data Quality depends on improving the processes by which data is produced. The Data Quality team needs to be able to assess and recommend changes to non-technical (as well as technical) processes that impact the qua lity of data. • Funding for remediation and improvement projects: Some organizations do not plan for remediating data, even when they are aware of Data Quality issues. Data will not fix itself. The costs and benefits of remediation and improvement projects should be docum ented so that work on improving data can be prioritized. • Funding for Data Quality Operations: Sustaining Data Quality requires ongoing operations to monitor Data Quality, report on findings and continue to manage issues as they are discovered. 5.1 Readiness Assessment / Risk Assessment Most organizations that depend on data have a lot of opportunity for improvement. How formal and well - supported a Data Quality function will be depend s on how mature the organization is from a data management perspective. See Chapter 15. Organizational readiness to adopt Data Quality practices can be assessed by considering the following characteristics: • The organization’s current understanding of the quality of its data: Before most organizations start their quality improvement journey, they generally have to understand the obstacles and pain points that signify poor quality data. Gaining knowledge of these is important. Through them, poor quality data can be directly associated with negative effects including direct and indirect costs on the organization. An understanding of pain points also helps identify and prioritize improvement projects. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 461 • The actual state of the data: Finding an objective way to describe the condition of data that is causing pain points is the first step to improving the data. Data can be measured and described through profiling and analysis, as well as through quantification of known issues and pain points. If the Data Quality team does not know the actual state of the data, then it will be difficult to prioritize and act on opportunities for improvement. • Risks associated with data creation, processing, or use: Identifying what can go wrong with data and the potential damage to an organization from poor quality data provides the basis for mitigating risks. If the organization does not recognize these risks, it may be challenging to get support for the Data Quality function . • Cultural and technical readiness for scalable Data Quality Monitoring: The quality of data can be negatively impacted by business and technical processes. Improving the quality of data depends on cooperation between business and IT teams. If the relationship between business and IT teams is not collaborative, then it will be difficult to make progress. Findings from a readiness assessment will help determine where to start and how quickly to proceed. Findings can also provide the basis for road -mapping improvements goals. If there is strong support for Data Quality improvement and the organization knows its own data, then it may be possible to launch a full strategic improvement program. If the organization does not know the actual state of its data, then it may be necessary to focus on building that knowledge before developing a full strategy. 5.2 Organization and Cultural Change The quality of data will not be improved through a collection of tools and concepts. Rather , improvement will come through a mindset that helps employees and stakeholders to act while always thinking of the quality of data and what the business and their customers need. Getting an organization to be conscientious about Data Quality often requires significant cultural change (see Chapter 17). Ultimately, employees need to think and act differently if they are to produce better quality data and manage data in ways that ensure quality. This requires training and reinforcement. Three common themes of how mindsets need to change in Data Quality Management are: • Lessening the grip of the Status Quo – “the data has always been bad, we have already tried everything to change it.” This attitude destroys the impetus to change and drains team morale. • Dealing with politics – “they need to do a better job with Data Quality instead of leaving it to me, but I do not have any resources, time, people or assistance to give them.” This attitude comes from siloed natures of organizations and effectively blocks enterprise Data Quality Management. • Removing hero cultures – “if I did not do this job, no -one else would, I am critical to making this regulatory report happen”. This attitude ensures opaqueness in Data Quality measurement, avoiding objective measurement and transparency (key Data Quality principle). The first step is promoting awareness about the role and importance of data to the organization. All employees must act responsibly and raise Data Quality issues, ask for good quality data as consumers, and provide quality Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "462 • DMBOK2 information to others. Every person who touches the data can impact the quality of that data. Data Quality is not just the responsibility of a Data Quality team or IT group. Just as the employees need to understand the cost to acquire a new customer or retain an existing customer, they also need to know the organizational costs of poor -quality data, as well as the conditions that cause data to be of poor quality. For example, if customer data is incomplete, a customer may receive the wrong product, creating direct and indirect costs to an organization. Not only will the customer return the product, but he or she may call and complain, using call center time, with the potential for reputational damage to the organization. If customer data is incomplete because the organization has not established clear requirements, then everyone who uses this data has a stake in clarifying requirements and following standards. 6. Data Quality and the other Knowledge Areas All data management knowledge areas contribute to the quality of data. High quality data that supports the organization should be the goal of all data management disciplines. Cross- functional commitment and coordination produces high quality data. Uninformed decisions or actions by anyone who interacts with data can result in poor quality data. Organizations and teams should be aware of this and should plan for high quality data by executing processes and projects in ways that account for risk related to un expected or unacceptable conditions in the data. This section identifies the key relationships that Data Quality has with other DMBOK Knowledge Areas. 6.1 Data Quality and Data Modeling and Design Data Quality is enabled by Data Modelling and Design during the design phase of projects by allowing for data elements to: • Hold Data Quality values. For example, after a customer address is validated, then the address may be marked as passing validation. This is an additional data element that needs to be accounted for in the design • Associate business rules. For example, if a customer address must always be completed to a certain level, then it is more efficient to store the rule in the data model against the element than in every business process that uses and updates the address. Tools used to model data and create ETL processes have a direct impact on the quality of data. If they are used without knowledge of the data, they can have detrimental effects. The Data Quality team should work with development teams to ensure that Data Quality risks are addressed and that the organization takes full advantage of the ways in which effective modeling and data processing can enable higher quality data. (See Chapters 5, 8, and 11.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 463 6.2 Data Quality and Metadata Management Metadata defines what the data represents. Having a robust process by which data is defined supports the ability of an organization to formalize and document the standards and requirements by which the quality of data can be measured. Data Quality is about meeting expectations. Metadata is a primary means of clarifying expectations. The most important way that Data Quality Management and Metadata Management are related is that Metadata is created during the Data Quality process. This Metadata should remain associated with the data. For example, if a customer address is marked as valid, complete and reasonable by a process, then this needs to be maintained as Metadata with the address. It assists the Data Quality principles of being standards driven with objective measurement and transparency. Metadata holds to express the “fitness” of the data. This enables consumers to assess the quality of data based on how well it meets the “purpose” of particular data consumer contexts. Conversely, if there is no Data Quality Metadata associated with data then users will not trust the data resulting in significant effort spent rechecking and validation. Data Quality teams should work closely with teams that manage Metadata to ensure that Data Quality requirements, rules, measurement results and documentation of issues are made available to data consumers. A Metadata repository can house results of Data Quality measurements so that these are shared across the organization and the Data Quality team can work toward consensus about priorities and drivers for improvement. (See Chapter 12.) 6.3 Data Quality and Master and Reference Data Management Reference and Master Data Management controls the copying of shared data across the organization. Shared data is used in many events to control the flow of business processes, enable decision making and validate data. Therefore, most (if not all) master and reference data is critical data within the Data Quality regime. A key source of poor Data Quality is Weak Master Data Management: Immature Master Data Management can lead to choosing unreliable sources for data which can cause Data Quality issues that are very difficult to find until the assumption that the data source is accurate is disproved. Reference and master data is a key source of data for Data Quality rule enforcement, particularly the Data Quality dimensions of validity, consistency and integrity. For example, a text value must be in a defined domain of values . These values will be managed Reference Data. For example, in order to have integrity, an order must be on an existing, approved customer. The customer record is master data that must be managed to be ready and available to Data Quality validation process ing. 6.4 Data Quality and Data Integration and Interoperability Data Quality is related to data integration and interoperability as the movement of data is a common source of errors within data processes. During data movement it is important that critical Metadata is maintained with the data (and integrations do not just remove the Metadata ). Data Quality Metadata provides trust and confidence in the data with assurance that it is fit for defined purposes. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "464 • DMBOK2 The other key value of data integrations is that is a key place to test the Data Quality. By monitoring Data Quality between processing stages (i.e., at key integration points), data that is corrupted can quickly be identified and isolated, avoiding propag ation of data errors through full data value chains. 6.5 Data Quality and Data Governance A Data Quality Function is more effective when part of a Data Governance Function. Often Data Quality issues are the reason for establishing enterprise -wide Data Governance (see Chapter 3). A Governance Organization can accelerate the work of a Data Quality Function by: • Setting priorities and providing direction • Providing mechanisms to facilitate staff involvement and knowledge sharing • Identifying and coordinating access to those who should be involved in Data Quality decisions and activities • Ensuring measurements of Data Quality are acted upon • Sharing Data Quality inspection results, identify opportunities for improvement, and build consensus • Resolving variations and conflicts Incorporating Data Quality efforts into the overall governance effort enables the Data Quality Function team to work with a range of stakeholders and enables: • Business process engineering and training staff who can help teams implement process improvements • Business and operational data stewards, and data owners who can identify critical data, define standards and quality expectations, and prioritize remediation of data issues. 7. Works Cited / Recommended Batini, Carlo, and Monica Scannapieco. Data Quality: Concepts, Methodologies and Techniques . Springer, 2006. Print. Brackett, Michael H. Data Resource Quality: Turning Bad Habits into Good Practices . Addison -Wesley, 2000. Print. Deming, W. Edwards. Out of the Crisis . The MIT Press, 2000. Print. English, Larry. Improving Data Warehouse and Business Information Quality: Methods For Reducing Costs And Increasing Profits . John Wiley and Sons, 1999. Print. English, Larry. Information Quality Applied: Best Practices for Improving Business Information, Processes, and Systems . Wiley Publishing, 2009. Print. Evans, Nina and Price, James. “Barriers to the Effective Deployment of Information Assets: An Executive Management Perspectiv e.” Interdisciplinary Journal of Information, Knowledge, and Management Volume 7, 2012. Accessed from http://bit.ly/2sVwvG4 . Fisher, Craig, Eitel Lauría, Shobha Chengalur -Smith and Richard Wang. Introduction to Information Quality . M.I.T. Information Quality Program Publications, 2006. Print. Advances in Information Quality Book Ser. Gottesdiener, Ellen. Requirements by Collaboration: Workshops for Defining Needs . Addison -Wesley Professional, 2002. Print. Hass, Kathleen B. and Rosemary Hossenlopp. Unearthing Business Requirements: Elicitation Tools and Techniques . Management Concepts, Inc, 2007. Print. Business Analysis Essential Library. Huang, Kuan -Tsae, Yang W. Lee and Richard Y. Wang. Quality Information and Knowledge . Prentice Hall, 1999. Print. Jugulum, Rajesh. Competing with High Quality Data . Wiley, 2014. Print. Lee, Yang W., Leo L. Pipino, James D. Funk and Richard Y. Wang. Journey to Data Quality . The MIT Press, 2006. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 465 Loshin, David. Enterprise Knowledge Management: The Data Quality Approach . Morgan Kaufmann, 2001. Print. Loshin, David. Master Data Management. Morgan Kaufmann, 2009. Print. Maydanchik, Arkady. Data Quality Assessment . Technics Publications, LLC, 2007 Print. McCallum, Ethan. Bad Data Handbook: Cleaning Up the Data So You Can Get Back to Work . 1st Edition. O'Reilly, 2012. McGilvray, Danette. Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information. Morgan Kaufmann, 2008. Print. Myers, Dan. “The Value of Using the Dimensions of Data Quality”, Information Management, August 2013. http://bit.ly/2tsMYiA . Olson, Jack E. Data Quality: The Accuracy Dimension . Morgan Kaufmann, 2003. Print. Redman, Thomas. Data Quality: The Field Guide . Digital Press, 2001. Print. Robertson, Suzanne and James Robertson. Mastering the Requirements Process: Getting Requirements Right. 3rd ed. Addison -Wesley Professional, 2012. Print. Sebastian -Coleman, Laura. Measuring Data Quality for Ongoing Improvement: A Data Quality Assessment Framework . Morgan Kaufmann, 2013. Print. The Morgan Kaufmann Series on Business Intelligence. Tavares, Rossano. Qualidade de Dados em Gerenciamento de Clientes (CRM) e Tecnologia da Informação [Data Quality in Managemen t of Customers and Information Technology]. São Paulo: Catálise. 2006. Print. Witt, Graham. Writing Effective Business Rules: A Practical Method . Morgan Kaufmann, 2012. Print. 8. Appendix 8.1 Data Quality ISO Standard ISO 8000, the international standard for Data Quality, is being developed to enable the exchange of complex data in an application -neutral form. In the introduction to the standard, ISO asserts: “The ability to create, collect, store, maintain, transfer, process and present data to support business processes in a timely and cost - effective manner requires both an understanding of the characteristics of the data that determine its quality, and an ability to measure, manage and report on Data Quality.” ISO 8000 defines dimensions that can be tested by any organization in the data supply chain to objectively determine conformance of the data to ISO 8000.76 The first published part of ISO 8000 (part 110, published in 2008) focused on the syntax, semantic encoding, and conformance to the data specification of Master Data. Other parts projected for the standard include part 100 - Introduction, part 120 - Proven ance, part 130 - Accuracy, and part 140 - Completeness.77 ISO 8000 defines quality data as “portable data that meets stated requirements.”78 The Data Quality standard is related to the ISO’s overall work on data portability and preservation. Data is considered ‘portable’ if it can be separated from a software application. Data that can only be used or read using a specific licensed software 76 http://bit.ly/2ttdiZJ. 77 http://bit.ly/2sANGdi . 78 http://bit.ly/2rV1oWC . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "466 • DMBOK2 application is subject to the terms of the software license. An organization may not be able to use data it created unless that data can be detached from the software that was used to create it. To meet stated requirements requires that these requirements be defined in a clear, unambiguous manner. ISO 8000 is supported through ISO 22745, a standard for defining and exchanging Master Data. ISO 22745 defines how data requirement statements should be constructed, provides examples in XML, and defines a format for the exchange of encoded data.81 ISO 22745 creates portable data by labeling the data using an ISO 22745 compliant Open Technical Dictionary such as the ECCMA Open Technical Dictionary (eOTD). The intention of ISO 8000 is to help organizations define what is and is not quality data, enable them to ask for quality data using standard conventions, and verify that they have received quality data using those same standards. When standards are follow ed, requirements can be confirmed through a computer program. ISO 8000 - Part 61 Information and Data Quality Management process reference model is under development. This standard will describe the structure and organization of Data Quality Management, including: • Data Quality Planning • Data Quality Control • Data Quality Assurance • Data Quality Improvement 8.2 Further Reading on Data Quality Dimensions Some dimensions can be measured objectively (completeness, validity, format conformity) and others that depend on heavily context or on subjective interpretation (usability, reliability, reputation). Whatever names are used, dimensions focus on whether the re is enough data (completeness), whether it is right (accuracy, validity), how well it fits together (consistency, integrity, uniqueness), whether it is up -to-date (timeliness), accessible, usable, and secure. Many leading thinkers in Data Quality have published sets of dimensions. 79 The three most influential are described here because they provide insight into how to think about what it means to have high quality data, as well as into how Data Quality can be measured. The Strong -Wang framework (1996) focuses on data consumers’ perceptions of data. It describes 15 dimensions across four general categories of Data Quality: • Intrinsic Data Quality o Accuracy o Objectivity o Believability o Reputation • Contextual Data Quality 79 In addition to the examples detailed here and numerous academic papers on this topic, see Loshin (2001), Olson (2003), McGilvray (2008), and Sebastian -Coleman (2013) for detailed discussions on Data Quality dimensions. See Myers (2013) for a comparison of dimensions. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 467 o Value -added o Relevancy o Timeliness o Completeness o Appropriate amount of data • Representational Data Quality o Interpretability o Ease of understanding o Representational consistency o Concise representation • Accessibility Data Quality o Accessibility o Access security In Data Quality for the Information Age (1996), Thomas Redman formulated a set of Data Quality dimension s rooted in data structure.80 Redman defines a data item as a “representable triple”: a value from the domain of a data element within an entity. Dimensions can be associated with any of the component pieces of data – the model (entities and data elements) as well as the values. Redma n includes the dimension of representation, which he defines as a set of rules for recording data items. Within these three general categories (data model, data values, representation), he describes more than two dozen dimensions. They include the followin g: Data Model: • Content: o Relevance of data o The ability to obtain the values o Clarity of definitions • Level of detail: o Data element granularity o Precision of data element domains • Composition: o Naturalness: The idea that each data element should have a simple counterpart in the real world and that each data element should bear on a single fact about the entity o Identify- ability: Each entity should be distinguishable from every other entity o Homogeneity o Minimum necessary redundancy • Consistency: o Semantic consistency of the components of the model o Structure consistency of data elements across entity types • Reaction to change: o Robustness o Flexibility 80 Redman expanded and revised his set of dimensions in Data Quality: The Field Guide (2001). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "468 • DMBOK2 Data Values: • Accuracy • Completeness • Currency • Consistency Representation: • Appropriateness • Interpretability • Portability • Format precision • Format flexibility • Ability to represent null values • Efficient use of storage • Physical instances of data being in accord with their formats Redman recognizes that consistency of entities, values, and representation can be understood in terms of constraints. Different types of consistency are subject to different kinds of constraints. In Improving Data Warehouse and Business Information Quality (1999), Larry English presents a comprehensive set of dimensions divided into two broad categories: inherent and pragmatic.81 Inherent characteristics are independent of data use. Pragmatic characteristics are associated with data presentation and are dynamic; their value (quality) can change depending on the uses of data. • Inherent quality characteristics o Definitional conformance o Completeness of values o Validity or business rule conformance o Accuracy to a surrogate source o Accuracy to reality o Precision o Non -duplication o Equivalence of redundant or distributed data o Concurrency of redundant or distributed data • Pragmatic quality characteristics o Accessibility o Timeliness o Contextual clarity o Usability o Derivation integrity o Rightness or fact completeness 81 English expanded and revised his dimensions in Information Quality Applied (2009). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA QUALITY MANAGEMENT • 469 In 2013, DAMA UK produced a white paper describing six core dimensions of Data Quality: • Completeness : The proportion of data stored against the potential for 100%. • Uniqueness : No entity instance (thing) will be recorded more than once based upon how that thing is identified. • Timeliness : The degree to which data represent reality from the required point in time. • Validity : Data is valid if it conforms to the syntax (format, type, range) of its definition. • Accuracy: The degree to which data correctly describes the ‘real world’ object or event being described. • Consistency : The absence of difference, when comparing two or more representations of a thing against a definition. The DAMA UK white paper also describes other characteristics that have an impact on quality. While the white paper does not call these dimensions, they work in a manner similar to Strong and Wang’s contextual and representational Data Quality and English’s pragmatic characteristics. • Usability : Is the data understandable, simple, relevant, accessible, maintainable and at the right level of precision? • Timing issues (beyond timeliness itself): Is it stable yet responsive to legitimate change requests? • Flexibility : Is the data comparable and compatible with other data? Does it have useful groupings and classifications? Can it be repurposed? Is it easy to manipulate? • Confidence : Are Data Governance, Data Protection, and Data Security processes in place? What is the reputation of the data, and is it verified or verifiable? • Value : Is there a good cost / benefit case for the data? Is it being optimally used? Does it endanger people’s safety or privacy, or the legal responsibilities of the enterprise? Does it support or contradict the corporate image or the corporate message? 8.3 Statistical Process Control Statistical Process Control (SPC) is a method to manage processes by analyzing measurements of variation in process inputs, outputs, or steps. The technique was developed in the manufacturing sector in the 1920s and has been applied in other industries, in improvement methodologies such as Six Sigma, and in Data Quality Management. Simply defined, a process is a series of steps executed to turn inputs into outputs. SPC is based on the assumption that when a process with consistent inputs is executed consistently, it will produce consistent outputs. It uses measures of central tendency (how values cluster around a central value, such as a mean, median, or mode) and of variability around a central value (e.g., range, variance, standard deviation), to establis h tolerances for variation within a process. The primary tool used for SPC is the control chart ( Figure 95), which is a time series graph that includes a central line for the average (the measure of central tendency) and depicts calculated upper and lower control limits (variability around a central value). In a stable process, measurement results outside the control limits indicate a special cause. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "470 • DMBOK2 Figure 95 Control Chart of a Process in Statistical Control SPC measures the predictability of process outcomes by identifying variation within a process. Processes have variation of two types: Common Causes that are inherent in the process and Special Causes that are unpredictable or intermittent. When the only so urces of variation are common causes, a system is said to be in (statistical) control and a range of normal variation can be established. This is the baseline against which change can be detected. Applying SPC to Data Quality measurement is based on the working assumption that, like a manufactured product, data is the product of a process. Sometimes the process that creates data is very simple (e.g., a person fills out a form). Other times, processe s are quite complex: a set of algorithms aggregates medical claim data in order to follow trends related to the effectiveness of particular clinical protocols. If such a process has consistent inputs and is executed consistently, it will produce consistent results each time it is run. However, if the inputs or execution change, then so will the outputs. Each of these components can be measured. The measurements can be used to detect special causes. Knowledge of the special causes can be used to mitigate risks associated with data collection or processing. SPC is used for control, detection, and improvement. The first step is to measure the process in order to identify and eliminate special causes. This activity establishes the control state of the process. Next is to put in place measurements to detect unexpected variation as soon as it is detectable. Early detection of problems simplifies investigation of their root causes. Measurements of the process can also be used to reduce the unwanted effects of common causes of variation, allowing for increased effic iency. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "471 CHAPTER 1 4 Big Data and Data Science 1. Introduction ince the early 2000s, the terms Big Data and Data Science have, unfortunately, been bandied about as buzzwords. The concepts and their implications are misunderstood – or, at least, there is limited consensus on their meaning. Even the meaning of ‘Big’ is relative. That said, both Big Data and Data Science are connected to significant technological changes that have allowed people to generate, store, and analyze larger and larger amounts of data. More importantly, people can use that data to predict and influence behavior, as well as to gain insi ght on a range of important subjects, such as healthcare practices, natural resource management, and economic development. Big Data refers not only to the volume of data, but also to its variety (structured and unstructured, documents, files, audio, video, and streaming data, etc.), and the speed at which it is produced (velocity). People who mine and develop predictive, machine learning, and prescriptive models and analytics from these and deploy results for analysis by interested parties are called Data Scientists. Data Science has existed for a long time; it used to be called ‘applied statistics’. But the capability to explore data patterns has quickly evolved in the twenty- first century with the advent of Big Data and the technologies that support it. Traditional B usiness Intelligence provides ‘rear -view mirror’ reporting – analysis of structured data to describe past trends. In some cases, BI patterns are used to predict future behavior, but not with high confidence. Until recently, in -depth analysis of enormous data sets has been limited by technology. Analyses have relied on sampling or other means of abstraction to approximate patterns. As the capacity to collect and analyze large data sets has grown, Data Scientists have integrated methods from mathematics, stat istics, computer science, signal processing, probability modeling, pattern recognition, machine learning, uncertainty modeling, and data visualization in order to gain insight and predict behaviors based on Big Data sets. In short, Data Science has found n ew ways to analyze and get value from data. As Big Data has been brought into data warehousing and Business Intelligence environments, Data Science techniques are used to provide a forward - looking (‘windshield’) view of the organization. Predictive capabilities, real -time and model -based, using different types of data sources, offer organizations better insight into where they are heading. (See Figure 96 .) To take advantage of Big Data, however, requires change in the way that data is managed. Most data warehouses are based on relational models. Big Data is not generally organized in a relational model. Most data warehousing depends on the concept of ETL (Extract, Transform, and Load). Big Data solutions, like data lakes, depend on the concept of E LT – loading and then transforming. As importantly, the speed and volume of data present S Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "472 • DMBOK2 challenges that require different approaches to critical aspects of data management, such as integration, Metadata Management, and Data Quality assessment. Figure 96 Abate Information Triangle 1.1 Business Drivers The biggest business driver for developing organizational capabilities around Big Data and Data Science is the desire to find and act on business opportunities that may be discovered through data sets generated through a diversified range of processes. Big Data can stimulate innovation by making more and larger data sets available for exploration. This data can be used to define predictive models that anticipate customer needs and enable personalized presentation of products and services. Data Science can i mprove operations. Machine learning algorithms can automate complex time -consuming activities, thus improving organizational efficiency, reducing costs, and mitigating risks. 1.2 Principles The promise of Big Data – that it will provide a different kind of insight – depends on being able to manage Big Data. In many ways, because of the wide variation in sources and formats, Big Data management will require more discipline than relational data management. Principles related to Big Data management have yet to fully form, but one is very clear: Organizations should carefully manage Metadata related to Big Data sources in order to have an accurate inventory of data files, their origins, and their value. DATA INFORMATION KNOWLEDGE INSIGHTRaw data Data with basic context (associative meta- data) Data with businessContext or function Understanding the question – Business c ontext, function and related Information SMART DATA Trusted Source For Business Decisions Data Science: Finding patterns/clusters in information; providing insight where one would not know to look forBIG DATAMaster DataAbate Information Triangle Business Intelligence Drill- Down Warehousing Data Visualizations Exception ReportingPast Present FutureData Science Predictive Analytics Prescriptive Analytics Machine Learning Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 473 Figure 97 Context Diagram: Big Data and Data Science Definition : The handling of large amounts of data (Big Data) and paradigm and statistical analytics (Data Science) of many different types of data to find answers and insights. Goals: 1. Discover relationships between data and the business. 2. Discover and analyze new factors that might affect the business. 3. Package communications of model outputs for stakeholders and decision -makers. 4. Integrate existing organizational practices with best practices in data management, big data, and data science. Activities : 1.Define Big Data Strategy & Business Needs (P) 2.Establish Big Data Environments (D) 3.Choose Data Sources (P) 4.Acquire & Ingest Data Sources (D) 5.Develop Hypotheses & Methods (D) 6.Integrate/Align Data For Analysis (D) 7.Explore Data Using Models (D) 8.Communicate Output to Stakeholders (D) 9.Deploy and Monitor (O,C)Inputs : •Business Strategy •Business Case •Information Requirements •Information Sources & Metadata •IT Standards •Analytical Models •Data ModelsDeliverables : •Big Data Strategy & Standards •Big Data Landscape •Data Sourcing Plan •Acquired Data Sources •Data Analysis and Hypotheses •Data Insights and Findings •Operations Plan •Model Performance •Model Enhancement Plan Suppliers : •Big Data Platform Architects •Data Scientists •Data Producers •Data Suppliers •Information ConsumersConsumers : •Business Partners •Business Executives •IT Executives •Intermediaries •Stakeholders •CustomersParticipants : •Data Curators •Big Data Platform Architects •Ingestion Architects •Data SME’s •Data Scientists •Analytic Design Lead •Data Quality and Governance Specialists •Metadata Specialists T echniques : •Analytic Modeling •Machine Learning T echniques •Big Data Modeling •Columnar Compression •Advanced Supervised LearningT ools : •MPP Shared -Nothing T echnologies •Distributed File -based Databases •In-memory Computing and Databases •In-database Algorithms •Big Data Cloud Solutions •Statistical Computing and Graphical Languages •Data Visualization T oolsMetrics : •T echnical Usage Metrics •Loading and Scanning Metrics •Learnings and Stories •Response and Performance Metrics •Data Loading and Scanning Metrics (P) Planning, (C) Control, (D) Development, (O) OperationsBig Data and Data Science Business Drivers Technical Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "474 • DMBOK2 1.3 Essential Concepts 1.3.1 Data Science As noted in the chapter introduction, Data Science merges data mining, statistical analysis, and machine learning with data integration and data modeling capabilities, to build predictive models that explore data content patterns. Developing predictive models is sometimes called Data Science because the data analyst, or data scientist, uses the scientific method to develop and assess a model. The data scientist develops a hypothesis about behavior that can be observed in the data prior to a particular action. For example, the purchase of one type of item is usually followed by the purchase of another type of item (the purchase of a house is usu ally followed by the purchase of furniture). Then, the data scientist analyzes large amounts of historical data to determine how frequently the hypothesis has been true in the past and to statistically verify the probable accuracy of the model. If a hypothesis is valid with sufficient frequency, and if the behavior it predicts is useful, then the model may become the basis for an operational intelligence process to predict future behavior, even possibly in real time , such as suggestive selling advertisements. Developing Data Science solutions involves the iterative inclusion of data sources into models that develop insights. Data Science depends on: • Rich data sources : Data with the potential to show otherwise invisible patterns in organizational or customer behavior • Information alignment and analysis : Techniques to understand data content and combine data sets to hypothesize and test meaningful patterns • Information delivery : Running models and mathematical algorithms against the data and producing visualizations and other output to gain insight into behavior • Presentation of findings and data insights : Analysis and presentation of findings so that insights can be shared Table 33 compares the role of traditional DW/BI to predictive and prescriptive analytics that can be achieved through Data Science techniques. Table 33 Analytics Progression DW / Traditional BI Data Science Descriptive Predictive Prescriptive Hindsight Insight Foresight Based on history: What happened? Why did it happen? Based on predictive models: What is likely to happen? Based on scenarios: What should we do to make things happen? Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 475 1.3.2 The Data Science Process Figure 98 illustrates the iterative phases of the Data Science process. The outputs of each step become the inputs into the next. ( See Section 2.) Figure 98 Data Science Process The Data Science process follows the scientific method of refining knowledge by making observations, formulating and testing hypotheses, observing results, and formulating general theories that explain results. Within Data Science, this process takes the form of observing data and creating and evaluating models of behavior: • Define Big Data strategy and business needs: Define the requirements that identify desired outcomes with measurable tangible benefits. • Choose data sources : Identify gaps in the current data asset base and find data sources to fill those gaps. • Acquire and ingest data sources: Obtain data sets and onboard them. • Develop Data Science hypotheses and methods : Explore data sources via profiling, visualization, mining, etc.; refine requirements. Define model algorithm inputs, types, or model hypotheses and methods of analysis (i.e., groupings of data found by clustering, etc.). • Integrate and align data for analysis : Model feasibility depends in part on the quality of the source data. Leverage trusted and credible sources. Apply appropriate data integration and cleansing techniques to increase quality and usefulness of provisioned data sets. • Explore data using models : Apply statistical analysis and machine learning algorithms against the integrated data. Validate, train, and over time, evolve the model. Training entails repeated runs of the model against actual data to verify assumptions and make adjustments, such as id entifying outliers. Through this process, requirements will be refined. Initial feasibility metrics guide evolution of the 1. Define Big Data Strategy & Business Need(s) 2. Choose Data Source(s) 3. Acquire & Ingest Data Source(s) 4. Develop Data Science Hypotheses & Methods5. Integrate / Align Data For Analysis6. Explore Data Using Models7. Deploy & Monitor Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "476 • DMBOK2 model. New hypotheses may be introduced that require additional data sets and results of this exploration will shape the future modeling and outputs (even changing the requirements). • Deploy and monitor : Those models that produce useful information can be deployed to production for ongoing monitoring of value and effectiveness. Often Data Science projects turn into data warehousing projects where more vigorous development processes are put in place (ETL, D Q, Master Data, etc.). 1.3.3 Big Data Early efforts to define the meaning of Big Data characterized it in terms of the Three V’s: Volume, Velocity, Variety (Laney, 2001). As more organizations start to leverage the potential of Big Data, the list of V’s has expanded: • Volume : Refers to the amount of data. Big Data often has thousands of entities or elements in billions of records . • Velocity : Refers to the speed at which data is captured, generated, or shared. Big Data is often generated and can also be distributed and even analyzed in real- time . • Variety / Variability : Refers to the forms in which data is captured or delivered. Big Data requires storage of multiple formats; data structure is often inconsistent within or across data sets. • Viscosity : Refers to how difficult the data is to use or integrate . • Volatility : Refers to how often data changes occur and therefore how long the data is useful . • Veracity : Refers to how trustworthy the data is . Big Data volumes are exceptionally large (greater than 100 Terabyte and often in the Petabyte and Exabyte range). In warehousing and analytic solutions, very large volumes of data pose challenges to data loading, modeling, cleansing, and analytics. These challenges are often solved using massively parallel processing, or parallel processing and distributed data solutions. However, they have much wider implications. The size of data sets requires changes in the overall way that data is stored and accessed, as well as in how data is understood (e.g., much of our current way of thinking about data is based on relational database structures), as well as how data is managed (Adams, 2009). Figure 99 presents a visual summary of the range of data that has become available through Big Data technologies and the implications on data storage options. 1.3.4 Big Data Architecture Components The selection, installation, and configuration of a Big Data and Data Science environment require specialized expertise. End -to-end architectures must be developed and rationalized against existing data exploratory tools and new acquisitions. Figure 100 describes the DW/BI and Big Data Architecture. (Details about DW/BI components are described in Chapter 11.) The biggest difference between DW/BI and Big Data processing is that in a traditional data warehouse, data is integrated as it is brought into the warehouse (extract, TRANSFORM, load); while in a Big Data environment, data is ingested and loaded before it is integrated (extract, LOAD, transform). In some cases, data may not be integrated at all, in the traditional sense. Instead of Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 477 being integrated in preparation for use, it is often integrated through particular uses (e.g., the process of building predictive models drives the integration of particular data sets). Figure 99 Data Storage Challenges82 Figure 100 Conceptual DW/BI and Big Data Architecture 82 Sourced and used with permission from Robert Abate / EMC Corporation. Exabyte T erabyte GigabytePetabyte Velocity Variety Veracity EDW/BW Web 2.0 Internet of ThingsMobile EDW/BW Customers ProductsWeb 2.0 eCommerce Web Logs CollaborationSocial SitesInternet of Things Sensors/Scanners Log Files Blogs/Wikis GPS T exts/Images StorageAudio/Video Marketing AdvertisingVolume © DATALEADERS.ORG Report Interact Compare Evaluate Predict Learn Data Visualization DaaS Big Data Results MDMReference & Master Data Conformed DimensionsData Warehouse Data Mastering Data Quality Intervention Enrichment & AugmentationSources BI Conceptual DW/BI and Big Data Architecture Application Operational Reporting Big Data Data & T ext Mining Unstructured Analytics Predictive Analytics Machine Learning Operational Reporting & Analytics Geospatial and Demographic Analytics Performance Management CubesODS Data Mart Dependent Data Stores Evaluate Model Explore Integrate Ingest Data LakeEmail Multimedia Sensors IoT Social Network Web DaaS DW Operational Systems Central Warehouse Subject -Oriented Non-Volatile Time-Variant Atomic Historical Data Staging Area Clean Integrate Enrich Standardize Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "478 • DMBOK2 1.3.5 Sources of Big Data Because so much of human activity is executed electronically, massive amounts of data are accumulating every day as we move through the world, interact with each other, and transact business. Big Data is produced through email, social media, online orders, and even online video games. Data is generated not only by phones and point -of-sale devices, but also by surveillance systems, sensors in transportation systems, medical monitoring systems, industrial and utility monitoring systems, satellites, and military equipment. For example, one airline flight can generate a terabyte of data. Devices that interact directly with the Internet generate a large portion of Big Data. The connections between devices an d the Internet are sometimes called the Internet of Things (IoT). 1.3.6 Data Lake A data lake is an environment where a vast amount of data of various types and structures can be ingested, stored, assessed, and analyzed. Data lakes can serve many purposes. For example, providing • An environment for Data Scientists to mine and analyze data • A central storage area for raw data, with minimal, if any, transformation • Alternate storage for detailed historical data warehouse data • An online archive for records • An environment to ingest streaming data with automated pattern identification A data lake can be implemented as a complex configuration of data handling tools including Hadoop or other data storage systems, cluster services, data transformation, and data integration. These handlers have facilitated cross -infrastructure, analytic facilitation software to bring the configuration together. The risk of a data lake is that it can quickly become a data swamp – messy, unclean, and inconsistent. In order to establish an inventory of what is in a data lake, it is critical to manage Metadata as the data is ingested. In order to understand how the d ata in a data lake is associated or connected, data architects or data engineers often use unique keys or other techniques (semantic models, data models, etc.) so that data scientists and other visualization developers know how to use the information store d within the data lake. (See Chapter 9 .) 1.3.7 Services -Based Architecture Services -based architecture (SBA) is emerging as a way to provide immediate (if not completely accurate or complete) data, as well as update a complete, accurate historical data set, using the same source (Abate, Aiken, Burke, 1997). The SBA architecture is similar to the DW architectures which send data directly to an ODS for immediate access, as well as to the DW for historical accumulation. SBA architectures have three main components, a batch layer, a speed layer, and a serving layer. (See Figure 101.) • Batch layer : A data lake serves as the batch layer, containing both recent and historical data • Speed layer : Contains only real-time data • Serving layer: Provides an interface to join data from the batch and speed layers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 479 Data is loaded into both the batch and speed layers. All analytic computations are performed on data in both the batch and speed layers, which most likely requires implementation in two separate systems. Organizations address synchronization issues through trade -offs between completeness, latency , and complexity of merged views defined in the serving layer. Cost/benefit assessment is required to determine whether reducing latency or improving data completeness is worth the associated cost and complexity. The batch layer is often referred to as the structure-over -time component (here every transaction is an insert), whereas in the speed layer (often referred to as an Operational Data Store or ODS), all transactions are updates (or inserts only if required). In this manner, the architecture prevents synchronization issues while simultaneously creating a current state and a history layer. This architecture usually provides its data through a serving or data services layer that abstracts the data utilizing Meta data. This services layer determines where the data is to be ‘served’ from and appropriately provides the data requested. Figure 101 Services -based Architecture 1.3.8 Machine Learning Machine Learning explores the construction and study of learning algorithms. It can be viewed as a union of unsupervised learning methods, more commonly referred to as data mining, and supervised learning methods deeply rooted in mathematical theory, specifically statistics, combinatorics , and optimization. A third branch is now forming called reinforcement learning, where goal performance is earned but not specifically teacher recognized – driving a vehicle for example. Programming machines to quickly learn from queries a nd adapt to changing data sets led to a completely new field within Big Data referred to as machine learning. 83 Processes run, and results are stored that are then used in subsequent runs to iteratively inform the process and refine the results. Machine Learning explores the construction and study of learning algorithms. These algorithms fall into three types: • Supervised learning : Based on generalized rules; for example, separating SPAM from non -SPAM email • Unsupervised learning : Based on identifying hidden patterns (i.e., data mining) 83 Refer to the Machine Learning resources periodic table at http://bit.ly/1DpTrHC for an interactive guide to the differing platforms available for the machine learning developer, scientist, and practitioner. Source DataSpeed Layer Real-time, no history Batch LayerServing Layer Fully processed historyMerged view Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "480 • DMBOK2 • Reinforcement learning : Based on achieving a goal (e.g., beating an opponent at chess) Statistical modeling and machine learning have been employed to automate otherwise costly research and development projects, by performing several trial and error passes on a vast set of data, repeating trials with the results collected, analyzed, and erro rs corrected. This approach can reduce time to answer dramatically and guide organizational initiatives with insights based on cost effective repeatable processes. For example, CIVDDD uses machine learning and complex scientific data visualization techniqu es to help government agencies and peacekeepers meet the challenge of dealing with the masses of threat- related information. 84 While it taps into data in new ways, machine learning has ethical implications, especially with respect to the principle of transparency. Evidence shows that deep learning neural networks (DLNN) work. They learn things. However, it is not always clear how they learn. As the algorithms that drive these processes become more complex, they also become more opaque, functioning as ‘black boxes’. As they account for a greater number of variables and as those variables themselves are more abstract, algorithms test the limits of human ability to interpret the machine (Davenport, 2017). The need for transparency – the ability to see how decisions are made – will likely increase as this functionality evolves and is put to use in a wider array of situations. (See Chapt er 2.) 1.3.9 Sentiment Analysis Media monitoring and text analysis are automated methods for retrieving insights from large unstructured or semi -structured data, such as transaction data, social media, blogs, and web news sites. This is used to understand what people say and feel about brands, products, or services, or other types of topics. Using Natural Language Processing (NLP) or by parsing phrases or sentences, semantic analysis can detect sentiment and also reveal changes in sentiment to predict possible scenarios. Consider the case of looking for key words in a posting. If the words good or great are present, this might be a positive response, versus seeing awful or bad might be signs that this could be a negative response. Categorizing the data into the types of responses, the ‘sentiment’ of the whole community or posting (social media such as Twitter, blogs, etc.) is exposed. That said, sentiment is not easily gained, as the words by themselves do not tell the whole story (i.e., I had a Great problem with their customer service). Sentiment must interpret words in context. This requires an understanding of the meaning of the post – this interpretation often requires work using NLP functions found in such systems as IBM’s Watson. 1.3.10 Data and Text Mining Data mining is a particular kind of analysis that reveals patterns in data using various algorithms. It began as an offshoot from Machine Learning, a subfield of Artificial Intelligence. The theory is a subset of statistical analysis known as unsupervised learning where algorithms are applied to a data set without knowledge or intent of the 84 CIVDDD, the Centre for Innovation in Information and Data -Driven Design, is a research grant in big data analytics and visualization to develop next generation data discovery, design, and visualization techniques for new computational tools, representational strategies, and interfaces. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 481 desired outcome. While standard query and reporting tools ask specific questions, data mining tools help discover unknown relationships by revealing patterns. Data mining is a key activity during the exploration phase as it facilitates rapid identification of studied data elements, identifies new relationships previously unknown, unclear, or unclassified, and provides structure for the classification of studied data elements. Text mining analyzes documents with text analysis and data mining techniques to classify content automatically into workflow -guided and SME -directed ontologies. Thus, electronic text media can be analyzed without restructuring or reformatting. Ontologies can be linke d into search engines, allowing for web -enabled querying against these documents. (See Chapter 9. ) Data and text mining use a range of techniques, including: • Profiling : Profiling attempts to characterize the typical behavior of an individual, group, or population. Profiling is used to establish behavioral norms for anomaly detection applications, such as fraud detection and monitoring for intrusions to computer systems. Profile results are inputs for many unsupervised learning components. • Data reduction : Data reduction replaces a large data set with a smaller set of data that contains much of the important information in the larger set. The smaller data set may be easier to analyze or process. • Association: Association is an unsupervised learning process to find relationships between studied elements based on transactions involving them. Examples of association include: Frequent item set mining, rule discovery, and market- based analysis. Recommendation systems on the internet use this process as well. • Clustering : Clustering group elements in a study together by their shared characteristics. Customer segmentation is an example of clustering. • Self-organizing maps : Self -organizing maps are a neural network method of cluster analysis. Sometimes referred to as Kohonen M aps, or topologically ordered maps, they aim to reduce the dimensionality in the evaluation space while preserving distance and proximity relationships as much as possible, akin to multi -dimension scaling. Reducing the dimensionality is like removing one variable from the equation without violating the outcome. This makes it easier to solve and visualize. 1.3.11 Predictive Analytics Predictive Analytics is the sub -field of supervised learning where users attempt to model data elements and predict future outcomes through evaluation of probability estimates. Rooted deeply in mathematics specifically statistics, predictive analytics shares many components with unsupervised learning, with the prescribed difference to the measurement of a desired predictive outcome. Predictive Analytics is the development of probability models based on variables, including historical data, related to possible events (purchases, changes in price, etc.). When it receives other pieces of information, the model triggers a reaction by the organization. The triggering factor may be an event, such as a customer adding a product to an on -line shopping basket, or it may be data in a data stream, such as a news feed or utility sensor Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "482 • DMBOK2 data, or an increased volume of service requests. The triggering factor may be an external event. News being reported about a company is a big predictor of a change in stock price. Predicting stock movement should include monitoring news and determining if news about a company is likely to be good or bad for the stock price. Frequently, the triggering factor is the accumulation of a large volume of real -time data, such as an extremely high number of trades or requests for service or volatility of the environment. Monitoring a data event stream includes incrementally building on the populated models until a threshold is reached as defined in the model. The amount of time that a predictive model provides between the prediction and event predicted is frequently very small (seconds or less than a second). Investment in very low latency technology solutions, such as in-memory databases, high -speed networks, and even physically proximity to the source of the data, optimizes an organization’s ability to react to the prediction. The simplest form of predictive model is the forecast. Many techniques exist for trending or forecasting based on regression analysis and benefit from smoothing. The simplest way to smooth data is through a moving average, or even a weighted moving average . More advanced techniques can be useful, like the exponential moving average, which introduces a smoothing factor to be applied. Minimizing the error residual from the least squares can be a starting point, but several runs are necessary to determine and optimize the smoothing factor. Double and triple exponential smoothing models exist to address trend and seasonality components. 1.3.12 Prescriptive Analytics Prescriptive analytics take predictive analytics a step farther to define actions that will affect outcomes, rather than just predicting the outcomes from actions that have occurred. Prescriptive analytics anticipates what will happen, when it will happen, and implies why it will ha ppen. Because prescriptive analytics can show the implications of various decisions, it can suggest how to take advantage of an opportunity or avoid a risk. Prescriptive analytics can continually take in new data to re -predict and re -prescribe. This process can improve prediction accuracy and result in better prescriptions. 1.3.13 Unstructured Data Analytics Unstructured data analytics combines text mining, association, clustering, and other unsupervised learning techniques to codify large data sets. Supervised learning techniques can also be applied to provide orientation, oversight, and guidance in the coding process leveraging human intervention to resolve ambiguity when necessary. Unstructured data analytics is becoming more important as more unstructured data is generated. Some analysis is impossible without the ability to incorporate unstructured data into analytical models. Howeve r, unstructured data is difficult to analyze without some way to isolate the elements of interest from extraneous elements. Scanning and tagging is one way to add ‘hooks’ to unstructured data that allow filtering and linking to related structured data. However, knowing what tags to generate based on what conditions is difficult. It is an iterative process, from when proposed ta g conditions are identified, tags are assigned as data is ingested, then analytics Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 483 uses those tags to validate the tag condition, and analyze the tagged data, which then leads to potentially changed tag conditions, or more tags. 1.3.14 Operational Analytics The concept of operational analytics (also known as operational BI or streaming analytics) has emerged from the integration of real -time analytics into operations. Operational analytics includes activities like user segmentation, sentiment analysis, geocoding, and other techniques applied to data sets for marketing campaign analysis, sales penetration, product adoption, asset optimization, and risk management. Operational analytics involves tracking and integrating real -time streams of information, deriving conclusions based on predictive models of behavior, and triggering automatic responses and alerts. Designing the model, triggers, and responses required for successful analysis takes more analysis on the data itself. An operational analytics solution includes the preparation of historical data for pre -population of the models of behavior. For example, in a retail product model, populating a shopping basket ana lysis that identifies products often purchased together. In predicting behavior of financial markets, historical price information and historical price rate of change are regularly used. Pre- population calculations are usually performed in advance to enable timely responses to triggering events. Once predictive models have been determined to be both useful and cost effective, solutions which integrate historical and current data (including real -time and streaming data, structured and unstructured) are implemented to populate the predictive models and trigger actions based on the predictions. The solution must ensure real -time data streams using the model rules are processed correctly and automated responses to meaningful events in the data are generated correctly. 1.3.15 Data Visualization85 Visualization is the process of interpreting concepts, ideas, and facts by using pictures or graphical representations. Data visualization facilitates understanding of the underlying data by presenting it in a visual summary, such as a chart or graph. Data visualizatio ns condense and encapsulate characteristics da ta, making them easier to see. In doing so, they can surface opportunities, identify risks, or highlight messages. Data visualizations can be delivered in a static format, such as a published report, or a more interactive on -line format; and some support end -user interaction where drilling or filtering capabilities facilitate analysis of data within the visualization. Others allow the visualization to be changed by the user on demand through innovative displays, such as data maps and moving landscapes of data over time. Visualization has long been critical to data analysis. Traditional BI tools include visualization options such as tables, pie charts, lines charts, area charts, bar charts, histograms, and turnkey boxes (candlesticks). To meet 85 Data visualization is an evolving field. Principles applied in data visualization are based on design principles. See Tufte, 2001 and McCandless 2012. Numerous web -based resources exist with examples and counter examples. See the Periodic Table of Visuali zation Methods at Visual Literacy.Org http://bit.ly/IX1bvI . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "484 • DMBOK2 the growing need to understand data, the number of visualization tools has increased and techniques have improved. As data analytics matures, visualizing data in new ways will offer strategic advantages. Seeing new patterns in data can result in new business opportunities. As data visualization continues to evolve, organizations will have to grow their Business Intelli gence teams to compete in an increasingly data -driven world. Business analytical departments will seek data experts with visualization skills, including data scientists, data artists, and data vision experts, in addition to traditional information architec ts and data modelers, especially given the risks associated with misleading visualization. (See Chapter 2. ) 1.3.16 Data Mashups Mashups combine data and services to create visualization for insight or analysis. Many virtualization tools enable mashups through functionality that relates data sources by common data elements, originally used to relate a name or descriptive text to a stored code. This client presentation mashup technique is ideal during discovery or exploration phases as they provide immediate benefits. This technique can be readily applied to the web where secured data mashups enable sharing of personal or confidential information across suppliers or providers. These can couple with artificial intelligence learning algorithms to expose internet- based services with natural language interfaces. 2. Activities 2.1 Define Big Data Strategy and Business Needs An organization’s Big Data strategy needs to be aligned with and support its overall business strategy and business requirements and be part of its data strategy. A Big Data strategy must include criteria to evaluate: • What problems the organization is trying to solve. What it needs analytics for : While one advantage of Data Science is that it can provide a new perspective on an organization, the organization still needs to have a starting point. An organization may determine that the data is to be used to understand the business or the business environment; to prove ideas about the value of new products; to explore something that is unknown; or to invent a new way to do business. It is important to establish a gating process to evaluate these initiatives at several phases during the implementation. The value and feasibility of initiatives need to be evaluated at several points in time. • What data sources to use or acquire: Internal sources may be easy to use, but may also be limited in scope. External sources may be useful, but are outside operational control (managed by others, or not controlled by anyone, as in the case of social media). Many vendors are competing in this space and often multiple sources exist for the desired data elements or sets. Acquiring data that integrates with existing ingestion items can reduce overall investment costs. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 485 • The timeliness and scope of the data to provision: Many elements can be provided in real -time feeds, snapshots at a point in time, or even integrated and summarized. Low latency data is ideal, but often comes at the expense of machine learning capabilities – there is a huge difference between computational algorithms directed to data- at-rest versus streaming. Do not minimize the level of integration required for downstream usage. • The impact on and relation to other data structures : There may need to be structure or content changes in other data structures to make them suitable for integration with Big Data sets. • Influences to existing modeled data : Including extending the knowledge on customers, products, and marketing approaches . The strategy will drive the scope and timing of an organization’s Big Data capability roadmap. 2.2 Choose Data Sources As with any development project, the choice of data sources for Data Science work must be driven by the problems the organization is trying to solve. The difference with Big Data / Data Science development is that the range of data sources is wider. It is not limited by format and can include data both external to and internal to an organization. The ability to incorporate this data into a solution also comes with risks. The quality and reliability of the data needs to be evaluated and a plan for use over time needs to be put into place. Big Data environments make it possible to quickly ingest lots of data, but to use that data and manage it over time, it is still necessary to know basic facts: • Its origin • Its format • What the data elements represent • How it connects to other data • How frequently it will be updated As more data becomes available (like US Census Bureau Statistics, shopping demographics, weather satellite data, research data sets), data needs to be evaluated for worth and reliability. Review the available data sources, and the processes that create tho se sources and manage the plan for new sources. • Foundational data : Consider foundational data components such as POS (Point of Sale) in a sales analysis. • Granularity : Ideally, obtain data in its most granular form (not aggregated). That way it can be aggregated for a range of purposes. • Consistency : If possible, select data that will appear appropriately and consistently across visualizations, or recognize limitations. • Reliability : Choose data sources that are meaningful and credible over time. Use trusted, authoritative sources . • Inspect/profile new sources : Test changes before adding new data sets. Unexpected material or significant changes in visualization outcomes can occur with the inclusion of new data sources. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "486 • DMBOK2 Risks associated with data sources include privacy concerns. The ability to rapidly ingest and integrate data from a variety of sources at scale affords communities the ability to recombine data sets that were otherwise secured. Similarly, the published analysis may describe, through summary, aggregate, or modeled state, a sub - set of the public that make it suddenly identifiable; this is a side effect of the ability to perform mass computation on very large populations , but publish to a very specific local or region. For example, when demographics computed at a national or country level quickly become non -identifiable, but not when published after filtering for a postal code or household level. 86 Criteria used to select or filter data also pose a risk. These criteria should be objectively managed to avoid biases or skews. Filtering can have a material impact on visualization. Discretion is necessary when removing outliers, restricting data sets to a limited domain, or dropping sparse elements. It is common practice to focus the provisioned data to emphasize isolation results, but it must be done objectively and uniformly. 87 (See Chapter 2.) 2.3 Acquire and Ingest Data Sources Once the sources are identified, they need to be found, sometimes purchased, and ingested (loaded) into the Big Data environment. During this process, capture critical Metadata about the source, such as its origin, size, currency, and additional knowledge about content. Many ingestion engines profile data as it is ingested, providing analysts with at least partial Metadata. Once the data is in a data lake, it can be assessed for suitability for multiple analysis efforts. Because building Data Science models is an iterative process, so is data ingestion. Iteratively identify gaps in the current data asset base and onboard those sources. Explore these data sources using profiling, visualization, mining, or other Data Science methods to define model algorithm inputs, or model hypotheses. Before integrating the data, assess its quality. Assessment can be as simple querying to find out how many fields contain null values, or as complex as running a Data Quality toolset or data analytic utility against the data to profile, classify, and identify relationships between data elements. Such assessment provides insight into whether the data provides a valid sample from which to work, and, if so, how the data can be stored and accessed (scattered across logical processing units [MPP], federated, distributed by key, etc.). This work involves SMEs (usually the data scientists themselves) and platform engineers. The assessment process provides valuable insight into how the data can be integrated with other data sets, such as Master Data or historical warehouse data. It also provides information that can be used in model training sets and validation activities. 86 See Martin Fowler, Datensparsamkeit . Blog, 12 December 2013. Fowler brings into question the assumption that we should always capture as much data as possible. He points out that the “capture it all” approach brings up privacy risks. In its place, he puts forth the idea of data minimization or data sparsity (from the German term Datensparsamkeit) http://bit.ly/1f9Nq8K . 87 For more information on the impact of bias, which can profoundly affect the interpretation of scientific results, consult the following websites: INFORMS is the leading international association for Operations Research and Analytics professionals. http://bit.ly/2sANQRW , Statistical Society of Canada: http://bit.ly/2oz2o5H and American Statistical Association: http://bit.ly/1rjAmHX . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 487 2.4 Develop Data Hypotheses and Methods Data Science is about building answer sets that can find meaning or insights within the data. The development of Data Science solutions entails building statistical models that find correlations and trends within and between data elements and data sets. There will be multiple answers to a question based upon inputs to a model. For example, one must choose a rate of return to calculate the future value of a financial portfolio. Models often have more than one variable , so the best practice is to find deterministic outco mes – or, in other words, use best guesses as to the values to be expected. However, best guesses themselves should be educated. Each model will operate depending on the analysis method chosen. It should be tested for a range of outcomes, even the ones that appear least probable. Models depend on both the quality of input data and the soundness of the model itself. Data models can often give insight into how to correlate the information found. An example of this is using K -Means clustering to determine the number of groupings of data to analyze further. (See Chapter 13.) 2.5 Integrate / Align Data for Analysis Preparing the data for analysis involves understanding what is in the data, finding links between data from the various sources, and aligning common data for use. In many cases, joining data sources is more an art than a science. For example, consider one data set based upon daily updates and another based upon monthly updates. The daily data, in order to be aligned, would have to be aggregated so that there would be an alignment pattern that could be used in the Data Science investigation. One method is to use a common model that integrates the data using a common key. Another way is to scan and join data using indexes within the database engines for similarity and record linkage algorithms and methods. Often data is inspected during the initial phases to understand how the data could be analyzed. Clustering helps determine the grouping of the data outputs. Other methods can find correlations that w ill be used to build the model to display results. Consider using techniques during the initial phases that will aid in understanding how the model will show results once published. Most solutions require the integration of Master Data and Reference Data in order to interpret results of the analytics. (See Chapter 10.) 2.6 Explore Data Using Models 2.6.1 Populate Predictive Model Configuring predictive models includes pre -populating the model with historical information concerning the customer, market, products, or other factors that are included in the model other than the triggering factor. Pre-population calculations are usually performed in advance to enab le the fastest response to triggering events. For example, customer purchase history would be needed to pre- populate a retail market -basket Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "488 • DMBOK2 recommendation model. In predicting behavior of retail markets, historical price and price change information are combined with customer, demographic, and weather information. 2.6.2 Train the Model Execute the model against the data in order to ‘train’ the model. Training includes repeated runs of the model against the data to verify assumptions. Training will result in changes to the model. Training requires balance. Avoid over -fitting by training against a limited data fold. Model validation must be complete before transitioning to production. Address any population imbalances or data biases with model offsets that are trained and validated; this can be tweaked in production as the initial offset is gradually adjusted through actual population interactions. Optimizing feature mix can be accomplished with Bayesian co -selection, classifier inversion, or rule induction. Models can also be combined for ensemble learning where the predictor model is built by combining the collected strengths of simpler models. Identifying outliers or anomalies (data objects that do not comply with the general behavior exhibited by the studied elements) is critical to evaluating the model. For more volatile datasets, apply a variance test based on the average and standard deviation. Both tests can be readily applied to profiled results. It may be that the outliers are the target of the exercise, as opposed to finding and validating trends in the majority of the data. For predictive analytics , use a real -time data stream to finish the population of the predictive model and trigger a response, which might be an alert or an event. The data stream may require special focus on design and development of an extreme low latency processing capability. In some models, the difference in value of the predictions between fractions of a second is extreme and solutions may require innovative technology with speed of light limitations. Models can use many statistical functions and techniques that are available in open source libraries, one of which is ‘R.’ The R Project for Statistical Computing is a free software environment for statistical computing; it contains many functions as service calls. 88 Custom functions can be developed leveraging the scripting language and shared across tools, platforms, and organizations. Once the solution design has been created and development and operation estimated, the organization may decide whether to develop the solution to predict behavior. Real -time operational analytics solutions frequently require substantial amounts of new architecture and development and could possibly not be cost effective. 2.6.3 Evaluate Model Once the data is placed onto a platform and ready for analysis, the Data Science begins. The model is constructed, evaluated against training sets, and validated. Refinements to the business requirements are 88 For more information, visit the R -Project website: http://bit.ly/19WExR5 . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 489 expected at this point and early feasibility metrics can guide the management efforts towards further processing or discarding. It is entirely possible that testing a new hypothesis will require additional data sets. Data scientists run queries and algorithms against the data to see if any insights become apparent. Often times a number of different mathematical functions will be run to see if any insight is found (clusters in the data, patterns that start to emerge between data element periods, etc.). During this period, data scientists are often building upon insights found in iterative batches. From these, models can be developed that display the correlation between data elements and insights. There is an ethical component to practicing Data Science and it needs to be applied when evaluating models. Models can have unexpected results or unintentionally reflect the assumptions and biases of the people who create them. Ethical training should be r equired for all artificial intelligence (AI) practitioners. Ideally, the curriculum for every student learning AI, computer science, or Data Science should include ethics and security topics. However, ethics alone is not sufficient. Ethics can help practitioners understand their responsibilities to all stakeholders, but ethical training needs to be augmented with the technical capability to put good intentions into practice by taking technical precautions as a system is built and tested (Executive Office, 2 016). (See Chapter 2.) 2.6.4 Create Data Visualizations Data visualization based on the model must meet the specific needs related to the purpose of the model. Each visualization should answer a question or provide an insight. Establish the purpose and parameters for the visualization: a point in time status, trends vs. exceptio ns, relationships between moving parts, geographical differences, or some other point. Select the appropriate visual to fulfill that purpose. Ensure that the visualization addresses an audience; adjust the layout and complexity to highlight and simplify accordingly. Not all audiences are ready for a complex interactive chart. Support visualizations with explanatory text. Visualizations should tell a story. Data ‘story telling’ can link new questions to the context of data exploration. Data stories must be supported by related data visualizations to have the best effect. 2.7 Deploy and Monitor A model that meets business needs in a feasible manner can be deployed to production for ongoing monitoring. Such models will require refinement and maintenance. Several modeling techniques are available for implementation. Models can serve batch processes as well as real- time integration messages. They can also be embedded into analytics software as input into decision management systems, historical analysis, or performance management dashboards. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "490 • DMBOK2 2.7.1 Expose Insights and Findings The presentation of findings and data insights, usually through data visualization, is the final step in a Data Science investigation. Insights should be connected to action items so that the organization benefits from the Data Science work. New relationships may be explored through data visualization techniques. As a model is used, changes in the underlying data and relationships may surface, telling a new story about the data. 2.7.2 Iterate with Additional Data Sources The presentation of findings and data insights usually generates questions that start a new process of research. Data Science is iterative, so Big Data development is iterative to support it. This process of learning from a specific set of data sources often leads to the need for different or additiona l data sources to both support the conclusions found and to add insights to the existing model(s). 3. Tools Advances in technology (Moore’s Law, the proliferation of hand held devices, and IoT, to name a few) have created the Big Data and Data Science industry. To understand the industry, one must understand its drivers. This section will explain the tools and technologies that have enabled Big Data Science to emerge. The advent of Massively Parallel Processing (MPP) was one of the first enablers to Big Data and Data Science as it provided the means to analyze huge volumes of information in relatively short amounts of time. Finding the needle in the haystack of information, or using machinery to plow through tons of dirt to find the gold nuggets is what we are doing today. Th is trend will continue. Other advances that have changed the way we look at data and information include: • Advanced in -database analytics • Analytics on unstructured data (Hadoop, MapReduce) • Integration of analytic results with operational systems • Data visualizations across multiple media and devices • Linking structured and unstructured information using semantics • New data sources using IOT • Advanced visualization capabilities • Data enrichment capabilities • Collaboration technologies and toolsets Existing data warehouses, data marts, and operational data stores (ODS) are being augmented to carry Big Data workload. NoSQL technologies allow storage and query of unstructured and semi -structured data. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 491 Access to unstructured data used to occur largely through a batch query interface that resulted in slow scheduled execution and poor response times. Several NoSQL databases are now available with designs that address specific limitations in this acquisition process. Scalable distributed databases automatically provide sharding capabilities (the ability to scale across servers natively) for parallel query execution. Of course, as with any other database, structural definition and mapping to unstructured data sets remain largely manual processes. Immediate query, reporting, and analysis capabilities can be satisfied with Big Data in -memory technologies that allow end users to construct SQL -like queries to access unstructured data. There are also adaptors to SQL for some tools that will transmit a N oSQL process and return a SQL compliant query – with limitations and caveats. Adaptor technologies can allow existing tools to be used for unstructured data query. Decision criteria tool sets, process implementation tools, and professional services offerings can both facilitate and expedite the process of choosing an initial set of tools. As when acquiring BI tools, it is critical to evaluate all options: build, buy, or rent (provisioned as software -as-a-service). As noted in Chapter 11, cloud sourcing tools and the associated expertise should be weighed against the cost of building from scratch or deploying purchased products from vendors. Ongoing upgrade and potential replacement costs must be considered as well. Alignment to a set OLA can bridge forecasted costs and provide input into setting compelling fees and penalties for term violations. 3.1 MPP Shared -nothing Technologies and Architecture Massively Parallel Processing (MPP) Shared -nothing Database technologies have become the standard platform for Data Science -oriented analysis of Big Data sets. In MPP databases, data is partitioned (logically distributed) across multiple processing servers (computational nodes), with each server having its own dedicated memory to process data locally. Communication between processing servers is usually controlled by a master host and occurs over a network interconnect. There is no disk sharing or memory contention, hence the name, ‘shared - nothing’. MPP has evolved because traditional computing paradigms (indexes, distributed data sets, etc.) did not provide acceptable response times on massive tables. Even the most powerful of computing platforms (Cray computer) would take many hours or even days to compute a complex algorithm against a trillion -row table. Consider now a number of commodity hardware servers, all lined up in a row and controlled via a host. Each is sent part of the query to run against this segmented or distributed trillion -row table. If there are, for example, 1000 processing servers, the query changes from accessing a trillion rows in one table to accessing 1000 billi on-row tables. This type of computing architecture is linearly scalable, which adds to the appeal for data scientist and Big Data users requiring a scalable platform to incorporate growth. This technology also enabled in-database analytical functions – the ability to execute analytical functions (like K-means Clustering, Regression, etc.) at the processor level. Distribution of workload to the processor level greatly speeds up analytical que ries – thereby fueling innovation in Data Science. A system that automatically distributes data and parallelizes query workloads across all available (localized) hardware is the optimum solution for Big Data analytics. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "492 • DMBOK2 Figure 102 Columnar Appliance Architecture89 Data volumes are growing fast. Companies can grow the capacity and performance of their systems over time by adding new nodes. MPP makes it easy to expand the parallelism of hundreds or thousands of cores across an ever -growing pool of machines. A massively parallel, shared -nothing architecture fully uses each core, with linear scalability and increased processing performance on large data sets. 3.2 Distributed File -based Databases Distributed file -based solutions technologies, such as the open source Hadoop , are an inexpensive way to store large amounts of data in different formats. Hadoop stores files of any type – structured, semi- structured, and unstructured. Using a configuration similar to MPP Shared -nothing (an MPP foundation for file storage), it shares files across processing servers. It is ideal for storing data securely (as many copies are made), but has challenges when trying to allow access to data via structured or analytical mechanism (like SQL). Due to its relatively low cost, Hadoop has become the landing zone of choice for many organizations. From Hadoop, data can be moved to MPP Shared -nothing databases to have algorithms run against it. Some organizations run complex Data Science queries in Hadoop, and are not concerned with response times in the order of hours and days (rather than minutes for the former architecture). The language used in file- based solutions is called MapReduce. This language has three main steps: • Map : Identify and obtain the data to be analyzed • Shuffle : Combine the data according to the analytical patterns desired 89 Image Source: “Greenplum Database 4.0: Critical Mass Innovation”, White Paper, August 2010 . SQL MapReduce Master Servers Interconnect Bus Segment Servers External Sources Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 493 • Reduce : Remove duplication or perform aggregation in order to reduce the size of the resulting data set to only what is required These steps can be combined in many different tools in different ways, both in sequence and in parallel, to do complex manipulations. 3.3 In-database Algorithms An in -database algorithm uses the principle that each of the processors in a MPP Shared -nothing platform can run queries independently, so a new form of analytics processing could be accomplished by providing mathematical and statistical functions at the computing node level. Open -source libraries of scalable in- database algorithms for machine learning, statistics, and other analytical tasks were designed both for in - and out-of-core execution, and for the shared -nothing parallelism offered by modern parallel database engines, ensuring that computation is done close to the data. By moving the computation closer to the data, the computing time is dramatically reduced for complex algorithms (such as K -means Clustering, Logistic or Linear regression, Mann- Whitney U Test, Conjugate Gradient, Cohort Analysis, etc.). 3.4 Big Data Cloud Solutions There are vendors who provide cloud storage and integration for Big Data, including analytic capabilities. Based on defined standards, customers load their data to a cloud environment. The vendor enhances the data, either as open data sets or as provided by other organizations. The customer can do analytics and Data Science using the combined data set. One application uses retail offers as the subject for the data, c ombines it with geographic and sales data, and offers airline miles for customers who agree to have their data used in this way. 3.5 Statistical Computing and Graphical Languages R is an open source scripting language and environment for statistical computing and graphics. It provides a wide variety of statistical techniques , such as linear and nonlinear modeling, classical statistical tests, time - series analysis, classification, and clustering. Because it is a scripting language, models developed in R can be implemented in a variety of environments, differing platforms and collaborated development across multiple geographic and organizational boundaries. The R environment can also produce publication -quality plots, including mathematical symbols and formulae, within the control of the end user. 3.6 Data Visualization Tools Traditional tools in data visualization have both a data and a graphical component. Advanced visualization and discovery tools use in -memory architecture to allow users to interact with the data. Patterns in a large data set can be difficult to recognize in a numbers display. A visual pattern can be picked up quickly when thousands of data points are loaded into a sophisticated display. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "494 • DMBOK2 Information graphics or infographics are graphical representations stylized for effective interaction and comprehension. Marketing adopted these to provide visual appeal to presentations. Journalists, bloggers, and teachers found infographics useful for tr end analysis, presentation, and distribution. Information visualization methods like radar charts, parallel coordinate plots, tag charts, heat maps, and data maps are now supported by many toolsets. These allow users to rapidly discern changes in data over time, gain insights into related items, and understand potential cause and effect relationships before impacts occur. These tools have several benefits over traditional visualization tools: • Sophisticated analysis and visualization types, such as small multiples, spark lines, heat maps, histograms, wate rfall charts, and bullet graphs • Built-in adherence to visualization best practices • Interactivity enabling visual discovery 4. Techniques 4.1 Analytic Modeling Several open source tools are available for development, as well as cloud data processing for model development, for visual development process, for web scraping, and for linear programming optimization. To share and execute models by other applications, look for tools that support the predictive model markup language (PMML) , an XML -based file format. Real -time access can resolve many latency issues from batch processing. The Apache Mahout is an open source project aimed at creating a machine -learning library. Mahout is positioned to automate Big Data exploration through recommendation mining, document classification, and item clustering. This branch of development efforts bypasses the traditional batch query MapReduce data access techniques. Leveraging an API interface directly into the storage layer HDFS , a variety of data access techniques can be provided such as SQL, content streaming, machine learning, and graphics libraries for data visualization. Analytic models are associated wi th different depths of analysis: • Descriptive modeling summarizes or represents the data structures in a compact manner. This approach does not always validate a causal hypothesis or predict outcomes. However, it does use algorithms to define or refine relationships across variables in a way that could provid e input to such analysis. • Explanatory modeling is the application of statistical models to data for testing causal hypothesis about theoretical constructs. While it uses techniques similar to data mining and predictive analytics, its purpose is different. It does not predict outcomes; it seeks to matc h model results only with existing data. Key to predictive analytics is to learn by example through training the model. Performance of a learning method relates its predictive abilities on independent test data. Assessment guides the choice of learning and measures Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 495 the quality of the chosen model. Model selection estimates performance where assessment evaluates the generalization error on new data. Avoid over -fitting – a situation that occurs when the model is trained against non -representative datasets, is overly complex in relation to its data, or has described noise instead of the underlying relationship(s). Use additional techniques such as K-fol d validation to indicate when training is no longer resulting in better generalization. Train ing error consistently decrease with model complexity and can drop off to zero. Therefore, it is not a useful estimate of the test error. Randomly divide the data set into three parts to form training, testing, and validation sets. The training set is used to fit the model, the validation set is used to predict error for selection, and the test set is used for assessment of the generalization error of the final model. Reusing the same test-set repeatedly can underestimate the true test error. Ideally, perform cross- validation by randomly dividing the data set into a set of K -folds or cross- validation groups. Perform training on all but one set of data based on strongly correlated predictor variables. Test the model on the remaining piece and determine generalization error based on all K -folds. Several statistical tests can be applied and performed to numerical ly assess contextual model validity. 4.2 Big Data Modeling Modeling Big Data is a technical challenge but critical if an organization that wants to describe and govern its data. Traditional Enterprise Data Architecture principles do apply; data needs to be integrated, specified, and managed. The main driver to physically model a data warehouse is to enable population of data for query performance. This driver is not in play for Big Data. This is not an excuse to abandon the modeling process or to hand it off to a developer. The value of modeli ng the data is that it enables people to understand data content. Apply proven data mode ling techniques while accounting for the variety of sources. Develop the subject area model, at least in a summarized way, so it can be related to proper contextual entities and placed into the overall roadmap, just like any other kind of data. The challenge is to make an understandable and useful picture out of these large data sets, and for a justifiable cost. Understand how the data links between data sets. For data o f different granularity, prevent combinations that count data elements or values more than once; for example, don’t combine atomic and aggregate sets. 5. Implementation Guidelines Many of the general principles of managing warehouse data apply to managing Big Data: ensuring that the data sources are reliable, having sufficient Metadata to enable data use, managing the quality of data, figuring out how to integrate data from differen t sources, and ensuring that data is secure and protected. (See Chapters 6, 7, and 8. ) The differences in implementing a Big Data environment are connected to a set of unknowns: how the data will be used, which data will be valuable, how long it needs to b e retained. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "496 • DMBOK2 Data velocity may lead people to think they do not have time to implement controls. This is a dangerous assumption. With larger data sets, managing ingestion and inventorying data in a lake is critical to preventing it from becoming a swamp. Ingestion may not always require organizational ownership or commitment to the data set being studied. Consider leasing a Big Data platform for finite periods to explore data of interest. Exploration can quickly determine which areas show potential value. Do this before ingesting into the organizational data lake, data store, or data staging area; once landed, it can be awkward to remove. 5.1 Strategy Alignment Any Big Data / Data Science program should be strategically aligned with organizational objectives. Establishing a Big Data strategy drives activities related to user community, data security, Metadata management, including lineage, and Data Quality Manage ment. The strategy should document goals, approach, and governance principles. The ability to leverage Big Data requires building organizational skills and capabilities. Use capability management to align business and IT initiatives and project a roadmap. Strate gy deliverables should account for managing: • Information lifecycle • Metadata • Data Quality • Data acquisition • Data access and security • Data Governance • Data privacy • Learning and adoption • Operations 5.2 Readiness Assessment / Risk Assessment As with any development project, implementation of a Big Data or Data Science initiative should align with real business needs. Assess organizational readiness in relation to critical success factors: • Business relevance: How well do the Big Data / Data Science initiatives and their corresponding use cases align with the company’s business? To succeed, they must strongly enforce a business function or process. • Business readiness: Is the business partner prepared for a long -term incremental delivery? Have they committed themselves to establishing centers of excellence to sustain the product in future releases? How broad is the average knowledge or skill gap within the target communi ty and can that be crossed within a single increment? Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 497 • Economic viability : Has the proposed solution considered conservatively the tangible and intangible benefits? Has assessment of ownership costs accounted for the option of buying or leasing items versus building from scratch? • Prototype : Can the proposed solution be prototyped for a subset of the end user community for a finite timeframe to demonstrate proposed value? Big bang implementations can cause big dollar impacts and a proving ground can mitigate these delivery risks. Likely the most challenging decisions will be around data procurement, platform development, and resourcing. • Many sources exist for digital data stores and not all need to be in -house owned and operated. Some can be procured while others can be leased. • Multiple tools and techniques are on the market; matching to general needs will be a challenge. • Securing staff with specific skills in a timely manner and retaining top talent during an implementation may require consideration of alternatives including professional services, cloud sourcing or collaborating. • The time to build in -house talent may well exceed the delivery window. 5.3 Organization and Cultural Change Business people must be fully engaged in order to realize benefits from the advanced analytics. A communications and education program is required to affect this. A Center of Excellence can provide training, start -up sets, design best practices, data source tips and tricks, and other point solutions or artifacts to help empower business users towards a self -service model. In addition to knowledge management, this center can provide timel y communications across the developer, designer, analyst, and data consumer communities. As with DW/BI, a Big Data implementation will bring together of a number of key cross -functional roles, including: • Big Data Platform Architect : Hardware, operating systems, filesystem s, and services . • Ingestion Architect : Data analysis, systems of record, data modeling, and data mapping. Provides or supports mapping of sources to the Hadoop cluster for query and analysis. • Metadata Specialist : Metadata interfaces, Metadata A rchitecture, and contents. • Analytic Design Lead : End user analytic design, best practice guidance implementation in related toolsets, and end user result set facilitation. • Data Scientist : Provides architecture and model design consultation based on theoretical knowledge of statistics and computability, delivery o f appropriate tools , and technical application to functional requirements. 6. Big Data and Data Science Governance Big Data, like other data, requires governance. Sourcing, source analysis, ingestion, enrichment, and publishing processes require business as well as technical controls, addressing such questions as: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "498 • DMBOK2 • Sourcing : What to source, when to source, what is the best source of data for particular study • Sharing : What data sharing agreements and contracts to enter into, terms and conditions both inside and outside the organization • Metadata : What the data means on the source side, how to interpret the results on the output side • Enrichment : Whether to enrich the data, how to enrich the data, and the benefits of enriching the data • Access : What to publish, to whom, how , and when An enterprise view of data should drive decisions on data handling. 6.1 Visualization Channels Management A critical success factor in implementing a Data Science approach is the alignment of the appropriate visualization tools to the user community. Depending on the size and nature of the organization, there are likely many different visualization tools being applied in a variety of processes. Ensure that users understand the relative complexity of the visualization tools. Sophisticated users will have increasingly complex demands. Coordination between enterprise architecture, portfolio management, and mainten ance teams will be necessary to control visualization channels within and across the portfolio. Be aware that changing data providers or selection criteria will likely have downstream impacts to the elements available for visualization, which can impact th e effectiveness of tools. 6.2 Data Science and Visualization Standards It is a best practice to establish a community that defines and publishes visualization standards and guidelines and reviews artifacts within a specified delivery method; this is particularly vital for customer - and regulatory - facing content. Standards may include: • Tools standards by analytic paradigm, user community, subject area • Requests for new data • Data set process standard • Processes for neutral and expert presentation to avoid biased results, and to ensure that all elements included have been done so in a fair and consistent manner including: • Data inclusion and exclusion • Assumptions in the models • Statistical validity of results • Validity of interpretation of results • Appropriate methods applied Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 499 6.3 Data Security Having a reliable process to secure data is itself an organizational asset. Policies for hand ling and securing Big Data should be established and monitored. These policies should account for how to prevent misuse of personal data and to secure it through its overall lifecycle. Securely provision appropriate levels of data for authorized personnel and make subscription data accessible according to agreed -upon levels. Align services to user communities so that special services can be created to provision private data for those communities allowed to ingest it, and mask the data for others. Often organizations create policies for access to information that are not to be violated (such as no access by name, address, or phone number). In order to secure information that is highly sensitive (social security number , credit card numbers , etc.), data will be stored using encryption techniques that obfuscate the information. Encryption can be chosen that, for example, has the same ‘content’ when encrypted, so that patterns may be exposed without knowing the actual values. Recombination measures the ability to reconstitute sensitive or private data. This capability must be managed as part of the Big Data security practice. The outcomes of the analysis may violate privacy, even though the actual data elements can only be inferred. Understanding the outcomes at the Metadata Management level is critical to avoid this and other potential security viol ations. This requires knowledge of the intended consumption or analysis to be performed and by what role. Some trusted persons within the organization will be granted the ability to read this data when necessary, but not everyone, and certainly not for deep analysis. (See Chapters 2 and 7. ) 6.4 Metadata As part of a Big Data initiative, an organization will bring together data sets that were created using different approaches and standards. Integration of such data is challenging. Metadata related to these data sets is critical to their successful use. Me tadata needs to be carefully managed as part of data ingestion, or the data lake will quickly become a data swamp. The user community must have tools that enable them to create a master list of data sets with Metadata that characterizes the structure, content, and quality of the data, including the source and lineage of the data and the definition and intended uses of entities and data elements. Technical Metadata can be harvested from a variety of Big Data tools including data storage layers, data integration, MDM, and even the source filesystems. Consideration of real -time feeds versus data at rest versus computational data elements is necessary to complete the source side lineage. 6.5 Data Quality Data Quality is a measure of deviation from an expected result: the smaller the difference, the better the data meets expectation, and the higher the quality. In an engineered environment, standards for quality should be easy to define (though practice shows that they are not or that many organizations do not take the time to define them). Some people have raised the question of whether Data Quality even matters for Big Data. Common sense says it does. For analytics to be reliable, the underlying data must be reliable. In Big Data Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "500 • DMBOK2 projects, it may seem very difficult to determine the quality of data, but an effort needs to be made to assess quality in order to have confidence in the analysis. This can be done through an initial assessment, which is necessary to understand the data, and through that, the identification of measurements for subsequent instances of the data set. Data Quality assessment will produce valuable Metadata that will be necessary input to any effort to integrate data. Consider that most mature Big Data organizations scan data input sources using Data Quality toolsets to understand the information contained within. Most advanced Data Quality toolsets offer functionality that enables an organization to test assumptions and build knowledge about its data. For example: • Discovery: Where information resides within the data set • Classification: What types of information are present based upon standardized patterns • Profiling : How the data is populated and structured • Mapping : What other data sets can be matched to these values Just as in DW/BI, it is tempting to put Data Quality assessment last. Without it, though, it may be difficult to know what Big Data represents or how to make connections between data sets. Integration will be necessary, and the likelihood that data feeds will be provisioned with identical structures and ele ments is very nearly zero. This means for example, codes and other potential linking data will likely vary from data provider to data provider. Without initial assessment, such conditions will go unnotice d until an analytic need is expressed that attempts to merge or combine those providers. 6.6 Metrics Metrics are vital to any management process; they not only quantify activity, but can define the variation between what is observed and what is desired. 6.6.1 Technical Usage Metrics Many of the Big Data tools offer insightful administrator reporting capabilities that interact directly with the contents queried by the user community. Technical usage analysis looks for data hot spots (most frequently accessed data) in order to manage data distribution and preserve performance. Growth rates also feed into capacity planning. 6.6.2 Loading and Scanning Metrics Loading and scanning metrics define the ingestion rate and interaction with the user community. As each new data source is acquired, loading metrics are expected to spike and then level as that source is fully ingested. Real -time feeds may be served throug h service queries, but can also appear as scheduled extracts are processed; for these feeds, expect a constant increase in data loading. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "BIG DATA AND DATA SCIENCE • 501 The application layer(s) would likely provide the best data usage metrics from execution logs. Monitor the consumption or access through available Metadata, which can guide usage analysis by showing query execution plans that have occurred most frequently. Scanning metrics should be combined with any query processing that may occur outside of the analytical processing itself. Administrative tools should be able to provide this level of reporting, as well as overall service health. 6.6.3 Learnings and Stories In order to show value, the Big Data / Data Science program must measure tangible outcomes that justify the cost of developing solutions and managing process changes. Metrics can include quantification of benefits, cost prevention or avoidance, as well as length of time between initiation and realized benefits. Common measurements include • Counts and accuracy of models and patterns developed • Revenue realization from identified opportunities • Cost reduction from avoiding identified threats Sometimes, the outcomes of the analytics tell stories that can lead to organization re -direction, re -vitalization, and new opportunity. One measurement can be a count of new projects and initiatives generated by marketing and senior executives. 7. Works Cited / Recommended Abate, Robert, Peter Aiken and Joseph Burke. Integrating Enterprise Applications Utilizing A Services Based Architecture. John Wiley and Sons, 1997. Print. Arthur, Lisa. Big Data Marketing: Engage Your Customers More Effectively and Drive Value . Wiley, 2013. Print. Barlow, Mike. Real- Time Big Data Analytics: Emerging Architecture . O'Reilly Media, 2013. Kindle. Davenport, Thomas H. “Beyond the Black Box in analytics and Cognitive.” DataInformed (website), 27 February, 2017. http://bit.ly/2sq8uG0 Web . Davenport, Thomas H. Big Data at Work: Dispelling the Myths, Uncovering the Opportunities . Harvard Business Review Press, 2014. Print. EMC Education Services, ed. Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data . Wiley, 2015. Print. Executive Office of the President, National Science and Technology Council Committee on Technology. Preparing for the Future of Artificial Intelligence. October 2016. http://bit.ly/2j3XA4k . Inmon, W.H., and Dan Linstedt. Data Architecture: A Primer for the Data Scientist: Big Data, Data Warehouse and Data Vault . 1st Edition. Morgan Kaufmann, 2014. Jacobs, Adam. “Pathologies of Big Data.” AMCQUEU , Volume 7, Issue 6. July 6, 2009. http://bit.ly/1vOqd80 . Web Janssens, Jeroen. Data Science at the Command Line: Facing the Future with Time -Tested Tools . O'Reilly Media, 2014. Print. Kitchin, Rob. The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences . SAGE Publications Ltd, 2014. Print. Krishnan, Krish. Data Warehousing in the Age of Big Data . Morgan Kaufmann, 2013. Print. The Morgan Kaufmann Series on Business Intelligence. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "502 • DMBOK2 Lake, Peter and Robert Drake. Information Systems Management in the Big Data Era . Springer, 2015. Print. Advanced Information and Knowledge Processing. Lake, Peter. A Guide to Handling Data Using Hadoop: An exploration of Hadoop, Hive, Pig, Sqoop and Flume . Peter Lake, 2015. Kindle. Advanced Information and Knowledge Processing. Laney, Doug. “3D Data Management: Controlling Data Volume, Velocity, and Variety.” The Meta Group [Gartner]. 6 February 2001. http://gtnr.it/1bKflKH . Loshin, David. Big Data Analytics: From Strategic Planning to Enterprise Integration with Tools , Techniques, NoSQL, and Graph. Morgan Kaufmann, 2013. Print. Lublinsky, Boris, Kevin T. Smith, Alexey Yakubovich. Professional Hadoop Solutions . Wrox, 2013. Print. Luisi, James. Pragmatic Enterprise Architecture: Strategies to Transform Information Systems in the Era of Big Data . Morgan Kaufmann, 2014. Print. Marz, Nathan and James Warren. Big Data: Principles and best practices of scalable realtime data systems . Manning Publications, 2014. Print. McCandless, David. Information is Beautiful . Collins, 2012. Provost, Foster and Tom Fawcett. Data Science for Business: What you need to know about data mining and data- analytic thinking. O'Reilly Media, 2013. Print. Salminen, Joni and Valtteri Kaartemo, eds. Big Data: Definitions, Business Logics, and Best Practices to Apply in Your Business . Amazon Digital Services, Inc., 2014. Kindle. Books for Managers Book 2. Sathi, Arvind. Big Data Analytics: Disruptive Technologies for Changing the Game . Mc Press, 2013. Print. Sawant, Nitin and Himanshu Shah . Big Data Application Architecture Q &A: A Problem - Solution Approach . Apress, 2013. Print. Expert’s Voice in Big Data. Slovic, Scott, Paul Slovic, eds. Numbers and Nerves: Information, Emotion, and Meaning in a World of Data . Oregon State University Press, 2015. Print. Starbird, Michael. Meaning from Data: Statistics Made Clear (The Great Courses, Parts 1 and 2). The Teaching Company, 2006. Print. Tufte, Edward R. The Visual Display of Quantitative Information . 2nd ed. Graphics Pr., 2001. Print. van der Lans, Rick. Data Virtualization for Business Intelligence Systems: Revolutionizing Data Integration for Data Warehous es. Morgan Kaufmann, 2012. Print. The Morgan Kaufmann Series on Business Intelligence. van Rijmenam, Mark. Think Bigger: Developing a Successful Big Data Strategy for Your Business . AMACOM, 2014. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "503 CHAPTER 1 5 Data Management Maturity Assessment 1. Introduction apability Maturity Assessment (CMA) is an approach to process improvement based on a framework – a Capability Maturity Model (CMM) – that describes how characteristics of a process evolve from ad hoc to optimal. The CMA concept grew out of efforts by the U nited States Department of Defense to establish criteria through which to evaluate software contractors. In the mid -1980s, the Capability Maturity Model for Software was published by the Software Engineering Institute of Carnegie -Mellon University. While f irst applied to software development, CMMs have been developed for a range of other fields, including data management. Maturity models are defined in terms of a progression through levels that describe process characteristics. When an organization gains an understanding of process characteristics, it can evaluate its level of maturity and put in place a plan to improve its capabilities. It can also measure improvement and compare itself to competitors or partners, guided by the levels of the model. With each new level, process execution becomes more consistent, predictable, and reliable. Processes improve as they take on ch aracteristics of the levels. Progression happens in a set order. No level can be skipped. Levels commonly include: 90 • Level 0 : Absence of capability • Level 1 : Initial or Ad Hoc: Success depends on the competence of individuals • Level 2 : Repeatable: Minimum process discipline is in place • Level 3 : Defined: Standards are set and used • Level 4 : Managed: Processes are quantified and controlled • Level 5 : Optimized: Process improvement goals are quantified Within each level, criteria are described across process features. For example, a maturity model may include criteria related to how processes are executed, including the level of automation of those processes. It may focus on policies and controls, as well as process details. Such an assessment helps identify what is working well, what is not working well, and where an organization has gaps. Based on the findings, the organization can develop a roadmap to target: • High -value improvement opportunities related to processes, methods, resources, and automation • Capabilities that align with business strategy 90 Adapted from Select Business Solutions, “What is the Capability Maturity Model?” http://bit.ly/IFMJI8 (Accessed 2016- 11-10). C Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "504 • DMBOK2 • Governance processes for periodic evaluation of organizational progress based on characteristics in the model Figure 103 Context Diagram: Data Management Maturity Assessment Definition : A method for ranking practices against maturity levels for handling data used by the organization to characterize the current state of data management and opportunities for improvement Goals : 1.To evaluate the current state of critical data management activities in order to plan for improvement. 2.To support the organization's operational and strategic direction. 3.To develop a cohesive vision for data that supports an overall organizational strategy. 4.To enable an integrated plan for improvement . Activities : 1. Plan the Assessment Activities (P) 1. Define Objectives 2. Choose a Framework 3. Define Organizational Scope 4. Define Interaction Approach 5. Plan Communications 2. Perform Maturity Assessment (C) 1. Gather Information 2. Perform Assessment 3. Interpret Results (D) 1. Report Assessment Results 2. Develop Executive Briefings 4. Create Targeted Program for Improvements (P) 1. Identify Actions and Create a Roadmap 5. Re-assess Maturity (C)Inputs : •Business Strategy & Goals •Culture & Risk Tolerance •Maturity Frameworks •DAMA -DMBOK •Policies, Processes, Standards, Operating Models •BenchmarksDeliverables : •Executive Briefings •Readiness Assessment •Risk Assessment •Assessment Engagement •Ratings and Ranks •Investment and Outcomes Options •Maturity Baseline •Roadmap Suppliers : •Executives •Data Stewards •DM Executives •Subject Matter Experts •Employees •Benchmark ProviderConsumers : •Executives •Audit / Compliance •Regulators •Data Stewards •Data Governance Bodies •Organizational Change ManagementParticipants : •CDO/CIO •Business Management •DM Executives & Data Governance Bodies •Data Governance Office •Maturity Assessors •Employees T echniques : •Data Management Maturity Frameworks Selection •Using DAMA -DMBOK •Reference Existing BenchmarksT ools : •Data Management Maturity Frameworks •Communications Plan •Collaboration T ools •Knowledge Management and Metadata Repositories •Data Profiling T oolsMetrics : •DMMA Ratings and Ranks •Resource Utilization Rates •Risk Exposure •Spend Management •Inputs to DMMA •Rate of Change (P) Planning, (C) Control, (D) Development, (O) OperationsData Management Maturity Assessment Technical DriversBusiness Drivers Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 505 A Data Management Maturity Assessment (DMMA) can be used to evaluate data management overall, or it can be used to focus on a single Knowledge Area or even a single process. Whatever the focus, a DMMA can help bridge the gap between business and IT perspectives on the health and effectiveness of data management practices. A DMMA provides a common language for depicting what progress looks like across Data Management Knowledge Areas and offers a stage -based path to improvement, which can be tailored to an organization’s strategic priorities. 91 Thus, it can be used both to set and to measure organizational goals, as well as to compare one’s organization against other organizations or industry benchmarks. Before beginning any DMMA, an organization has to establish a baseline understanding of its current state capabilities, assets, goals, and priorities. A certain level of organizational maturity is required to conduct the assessment in the first place, as well as to effectively respond to the assessment results by setting targets, establishing a roadmap, and monitoring progress. 1.1 Business Drivers Organizations conduct capability maturity assessments for a number of reasons: • Regulation : Regulatory oversight requires minimum levels of maturity in data management. • Data Governance : The Data Governance function requires a maturity assessment for planning and compliance purposes. • Organizational readiness for process improvement : An organization recognizes a need to improve its practices and begins by assessing its current state. For example, it makes a commitment to manage Master Data and needs to assess its readiness to deploy MDM processes and tools. • Organizational change : An organizational change, such as a merger, presents data management challenges. A DMMA provides input for planning to meet these challenges. • New technology : Advancements in technology offer new ways to manage and use data. The organization wants to understand the likelihood of successful adoption. • Data management issues: There is need to address Data Quality issues or other data management challenges and the organization wants to baseline its current state in order to make better decisions about how to implement change. 1.2 Goals and Principles The primary goal of a data management capability assessment is to evaluate the current state of critical data management activities in order to plan for improvement. The evaluation places the organization on the maturity scale by clarifying specific strengths and weaknesses. It helps the organization identify, prioritize, and implement improvement opportunities. In meeting its primary goal, a DMMA can have a positive impact on culture. It helps: • Educate stakeholders about data management concepts, principles, and practices • Clarify stakeholder roles and responsibilities in relation to organizational data 91 http://bit.ly/1Vev9xx July 18 2015. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "506 • DMBOK2 • Highlight the need to manage data as a critical asset • Broaden recognition of data management activities across the organization • Contribute to improving the collaboration necessary for effective Data Governance Based on assessment results, an organization can enhance its Data Management function so it supports the organization’s operational and strategic direction. Typically, Data Management function s develop in organizational silos. They rarely begin with an enterprise view of the data. A DMMA can equip the organization to develop a cohesive vision that supports overall organizational strategy. A DMMA enables the organization to clarify priorities, crystalize objectives, and develop an integrated plan for improvement. 1.3 Essential Concepts 1.3.1 Assessment Levels and Characteristics CMMs usually define five or six levels of maturity, each with its own characteristics that span from non -existent or ad hoc to optimized or high performance. See Figure 104 for a sample visualization. The following is a generic summary of macro states of data management maturity. A detailed assessment would include criteria for sub - categories like strategy, policy, standards, role definition, etc. within each of the Knowledg e Areas. • Level 0: No Capability : No organized data management practices or formal enterprise processes for managing data. Very few organizations exist at a Level 0. This level is acknowledged in a maturity model for purposes of definition. • Level 1 Initial / Ad Hoc : General -purpose data management using a limited tool set, with little or no governance. Data handling is highly reliant on a few experts. Roles and responsibilities are defined within silos. Each data owner receives, generates, and sends data autonomously. Controls, if they exist, are applied inconsistently. Solutions for managing data are limited. Data Quality issues are pervasive but not addressed. Infrastructure supports are at the business unit level. Assessment criteria may include the presence of any process controls, such as logging of Data Quality issues. • Level 2 Repeatable: Emergence of consistent tools and role definition to support process execution. In Level 2, the organization begins to use centralized tools and to provide more oversight for data management. Roles are defined and processes are not dependent solely on sp ecific experts. There is organizational awareness of Data Quality issues and concepts. Concepts of Master and Reference Data begin to be recognized. Assessment criteria might include formal role definition in artifacts like job descriptions, the existence of process documentation, and the capacity to leverage tool sets. • Level 3 Defined : Emerging data management capability. Level 3 sees the introduction and institutionalization of scalable data management processes and a view of DM as an organizational enabler. Characteristics include the replication of data across an organization with some controls in place and a general increase in overall Data Quality , along with coordinated policy definition and Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 507 management. More formal process definition leads to a significant reduction in manual intervention. This, along with a centralized design process, means that process outcomes are more predictable. Figure 104 Data Management Maturity Model Example Assessment criteria might include the existence of data management policies, the use of scalable processes, and the consistency of data models and system controls. • Level 4 Managed : Institutional knowledge gained from growth in Levels 1 -3 enables the organization to predict results when approaching new projects and tasks and to begin to manage risks related to data. Data management includes performance metrics. Characteristics of Level 4 include standardized tools for data management from desktop to infrastructure, coupled with a well -formed centralized planning and governance function. Expressions of this level are a measurable increase in Data Quality and organization -wide capabilities such as end -to-end data audits. Assessment criteria might include metrics related to project success, operational metrics for systems, and Data Quality metrics. • Level 5: Optimization : When data management practices are optimized, they are highly predictable, due to process automation and technology change management. Organizations at this level of maturity focus on continuous improvement. At Level 5, tools enable a view data across pr ocesses. The proliferation of data is controlled to prevent needless duplication. Well- understood metrics are used to manage and measure Data Quality and processes. Assessment criteria might include change management artifacts and metrics on process improvement. 1.3.2 Assessment Criteria Each capability level will have specific assessment criteria related to the processes being evaluated. For example, if the maturity of the data modeling function is being evaluated, level 1 may ask whether a data modeling practice exists at all and how many systems it extends to; level 2 may ask whether an approach to Level 1 Initial / Ad Hoc•Little or no governance •Limited tool set •Roles defined within silos •Controls applied inconsistently, if at all •Data quality issues not addressed Level 2 Repeatable•Emerging governance •Introduction of a consistent tool set •Some roles and processes defined •Growing awareness of impact of data quality issues Level 3 Defined•Data viewed as an organizational enabler •Scalable processes and tools; reduction in manual processes Process outcomes, including data quality, are more predictable Level 4 Managed•Centralized planning and governance •Management of risks related to data •Data Management performance metrics •Measurable improvements in data qualityLevel 5 Optimized•Highly predictable processes •Reduced risk •Well understood metrics to manage data quality and process quality Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "508 • DMBOK2 enterprise data modeling has been defined; level 3 will ask the degree to which the approach has been implemented; level 4 will ask whether modeling standards have been effectively enforced; and level 5 will ask about processes in place to improve modeling practices. (See Chapter 5.) At any level, assessment criteria will be evaluated along a scale, such as 1 – Not started, 2 – In process, 3 – Functional, 4 – Effective, showing progress within that level and movement toward the next level. Scores can be combined or visually displayed to enable understanding of the variance between current and desired state. When assessing using a model that can be mapped to a DAMA -DMBOK Data Management Knowledge Area, criteria could be formulated based on the categories in the Context Diagram: • Activity : To what degree is the activity or process in place? Are criteria defined for effective and efficient execution? How well defined and executed is the activity? Are best practice outputs produced? • Tools : To what degree is the activity automated and supported by a common set of tools? Is tool training provided within specific roles and responsibilities? Are tools available when and where needed? Are they configured optimally to provide the most effective a nd efficient results? To what extent is long -term technology planning in place to accommodate future state capabilities? • Standards : To what degree is the activity supported by a common set of standards? How well documented are the standards? Are standards enforced and supported by governance and change management? • People and resources : To what degree is the organization staffed to carry out the activity? What specific skills, training, and knowledge are necessary to execute the activity? How well are roles and responsibilities defined? Figure 105 illustrates one way of presenting a visual summary of findings from a DMMA. For each of the capabilities (Governance, Architecture, etc.) the outer ring of the display shows the level of capability the organization has determined it needs to compete succe ssfully. The inner ring displays the level of capability as determined via the assessment. Areas where the distance between the two rings is largest represent the greatest risks to the organization. Such a report can help set priorities. It can also be used to measure progress over time. Figure 105 Example of a Data Management Maturity Assessment Visualization 012345Governance Architecture Modeling Storage & Ops Security DII D&CR&MDDW&BIMetadataDQDMM Assessment Chart Desired rank Current Rank Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 509 1.3.3 Existing DMMA Frameworks92 A data management maturity assessment framework is segmented into discrete data management topics. Framework focus and content vary depending on whether they have a general or industry- specific focus. However, most address subjects that can be mapped to DA MA-DMBOK Knowledge Areas. The examples below are intended to illustrate the range of Capability Maturity Models that have been developed in the data management space. Many vendors have developed their own models. Organizations should evaluate several model s before choosing a vendor or before developing their own framework. 1.3.3.1 CMMI Data Management Maturity Model (DMM) The CMMI (Capability Maturity Model Institute) has developed the CMMI -DMM (Data Management Maturity Model) , which provides assessment criteria for the following data management areas: • Data Management Strategy • Data Governance • Data Quality • Platform and Architecture • Data Operations • Supporting Processes Within each of these processes, the model identifies sub -processes for evaluation. For example, the Data Quality section accounts for Data Quality Strategy and Data Quality Assessment, Profiling, and Cleansing. The model also accounts for the relation between the data management areas. For example the need for stakeholder alignment and the relation between business processes and Data Quality Management. 93 1.3.3.2 EDM Council DCAM94 The Enterprise Data Management Council, an industry advocacy organization for financial services headquartered in the United States, has developed the DCAM (Data Management Capability Assessment Model). The result of a membership -driven effort to get conse nsus on data management best practices, the DCAM describes 37 capabilities and 115 sub -capabilities associated with the development of a sustainable Data Management function . Scoring focuses on the level of stakeholder engagement, formality of process, and existence of artifacts that demonstrate the achievement of capabilities. 92 For additional information and review of existing Data Management CMMs, see: Alan McSweeney, Review of Data Management Maturity Models , SlideShare.net, published 2013- 10-23. http://bit.ly/2spTCY9 . Jeff Gorball, Introduction to Data Management Maturity Models , SlideShare.net, published 2016- 08-01. McSweeney includes the DAMA -DMBOK as one of his maturity models, although the DMBOK is not structured as such. 93 http://bit.ly/ 1Vev9xx accessed 2015- 07-18. 94 http://bit.ly/2sqaSga accessed 2015- 07-18. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "510 • DMBOK2 1.3.3.3 IBM Data Governance Council Maturity Model95 The IBM Data Governance Council Maturity Model was based on input from a council of 55 organizations. Council members collaborated to define a common set of observable and desired behaviors that organizations can use to evaluate and design their own Data Governance function s. The purpose of the model is to help organizations build consistency and quality control in governance through proven business technologies, collaborative methods, and best practices. The model is organized around four key categories: • Outcomes: Data risk management and compliance, value creation • Enablers : Organizational structure and awareness, policy, stewardship • Core disciplines : Data Quality Management, information lifecycle management, information security and privacy • Supporting Disciplines : Data Architecture, classification and Metadata, audit information, logging and reporting The IBM model is presented both as a Maturity Framework and as a set of assessment questions with answers constructed to indicate maturity levels. 1.3.3.4 Stanford Data Governance Maturity Model 96 The Stanford Data Governance Maturity Model was developed for use by the University; it was not intended to be an industry standard. Even still, it serves as a solid example of a model that provides guidance and a standard of measurement. The model focuses on Data Governance , not data management, but it nevertheless provides a basis for evaluating data management overall. The model differentiates between foundational (awareness, formalization, Metadata) and project (data stewardship, Data Quality, Master Data) components. Within each, it articulates drivers for people, policies, and capabilities. It then articulates characteristics of each level of maturity. It also provides qualitative and quantitative measurements for each level. 1.3.3.5 Gartner’s Enterprise Information Management Maturity Model Gartner has published an EIM maturity model, which establishes criteria for evaluating vision, strategy, metrics, governance, roles and responsibilities, lifecycle, and infrastructure. 2. Activities Data Management Maturity Assessments require planning. To ensure practical, actionable results, allow time within the plan for preparation of materials and evaluation of results. Assessments should be conducted in a 95 https://ibm.co/2sRfBIn (accessed 2016- 12-04). 96 http://stanford.io/2sBR5bZ (accessed 2016- 12-04) and http://stanford.io/2rVPyM2 (accessed 2016- 12-04). Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 511 short, defined timeframe. The purpose of the evaluation is to expose current strengths and opportunities for improvement – not to solve problems. Evaluations are conducted by soliciting knowledge from business, data management, and information technology participants. The goal is to reach a consensus view of current state capabilities, supported by evidence. Evidence may come from examination of art ifacts (such as whether database backups exist), through interviews (verifying someone is performing system of record evaluation for re -use), or both. Assessments can and should be scaled to fit the needs of the organization. However, amend with care. Models may lose rigor or traceability to original intent if shortened or edited. Keep the integrity of the model intact when customizing. 2.1 Plan Assessment Activities Planning for an assessment includes defining the overall approach and communicating with stakeholders before and during the assessment to ensure they are engaged. The assessment itself includes collecting and evaluating inputs and communicating results, recommendations, and action plans. 2.1.1 Define Objectives Any organization that decides it should assess its data management maturity level is already engaged in the effort to improve its practices. In most cases, such an organization will have identified the drivers for the assessment. These drivers must be clar ified in the form of objectives that describe the focus and influence the scope of the assessment. The objectives for the assessment must be clearly understood by executives and the lines of business, who can help ensure alignment with the organization’s strategic direction. Assessment objectives also provide criteria by which to evaluate which assessment model to adopt, which business areas to prioritize for assessment, and who should provide direct input to the process. 2.1.2 Choose a Framework As described in Section 1.3.3, existing frameworks focus on different aspects of data management. Review these frameworks in the context of assumptions about current state and assessment objectives in order to choose one that will inform the organization in meaningful ways. Focus areas of the assessment model can be customized based on organizational focus or scope. The choice of framework influences how the assessment is conducted. The team working on it should have expertise in the model and the methodology on which it depends. 2.1.3 Define Organizational Scope Most DMM Frameworks are designed to apply to an entire enterprise. However, an enterprise -wide scope may be impractical. For a first assessment, it is usually best to define a manageable scope, such as a single business Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "512 • DMBOK2 area or program. The areas chosen represent a meaningful subset of the organization and participants should be able to influence key business processes that affect the data assets within scope. As part of a phased approach, evaluation can be repeated for o ther parts of the organization. There are trade -offs between local and enterprise assessments: • Localized assessments can go much deeper into the details. They can also be done more quickly because the scope is contained. To do a localized assessment, select a function that is highly regulated, such as financial reporting within a public company. The inputs, roles, tools, and consumers may be outside of the functions being assessed, which can complicate the scoping and execution of the assessment. Well -planned localized assessments can often be aggregated and weighted to form an enterprise assessment, since many data assets are shared. • Enterprise assessments focus on the broad and sometimes disconnected parts of an organization. An enterprise assessment may be created from localized DMMA’s or it can be a separate undertaking. For example, an organization may evaluate different functions (research and developm ent, manufacturing, and financing) based on the same criteria. The inputs, roles, tools, and consumers are typically pan -enterprise and multi-leveled. 2.1.4 Define Interaction Approach In conducting a DMMA, an organization should follow recommendations for the selected model. Information gathering activities may include workshops, interviews, surveys, and artifact reviews. Employ methods that work well within the organizational culture, minimize the time commitment from participants, and enable the assessment to be completed quickly so that actions from the assessment can be defined while the process is fresh in participants’ minds. In all cases, responses will need to be formalized by having participants rate the assessment criteria. In many cases, assessment will also include actual inspection and evaluation of artifacts and other evidence. If there are delays in completing the assessment, stakeholders are likely to lose enthusiasm for the Data Management function and the impetus for contributing to positive change. It is advisable to avoid detailed and comprehensive analysis and to emphasize sound judgment based on the expertise of the assessment leaders. The DMM Frameworks provide the measurement criteria and an embedded path to improvement. These enable synthesis of a complete picture of the current Data Management function and its parts. 2.1.5 Plan Communications Communications contribute to the overall success of the assessment and the action items coming out of it. Communication will be directed at participants and other stakeholders. Findings may impact people’s jobs, through changes in methodology and organizat ional alignment, so it is important to communicate clearly about the purpose, the process, and specific expectations for individuals and groups. Ensure participants understand the assessment model, as well as how the findings will be used. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 513 Before the assessment begins, stakeholders should be informed about expectations for the assessment. Communications should describe: • The purpose of the DMMA • How it will be conducted • What their involvement may be • The schedule of assessment activities During any activity of the assessment (for example, a focus group meeting), ensure there is a clear agenda, including a plan to answer any follow up questions. Continually remind participants of the goals and objectives. Always thank the participants and d escribe next steps. Determine if the planned approach is likely to be successful across the targeted business scope, including such factors as resistance / cooperation, possible internal legal concerns about exposure to outside inspection if troubling gaps are found, or possible Human Resources concerns. The communications plan should include a schedule to report on findings and recommendations at all levels, including general reports and executive briefings. 2.2 Perform Maturity Assessment 2.2.1 Gather Information The next step is to gather appropriate inputs for the assessment, bas ed on the interaction model. At a minimum, the information gathered will include formal ratings of assessment criteria. It may also include input from interviews and focus groups, system analysis and design documentation, data investigation, email strings, procedure manuals, standards, policies, file repositories, approval workflows, various work products, Metadata repositories, data and integration reference architectures, templates, and forms. 2.2.2 Perform the Assessment The overall rating assignments and interpretation are typically multi- phased. Participants will have different opinions generating different ratings across the assessment topics. Discussion and rationalization will be needed to reconcile the ratings. Input is provided by the participants and then refined through artifact reviews or examination by the assessment team. The goal is to come to a consensus view of current state. This view should be supported by evidence (i.e., proof of practice demonstrated by b ehavior and artifacts). If stakeholders do not have consensus on current state, it is difficult to have consensus on how to improve the organization. The refinement generally works as follows: • Review results against the rating method and assign a preliminary rating to each work product or activity. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "514 • DMBOK2 • Document the supporting evidence. • Review with participants to come to consensus on a final rating for each area. If appropriate, use weight modifiers based on the importance of each criterion. • Document the interpretation of the rating using the model criteria statements and assessor comments. • Develop visualizations to illustrate results of the assessment. 2.3 Interpret Results Interpretation of the results consists of identifying improvement opportunities aligned with organizational strategy and recommending actions required to take advantage of these opportunities. In other words, interpretation defines next steps toward a targ et state. When the assessment is complete, organizations need to plan for the target state that they aspire to achieve in data management. The amount of time and effort required to achieve the desired target will vary, depending on the starting point, the culture of the organization, and the drivers for change. When presenting assessment results, start with the meaning of the ratings for the organization. The ratings can be expressed with respect to organizational and cultural drivers as well as business goals, such as customer satisfaction or increased sales. Illustrate the linkage between the current capabilities of the organization and the business processes and strategies that they support, and the benefits of improving these capabilities by moving to the target state. 2.3.1 Report Assessment Results The assessment report should include: • Business drivers for the assessment • Overall results of the assessment • Ratings by topic with gaps indicated • A recommended approach to close gaps • Strengths of the organization as observed • Risks to progress • Investment and outcomes options • Governance and metrics to measure progress • Resource analysis and potential future utilization • Artifacts that can be used or re -used within the organization The assessment report is an input to the enhancement of the Data Management function , either as a whole or by Data Management Knowledge Area. From it, the organization can develop or advance its data management strategy. Strategy should include initiatives that further business goals through improved governance of processes and standards. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 515 2.3.2 Develop Executive Briefings The assessment team should prepare executive briefings that summarize findings – strengths, gaps, and recommendations – that executives will use as input to decisions about targets, initiatives, and timelines. The team must tailor the messages to clarify l ikely impacts and benefits for each executive group. Often executives wish to aim higher than the assessment recommendations. In other words, they want to skip levels in the maturity model. Targeting a higher level of maturity has to be reflected in the impact analysis for the recommendations. There is a cost to this kind of acceleration, and costs must be balanced against benefits. 2.4 Create a Targeted Program for Improvements The DMMA should have a direct impact on data strategy and IT governance, as well as the Data Management function and strategy. Recommendations from the DMMA should be actionable. These should describe capabilities the organizational requires. In doing so, an assessment can be a powerful tool for IT and business leaders to set organizational priorities and allocate r esources. 2.4.1 Identify Actions and Create a Roadmap DMMA ratings highlight items for management attention. Initially, a rating is likely to be used as a standalone metric to determine how well an organization is doing a specific activity. However, ratings can be quickly operationalized into ongoing measures, especially for activities where change is desired (e.g., “The target is level ‘n’ because we need or want to be able to do something ‘z’”). If the assessment model is used for ongoing measurement, its criteria not only guide s the organization to higher levels of maturity, its criteria also keep s organizational attention on improvement efforts. The DMM assessment results should be detailed and comprehensive enough to support a multiple year data management improvement program, including initiatives that will build data management capability as the organization adopt s best practices. Since change largely happens in organizations through projects, new projects must be influenced to adopt better practices. The roadmap or reference plan should contain: • Sequenced activities to effect improvements in specific data management functions • A timeline for implementing improvement activities • Expect ed improvements in DMMA ratings once activities have been implemented • Oversight activities, including the maturing this oversight over the timeline The roadmap will give targets and a pace for change within prioritized work streams, and accompanied by an approach for measuring progress. 2.5 Re-assess Maturity Re-assessments should be conducted at regular intervals. They are part of the cycle of continuous improvement: Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "516 • DMBOK2 • Establish a baseline rating through the first assessment • Define re-assessment parameters, including organizational scope • Repeat DMM assessment as necessary on a published schedule • Track trends relative to the initial baseline • Develop recommendations based on the re- assessment findings Re-assessment can also re -invigorate or refocus effort. Measurable progress assists in maintaining commitment and enthusiasm across the organization. Changes to regulatory frameworks, internal or external policy, or innovations that could change the approa ch to governance and strategies are additional reasons to re -assess periodically. 3. Tools • Data Management Maturity Framework : The primary tool used in a maturity assessment is the DMM framework itself. • Communication Plan : A communication plan includes an engagement model for stakeholders, the type of information to be shared, and the schedule for sharing information. • Collaboration Tools : Collaboration tools allow findings from the assessment to be shared. In addition, evidence of data management practices may be found in email, completed templates, and review documents created via standard processes for collaborative design, operations, incident tracking, reviews, and approvals. • Knowledge Management and Metadata Repositories : Data standards, policies, methods, agendas, minutes of meetings or decisions, and business and technical artifacts that serve as proof of practice may be managed in these repositories. In some CMMs, lack of such repositories is an indicator of lower matu rity in the organization. Metadata repositories can exist in several constructs, which may not be obvious to the participants. For example, some Business Intelligence applications rely completely on Metadata to compile their views and reports, while not referring to it as a separate distinct repository. 4. Techniques Many techniques related to executing a DMMA are defined by the methodology of the DMM framework chosen. Techniques that are more general are described here. 4.1 Selecting a DMM Framework The following criteria should be considered when selecting a DMM framework. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 517 • Accessibility : Practices are stated in non -technical terms that convey the functional essence of the activity. • Comprehensiveness : The framework addresses a broad scope of data management activities and includes business engagement, not merely IT processes. • Extensible and flexible: The model is structured to enable enhancement of industry -specific or additional disciplines and can be used either in whole or in part, depending on the needs of the organization. • Future progress path built -in: While specific priorities differ from organization to organization, the DMM framework outlines a logical way forward within each of the functions it describes. • Industry -agnostic vs. industry -specific: Some organizations will benefit from an industry- specific approach, others from a more generic framework. Any DMM framework should also adhere to data management best practices that cross verticals. • Level of abstraction or detail : Practices and evaluation criteria are expressed at a sufficient level of detail to ensure that they can be related to the organization and the work it performs. • Non-prescriptive : The framework describes what needs to be performed, not how it must be performed. • Organized by topic : The framework places data management activities in their appropriate context, enabling each to be evaluated separately, while recognizing the dependencies. • Repeatable: The framework can be consistently interpreted, supporting repeatable results to compare an organization against others in its industry and to track progress over time. • Supported by a neutral, independent organization: The model should be vendor neutral in order to avoid conflicts of interest, and widely available to ensure a broad representation of best practices. • Technology neutral : The focus of the model should be on practices, rather than tools. • Training support included : The model is supported by comprehensive training to enable professionals to master the framework and optimize its use. 4.2 DAMA -DMBOK Framework Use The DAMA -DMBOK can be used to prepare for or establish criteria for a DMMA. Execution owners will see a direct linkage between segmented functions (the Knowledge Areas) and the corresponding tasks (activities). The DMBOK Knowledge Areas, activities, and de liverables (work products) can be configured to a specific DMM framework based on the areas measured, their supporting activities, relevancy, and time available. This fast checklist approach can be used to determine areas that need deeper analysis, represent gaps, or point to hot spots for remediation. The DMBOK offers an additional advantage as an assessment -planning tool: There is a large community of knowledge professionals using the DMBOK as a guide across multiple industries, creating a community of practice around its use. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "518 • DMBOK2 5. Guidelines for a DMMA 5.1 Readiness Assessment / Risk Assessment Before conducting a maturity assessment, it is helpful to identify potential risks and some risk mitigation strategies. Table 34 summarizes risks and mitigation approaches. Table 34 Typical Risks and Mitigations for a DMMA Risk Mitigation Lack of organizational buy -in Socialize the concepts related to the assessment. Establish benefit statements before conducting the assessment. Share articles and success stories. Engage an executive sponsor to champion the effort and review the results. Lack of DMMA expertise Lack of time or in -house expertise Lack of communication planning or standards Use third party resources or specialists. Require knowledge transfer and training as part of the engagement. Lack of ‘Data Speak’ in the organization; Conversations on data quickly devolve into discussions about systems Relate the DMMA to specific business problems or scenarios. Address in the communications plan. The DMMA will educate all participants regardless of background and technical experience. Orient participants to key concepts prior to the DMMA. Incomplete or out -of-date assets for analysis Flag ‘as of’ or balance the rating accordingly. For example, give a -1 to everything that is over 1 year out -of-date. Narrow focus Reduce the investigation depth to a simple DMMA and go to other areas for a quick assessment to establish ratings for a later comparative baseline. Conduct the first DMMA as a pilot, then apply lessons learned to address a broader scope. Present in -scope focus of proposed assessment in context of DAMA -DMBOK Knowledge Areas. Illustrate what is being left out of scope and discuss the need to include. Inaccessible staff or systems Reduce the horizontal scope of the DMMA by focusing only on available Knowledge Areas and staff . Surprises arise , such as a regulation changes Add flexibility into the assessment work stream and focus. 5.2 Organizational and Cultural Change Establishing or enhancing a Data Management function includes changes to processes, methods, and tools. With these changes, culture must also change. Organizational and cultural transformation begins with acknowledging that things can be better. Measurement functions typically usher in meaningful change. The DMMA locates the organization on a maturity scale and provides a roadmap for improvement. Doing so, it can point an organization forward through change. The DMMA results should be part of a larger discussion within an organization. When properly supporte d by effective Data Governance , DMMA results can coalesce differing perspectives, result in a shared vision, and accelerate an organization’s progress. ( See Chapter 17. ) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT MATURITY ASSESSMENT • 519 6. Maturity Management Governance Typically, a DMMA is part of an overall set of Data Governance activities, each of which has a lifecycle. The lifecycle of a DMMA consists of the initial planning and initial assessment, followed by recommendations, an action plan, and periodic re -evaluation. The lifecycle itself should be governed. 6.1 DMMA Process Oversight Oversight for the DMMA process belongs to the Data Governance team. If formal Data Governance is not in place, then oversight defaults to the steering committee or management layer that initiated the DMMA. The process should have an executive sponsor, idea lly the CDO, to ensure improvements in data management activities map directly to business objectives. The breadth and depth of oversight depend on the DMMA’s scope. Each function involved in the process has a voice in the execution, method, results, and r oadmaps coming from the overall assessment. Each involved data management area and organization function will have an independent view, but also will have a common language through the DMM framework. 6.2 Metrics In addition to being a core component of any improvement strategy, metrics are a key communications tool. Initial DMMA metrics are the ratings representing the current state of data management. These can be periodically reassessed to show improvement trends. Each organization should develop metrics tailored to its target state roadmap. Sample metrics could include: • DMMA ratings : DMMA ratings present a snapshot of the organization’s capability level. The ratings may be accompanied by a description, perhaps a custom weighting for the rating across an assessment or specific topic area, and a recommended target state. • Resource utilization rates : Powerful examples of metrics that help express the cost of data management in the form of head count. An example of this type of metric is: “Every resource in the organization spends 10% of their time manually aggregating data.” • Risk exposure or the ability to respond to risk scenarios expresses an organization’s capabilities relative to their DMMA ratings. For example, if an organization wanted to begin a new business that required a high level of automation but their current operating model is based on manual data management (Level 1), they would be at risk of not delivering. • Spend management expresses how the cost of data management is allocated across an organization and identifies the impacts of this cost on sustainability and value. These metrics overlap with Data Governance metrics. o Data management sustainability o Achievement of initiative goals and objectives o Effectiveness of communication o Effectiveness of education and training o Speed of change adoption o Data management value Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "520 • DMBOK2 o Contributions to business objectives o Reductions in risks o Improved efficiency in operations • Inputs to the DMMA are important to manage as they speak to the completeness of coverage, level of investigation, and detail of the scope relevant for interpretation of the scoring results. Core inputs could include the following: count, coverage, availability, number of systems, data volumes, teams involved, etc. • Rate of Change The rate at which an organization is improving its capability. A baseline is established through the DMMA. Periodic reassessment is used to trend improvement. 7. Works Cited / Recommended Afflerbach, Peter. Essential Readings on Assessment. International Reading Association, 2010. Print. Baskarada, Sasa. IQM -CMM: Information Quality Management Capability Maturity Model . Vieweg+Teubner Verlag, 2009. Print. Ausgezeichnete Arbeiten zur Informationsqualität. Boutros, Tristan and Tim Purdie. The Process Improvement Handbook: A Blueprint for Managing Change and Increasing Organizational Performance . McGraw -Hill Education, 2013. Print. CMMI Institute (website). http://bit.ly/1Vev9xx . Crawford, J. Kent. Project Management Maturity Model . 3rd ed. Auerbach Publications, 2014. Print. PM Solutions Research. Enterprise Data Management Council (website). Freund, Jack and Jack Jones. Measuring and Managing Information Risk: A FAIR Approach . Butterworth -Heinemann, 2014. Print. Ghavami, Peter PhD. Big Data Governance: Modern Data Management Principles for Hadoop, NoSQL and Big Data Analytics . CreateSpace Independent Publishing Platform, 2015. Print. Honeysett, Sarah. Limited Capability - The Assessment Phase . Amazon Digital Services LLC., 2013. Social Insecurity Book 3. IBM Data Governance Council. https://ibm.co/2sUKIng . Jeff Gorball, Introduction to Data Management Maturity Models . SlideShare.net, 2016- 08-01. http://bit.ly/2tsIOqR . Marchewka, Jack T. Information Technology Project Management: Providing Measurable Organizational Value . 5th ed. Wiley, 2016. Print. McSweeney, Alan. Review of Data Management Maturity Models . SlideShare.net, 2013- 10-23. http://bit.ly/2spTCY9 . Persse, James R. Implementing the Capability Maturity Model . Wiley, 2001.Print. Saaksvuori, Antti. Product Management Maturity Assessment Framework . Sirrus Publishing Ltd., 2015. Print. Select Business Solutions. “What is the Capability Maturity Model?” http://bit.ly/IFMJI8 (Accessed 2016- 11-10). Stanford University. Stanford Data Governance Maturity Model . http://stanford.io/2ttOMrF. Van Haren Publishing. IT Capability Maturity Framework IT -CMF . Van Haren Pub, 2015. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "521 CHAPTER 1 6 Data Management Organization and Role Expectations 1. Introduction he data landscape is quickly evolving and with it, organizations need to evolve the ways they manage and govern data. Most organizations today are faced with an increasing volume of data captured through a wide range of processes in a range of formats. The increase in volume and variety adds complexity to data management. At the same time, data consumers now demand quick and easy access to data. They want to be able to understand data and use it to address critical business questions in a timely manner. Dat a management and Data Governance organizations must be flexible enough to work effectively in this evolving environment. To do so, they need to clarify basic questions about ownership, collaboration, accountability, and decision - making. This section will describe a set of principles that should be considered when putting together a data management or Data Governance organization. It refers to both Data Governance and data management because Data Governance provides the guidance and business context for the activities executed by the Data Management Organization. There is no perfect organizational structure for either. While common principles should be applied to organizing around Data Governance and data management, much of the detail will depend on the drivers of that enterprise’s industry and the corporate culture of the enterprise itself. 2. Understand Existing Organization and Cultural Norms Awareness, ownership, and accountability are the keys to activating and engaging people in data management initiatives, policies, and processes. Before defining any new organization or attempting to improve an existing one, it is important to understand cu rrent state of component pieces, related to culture, the existing operating model, and people. See Figure 106. For example: • The role of data in the organization: What key processes are data -driven? How are data requirements defined and understood? How well -recognized is the role that data plays in organizational strategy? T Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "522 • DMBOK2 • Cultural norms about data : Are there potential cultural obstacles to implementing or improving management and governance structures? • Data management and Data Governance practices : How and by whom is data -related work executed? How and by whom are decisions about data made? • How work is organized and executed: For example, what is the relation between project -focused and operational execution? What committee structures are in place that can support the data management effort? • How reporting relationships are organized: For example, is the organization centralized or decentralized, hierarchical or flat? • Skill levels : What is the level of data knowledge and data management knowledge of SMEs and other stakeholders, from line staff to executives? Figure 106 Assess Current State to Create an Operating Model After forming a picture of current state, assess the level of satisfaction with current state in order to gain insight into the organization’s data management needs and priorities. For example: • Does the organization have the information it needs to make sound, timely business decisions? • Does the organization have confidence in its revenue reports? • Can it track the organizational key performance indicators? • Is the organization in compliance with all laws regarding management of data? Most organizations that seek to improve their data management or governance practices are in the middle of the capability maturity scale (i.e., they are neither 0’s nor 5’s on the CMM scale ). (See Chapter 15.) To craft a relevant Data Management Organization, it is important to understand and accommodate the existing company culture and organizational norms. If the Data Management Organization is not aligned to the existing decision - making and committee constru cts, it will be challenging to sustain it over time. Therefore, it makes sense to evolve these organizations, rather than imposing radical changes. A Data Management Organization should align with a company’s organizational hierarchy and resources. Finding the right people requires an understanding of both the functional and the political role of data management within an organization. The aim should be cross -functional participation from the various business stakeholders. To accomplish this: •How are decisions made? •Who makes them? •How are committees used? •Who currently manages data?•Centralized •Decentralized •Hybrid/Federated•Data Management Owner •Data Governance Owner •Subject Matter Experts •LeadershipOperating Model People Culture Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 523 • Identify employees currently performing data management functions; recognize and involve them first. Hire additional resources only as data management and governance needs grow. • Examine the methods the organization is using to manage data and determine how processes can be improved. Determine how much change is likely to be required to improve data management practices. • Roadmap the kinds of changes that need to take place from an organizational perspective to better meet requirements. 3. Data Management Organizational Constructs A critical step in Data Management Organization design is identifying the best -fit operating model for the organization. The operating model is a framework articulating roles, responsibilities, and decision -making processes. It describes how people and fun ctions will collaborate. A reliable operating model helps create accountability by ensuring the right functions within the organization are represented. It facilitates communication and provides a process to resolve issues. While it forms the basis for the organizational structure, the operating model is not an org chart – it is not about putting names in boxes, but about describing the relationship between the component pieces of the organization. This section will present a high -level overview of the pros and cons of decentralized, network, hybrid, federated, and centralized operating models. 3.1 Decentralized Operating Model In a decentralized model, data management responsibilities are distributed across differ ent lines of business and IT (s ee Figure 107). Collaboration is committee -based; there is no single owner. Many Data Management improvement programs start as grass root efforts to unify the data management practices across an organization and therefore have a decentralized structure. Figure 107 Decentralized Operating Model LOB/BU Data Management Steering Committee Data StewardsApplication ArchitectsBusiness AnalystsData AnalystsLOB/BU Data Management Group Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "524 • DMBOK2 The benefits of this model include its relatively flat structure and its alignment of data management to lines of business or IT. This alignment generally means there is a clear understanding of data requirements. It is also relatively easy to implement or improve. The drawbacks include the challenge of having many participants involved with governance bodies and in decision -making. It is generally harder to implement collaborative decisions than centralized edicts. Decentralized models are generally less formal and because of this, they can be harder to sustain over time. To be successful, they need to have ways to enforce consistency of practices. This can be difficult to coordinate. It is also often difficult to define data ownership with a decentralized model. 3.2 Network Operating Model Decentralized informality can be made more formal through a documented series of connections and accountabilities via a RACI (Responsible, Accountable, Consulted, and Informed) matrix. This is called a networked model because it operates as a series of kno wn connections between people and roles and can be diagrammed as a ‘network.’ (See Figure 108.) Figure 108 Network Operating Model A network model’s benefits are similar to those of a decentralized model (flat structure, alignment, quick set up). The addition of a RACI helps create accountability without impacting the organizational charts. The additional drawback is the need to maint ain and enforce expectations related to the RACI. 3.3 Centralized Operating Model The most formal and mature data management operating model is a centralized one (see Figure 109). Here everything is owned by the Data Management Organization. Those involved in governing and managing data report directly to a data management leader who is responsible for Governance, Stewardship, Metadata Management, Data Quality Management, Master and Reference Data Management, Data Architecture, Business Analysis, etc. DATA MANAGEMENT OFFICE Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 525 Figure 109 Centralized Operating Model The benefit of a centralized model is that it establishes a formal executive position for data management or Data Governance . There is one person at the top. Decision -making is easier because accountability is clear. Within the organization, data can be managed by type or subject area. The drawback is that implementation of a centralized model generally requires significant organizational change. There is also a risk that formal separation of the data management role moves it away for core business processes and can result in knowledge being lost over time. A centralized model generally requires a new organization. The question arises: Where does the Data Management Organization fit within the overall enterprise? Who leads it and to whom does the leader report? It is becoming more common for a Data Management Organization not to report to the CIO because of the desire to maintain a business, rather than IT, perspective on data. These organizations are also commonly part of a shared services or operations team or part of the Chief Data Officer’s organization . (See Section 6.1 .) 3.4 Hybrid Operating Model As its name implies, the hybrid operating model encompasses benefits of both the decentra lized and centralized models (s ee Figure 110). In a hybrid model, a centralized data management Center of Excellence works with decentralized business unit groups, usually through both an executive steering committee representing key lines of business and a set of tactical working groups addressing sp ecific problems. In this model, some roles remain decentralized. For example, Data Architects may stay within an Enterprise Architecture group; lines of business may have their own Data Quality teams. Which roles are centralized and which stay decentralized can vary widely, depend ing largely on organizational culture. Executive Sponsor Data Management Lead Business Analysis GroupData Management GroupData Architecture GroupT echnical Data Analysis GroupSteering Committee Bus / LOBsBusiness Support T echnical Support Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "526 • DMBOK2 Figure 110 Hybrid Operating Model The primary benefit of a hybrid model is that it establishes appropriate direction from the top of the organization. There is an executive accountable for data management and/or governance. Business Unit teams have broad accountability and can align to business priorities to provide greater focus. They benefit from the support of a dedicated data management Center of Excellence that can help bring focus to specific challenges. The challenges include getting the organization set up, since doing so generally requires additional headcount to staff a Center of Excellence. Business Unit teams may have different priorities, and these will need to be managed from an enterprise perspective. In addition, there are sometimes conflicts between the priorities of the central organization and those of the decentralized organization s. 3.5 Federated Operating Model A variation on the hybrid operating model, the federated model provides additional layers of centralization / decentralization, which are often required in large global enterprises. Imagine an enterprise Data Management Organization with multiple hybrid da ta management models delineated based on division or region. (See Figure 111.) A federated model provides a centralized strategy with decentralized execution. Therefore, for large enterprises it may be the only model that can work. A data management executive who is accountable across the organization runs the enterprise Center of Excellence. Of course, different lines of business are empowered to meet requirements based on their needs and priorities. Federation enables the organization to prioritize based on specific data entities, divisional challenges, or regional priorities. The main drawback is complexity. There are a lot of layers, and there needs to be a balance between autonomy for lines of business and the needs of the enterprise. This balance can impact enterprise priorities. Steering Committee Data Management Center of Excellence Data Management Business Unit T eams Business Stakeholders IT Enablement BU Data ManagementData Management Organization Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 527 Figure 111 Federated Operating Model 3.6 Identifying the Best Model for an Organization The operating model is a starting point for improving data management and Data Governance practices. Introducing it requires an understanding of how it may impact the current organization and how it will likely need to evolve over time. Since the operating model will serve as the structure through which policies and processes will be defined, approved, and executed, it is critical to identify the best fit for an organization. Assess whether the current organizational structure is centralized, decentralized, or a combination, hierarchical or relatively flat. Characterize how independent divisions or regions are. Do they operate almost self - sufficiently? Are their requirements and goals very different from each other? Most importantly, try to determine how decisions are made (e.g., democratically or by fiat) , as well as how they are implemented. The answers should give a starting point to understand the organization’s location on the spectrum between decentralized and centralized. 3.7 DMO Alternatives and Design Considerations Most organizations start with a decentralized model before they move toward a formal Data Management Organization (DMO). As an organization sees the impact of improvements in Data Quality , it may start to formalize accountability through a data management RACI matrix and evolve into a network model. Over time, synergies between the distributed roles will become more obvious and economies of scale will be identified that will pull some roles and people into organized groups. Eventually, this can morph into a hybrid or federated model. Some organizations don’t have the luxury of going through this maturity process. They are forced to mature quickly based on a market shock or new government regulations. In such a case, it is important to proactively Data Management Organization Enterprise Information Management Steering Committee Enterprise Data Management Center of Excellence Data Management Groups Divisional Data Management Group Business Stakeholders IT EnablementDivisional Data Management Group Business Stakeholders IT EnablementDivisional Data Management Group IT EnablementBusiness Stakeholders Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "528 • DMBOK2 address the discomfort associated with the organizational change if it is to be successful and sustainable. (See Chapter 17.) Whichever model is chosen, remember that simplicity and usability are essential for acceptance and sustainability. If the operating model fits the culture of a company, then data management and proper governance can be embedded in operations and aligned with strategy. Keep these tips in mind when constructing an Operating Model: • Determine the starting point by assessing current state • Tie the operating model to organization structure • Take into account: o Organization Complexity + Maturity o Domain Complexity + Maturity o Scalability • Get executive sponsorship – a must for a sustainable model • Ensure that any leadership forum (steering committee, advisory council, board) is a decision -making body • Consider pilot programs and waves of implementation • Focus on high -value, high -impact data domains • Use what already exists • Never take a One- Size-Fits-All approach 4. Critical Success Factors Ten factors have been consistently shown to play a key role in the success of effective Data Management Organizations, regardless of their structure: 1. Executive sponsorship 2. Clear vision 3. Proactive change management 4. Leadership alignment 5. Communication 6. Stakeholder engagement 7. Orientation and training 8. Adoption measurement 9. Adherence to guiding principles 10. Evolution not revolution 4.1 Executive Sponsorship Having the right executive sponsor ensures that stakeholders affected by a Data Management improvement program receive the necessary guidance to transition efficiently and effectively through the changes needed to Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 529 put the new data- focused organization together and sustain it for the long term. The executive sponsor should understand and believe in the initiative. He or she must be able to effectively engage other leaders in support of the changes. 4.2 Clear Vision A clear vision for the Data Management Organization, along with a plan to drive it, is critical to success. Organizational leaders must ensure that all stakeholders who are affected by data management – both internal and external – understand and internali ze what data management is, why it is important, and how their work will affect and be affected by it. 4.3 Proactive Change Management Managing the change associated with creating a Data Management Organization requires planning for, managing, and sustaining change. Applying organizational change management to the establishment of a Data Management Organization addresses the people challenges and increases the likelihood that desired Data Management Organization is sustainable over time. (See Chapter 17.) 4.4 Leadership Alignment Leadership alignment ensures that there is agreement on – and unified support for – the need for a Data Management improvement program and that there is agreement on how success will be defined. Leadership alignment includes both the alignment between the leaders’ goals and the data management outcomes and value and alignment in purpose amongst the leaders. If leaders are not aligned with each other, they will end up sending mixed messages that can lead to resistance and eventually derail the change. Therefore, it is critical to assess – and regularly re -assess – leaders at all levels to identify disconnects and take steps to quickly address them. 4.5 Communication Communication should start early and continue openly and often. The organization must ensure that stakeholders have a clear understanding of what data management is and why it is important to the company, what is changing, and what changes in behavior are required. People can’t improve the way they manage data if they don’t know what they are supposed to do differently. Creating a story around the data management initiative and building key messages around it helps these processes. Messages must be consistent, underscoring the importance of data management. In addition, they should be customized according to stakeholder group. For example, the level of education or amount of training needed by different groups concerning data management will vary. Messages should be repeated as needed and continually tested over time to ensure they are effectively getting out there and that awareness and understanding are building. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "530 • DMBOK2 4.6 Stakeholder Engagement Individuals, as well as groups, affected by a data management initiative will react differently to the new program and their role within it. How the organization engages these stakeholders – how they communicate with, respond to, and involve them – will ha ve a significant impact on the success of the initiative. A stakeholder analysis helps the organization better understand those affected by data management changes. By taking that information and mapping stakeholders according to level of influence within the organization and level of interest in (or affect due to) the data management implementation, the organization can determine the best approach to engaging different stakeholders in the change process . (See Section 5.3 .) 4.7 Orientation and Training Education is essential to making data management happen, although different groups will require different types and levels of education. Leaders will need orientation to the broader aspects of data management and the value to the company. Data stewards, owners, and custodians (i.e., those on the frontlines of change) will require in -depth understanding of the data management initiative. Foc used training will allow them to perform their roles effectively. This means training on new policies, processes, techniques, procedures, and even tools. 4.8 Adoption Measurement It is important to build metrics around the progress and adoption of the data management guidelines and plan to know that the data management roadmap is working and that it will continue working. Plan to measure: • Adoption • Amount of improvement, or the delta from a previous state • The enabling aspects of data management – how well does data management influence solutions with measurable results? • Improved processes, projects • Improved identification and reaction to risk • The innovation aspect of data management – how well does data management fundamentally change how business is conducted? • Trusted a nalytics The enabling aspect of data management could focus on the improvement of data -centric processes, such as month -end closing, identification of risk, and efficiency of project execution. The innovation aspect of data management could focus on improvement in decision -making and analytics through improved and trusted data . 4.9 Adherence to Guiding Principles A guiding principle is a statement that articulates shared organizational values, underlies strategic vision and mission, and serves as a basis for integrated decision-making. Guiding principles constitute the rules, Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 531 constraints, overriding criteria, and behaviors by which an organization abides in its daily activities in the long term. Regardless of whether there is a decentralized or centralized operating model, or anything in between, it is critical to establish and agree upon guiding principles so that all participants behave in synchronistic ways. The guiding principles serve as the reference points from which all decisions will be made. Establishing them is an important first step in creating a Data Management imp rovement program that effectively drives changes in behavior. 4.10 Evolution Not Revolution In all aspects of data management, the philosophy of ‘evolution not revolution’ helps to minimize big changes or large -scale high -risk projects. It is important to establish an organization that evolves and matures over time. Incrementally improving the way that data is managed and prioritized by business objectives will ensure that new polici es and processes are adopted and behavioral change is sustained. Incremental change is also much easier to justify, so it is easier to gain stakeholder support and buy-in and get those critical participants involved. 5. Build the Data Management Organization 5.1 Identify Current Data Management Participants When implementing the operating model, start with teams already engaged in data management activities. This will minimize the effect on the organization and will help to ensure that the focus of the team is data, not HR or politics. Start by reviewing existing data management activities, such as who creates and manages data, who measures Data Quality , or even who has ‘data’ in their job title. Survey the organization to find out who may already be fulfilling needed roles and responsibilities. Such individuals may hold different titles. They are likely part of a distributed organization and not necessarily recognized by the enterprise. After compiling a list of ‘data people,’ identify gaps. What additional roles and skill sets are required to execute the data strategy? In many cases, people in other parts of the organization have analogous, transferrable skill sets. Remember, people already in the organization bring valuable knowledge and experience t o a data management effort. Once an inventory is complete, and people are assigned to the roles, review their compensation and align it with the expectations of data management. Likely, the Human Resources department will get involved to validate the titles, roles, compensation , and performance objectives. Ensure that the roles are assigned to the right people at the right level within the organization, so that when they are involved in decision -making, they have the credibility to make decisions that stick. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "532 • DMBOK2 5.2 Identify Committee Participants No matter which operating model an organization chooses, some governance work will need to be done by a Data Governance Steering Committee and by working groups. It is important to get the right people on the Steering Committee and to use their time well. Keep them well -informed and focused on the ways that improved data management will help them reach business objectives, including strategic goals. Many organizations are reluctant to start yet another committee since there are so many already existing. It is often easier to take advantage of existing committees to advance data management topics than it is to start a new one. But take this route cautiously. The main risk in using an existing committee is that data management may not get the attention it requires, especially in the early stages. The process to staff either a senior steering committee or a more tactical working group requires conducting stakeholder analysis and, through that, identifying executive sponsors. 5.3 Identify and Analyze Stakeholders A stakeholder is any person or group who can influence or be affected by the Data Management improvement program. Stakeholders can be internal to or external to the organization. They include individual SMEs, senior leaders, teams of employees, committees, customers, government or regulatory agencies, brokers, agents, vendors, etc. Internal stakeholders can come from IT, operations, compliance, legal, HR, finance or other lines of business. External stakeholders can be influential, and it is important that their needs be accounted for by the Data Management Organization. A stakeholder analysis can help the organization determine the best approach to engaging participants in the data management process and leveraging their roles within the operating model. Insight gained from the analysis is also helpful in determining how to best allocate time and other limited resources. The earlier this analysis is conducted, the better, since the more the organization is able to anticipate reactions to change, the more it can plan for them. A stakeholder analysis will help answer questions like: • Who will be affected by data management? • How will roles and responsibilities shift? • How might those affected respond to the changes? • What issues and concerns will people have? The analysis will result in a list of stakeholders, their goals and priorities, and why those goals are important to them. Figure out what actions are needed for stakeholders based on the analysis. Pay particular attention to what needs to be done to bring along critical stakeholders, those that can make or break an organization’s data management success, especially its initial priorities. Consider: • Who controls critical resources • Who could block data management initiatives, either directly or indirectly • Who could influence other critical constituents • How supportive stakeholders are of the upcoming changes Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 533 Figure 112 provides a simple map to help prioritize stakeholders based on their influence, their level of interest in the Data Management improvement program, or the degree to which the program will impact them. 5.4 Involve the Stakeholders After identifying the stakeholders and a good Executive Sponsor, or a short list from which to choose, it is important to clearly articulate why each of the stakeholders should be involved. They may not jump at the chance. The person or team driving the data management effort should articulate the reasons each stakeholder is necessary to the success of the Data Management improvement program . This means understanding their personal and professional goals, and being able to link the output from data manageme nt processes to their goals, so they can see a direct connection. Without an understanding of this direct connection, they may be willing to help out in the short term, but they will not provide long- term support or assistance. Figure 112 Stakeholder Rating Map 6. Interactions Between the DMO and Other Data-o riented Bodies Once the operating model is established and participants are identified, it is time to move the people into the newly authorized roles. Operationalizing the organization means establishing the committees and engaging with stakeholders. In a centralized mod el, most of the data management activity will be controlled within one organization. With a decentralized or network model, though, the Data Management Organization will need to work with other groups that have a significant impact on the way that data is managed. Those groups are typically: • Chief Data Officer Organization • Data Governance Bodies • Data Quality • Enterprise Architecture Meet Their NeedsKey Player Prioritization of Stakeholders Lower PriorityShow Consideration Stakeholder InterestStakeholder Influence Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "534 • DMBOK2 6.1 The Chief Data Officer While most companies recognize at some level that data is a valuable corporate asset, only a few have appointed a Chief Data Officer (CDO) to help bridge the gap between technology and business and evangelize an enterprise -wide data management strategy at a senior level. This role is on the rise, however, with Gartner estimating that half of all regulated companies will employ a CDO by 2017 (Gartner, 2015). While the requirements and functions of a CDO are specific to each company’s culture, organizational structure, and business needs, many CDOs tend to be part business strategist, adviser, Data Quality steward and all around data management ambassador. In 2014, Dataversity published research outlining common mandates for a CDO. 97 These included: • Establishing an organizational data strategy • Aligning data- centric requirements with available IT and business resources • Establishing Data Governance standards, policies and procedures • Providing advice (and perhaps services) to the business for data -dependent initiatives, such as business analytics, Big Data, Data Quality, and data technologies • Evangelizing the importance of good information management principles to internal and external business stakeholders • Oversight of data usage in analytics and Business Intelligence Dataversity’s findings also highlighted shifting focuses across different industries. Regardless of industry, it is common for a Data Management Organization to report up through the CDO. In a more decentralized operating model, the CDO is responsible for the data strategy, but resources that are in IT, operations, or other lines of business execute that strategy. Some DMOs are established initially with the CDO just determining the strategy, and over time other aspects of data management, governance, and analytics are folded under the CDO umbrella as efficiencies and economies of scale are identified. 6.2 Data Governance Data Governance is the organizing framework for establishing the strategy, objectives, and policy for effectively managing corporate data. It consists of the processes, policies, organization, and technologies required to manage and ensure the availability, usability, integrity, consistency, auditability, and security of data. Since a Data Governance function consists of the inter -workings of strategy, standards, policies and communication regarding data, it has a synergistic relationship with data manageme nt. Governance provides a framework for data management to engage and align with the business priorities and stakeholders. Within a centralized model, the Data Governance Office can report to the Data Management Organization or vice versa. When a Data Management function is focused on establishing policies and guidelines needed to manage data as an asset, the Data Governance Office can act as the lead, and the Data Management Organization 97 http://bit.ly/2sTf3Cy . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 535 reports to (or is matrixed to) the Data Governance Office. This occurs many times in highly regulated environments where the emphasis is on policy and accountability. Even in a very decentralized model, there should be a tight partnership between the Data Governance Office, which creates the guidelines and policies for how data should be managed, and the Data Management Organization that implements them. John Ladley succinctly clarifies this relationship: Data Governance is about ‘Doing the right things’ and data management is about ‘Doing things right’ (Ladley, 2012). They are two sides of the equation needed to produce valuable data. In this way, Data Governance provid es the marching orders for data management. Most importantly, there needs to be an understanding of this synergy and agreement upon roles, responsibilities, and accountabilities that support the guidelines of Data Governance and the efficiencies of data management. Participants in a Data Governance Working Group can be drawn from a Data Management Organization, and a Data Management Organization can use the mandate and ‘air cover’ provided by the governance oversight. 6.3 Data Quality Data Quality Management is a key capability of a data management practice and organization. Many Data Management Organizations start with a focus on the quality of data because there is a desire to measure and improve the quality of data across the organization. It is possible to address Data Quality within a line of business, or even within an application, without having to involve other groups or manage cross -functional complexities. However, as a Data Quality practice matures, the organization will benefit from a unified approach to Data Quality; for example, by estab lishing a Center of Excellence. The goal shifts to improving the quality of data that is shared across lines of business or applications, often with a f ocus on Master Data Management. It is common that a Data Management Organization develops organically out of a Data Quality initiative as the investment in improving Data Quality adds value across the company, and efforts associated with improving quality expand into other disciplines like Master, Reference, and Metadata Management. A Data Quality function can evolve into similar operating models as an over -arching Data Management function , although it is rare for Data Quality functions to become completely centralized in any sizable company because there is most often aspects of Data Quality that are executed on a line- of-business or application level. Because a Data Quality function can be decentralized, networked, or a hybrid (using a Center of Excellence approach), align the Data Quality operating model to that of the overall Data Management Organization, in order to use consistent stakeholders, relationships, accountabilities, standards, processes, and even tools. 6.4 Enterprise Architecture An Enterprise Architecture group designs and documents the master blueprints for an organization to articulate and optimize how to meet its strategic objectives. The disciplines within an Enterprise Architecture practice include: • Technology Architecture Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "536 • DMBOK2 • Application Architecture • Information (or Data) Architecture • Business Architecture Data Architecture is a key capability of an effective Data Management Organization. Therefore, Data Architects can sit in either group, with a dotted line to the other group. When Data Architects sit within a Data Management Organization, typically they interface with the rest of their architecture peers via Architecture Review Boards (ARB), committees that review and give guidance on the way that architecture standards are implemented or affected by projects and programs. An ARB can approve or disapprove new projects and systems based on their level of adherence to architectural standards. When an organization does not have Data Architects, Data Management can interface with the Architecture organization in a few ways: • Through Data Governance : Since both Data Management and Enterprise Architecture participate in a Data Governance function , the governance working group and committee structure can provide a platform for aligning goals, expectations, standards, and activities. • Through the ARB : As data management projects are brought to the ARB, the Architecture group would provide guidance, feedback, and approvals. • Ad-hoc: If there are no formal committees, then the Data Management Lead should periodically meet with the Architecture Lead to ensure there is shared knowledge and understanding of projects and processes that impact the other party. Over time, the difficulty of m anaging this ad hoc process will likely lead to the development of a formal role or committee to facilitate discussions and decisions. If there were Data Architects, then they would represent architecture in governance discussions and would lead the discussions in the ARB. 6.5 Managing a Global Organization Global companies face complex data management challenges based on the volume and variety of country - specific laws and regulations, especially those regarding the privacy and security of certain types of data. Add these issues to the typical management challenges of a global organization (distributed work force, systems, time zones , and languages), and the task of efficiently and effectively managing data can seem like an endless exercise of herding cats. Global organizations need to pay special attention to : • Adhering to standards • Synchronizing processes • Aligning accountability • Training and communication • Monitoring and measuring effectively • Developing economies of scale • Reducing duplication of effort Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 537 As Data Management functions and Organizations become more global, the networked or federated models become more attractive where accountabilities can be aligned, standards can be followed, and regional variations can still be accommodated. 7. Data Management Roles Data management roles can be defined at the functional or individual level. Names for the roles will differ between organizations and some organizations will have greater or lesser need for some of the roles. All IT roles can be mapped to points in the data lifecycle, so they all impact the management of data, whether directly (as with a Data Architect who designs a data warehouse) or indirectly (as with a Web Developer who programs a website). Likewise, many b usiness roles create, access, or manipulate data. Some roles, such as Data Quality Analyst, require a mix of technical skills and business knowledge. The functions and roles described below focus on those that are directed engaged in the management of data . 7.1 Organizational Roles IT Data Management Organizations provide a range of services from data, application, and technical architecture to database administration. A centralized Data Management Services Organization is focused solely on data management. This team may include a DM Executive, other DM Managers, Data Architects, Data Analysts, Data Quality Analysts, Database Administrators, Data Security Administrators, Metadata Specialists, Data Modelers, Data Administrators, Data Warehouse Architects, Data Integration Architects, a nd Business Intelligence Analysts. A federated Data Management Services approach will include a set of IT units, each focused on a facet of data management. Especially in large organizations, IT functions are often decentralized. For example, each business function may have its own team of Software Developers. A hybrid approach is also taken. For example, while each business function may have its own developers, the DBA function may be centralized. Business functions focused on data management are most often associated with Data Governance or Enterprise Information Management teams. For example, Data Stewards are often part of a Data Governance Organization. Such an organization will facilitate Data Governance bodies, such as the Data Governance Council. 7.2 Individual Roles Individual roles may be defined under business or IT. Some are hybrid roles that require knowledge of systems and business processes. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "538 • DMBOK2 7.2.1 Executive Roles Data Management executives may be on the business or technology side of the house. Chief Information Officer and Chief Technology Officer are well -established roles in IT. The concept of Chief Data Officer on the business- side has gained a lot of credibility in the past decade and many organizations have hired CDOs. 7.2.2 Business Roles Business roles focus largely on Data Governance functions, especially stewardship. Data Stewards are usually recognized subject matter experts who are assigned accountability for Metadata and Data Quality of business entities, subject areas, or databases. Stewards play different roles, depending on organizational priorities. The initial focus of stewardship is often on defining business terms and valid values for their subject areas. In many organizations, Stewards also define and maintain Data Quality requ irements and business rules for assigned data attributes, help identify and resolve data issues, and provide input into data standards, policies, and procedures. Stewards may function at the enterprise, business unit, or functional level. Their role may be formal (‘data steward’ is part of the title) or informal (they steward data, but have another job title). In addition to Data Stewards, Business Process Analysts and Process Architects contribute to ensuring that business process models and the actual processes that create data are sound and support downstream uses. Other business- based knowledge workers, such as business analyst consumers of data and information who add value to the data for the organization, contribute to the overall management of data. 7.2.3 IT Roles IT Roles include different types of architects, developers at different levels, database administrators, and a range of supporting functions. • Data Architect : A senior analyst responsible for Data Architecture and data integration. Data Architects may work at the enterprise level or a functional level. Data Architects may specialize in data warehousing, data marts, and their associated integration processes. • Data Modeler : Responsible for capturing and modeling data requirements, data definitions, business rules, Data Quality requirements, and logical and physical data models. • Data Model Administrator : Responsible for data model version control and change control. • Database Administrator : Responsible for the design, implementation, and support of structured data assets and the performance of the technology that makes data accessible. • Data Security Administrator: Responsible for ensuring controlled access to data requiring different levels of protection. • Data Integration Architect : A senior data integration developer responsible for designing technology to integrate and improve the quality of enterprise data assets. • Data Integration Specialist : A software designer or developer responsible for implementing systems to integrate (replicate, extract, transform, load) data assets in batch or near -real-time. • Analytics / Report Developer : A software developer responsible for creating reporting and analytical application solutions. • Application Architect : Senior developer responsible for integrating application systems. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT ORGANIZATION AND ROLE EXPECTATIONS • 539 • Technical Architect : Senior technical engineer responsible for coordinating and integrating the IT infrastructure and the IT technology portfolio. • Technical Engineer : Senior technical analyst responsible for researching, implementing, administering, and supporting a portion of the information technology infrastructure. • Help Desk Administrator : Responsible for handling, tracking, and resolving issues related to use of information, information systems, or the IT infrastructure. • IT Auditor : An internal or external auditor of IT responsibilities, including Data Quality and data security. 7.2.4 Hybrid Roles Hybrid roles require a mix of business and technical knowledge. Depending on the organization, people in these roles may report through the IT or business side. • Data Quality Analyst : Responsible for determining the fitness of data for use and monitoring the ongoing condition of the data; contributes to root cause analysis of data issues and helps the organization identify business process and technical improvements that contribute to higher quality data. • Metadata Specialist : Responsible for integration, control, and delivery of Metadata, including administration of Metadata repositories. • Business Intelligence Architect : A senior Business Intelligence analyst responsible for the design of the Business Intelligence user environment. • Business Intelligence Analyst / Administrator : Responsible for supporting effective use of Business Intelligence data by business professionals. • Business Intelligence Program Manager : Coordinates BI requirements and initiatives across the corporation and integrates them into a cohesive prioritized program and roadmap. 8. Works Cited / Recommended Aiken, Peter and Juanita Billings. Monetizing Data Management: Finding the Value in your Organization's Most Important Asset. Technics Publications, LLC, 2013. Print. Aiken, Peter and Michael M. Gorman. The Case for the Chief Data Officer: Recasting the C -Suite to Leverage Your Most Valuable Asset . Morgan Kaufmann, 2013. Print. Anderson, Carl. Creating a Data -Driven Organization . O'Reilly Media, 2015. Print. Arthur, Lisa. Big Data Marketing: Engage Your Customers More Effectively and Drive Value . Wiley, 2013. Print. Blokdijk, Gerard. Stakeholder Analysis - Simple Steps to Win, Insights and Opportunities for Maxing Out Success . Complete Publishing, 2015. Print. Borek, Alexander et al. Total Information Risk Management: Maximizing the Value of Data and Information Assets . Morgan Kaufmann, 2013. Print. Brestoff, Nelson E. and William H. Inmon. Preventing Litigation: An Early Warning System to Get Big Value Out of Big Data . Business Expert Press, 2015. Print. Collier, Ken W. Agile Analytics: A Value -Driven Approach to Business Intelligence and Data Warehousing . Addison -Wesley Professional, 2011. Print. Agile Software Development Ser. Dean, Jared. Big Data, Data Mining, and Machine Learning: Value Creation for Business Leaders and Practitioners . Wiley, 2014. Print. Wiley and SAS Business Ser. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "540 • DMBOK2 Dietrich, Brenda L., Emily C. Plachy and Maureen F. Norton. Analytics Across the Enterprise: How IBM Realizes Business Value from Big Data and Analytics . IBM Press, 2014. Print. Freeman, R. Edward. Strategic Management: A Stakeholder Approach . Cambridge University Press, 2010. Print. Gartner, Tom McCall, contributor. “Understanding the Chief Data Officer Role.” 18 February 2015. http://gtnr.it/1RIDKa6 . Gemignani, Zach, et al. Data Fluency: Empowering Your Organization with Effective Data Communication. Wiley, 2014. Print. Gibbons, Paul. The Science of Successful Organizational Change: How Leaders Set Strategy, Change Behavior, and Create an Agile Culture . Pearson FT Press, 2015. Print. Harrison, Michael I. Diagnosing Organizations: Methods, Models, and Processes . 3rd ed. SAGE Publications, Inc, 2004. Print. Applied Social Research Methods (Book 8) . Harvard Business Review, John P. Kotter et al. HBR's 10 Must Reads on Change Management. Harvard Business Review Press, 2011. Print. HBR's 10 Must Reads. Hatch, Mary Jo and Ann L. Cunliffe. Organization Theory: Modern, Symbolic, and Postmodern Perspectives . 3rd ed. Oxford University Press, 2013. Print. Hiatt, Jeffrey and Timothy Creasey. Change Management: The People Side of Change. Prosci Learning Center Publications, 2012. Print. Hillard, Robert. Information- Driven Business: How to Manage Data and Information for Maximum Advantage . Wiley, 2010. Print. Hoverstadt, Patrick. The Fractal Organization: Creating sustainable organizations with the Viable System Model . Wiley, 2009. Print. Howson, Cindi. Successful Business Intelligence: Unlock the Value of BI and Big Data . 2nd ed. Mcgraw -Hill Osborne Media, 2013. Print. Kates, Amy and Jay R. Galbraith. Designing Your Organization: Using the STAR Model to Solve 5 Critical Design Challenges . Jossey- Bass, 2007. Print. Kesler, Gregory and Amy Kates. Bridging Organization Design and Performance: Five Ways to Activate a Global Operation Model . Jossey- Bass, 2015. Print. Little, Jason. Lean Change Management: Innovative practices for managing organizational change . Happy Melly Express, 2014. Print. National Renewable Energy Laboratory. Stakeholder Analysis Methodologies Resource Book . BiblioGov, 2012. Print. Prokscha, Susanne. Practical Guide to Clinical Data Management. 2nd ed. CRC Press, 2006. Print. Schmarzo, Bill. Big Data MBA: Driving Business Strategies with Data Science . Wiley, 2015. Print. Soares, Sunil. The Chief Data Officer Handbook for Data Governance . Mc Press, 2015. Print. Stubbs, Evan. The Value of Business Analytics: Identifying the Path to Profitability . Wiley, 2011. Print. Tompkins, Jonathan R. Organization Theory and Public Management. Wadsworth Publishing, 2004. Print. Tsoukas, Haridimos and Christian Knudsen, eds. The Oxford Handbook of Organization Theory: Meta -theoretical Perspectives . Oxford University Press, 2005. Print. Oxford Handbooks. Verhoef, Peter C., Edwin Kooge and Natasha Walk. Creating Value with Big Data Analytics: Making Smarter Marketing Decisions . Routledge, 2016. Print. Willows, David and Brian Bedrick, eds. Effective Data Management for Schools . John Catt Educational Ltd, 2012. Print. Effective International Schools Ser. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "541 CHAPTER 1 7 Data Management and Organizational Change Management 1. Introduction or most organizations, improving data management practices requires changing how people work together and how they understand the role of data in their organizations, as well as the way they use data and deploy technology to support organizational processes. Successful data management practices require, among other factors: • Learning to manage on the horizontal by aligning accountabilities along the Information Value chain • Changing focus from vertical (silo) accountability to shared stewardship of information • Evolving information quality from a niche business concern or the job of the IT department into a core value of the organization • Shifting thinking about information quality from ‘data cleansing and scorecards’ to a more fundamental organizational capability • Implementing processes to measure the cost of poor data management and the value of disciplined data management This level of change is not achieved through technology, even though appropriate use of software tools can support delivery. It is instead achieved through a careful and structured approach to the management of change in the organization. Change will be required at all levels. It is critical to manage and coordinate change to avoid dead -end initiatives, loss of trust, and damage to the credibility of the information management function and its leadership. Data management professionals who understand formal change management will be more successful in bringing about changes that will help their organizations get more value from their data. To do so, it is important to understand: • Why change fails • The triggers for effective change • The barriers to change • How people experience change F Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "542 • DMBOK2 2. Laws of Change Experts in organizational change management recognize a set of fundamental ‘Laws of Change ’ that describe why change is not easy. Recognizing these at the beginning of the change process enables success. • Organizations don’t change, people change : Change does not happen because a new organization is announced or a new system is implemented. It takes place when people behave differently because they recognize the value in doing so. The process of improving data management practices and implementing f ormal Data Governance will have far -reaching effects on an organization. People will be asked to change how they work with data and how they interact with each other on activities involving data. • People don’t resist change. They resist being changed : Individuals will not adopt change if they see it as arbitrary or dictatorial. They are more likely to change if they have been engaged in defining the change and if they understand the vision driving the change, as well as when and how change will take pla ce. Part of change management for data initiatives involves working with teams to build organizational understanding of the value of improved data management practices. • Things are the way they are because they got that way : There may be good historic reasons for things being the way they are. At some point in the past, someone defined the business requirements, defined the process, designed the systems, wrote the policy, or defined the business model that now requires change. Understanding the origins of current data management practices will help the organization avoid past mistakes. If staff members are given a voice in the change, they are more likely to understand new initiatives as improvements. • Unless there is push to change, things will likely stay the same: If you want an improvement, something must be done differently. As Einstein famously said: “You can’t solve a problem with the level of thinking that created it in the first place.” • Change would be easy if it weren’t for all the people: The ‘technology’ of change is often easy. The challenge comes in dealing with the natural variation that arises in people. Change requires Change Agents, people who pay attention to the people and not just the systems. Change Agents actively listen to employees, customers, and other stakeholders to catch problems before they arise and execute the change more smoothly. Ultimately, change requires a clear VISION of Change Goals communicated vividly and regularly to stakeholders to get engagement, buy -in, backing, and (importantly) continued support when challenges arise. 3. Not Managing a Change: Managing a Transition Change management expert William Bridges emphasizes the centrality of transition in the change management process. He defines transition as the psychological process that people go through to come to terms with the new situation. While many people think of change solely in terms of a new beginning, Bridges asserts that Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 543 change involves moving through three distinct phases, starting with the ending of the existing state. Endings are difficult because people need to let go of existing conditions. People then enter the Neutral Zone, in which the existing state has not quite ended and the new state has not quite begun. Change is complete when the new state is established (s ee Table 35). Of these three, the Neutral Zone is the least predictable and most confusing, because it is a mix of old and new. If the people in the organization do not transition through the Neutral Zone, then the organization is at risk of slipping back into old habits and failing to sustain the change. Bridges maintains that the single biggest reason organizational changes fail is that people driving change rarely think about endings and therefore do not manage the impact of endings on people. He states: “Most organizations try to start with a beginning, rather than finishing with it. They pay no attention to endings. They do not acknowledge the existence of the neutral zone, and then wonder why people have so much difficulty with change” (Bridges, 2009). When experiencing a change, all individuals go thr ough all three phases, but at different speeds. Progression depends on factors such as past experience, personal preferred style, the degree of involvement in recognizing the problem and developing possible solutions, and the extent to which they feel push ed towards a change rather than moving towards it voluntarily. Table 35 Bridges’s Transition Phases Transition Phase Description The Ending • When we acknowledge that there are things we need to let go of. • When we recognize that we have lost something. • Exampl e: Changing jobs – even when an individual chooses to change jobs, there are still losses such as losing close working friends. The Neutral Zone • When the old way has finished but the new way isn’t here yet. • When everything is in flux and it feels like no one knows what they should be doing. • When things are confusing and disorderly. • Example: Moving to a new house. The first few days or even months after moving, the new house is not home yet and things are quite probably in turmoil. The New Beginning • When the new way feels comfortable, right, and the only way. • Example: Having a baby. After a few months in the neutral zone of turmoil, you come to a stage when you cannot imagine life without your new baby. Figure 113 Bridges’s Transition Phases Ending LosingLetting GoUnfreezing theStatus QuoThe New Beginning Embedding ChangeRefreezing Values The Neutral Zone Time Level of Management Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "544 • DMBOK2 Bridges emphasizes that while the first task of the Change Manager is to understand the Destination (or VISION) and how to get there, the ultimate goal of transition management is to convince people that they need to start the journey. When managing change and transition, the role of the Change Agent, and of any manager or leader in the process, is to help people recognize that the process and the stages of a transition are perfectly natural. The following checklist for managing transition summarizes key point s managers should be aware of as they help people transition. The Ending • Help everyone to understand the current problems and why the change is necessary. • Identify who is likely to lose what. Remember that loss of friends and close working colleagues is as important to some as the loss of status and power is to others. • Losses are subjective. The things one person grieves about may mean nothing to someone else. Accept the importance of subjective losses. Don’t argue with others about how they perceive the loss, and don’t be surprised at other people’s reactions to loss. • Expect and accept signs of grieving and acknowledge losses openly and sympathetically. • Define what is over and what is not. People must make the break at some time and trying to cling on to old ways prolongs difficulties. • Treat the past with respect. People have probably worked extremely hard in what may have been very difficult conditions. Recognize that and show that the work is valued. • Show how ending something ensures the things that matter to people are continued and improved . • Give people information. Then do it again and again and again in a variety of ways – written information to go away and read, as well as the opportunity to talk and ask questions. • Use the stakeholder analysis to map out how best to approach different individuals – understand how their perspectives might need to be engaged to initiate the change and what likely points of resistance might be. The Neutral Zone • Recognize this as a difficult phase (mix of old and new) but that everyone must go through it. • Get people involved and working together; give them time and space to experiment and test new ideas. • Help people to feel that they are still valued. • Praise people with good ideas, even if not every good idea works as expected. The Plan, Do, Study, Act (PDSA) model encourages trying things out, and learning from each cycle. • Give people information; do it again and again and again in a variety of ways. • Provide feedback about the results of the ideas being tested and decisions made. The New Beginning • Do not force a beginning before its time. • Ensure people know what part they are to play in the new system. • Make sure policies, procedures, and priorities are clear; do not send mixed messages. • Plan to celebrate the new beginning and give the credit to those who have made the change. • Give people information; do it again and again in a variety of ways. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 545 4. Kotter’s Eight Errors of Change Management In Leading Change, John P. Kotter, one of the most respected researchers in the field of Change Management , outlines eight reasons why organization s fail to execute change. These provide perspective on issues that commonly arise in the context of information and data management. 4.1 Error #1: Allowing Too Much Complacency According to Kotter, the biggest mistake people make when trying to change organizations is plunging ahead without first establishing a high enough sense of urgency among their peers and superiors. (This is related to the need to drive up dissatisfaction with the status quo identified in the Gleicher formula; see Section 6. ) Kotter’s analysis provides valuable pointers for Change Managers looking to avoid the errors of others. Change Agents often: • Overestimate their ability to force big changes on the organization • Underestimate how difficult it can be to shift people out of their comfort zones • Don’t see how their actions and approach might reinforce the status quo by driving up defensiveness • Rush in where angels fear to tread – kicking off change activities without sufficient communication of what change is required or why change is required (the Vision) • Confuse urgency with anxiety, which in turn leads to fear and resistance as stakeholders retrench (often quite literally) in their silos While it is tempting to think that in the face of organizational crisis , complacency would not be a problem, often the opposite is the case. Stakeholders often cling to the status quo in the face of too many (often conflicting) demands for change (which are often processed as ‘if everything is important, then nothing is import ant’). 4.1.1 Examples in Information Management Context Table 36 describes examples of how complacency can manifest in an information management context: Table 36 Complacency Scenarios Example Scenario How it might manifest Response to a Regulatory Change “We’re OK. We haven’t been fined under the current rules.” Response to Business Change “We’ve been supporting the business successfully for years. We’ll be OK.” Response to Technology Change “That new technology is unproven. Our current systems are stable and we know how to work around issues.” Response to Problems or Errors “We can assign a troubleshooting team to that and patch the issues up. There are bound to be some people available in [Insert name of Department or Team here].” Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "546 • DMBOK2 4.2 Error #2: Failing to Create a Sufficiently Powerful Guiding Coalition Kotter identifies that major change is almost impossible without the active support from the head of the organization and without a coalition of other leaders coming together to guide the change. Leadership engagement is especially important in Data Governance efforts, as these require significant behavioral changes. Without commitment from top leaders, short- term self- interest will outweigh the argument for the long -term benefits of better governance. A Guiding Coalition is a powerful and enthusiastic team of volunteers from across the organization that helps to put new strategies into effect and transform the organization. A key challenge in developing a Guiding Coalition is identifying who needs to be involved. (See Section 5.2 .) 4.3 Error #3: Underestimating the Power of Vision Urgency and a strong guiding team are useless without a clear, sensible vision of the change . The vision provides the context of the change effort. It helps people understand the meaning of any individual component. A well - defined and communicated vision can help drive the level of energy required to properly implement the change. Without a public statement of vision to guide decision -making, every choice risks becoming a debate and any action could derail the change initiative or undermine it. Vision is not the same thing as planning or program management. The vision is not the project plan or project charter or a detailed breakdow n of all the components of the c hange. A Vision is a Clear and Compelling Statement of where the Change is leading. Communicating vision means connecting with people. For data management initiatives, the vision must articulate the challenges with existing data management practices, the benefits of improvement, and the path to get to a better future state. 4.3.1 Example in Information Management All too often in information management, the vision for a particular project is presented as the implementation of a new technology. The technology, while important, is not the change and not the vision. What the organization can do with the technology constitutes the vision. For example, stating, “We will implement a new integrated financial reporting and analytics suite built on [insert name of technology here] by the end of Quarter 1” is a laudable and measurable goal. However, it does little to communicate a clear and compelling statement of where the change will lead. On the other hand, asserting, “We will improve the accuracy and timeliness of financial reports and make them more readily available to all stakeholders. Improved understanding of how data flows into and out of our reporting processes will support trust in our numbers, save time, and reduce unnecessary stress during end- of- period processes. We will take our first step to achieve this by implementing [System X] by the end of Q1” clarifies what will be done, and why it is being done. If you can point out the benefits of the change to the organization, you will build support for change. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 547 4.4 Error #4: Under Communicating the Vision by a Factor of 10, 100, or 1000 Even if everyone agrees that the current situation is unsatisfactory, people will still not change unless they perceive the benefits of change as a significant improvement over the status quo. Consistent, effective communication of the vision, followed by action, is critical to successful change management. Kotter advises that communication comes in both words and deeds. Congruence between the two is critical for success. Nothing kills a change effort as fast as a situation where people receive the message: ‘Do as I say, not as I do.’ 4.5 Error #5: Permitting Obstacles to B lock the Vision New initiatives fail when people feel disempowered by huge obstacles in their path, even when they fully embrace the need for and direction of the proposed change. As part of its transformation, the organization must identify and respond to different kinds of roadblocks: • Psychological : Roadblocks that exist in people’s heads must be addressed based on their causes. Do they stem from fear, lack of knowledge, or some other cause? • Structural : Roadblocks due to organizational structures such as narrow job categories or performance appraisal systems that force people to choose between the Vision and their own self -interest must be addressed as part of the change management process. Change management should address structural incentives and disincentives to change. • Active resistance : What roadblocks exist due to people who refuse to adapt to the new set of circumstances and who make demands that are inconsistent with the Transformation? If key members of the organization make the right noises about the change vision but fail to alter their behaviors or reward the required behaviors or continue to operate in incompatible ways, the execution of the vision will falter and could fail. Kotter calls on “smart people” in organizations to confront these obstacles. If they do not, others will feel disempowered and change will be undermined. 4.6 Error #6: Failing to Create Short -Term Wins Real change takes time. Anyone who has ever embarked on a fitness regime or a weight -loss plan knows that the secret to keeping going is to have regular milestone targets that keep up momentum and motivation by marking progress. Anything that involves a lo ng-term commitment and investment of effort and resources requires some element of early and regular feedback of success. Complex change efforts require short- term goals in support of long -term objectives. Meeting these goals allows the team to celebrate and maintain momentum. The key thing is to create the short -term win rather than merely hoping for it. In successful transformations, managers actively establish early goals, achieve these goals, and reward the team. Without systematic efforts to guarantee success, change is likely to fail. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "548 • DMBOK2 4.6.1 Examples in Information Management Context In an information management context, the short- term wins and goals often arise from the resolution of an identified problem. For example, if the development of a Business Glossary is a key deliverable of a Data Governance initiative, a short- term win might come from solving a problem related to inconsistent understanding of data (i.e., two business areas report different KPI results because they used different rules in their calculations). Identifying the problem, solving it, and linking the solution to the overall long -term vision for the change allow the team to celebrate that goal and demonstrate the vision in action. It also provides valuable collateral for communication about the vision and helps to reinforce the change message. 4.7 Error #7: Declaring Victory Too Soon All too often in Change projects, particularly ones stretching over several years, there is a temptation to declare success at the first major performance improvement. Quick wins and early wins are powerful tools to keep up momentum and morale. However, an y suggestion that the job is done is usually a mistake. Until the changes are embedded in the culture of the organization , new approaches are fragile and old habits and practices can reassert themselves. Kotter suggests that changing an entire company can take between three and ten years. 4.7.1 Example in Information Management Context The classic example of ‘Mission Accomplished’ syndrome is the scenario where the implementation of a technology is viewed as the route to improving the management of information or resolving an issue with the quality or reliability of data. Once the technology has been deployed, it can be difficult to keep the project moving towards the goal, particularly if the overall vision has been poorly defined. Table 37 captures several examples related to the consequences of declaring victory too soon. Table 37 Declaring Victory Too Soon Scenarios Example Scenario How it might manifest Addressing Data Quality “We’ve bought a Data Quality tool. That’s fixed that now.” • No one in the organization is reviewing or acting on Data Quality reports Confusing capability delivery with implementation and operation “We’ve implemented the reporting stack for Regulation X. We are now compliant with the legislation.” • Regulatory requirement changes • Nobody is reviewing or acting on issues identified in reporting Migration of data “All the data in System X is now in System Y.” • Record counts match, but the data in System Y is incomplete, or truncated due to failures in the migration process. Manual interventions needed Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 549 4.8 Error # 8: Neglecting to A nchor Changes Firmly in the Corporate C ulture Organizations don’t change, people change. Until new behaviors are embedded in to the social norms and shared values of an organization, they are subject to decay and degradation as soon as the focus of the change effort is removed. Kotter is clear: You ignore culture at your peril when engaging in any change activity. The two keys to anchoring the change in the culture of the organization are : • Consciously showing people how specific behaviors and attitudes have influenced performance. • Taking sufficient time to embed the change of approach in the next generation of management. 4.8.1 Example in Information Management Context This risk highlights the importance of human factors in the overall change that might be implemented to bring about improvements in Data Governance execution, Metadata management and use, or Data Quality practices (to name but three). For example, an organization may have introduced a Metadata tagging requirement on all documentation to support automated classification and archiving processes in their content management system. Staff begin to comply in the first few weeks, but as time passes, they revert to old habits and do not correctly tag documents, leading to a massive backlog of unclassified records that needs to be reviewed manually to bring them into line with requirements of the technology solution. This highlights the simple fact that improvements in Information Management are delivered through a combination of processes, people, and technology. Very often that middle component is missed, leading to sub - optimal delivery and backsliding on progress ma de. It is important when introducing new technology or new processes to consider how the people will carry the change forward and sustain the gains. 5. Kotter’s Eight Stage Process for Major Change In addition to the Eight Errors of Change Management, Kotter recognizes a set of common obstacles to change: • Inward focused cultures • Paralyzing bureaucracy • Parochial politics • Low levels of trust • Lack of teamwork • Arrogance • Lack of or failure of leadership • Fear of the unknown Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "550 • DMBOK2 To combat these, he proposes an eight -step model for major change. Kotter’s model provides a framework within which each of these issues can be addressed in a way that supports sustainable long -term change. Each step is associated with one of the fundamental errors that under mine transformation efforts. The first four steps of the model soften entrenched status quo positions. As Kotter says, this effort is only needed because change is not easy. The next three steps (5 to 7) introduce new practices and ways of working. The last step locks the changes in place and provides the platform for future gains and improvement. Kotter advises that there is no short cut in following these steps. All successful change efforts must go through all eight steps. Focusing on steps 5, 6, and 7 is tempting. However, that does not provide a solid foundation for sustaining the change (no vision, no Guiding Coalition, no dissatisfaction with the status quo). Likewise, it is important to reinforce each step of as you move through the process, using quick wins to bolster the vision and the communication and highlight the problems with the statu s quo. Figure 114 Kotter’s Eight Stage Process for Major Change 5.1 Establishing a Sense of Urgency People will find a thousand ways to withhold cooperation from something they think is unnecessary. A clear and compelling sense of urgency is required to motivate a sufficient critical mass of people to support a change effort. Winning co -operation and collaboration requires a rallying call. The opposite of urgency is complacency. When complacency is high, it is difficult if not impossible to put together a sufficiently powerful group to create the change vision and guide the change effort. In rare instances, individuals can make some headway in the face of complacency but this is almost inevitably unsustainable. In the information management context, several factors can create a sense of urgency: • Regulatory changes • Threats to security of information • Risks to business continuity • Changes to business strategy • Mergers and acquisitions • Regulatory audit or litigation threats • Changes to technology • Changes to capability of competitors in the market • Media commentary about an organization’s or an industry’s information management issues 1-Establishing a Sense of Urgency 2-Ceating the Guiding Coalition 3-Developing a Vision and a Strategy 4-Communicating the Change Vision5-Empowering Broad -based Action 6-Creating Short T erm Wins 7-Consolidating Gains and Producing More Change 8-Anchoring New Approaches in the Culture Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 551 5.1.1 Sources of Complacency Kotter identifies nine reasons why organizations and people can be complacent. (See Figure 115) • In the absence of a visible crisis, it is difficult to raise a sense of urgency. • The trappings of success can drown out the urgency of some situations. • Measuring staff against low performance standards or standards that don’t compare against external benchmarks or internal long -term trends. • Overly narrow functional goals, with different performance metrics for different functional units, can lead to a situation where no one is responsible when overall organizational performance is poor or suffering. • If internal planning and control systems are (or can be) rigged or gamed to make it easy for everyone to reach their goals, it is easy to be complacent. • If the only source of performance feedback is from the faulty internal systems, there is no sanity check of the correctness of complacency. • Where problems are identified or where external performance feedback is gathered, it is often attacked as being damaging to morale, hurtful to others, or likely to cause an argument. Rather than take the information as an input into an evaluation of organization performance , the culture is to ‘kill the messenger’. • For very simple psychological reasons people don’t accept things they don’t want to hear. When evidence of a big problem appears, people will often ignore the information or reinterpret it in a less painful way. • Even in organizations where the first eight challenges are not significant, there is a risk that ‘happy talk’ from senior management or senior figures in the organization can create an unwarranted sense of security and success. Often this ‘happy talk’ is the result of a history of past successes. Past success can give individuals an ego and create an arrogant culture. Both factors can keep the sense of urgency low and hamper change. A good rule of thumb in any change initiative is never to underestimate the power of forces that might reinforce complacency and promote the status quo. The challenge of complacency must be addressed. An organization can’t make any important decisions with out tackling the real issues. 5.1.2 Pushing up the Urgency Level To push up the urgency level requires removal of the sources of complacency or reduction of their impact. Creating a strong sense of urgency requires that leaders take bold or even risky actions. It is worth recalling how Deming admonished management to institute leadership as part of his 14 Points of Transformation. 98 98 In Out of the Crisis (1982), W. Edwards Deming published his 14 Points for Management Transformation. http://bit.ly/1KJ3JIS . Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "552 • DMBOK2 Figure 115 Sources of Complacency Bold means doing something that might cause short term pain, not just something that looks good in a marketing email. In other words, it requires an adoption of the new philosophy (to borrow again from Deming). Moves bold enough to reduce complacency tend to cause short -term conflict and anxiety. However, if the conflict and anxiety can be channeled towards the change vision then a leader can capitalize on the short- term discomfort to build the long -term goals. Bold moves are difficult in the absence of supported and supportive leadership. Cautious senior managers who are unable to increase the sense of urgency will reduce the ability of an organization to change. 5.1.3 Using Crisis with Care One way to push up urgency levels is to latch on to a visible crisis. It is sometimes said that major change is not possible until the very economic survival of the organization is at risk. However, it is not necessarily that the change comes even then. An economic or financial crisis in an organization can often result in scarce but necessary resources being difficult to come by to support the change vision. T oo many visible resourcesAbsence of a major and visible crisis Human capacity for denial of problems, especially when busy or stressed T oo much ‘’Happy Talk’’ (Group Think) ‘’Killing the Messenger’’ -Low Candor/Low Confrontation Cultures A lack of performance feedback from external sourcesInternal measurement focusing on wrong performance measures Organizational Structures that focus employees on narrow functional goalsLow overall performance standards Complacency Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 553 It is possible to create a perceived crisis by bombarding the organization with information about problems, potential problems, potential opportunities, or by setting ambitious goals that disrupted the status quo. Kotter suggests that it is often easier to create a problem that (coincidentally) you have the plan to address. 5.1.4 The Role of Middle and Lower -level M anagers Depending on the scale of the target for the change (e.g., a department or business unit versus an entire organization), the key players will be the managers in charge of that unit. They will need to be able to reduce complacency in the teams under their direct control. If they have enough autonomy, they may be able to do this regardless of the pace of change in the rest of the organization. If there is not sufficient autonomy, then a change effort in a small unit can be doomed from the start as the external forces of inertia come to bear. Often senior executives need to reduce those forces. However, middle or lower -level managers can drive this kind of change if they act in a strategic way. For example, if they use analysis to clearly show the impact of not making the required change on a key strategic project. This is particularly effective when the debate can be diffused by directing it onto an external group such as an external consultancy who may have helped with the analysis. 5.1.5 How M uch Urgency is Enough? A sense of urgency about a problem leads people to conclude that the status quo is unacceptable. To sustain transformation for the long term, support from a critical mass of managers is required. Kotter suggests 75%. However, creating too much urgency can be counterproductive. Too much urgency may result in competing visions of change or cause a focus on ‘firefighting’. A sufficiently compelling sense of urgency will help get the change process started and give it momentum. Sufficient urgency will also help in getting the right level of leadership in the Guiding Coalition. Ultimately, the sense of urgency needs to be stro ng enough to prevent complacency from reasserting itself after initial successes are achieved. One key approach is to tap into the ‘voice of the customer’ and speak to external customers, suppliers, shareholders, or other stakeholders about their perspective on the level of urgency that is being created. 5.2 The Guiding Coalition No one person has all the answers, or all the insights necessary to create a vision, or has the right range and variation of connections to support the effective communication of a vision. For successful change, two specific scenarios must be avoided: • The Lone CEO / Lone Champion • The Low Credibility Committee Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "554 • DMBOK2 The Lone CEO scenario puts the success or failure of the change effort in the hands of one person. The pace of change in most organization these days is such that one person cannot possibly manage it all. The pace of decision -making and communication slows, unless decisions ar e being taken without a full assessment of the issues. Either option is a recipe for failure. The Low Credibility Committee arises where a capable champion is given a ‘task force’ with representatives from a variety of functional departments (and perhaps some external consultants). What the task force lacks is sufficient representation (if any) from people at a senior level on the executive pecking order. If it is seen as “important but not that important” (again, because of the lack of commitment from top brass), people don’t feel motivated to get a true understanding of the situation. Inevitably, the task force fails. It is essential to create a suitable Guiding Coalition that has the necessary management commitment to support the urgency of the need for change. In addition, the team has to support effective decision -making – which requires high levels of trust within t he team. A Guiding Coalition that works as a team can process more information faster. It also speeds the implementation of ideas because the decision -makers with power are truly informed and committed to key decisions. An effective Guiding Coalition has four key characteristics: • Position Power : Are enough key players on board, especially main line managers, so that those who are left out can’t easily block progress? • Expertise : Are relevant points of view adequately represented so that informed and intelligent decisions will be made? • Credibility : Are enough people with good reputations in the organization on the team so that it will be taken seriously? • Leadership : Does the team have enough proven leaders on board to drive the change process? Leadership is a key concern. There must be a good balance between management and leadership skills on the Guiding Coalition. Management keeps the whole process under control. Leadership drives the change. One without the other will not achieve a sustainabl e result. Key issues that arise in the context of building your Guiding Coalition include: How many people do I need to help me define and guide this change? The answer to this is a painfully consultant -like “It depends”, but the size of the coalition relates to the size of the overall group being influenced. A balance needs to be struck between having a group that is too big and having a group that leaves key stakeholders feeling left ‘outside the tent’. Who should be involved or invited to join the Guiding Coalition? The Guiding Coalition differs from a formal project or program steering committee in that it needs to provide a platform for influence throughout the organization. As such, the coalition needs to include representatives from different stakeholder communiti es. However, it is not a general stakeholder requirements gathering forum either. Seek perspectives from people who may be impacted in the information value chain of the organization. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 555 One key attribute of the members of the Guiding Coalition is their ability to influence their peers, either through formal authority in the hierarchy or through their status and experience in the organization. Behavior is key in the Guiding Coalition . In the formulation of the Guiding Coalition, change leaders need to avoid behaviors that weaken the effectiveness, function, and reach of the team. For example, avoid: • Naysaying : Naysayers can hamper positive and open dialogue needed for the Guiding Coalition to develop creative ideas, to refine, implement, and evolve the change vision and identify opportunities for growth. • Distraction : Guiding Coalition team members need to be focused on the change activity. Unfocussed individuals can take the team off course, leading to delays or the failure to capitalize on early wins. • Selfishness : The Guiding Coalition’s efforts move the organization as a whole and affect everyone. Hidden agendas must not be allowed to derail the team’s efforts. 5.2.1 The Importance of Effective Leadership in the Coalition There is a difference between management and leadership . A Guiding Coalition with good managers but no leaders will not succeed. Missing leadership can be addressed by hiring from the outside, promoting leaders from within, and encouraging staff to step up to the challenge of leading. When putting your coalition together you need to be wary of what Kotter terms ‘Egos’, ‘Snakes’, and ‘Reluctant Players’. ‘Egos’ are individuals who fill up the room and do not let others contribute. ‘Snakes’ are people who create and spread mistrust and di strust. ‘Reluctant Players’ are (usually) senior figures who see a moderate need for the change but don’t fully grasp the urgency. Any of these personality types can hijack or undermine the change effort. Efforts should be made to keep them off the team or manage them closely to keep them on message. 5.2.2 Example in Information Management Context In the context of an information management change initiative, the Guiding Coalition can help the organization identify opportunities to link initiatives in different areas that are engaged in different aspects of the same overall change. For example, in response to a regulatory requirement, a firm’s in- house counsel may have begun to develop a map of data flows and processes in the organization. At the same time, a data warehousing initiative may have begun to map the lineage of data for v erification of reporting accuracy and quality. A Data Governance change leader might bring the head of legal and the head of reporting together on their Guiding Coalition to improve documentation and control of information processes in the context of Data Governance. This in turn might require input from the front- line teams using and creating data to understand the impacts of any proposed changes. Ultimately, a good understanding of the information value chain will help identify potential candidates to include in the Guiding Coalition. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "556 • DMBOK2 5.2.3 Building an Effective Team An effective team is based on two simple foundations: trust and a common goal. An absence of trust is often caused by a lack of communications and other factors such as misplaced rivalry. The classic ‘Business vs. IT’ divide is a good example of where trust breaks down. To build trust, engage in team building activities that create and promote mutual understanding, respect, and caring. In achieving that mutual understanding, though, care should be taken to avoid ‘Group Think’. 5.2.4 Combating Group Think ‘Group Think’ is a psychological effect that arises in highly coherent and cohesive groups, particularly ones that are isolated from sources of information that might contradict their opinions, or those that are dominated by a leader who encourages people to agree with his or her position rather than opening up discussion. In Group Think, everyone goes along with a proposal even where they have reservations about it. Group Think is probably operating if : • No one raises objections • No alternatives are offered • Different perspectives ar e quickly dismissed and die for ever • Information that might challenge the thinking is not actively sought To prevent Group Think it is important to : • Encourage all participants to follow the scientific method of gathering data to help understand the nature and causes of a problem • Develop a list of criteria for evaluating all decisions • Learn to work together efficiently so that Group Think is not the short cut to getting things done faster • Encourage brainstorming • Leaders should speak last • Actively search for outside knowledge and input into meetings • Once a solution has been identified, have the team develop not just one plan but also a ‘Plan B’ (which forces them to rethink assumptions in the original plan) 5.2.5 Examples in Information Management Context Group Think can arise in a variety of contexts. One potential area is the traditional ‘Business vs IT divide’, in which different parts of the organization are resistant to changes proposed by the other. Another potential scenario is where the organization’s goal is to become data- driven with a focus on analytics and data gathering, which may result in privacy, security, or ethical issues in relation to information handling being discounted or deprioritized in the overall work plan. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 557 There are many reasons to apply Data Governance discipline in organizations. One key function is to ensure clarity about models and methods to be applied. This clarity will allow issues such as the Business / IT divide or balancing of competing priorities to be addressed appropriately and consistently. 5.2.6 Common Goals If every member of the Guiding Coalition is pulling in a different direction, trust will break down. Typical goals that bind people are a commitment to excellence or a desire to see the organization perform at the highest level possible in a given area. These goals should not be confused with the vision for change but should be complementary to it. 5.3 Developing a Vision and Strategy A common mistake in change management efforts is to rely on either authoritarian decree or micromanagement to get the change moving. Neither approach is effective if the change situation is complex. If the goal is behavior change, unless the boss is very powerful, authoritarian decree approaches work poorly even in simple situations. Without ‘the power of kings’ behind it, an authoritarian decree is unlikely to break through all the forces of resistance. The Change Agents tend to be ignored, undermined, or worked around. Almost inevitably, some change resister will call the Change Agent’s bluff to test the authority and clout behind the change process. Micromanagement tries to get around this weakness by defining in specific detail what employees should do and then monitoring compliance. This can overcome some of the barriers to change but will, over time, take increasing lengths of time as management ha ve to spend more time detailing the work practices and methods for the new changed behaviors as the level of complexity associated with the change increases. The only approach that consistently allows Change Agents to break through the status quo is to base change on a clear and compelling vision that provides momentum. Figure 116 Vision Breaks Through Status Quo Authoritarian DecreeMicromanagement Vision Forces that support the status quo Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "558 • DMBOK2 5.3.1 Why Vision is Essential A vision is a picture of the future with some implicit or explicit commentary about why people should strive to create that future. A good vision shares three important purposes: Clarification, motivation, and alignment. • Clarification : A good vision clarifies the direction of change and simplifies a range of more detailed decisions by setting key parameters. An effective vision (and supporting back up strategies) helps resolve issues that arise out of disagreements about direction or confusion about the motivation or drivers for the change. Endless debates can be avoided with a simple question: Is the planned action in line with the vision? Similarly, the vision can help clear the decks of clutter, allowing the team to focu s efforts on priority projects that are contributing to the transformation effort. • Motivation : A clear vision motivates people to take steps in the right direction, even if the initial steps are personally painful. This is particularly true in organizations where people are being forced out of their comfort zones on a regular basis. When the futur e is depressing and demoralizing, the right vision can give the people an appealing cause to fight for. • Alignment : A compelling vision helps to align individuals and coordinate the actions of motivated people in an efficient way. The alternative is to have a flurry of detailed directives or endless meetings. Experience shows that without a shared sense of direction interdependent people can end up in cycles of constant conflict and nonstop meetings. 5.3.2 The Nature of an Effective Vision A vision can be mundane and simple. It doesn’t need to be grand or overarching. It is one element in the system of tools and processes for change; this system also includes strategies, plans, budgets, and more. Nevertheless, a vision is a very important factor bec ause it demands that teams focus on tangible improvements. An effective vision has several key characteristics: • Imaginable : It conveys a picture of what the future looks like. • Desirable: It appeals to the long -term interests of employees, customers, shareholders, and other stakeholders. • Feasible : It comprises realistic and attainable goals. • Focused: It is clear enough to provide guidance in decision -making. • Flexible: It is general enough to allow individuals to take the initiative and to allow for alternative plans and responses when conditions or constraints change. • Communicable : It is easy to share and to communicate in five minutes or less. The key test for the effectiveness of a vision is how easy it is to imagine it and for it to be desirable. A good vision can demand sacrifice but must keep the long -term interests of the people involved in scope. Visions that don’t focus for the long term on the benefits to people are eventually challenged. Likewise, the vision must be rooted in the reality of the product or service market. In most markets, reality is that the end customer needs to be considered constantly. Key questions to ask are: • If this became real, how would it affect customers (internal and external)? Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 559 • If this became real how would it affect shareholders? Will it make them happier? Will it deliver longer -term value for them? • If this became real, how would it affect employees? Would the work place be better, happier, less stressed, more fulfilling? Will we be able to become a better place to work in? Another key test is the strategic feasibility of the vision. A feasible vision is more than a wish. It may stretch resources and stretch capabilities but people recognize that is can be reached. Feasible does not mean easy, however. The vision must be chal lenging enough to force fundamental rethinking. Regardless of which stretch goals are set, the organization must ground that vision in a rational understanding of the market trends and the organization’s capability. The vision must be focused enough to guide people but not so rigid that it handcuffs staff to increasingly irrational modes of behavior. Often , the best approach is to aim for simplicity of vision while at the same time embedding enough specific hooks that the vision is still a valuable cornerstone and reference point for decision - making: It is our goal to become the world leader in our industry within 5 years. In this context, leadership means managing information more effectively to deliver greater revenues, more profit, and a more rewarding place for our people to work. Achieving this ambition will require a solid foundation of trust in our ability to make decisions, clarity in our internal and external communications, an improved understanding of the information landscape in which we operate, and rational investments in appropriate tools and technologies to support a data- driven culture and ethos. This culture will be trusted and admired by shareholders, customers, employees, and communities. 5.3.3 Creating the Effective Vision Kotter advises that creating an effective vision is an iterative process that must have several clear elements to be successful. • First draft : A single individual makes an initial statement reflecting their dreams and the needs of the market place . • Role of the Guiding Coalition: The Guiding Coalition reworks the first draft to fit the wider strategic perspective. • Importance of teamwork : The group process never works well without teamwork. Encourage people to engage and contribute. • Role of the head and heart : Both analytical thinking and ‘blue sky dreaming’ are required throughout the activity. • Messiness of the process: This won't be a straightforward procedure; there will be much debate, rework, and change. If there isn’t, something is wrong with the vision or the team. • Time frame : The activity is not a one meeting deal. It can take weeks, months, or even longer. Ideally, the vision should be constantly evolving. • End product: A direction for the future that is desirable, feasible, focused, flexible, and can be conveyed in five minutes or less. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "560 • DMBOK2 Figure 117 Management/Leadership Contrast 5.4 Communicating the Change Vision A vision only has power when those involved in the change activity have a common understanding of its goals and direction, a common outlook on the desired future. Problems that commonly arise with communicating the vision include: • Failure to communicate, or to communicate enough. • Poor communication: Cumbersome or unwieldy wording that hides the sense of urgency; as a result, people don’t listen carefully. • Not communicating far enough : Managers are trained to communicate up and down. Leaders need to communicate out and into broader constituencies. This range of communication requires leaders to have a clear sense of the problem and how it can be solved. Another challenge is dealing with the questions that are about the vision, from stakeholders, the Guiding Coalition, and the team implementing the change itself. Often the Guiding Coalition spends a lot of time working out these questions and preparing answers to them only to dump them on the organization in one quick hit (an FAQ page, notes to a briefing). The resulting information overload clouds the vision, creates short - term panic and resistance. Given that, in the average organization, the change message will account for not much more than one -half of one percent of the total communication going to an employee it is clear that simply dumping information will not be effective. The message needs to be communicated in a way that increases its effectiveness and amplifies the communication. Kotter identifies seven key elements in effective communication of vision: • Keep it simple: Strip out the jargon, internal vocabulary, and complex sentences. • Use metaphor, analogy, and example: A verbal picture (or even a graphical one) can be worth a thousand words. Plans Specific steps and timetables to implement strategiesStrategies A logic for how the vision can be achievedVision A sensible and appealing picture of the future Budgets Plans converted into financial projections and goalsLeadership Management Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 561 • Use multiple forums: The message needs to be communicable across a variety of different forums from elevator pitch to broadcast memo, from small meeting to an all -hands briefing. • Repeat, repeat, repeat : Ideas have to be heard many times before they are internalized and understood. • Lead by example: Behavior from important people needs to be consistent with the vision. Inconsistent behavior overwhelms all other forms of communication. • Explain seeming inconsistencies : Loose ends and unaddressed disconnects undermine the credibility of all communication. • Give and take : Two -way communication is always more powerful than one- way communication. 5.4.1 Examples in Information Management Context In an information management context, the failure to define or communicate a clear and compelling vision for a change can often be seen in initiatives where a new technology or capability is being rolled out driven by a focus on technology deployment. In the absence of an understanding or appreciation of the potential information -handling benefits from the new technology or methods, there may be resistance on the part of stakeholders to adopt new ways of working. For example, if an organization is implementing Metadata -driven document and content management processes, business stakeholders may not engage with the up -front effort of understanding or applying Metadata tagging or classification of records if there is no clearly communicated vision of how this will be a benefit to the organization and to them. Absent that, the otherwise valuable initiative may get bogged down with lower than required levels of adoption and compliance. 5.4.2 Keeping it Simple It is hard to emotionally connect with language that is unnatural, densely written, or difficult to understand. These examples illustrate the communication problems that can arise when the vision is not kept simple. The example below illustrates this point. Our goal is to reduce our mean ‘time to repair’ parameter so that it is demonstrably lower than all major competitors in our target geographic and demographic markets. In a similar vein, we have targeted new -product development cycle times, order processin g times, and other customer -related process vectors for change. Translation: “We’re going to become faster than anyone in our industry at meeting customer needs.” When the vision is articulated in a simple way, it is easier for teams, stakeholders, and customers to understand the proposed change, how it might affect them, and their role in it. This, in turn, helps them to more easily communicate it to their peers. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "562 • DMBOK2 5.4.3 Use Many Different Forums The communication of vision is usually more effective when different channels are used. There are various reasons for this, ranging from the fact that some channels can be overloaded with information or with ‘baggage’ of previous change initiatives, to the fact that different people interpret and process information differently. If people are being hit with the same message through different channels , it increases the likelihood that the message will be heard, internalized, and acted on. Related to this ‘mu lti-channel / multi -format’ approach is the need to keep repeating the vision and communicating progress. 5.4.4 Repetition, Repetition, Repetition In many respects, change vision and change messages are like water in a river that encounters a boulder that must be overcome. The water does not burst through the dam immediately (unless it has a lot of force behind it, in which case it tends to do so des tructively) , but over time, through iterative erosion , the water wears down the boulder so it can flow around it. In the same way, change initiatives have to apply iterative retellings of the change vision in different forums and formats to engender a change that is ‘sticky’. Which of these scenarios would be more effective? • Senior management put out a video message to all staff and a voicemail drop announcement to brief everyone on the change. Details on execution will follow from line managers. The intranet carries three articles over the next six months about the Vision, and there is a briefing session at the quarterly management conference (delivered at the end of the day). The plan includes six instances of communication with no fleshing out of details. • Senior management undertake to find four chances each day to have a change conversation and tie it back to the ‘Big Picture’. They in turn task their direct reports with finding four chances, and with tasking their direct reports to find four chances. So, when Frank is meeting Product Development, he asks them to review their plans in the context of the Big Vision. When Mary is presenting a status update she ties it back to the contribution to the Vision. When Garry is presenting negative internal audit fin dings, he explains the impact in terms of the Vision. At each level of management, per manager there are countless opportunities for communication per year where the vision can be referenced. (This is also known as “Adopting the New Philosophy” and “Instit uting Leadership”, which are key points in W. Edwards Deming’s 14 Points for Transformation in Quality Management. ) 5.4.5 Walking the Talk There is no substitute for leadership by example. It makes the values and cultural aspects of the desired change tangible in a way that no amount of words can do. If for no other reason than that senior managers walking the talk engenders the development of stories about the vision and triggers discussion about the vision, this is an exceptionally powerful tool. The corollary is that telling people one thing and doing the opposite sends a clear message that the vision isn’t that important and can be ignored when push comes to shove. Nothing undermines the change vision and efforts more than a senior member of the Guiding Coalition acting incongruently to the vision. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 563 5.4.6 Example in Information Management Context In information management context, failure to ‘Walk the Talk’ can be as simple as a senior manager sending files containing personal information about customers by an unsecured or unencrypted email channel in contravention of the information security policy, but receiving no sanction. It can also be as simple as the team leading an information governance initiative applying the principles and rigor they are asking the rest of the organization to adopt to their own activities, information handling, reporting, and responses to issues and errors. Consider the impact in the implementation of a Metadata management project if the team were to apply Metadata standards and practices to their own internal project records. If nothing else, it would help them to understand the practicalities of the change, but would also provide them with a good demonstration for others of the benefits of properly tagged and classified records and information. 5.4.7 Explaining Inconsistencies Sometimes the inconsistency is unavoidable. It may be that, for tactical or operational reasons, or simply to get things moving within the overall organization system, a Change Agent might need to take an action that looks at variance with the stated vision. When this happens, it must be handled and addressed carefully to ensure the vision is sustained, even if a ‘scenic route’ is being taken. Examples of inconsistencies that can arise might include the use of external consultants when the organization is seeking to reduce costs or headcount. “Why is the organization bringing in these expensive suits when we’re rationing printer paper?” people may ask. There are two ways to deal with apparent inconsistency. One of them is guaranteed to kill your vision. The other gives you a fighting chance of being able to keep things on track. The first option is to either ignore the question or react defensively and shoot the messenger. Invariably, this winds up in an embarrassing climb down where the inconsistency is removed, and not always in a manner that is beneficial to the long -term objectives of the change. The second option is to engage with the question and explain the rationale for the inconsistency. The explanation must be simple, clear, and honest. For example, an organization bringing in consultants might respond like this: We appreciate that it looks odd spending money on consultants when we are shaving costs everywhere else to achieve our vision of being lean, mean, and sustainably profitable. However, to make the savings sustainable we need to break out of old habits of th inking and learn new skills. That requires us investing in knowledge. And where we don’t have that knowledge internally we must buy it in in the short term, and use that opportunity to build the knowledge up internally for the future. Every consultant is a ssigned to a specific project. And every project team has been tasked with learning as much as possible about their new function through shadowing the consultants and using them for formal training. In this way, we will make sure that we will have sustainable improvements into the future. The key thing is to be explicit about the inconsistency and explicit about why the inconsistency is valid, and how long it will exist for if it is only a transient inconsistency. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "564 • DMBOK2 5.4.8 Example in Information Management Context Explaining inconsistencies is a very good example of the importance of Data Governance models that create agreed upon protocols for decision -making and promote the formal recognition and control of exceptions to rules. For example, if a governance standard requires that no testing should be done with live production data but a project requires this to verify data matching algorithms or to prove the effectiveness of data cleansing routines, then there must be a clear and explicit explanation of this variance from the expected standard. That is arrived at through appropriate governance controls. Where that project executes testing using live data without having appropriate approvals and risk assessments in place, then there should be a sanction (‘walk the talk’) or the basis for the non- application of the sanction should be equally clearly and explicitly explained. 5.4.9 Listen and Be Listened To Stephen Covey advises people who want to be highly effective to “Seek first to understand, then to be understood .” In other words, listen so that you will be listened to (Covey, 2013). Often the leadership team don’t quite get the vision right, or they encounter a barrier or bottle neck that could have been avoided if they had been better informed. This lack of information leads to expensive errors and weakens the buy -in to and commitment to the Vision. Two -way conversations are an essential method of identifying and answering concerns people have about a change or about a vision for change. The Voice of the Customer is as important to the definition of and development of the vision as it is to any metric of quality in the data itself. And if every conversation is regarded as an opportunity to discuss the vision and to illicit feedback then, without having to formally tie people up in meetings, it is possible to have thousands of hours of discussion and to evolve the vision and how to execute it effectively. 5.4.10 Example in Information Management Context In an information management context, two -way communication is best illustrated by a scenario where the IT function’s view is that all data that is needed by key business stakeholders is available in a timely and appropriate manner, but business stakeholders are consistently expressing frustration at delays in getting information they require to do their jobs and so they have developed a cottage industry in spreadsheet- based reporting and data marts. A vision to improve the information management and governance capability that doesn’t identify and address the gap in perception between the IT function’s view of the information environment and the business stakeholders’ perception of their information environment will inevitably falter and fail to gain the bro ad-based support needed to ensure effective and sustainable change is delivered. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 565 6. The Formula for Change One of the most famous methods for describing the ‘recipe’ required for effective change, the Gleicher Formula, describes factors that need to be in place to overcome the resistance to change in the organization. 𝐶𝐶= (𝑉𝑉×𝑉𝑉×𝐸𝐸) >𝑅𝑅 A ccording to the Gleicher Formula, Change (C) occurs when the level of dissatisfaction with the status quo (D) is combined with a vision of a better alternative (V) and some actionable first steps to get there (F) and the product of the three is enticing enough to overcome resistance (R) in the organization. Influencing any of the four variables in the Gleicher formula increases the effectiveness and success of the change effort. However, as with any complex machine, it is important to be aware of the risks inherent in pushing buttons and pulling levers: • Increasing dissatisfaction within the organization with the way things are working is a powerful tool and needs to be wielded with care lest it increases Resistance. • Developing a vision of the future will require a concrete and vivid vision of what people will do differently, what people will stop doing, or what they will start doing that they aren’t doing now. Ensure that people can appreciate the new skills, attitudes, or work methods that will be required. Present these in a way that doesn’t scare people away or create political barriers to the change by causing people to defend the status quo. • When describing the first steps to change, ensure they are achievable and explicitly tie them back to the vision. • Act to reduce resistance and avoid increasing resistance to change. To be blunt: Avoid alienating people. This requires a good understanding of the Stakeholders. 7. Diffusion of Innovations and Sustaining Change Ultimately, training and education must be put in place to deliver a sustainable information quality and data management change in an organization. Implementing change requires understanding how new ideas spread around the organization. This aspect of change is known as Diffusion of Innovations. Diffusion of Innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. Formulated in 1962 by Everett Rogers, it is related to the pop culture concept of the Idea Virus ( http://bit.ly/2tNwUHD ) as popularized by Seth Godin . Diffusion of Innovations has been applied consistently across a diverse range of fields from medical prescribing , to changes in farm husbandry methods, to the adoption of consumer electronics. The Diffusion of Innovations theory asserts that changes are initiated by a very small percentage (2.5%) of the total population, the Innovators, who tend (in the context of the society being examined) to be young, high in social class, and financially secure enough to absorb losses on bad choices. They have contact with technological innovators and a high risk tolerance. These are then followed by a further 13.5% of the population, Early Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "566 • DMBOK2 Adopters, who share traits with Innovators, but are less tolerant of risk. Early Adopters understand how getting the choice right can help them maintain a central role in the society as people to be respected. Change is adopted next by the largest segments of the population, the Early and Late Majorities, which comprise 68% in total. Laggards are the last to adopt any specific innovation. (See Figure 118 and Table 38.) Figure 118 Everett Rogers Diffusion of Innovations Table 38 Diffu sion of Innovations Categories A dapted to Information Management99 Adopter Category Definition (Information Management Perspective) Innovators Innovators are the first individuals to spot a better way to tackle problems with the quality of information. They take risks trying to develop profiling of data, build tentative scorecards, and begin to put the symptoms experienced by the business into th e language of Information Management. Often these innovators will use their own resources to get information and develop skills about best practices. Early Adopters Early Adopters are the second fastest category of individuals to adopt an innovation. These individuals have the highest degree of opinion leadership among the other adopter categories. They are perceived as ‘visionary’ managers (or experienced managers, or managers responsible for emergent business strategy areas) who have realized information quality issues are a barrier to their success. Often they piggy back on the initial work of the Innovators to develop their business case and begin to formalize information practices. Early Majority It takes the Early Majority significantly longer than the Early Adopters to adopt an innovation. Early Majority tend to be slower in the adoption process, have above average social status, contact with early adopters, and seldom hold positions of opinion l eadership in a system. They could be in the ‘traditional core’ areas of the organization where the impact of poor quality data is masked as the ‘cost of business’. Late Majority Individuals in the Late Majority approach an innovation with a high degree of skepticism and after most society has adopted the innovation. Late Majority typically have below average social status, very little financial lucidity, in contact with others in late majority and early majority, very little opinion leadership. In Information Managem ent terms, these can be areas of the organization where tight budgets might combine with skepticism about the proposed changes to generate resistance. Laggards Laggards are the last to adopt an innovation. Individuals in this category show little to no opinion leadership. They are typically averse to change -agents and tend to be advanced in age. Laggards tend to focus on ‘traditions’. In Information Management , terms these are often the people or areas of the business who resist because the ‘new thing’ means having to do the ‘old thing’ differently or not at all. 99 © 2014 Daragh O Brien. Used with permission. Laggards 16%Late Majority34%EarlyMajority34%Early Adopters13.5%Innovators2.5%100 755025 0Market share % Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 567 7.1 The Challenges to be O vercome as Innovations Spread Two key challenge areas exist with the spread of innovations through the organization. The first is breaking past the Early Adopter stage. This requires careful management of change to ensure that the Early Adopters can identify a sufficient level of dissatisfaction with the status quo that they will make and persist with the change. This step is needed to reach the ‘Tipping Point’ where the innovation is adopted by enough people that it begins to become mainstream. The second key challenge point is as the innovation moves out of the Late Majority stage into the Laggards stage. The team needs to accept that they cannot necessarily convert 100% of the population to the new way of doing things. A certain percentage of the group may continue to resist change and the organization will need to decide what to do about this element of the group. 7.2 Key Elements in the Diffusion of Innovation Four key elements need to be considered when looking at how an innovation spreads through an organization: • Innovation: An idea, practice, or object that is perceived as new by an individual or other unit of adoption • Communication channels : The means by which messages get from one individual to another • Time : The speed at which the innovation is adopted by members of the social system • Social system : The set of interrelated units that are engaged in joint problem solving to accomplish a common goal In the context of information management, an innovation could be something as simple as the idea of the role of a Data Steward and the need for Stewards to work cross -functionally on common data problems rather than traditional ‘silo’ thinking. The process by which that innovation is communicated, and the channels through which it is communicated most effectively, are the communication channels which must be considered and managed. Finally, the idea of the Social System as a set of interrelated units that are engaged towards a joint venture. This is reminiscent of the System as described by W. Edwards Deming which must be optimized as a whole rather than piece -by-piece in isolation. An innovation that doesn’t spread outs ide of a single business unit or team is not a well diffused change. 7.3 The Five Stages of Adoption The adoption of any change tends to follow a five -step cycle. It starts with individuals becoming aware of the innovation (Knowledge), being persuaded as to the value of the innovation and its relevance to them (Persuasion), and reaching the point of making a Decision about their relation to the innovation. If they do not reject the innovation, they then move Implement and finally Confirm the adoption of the innovation. (See Table 39 and Figure 119.) Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "568 • DMBOK2 Of course, because an idea can always be Rejected rather than adopted, the Tipping Point of critical mass of the Early Adopters and Early Majority is important. Table 39 The Stages of Adoption (Adapted from Rogers, 1964) Stage Definition Knowledge In the knowledge stage the individual is first exposed to an innovation but lacks information about the innovation. During this stage the individual has not yet been inspired to find more information about the innovation. Persuasion In the persuasion stage the individual is interested in the innovation and actively seeks information about the innovation. Decision In the Decision stage the individual weighs the advantages and disadvantages of using the innovation and decides whether to adopt or reject it. Rogers notes that the individualistic nature of this stage makes it the most difficult stage about which to acquire empirical evidence. Implementation In the Implementation stage the individual employs the innovation and determines its usefulness or searches fo r further information about it. Confirmation In the Confirmation stage, the individual finalizes his/her decision to continue using the innovation and may end up using it to its fullest potential. Figure 119 The Stages of Adoption 7.4 Factors Affecting Acceptance or Rejection of an Innovation or Change People make largely rational choices when accepting or rejecting an innovation or change. Key to these is whether the innovation offers any relative advantage over the previous way of doing things. Consider the modern smartphone. It presented a clea r advantage over previous smartphones because it was easy to use, stylish to look at, and has an App store where the product’s capabilities could be extended quickly and easily. Likewise, implementation of data management tools, technologies, and techniques have relative advantages over manual rekeying of data, bespoke coding, or resource intensive manual data search and discovery activities. Knowledge Persuasion Decision Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 569 For example, in many organizations there can be resistance to simple document and content management changes such as tagging files with Metadata to provide context. However, the use of that Metadata in turn provides a relative advantage in terms of supporting security controls, retention schedules, and simple tasks such as information search and retrieval. Linking the hassle of tagging to the time saved either searching for information or dealing with issues where information is shared or disclosed without authorization can help demonstrate this relative advantage. Once individuals see that an improvement is proposed, they will ask whether the improvement is compatible with their life, their way of working , etc. Returning to the smartphone example, the fact that it blended a high quality mp3 player, email, phone , etc., meant that it was compatible with the lifestyle and ways of working of its target users. To understand compatibility, a consumer will (consciously or sub -consciously) consider several factors. For example, the complexity or simplicity of the change. If the innovation is too difficult to use, then it is less likely to be adopted. Again, the evolution of smartphone and tablet platforms is littered with failed attempts that didn’t achieve the goal of a simple user interface. The ones that did so redefined the expectation of the market and inspired similar interfaces on other devices. Trialability refers to how easy it is for the consumer to experiment with the new tool or technology. Hence freemium offers for tools. The easier it is to ‘kick the tires’ the more likely the user will adopt the new tool or innovation. The importance of this is that it helps establish the understanding of relative advantage, the compatibility with the life style and culture of the organization, and the simplicity of the change. As a set of first steps towards a change vision, iterative prototyping and ‘trying it out’ w ith stakeholders is essential and can help cement the Guiding Coalition as well as ensuring early adopters are on- board. Observability is the extent that the innovation is visible. Making the innovation visible will drive communication about it through formal and personal networks. This can trigger negative reactions as well as positive reactions. Plan on how to handle negative feedback. The experience of seeing people using a new technology or working with information in a particular way (e.g., visualization of traditionally ‘dry’ numbers) can influence how to better communicate back the experience. 8. Sustaining Change Getting change started requires a clear and compelling vision and clear and immediate first steps, a sense of urgency or dissatisfaction with the status quo, a Guiding Coalition, and a plan to avoid the pitfalls and traps that Change Agents can fall into as they begin their change journey. However, a common problem in information management initiatives (e.g., Data Governance function s) is that they are initiated in response to a specific driver or to a particular symptom of sub -optimal capability in the organization. As the symptom is addressed, the sense of dissatisfaction and urgency lessens. It becomes harder to sustain political or financial support, particularly when competing with other projects. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "570 • DMBOK2 It is outside the scope of this work to provide detailed analysis or tools for how these complex issues might be addressed. However, in the context of a Body Of Knowledge it is appropriate to refer back to the change management principles outlined in this chapter to provide some insight as to how solutions might be found. 8.1 Sense of Urgency / Dissatisfaction It is important to maintain the sense of urgency. The corollary of this is to be alert to emerging areas of dissatisfaction in the organization and how the information management change might help support improvement. For example, the scope of a Data Governance initiative that has been implemented to support a data privacy regulatory requirement can be broadened to address information quality issues in relation to personal data. That can be related back to the primary scope of the initiative, as most data privac y regulations have a Data Quality component and provide a right of access to data to individuals, so there is a risk of poor quality data being exposed. However, it opens the vision of the Data Governance function up to include information quality methods and practices which can be implemented as a ‘second wave’ once the core data privacy governance controls are in place. 8.2 Framing the Vision A common mistake is to confuse project scope with change vision. Many projects may be required achieve the vision. It is important the vision be set in a way that allows broad based action and does not create a cul -de-sac for the change leaders once the initial ‘low hanging fruit’ projects are delivered. There is a difference between a vision that says: We will implement a structured governance framework for personal data to ensure compliance with EU Data Privacy rules. and one that says: We will lead our industry in repeatable and scalable approaches and methods for managing our critical information assets to ensure profits, reduce risks, improve quality of service, and balance our ethical obligations as stewards of personal information. The first is, more or less, an objective. The second provides direction for the organization. 8.3 The Guiding Coalition Restricting the membership of the Guiding Coalition to the most immediately affected stakeholders will restrict change effectiveness. As with vision, it is important not to confuse project steering groups who are overseeing the delivery of specific deliverables with the coalition who are guiding and evolving the vision for change in the organization. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 571 8.4 Relative Advantage and Observability While the specific application or focus of a change initiative might be narrow, in most cases , the principles, practices, and tools that are applied may be transferrable to other initiatives. Being able to demonstrate how the approach and methods can give a relative advantage to other initiatives in the organization can help extend the Guiding Coalition and identify new areas of urgency or dissatisfaction that the change initiative can support. For example, in a utility company, Data Quality profiling and score -carding methods and tools that are implemented for a single view of customer implementation may be directly transferrable to a regulatory billing compliance program. Linking the two would lend itself to an Enterprise Data Quality Scorecard and associa ted Data Governance and remediation initiatives, particularly where sub -optimal approaches such as manual data clean -up might be the default option for billing data. 9. Communicating Data Management Value Helping an organization understand the importance of data management often requires a formal organizational change management plan, as described in this chapter. Such a plan helps the organization recognize the value of its data and the contribution of data management practices to that value. Once a Data Management function is established, however, it is also necessary to cultivate ongoing support. Ongoing communication promotes understanding and sustains support. If communications are structured as a two -way channel, a communications plan can help strengthen partnerships by enabling stakeholders to share concerns and ideas. This kind of communications effort requires planning. 9.1 Communications Principles The purpose of any communication is to send a message to a receiver. When planning communications, one needs to account for the message, the media used to convey it, and the audiences for which it is intended. To support this basic structure, certain general principles apply to any forma l communications plan, regardless of topic. These are very important when communicating about data management because many people do not understand the importance of data management to organizational success. An overall communications plan and each individ ual communication should: • Have a clear objective and a desired outcome • Consist of key messages to support the desired outcome • Be tailored to the audience / stakeholders • Be delivered via media that are appropriate to the audience / stakeholders While communications may be on a range of topics, the general goals of communicating boil down to: • Informing • Educating • Setting goals or a vision • Defining a solution to a problem Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "572 • DMBOK2 • Promoting change • Influencing or motivating action • Gaining feedback • Generating support Most importantly, in order to communicate clearly, it is necessary to have substantive messages to share with people. Overall communications about data management will be more successful if the data management team understands the current state of data management practices and has a vision and mission statement that connects improvement in data management practices directly to the strategic goals of the organization. Data management communications should strive to: • Convey the tangible and intangible value of data management initiatives • Describe how data management capabilities contribute to business strategy and results • Share concrete examples of how data management reduces costs, supports revenue growth, reduces risk, or improves decision quality • Educate people on fundamental data management concepts to increase the base of knowledge about data management within the organization 9.2 Audience Evaluation and Preparation Communications planning should include a stakeholder analysis to help identify audiences for the communications that will be developed. Based on results of the analysis, content can be then tailored to be relevant, meaningful, and at the appropriate level, based on the stakeholder needs. For example, if the goal of the communications plan is to gain sponsorship for an initiative, target the communications to the highest possible influencers, usually executives who want to know the bottom line benefit of any program they fund. Tactics for persuading people to act on communications include various ways of getting people to see how their interests align with the goals of the program. • Solve problems : Messages should describe how the data management effort will help solve problems pertinent to the needs of the stakeholders being addressed. For example, individual contributors have needs different from executives. IT has needs that are different from t hose of business people. • Address pain points : Different stakeholders will have different pain points. Accounting for these pain points in communications materials will help the audience understand the value of what is being proposed. For example, a compliance stakeholder will be interested in how a Data Management improvement program will reduce risk. A marketing stakeholder will be interested in how the program helps them generate new opportunities. • Present changes as improvements : In most cases, introducing data management practices requires that people change how they work. Communications need to motivate people to desire the proposed changes. In other words, they need to recognize changes as improvements from which they will benefit. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 573 • Have a vision of success: Describing what it will be like to live in the future state enables stakeholders to understand how the program impacts them. Sharing what success looks and feels like can help the audience understand the benefits of the Data Management improvement program. • Avoid jargon: Data management jargon and an emphasis on technical aspects will turn some people off and detract from the message. • Share stories and examples : Analogies and stories are effective ways to describe and help people remember the purposes of the Data Management improvement program. • Recognize fear as motivation: Some people are motivated by fear. Sharing the consequences of not managing data (e.g., fines, penalties) is a way to imply the value of managing data well. Examples of how the lack of data management practices has negatively affected a business unit wil l resonate. Effective delivery of communications involves monitoring the listeners’ reactions to the message. If a given tactic is not working, adapt and try a different angle. 9.3 The Human Element The facts, examples, and stories shared about a Data Management improvement program , are not the only things that will influence stakeholder perceptions about its value. People are influenced by their colleagues, and leaders. For this reason, communication should use the stakeholder analysis to find where groups have like interests and needs. As support broadens for the data management effort, supporters can help share the message with their peers and leadership. 9.4 Communication Plan A communication plan brings planning elements together. A good plan serves as a roadmap to guide the work towards the goals. The communication plan should include elements listed in Table 40. Table 40 Communication Plan Elements Element Description Message The informat ion that needs to be conveyed. Goal / Objective The desired outcome of conveying a message or set of messages (i.e., why the message needs to be conveyed). Audience Group or individual targeted by the communication. The plan will have different obje ctives for different audiences. Style Both the level of formality and the level of detail in messages should be tailored to the audience. Executives need less detail than teams responsible for implementation of projects. Style is also influen ced by organizational culture. Channel, Method, Medium The means and format through which the message will be conveyed (e.g., web page, blog, email, one- on-one meetings, small group or large group presentations, lunch and learn sessions, workshops, etc.) Different media have different effects. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "574 • DMBOK2 Element Description Timing How a message is received may be influenced by when it is received. Employees are more likely to read an email that comes out first thing Monday morning than one that comes out last thing on Friday afternoon. If the purpose of communication is to gain support in advance of a budget cycle, then it should be timed in relation to the budget cycle. Information about impending changes to processes should be shared in a timely manner and in ad vance of a change taking place. Frequency Most messages need to be repeated in order to ensure all stakeholders hear them. The communications plan should schedule the sharing of messages so that repetition is helpful in getting the message across and does not become an annoyance. In addition, ongo ing communications (for example, a newsletter) should be published based on an agreed -to schedule. Materials The communications plan should identify any materials that need to be created to execute the plan. For example, short and long versions of presentation s and other written communications, elevator speeches, executive summaries, and marketing materials like posters, mugs, and o ther means of visual branding. Communicators The communications plan should identify the person or people who will deliver communications. Often , the person delivering the message has a profound influence on the target audience. If the data management sponsor or other executive delivers a message, stakeholders will have a different response than if a lower level manager delivers it. Decisions about who will communicate which messages to which stakeholders should be base d on the goals of the message. Expected Response The communications plan should anticipate how different stakeholder groups, and sometimes how individual stakeholders, will respond to communications. This work can be accomplished by anticipating questions or objections and formulating responses. Thinking through potential responses is a good way to clarify goals and build ro bust messages to support them. Metrics The communications plan should include measures of its own effectiveness. The goal is to ensure that people have understood and are willing and able to act on the messages in the plan. This can be accomplished through surveys, interviews, focus groups, and other feedback mechanisms. Changes in behavior are the ultimate test of a communications plan’s success. Budget and Resource Plan The communications plan must account for what resources are needed to carry ou t goals within a given budget. 9.5 Keep Communicating A Data Management function is an ongoing effort, not a one- time project. Communications efforts that support the function need to be measured and sustained for ongoing success. New employees are hired and existing employees change roles. As changes happen, communication plans need to be refreshed. Stakeholder needs change over time as Data Management function s mature. Time is needed for people to absorb messages, and hearing messages multiple times helps stakeholders to retain this knowledge. The methods of communication and messages will also need to be adapted over time as understanding grows. The competition for funding never goes away. One goal of a communications plan is to remind stakeholders of the value and benefits of the Data Management improvement program. Showing progress and celebrating successes is vital to gaining continued support for the effort. Effective planning and ongoing communication will demonstrate the impact that data management practices have had on the organization over time. Over time, knowledge of data’s importance changes the organization’s Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "DATA MANAGEMENT AND ORGANIZATIONAL CHANGE MANAGEMENT • 575 way of thinking about data. Successful communication provides a better understanding that data management can generate business value from information assets and have a long lasting impact on the organization. 10. Works Cited / Recommended Ackerman Anderson, Linda and Dean Anderson. The Change Leader's Roadmap and Beyond Change Management. Two Book Set. 2nd ed. Pfeiffer, 2010. Print. Ackerman Anderson, Linda, Dean Anderson. Beyond Change Management: How to Achieve Breakthrough Results Through Conscious Change Leadership. 2nd ed. Pfeiffer, 2010. Print. Ackerman Anderson, Linda, Dean Anderson. The Change Leader's Roadmap: How to Navigate Your Organization's Transformation . 2nd ed. Pfeiffer, 2010. Print. Barksdale, Susan and Teri Lund. 10 Steps to Successful Strategic Planning. ASTD, 2006. Print. 10 Steps. Becker, Ethan F. and Jon Wortmann. Mastering Communication at Work: How to Lead, Manage, and Influence . McGraw -Hill, 2009. Print. Bevan, Richard. Changemaking: Tactics and resources for managing organizational change. CreateSpace Independent Publishing Platform, 2011. Print. Bounds, Andy. The Snowball Effect: Communication Techniques to Make You Unstoppable . Capstone, 2013. Print. Bridges, William. Managing Transitions: Making the Most of Change . Da Capo Lifelong Books, 2009. Print. Center for Creative Leadership (CCL), Talula Cartwright, and David Baldwin. Communicating Your Vision . Pfeiffer, 2007. Print. Contreras, Melissa. People Skills for Business: Winning Social Skills That Put You Ahead of The Competition . CreateSpace Independent Publishing Platform, 2013. Print. Covey, Stephen R. Franklin Covey Style Guide: For Business and Technical Communication . 5th ed. FT Press, 2012.Print. Covey, Stephen R. The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change . Simon and Schuster, 2013. Print. Franklin, Melanie. Agile Change Management: A Practical Framework for Successful Change Planning and Implementation. Kogan Page, 2014. Print. Garcia, Helio Fred. Power of Communication: The: Skills to Build Trust, Inspire Loyalty, and Lead Effectively. FT Press, 2012. Print. Godin, Seth and Malcolm Gladwell. Unleashing the Ideavirus . Hachette Books, 2001. Harvard Business School Press. Business Communication . Harvard Business Review Press, 2003. Print. Harvard Business Essentials. HBR’s 10 Must Reads on Change Management . Harvard Business Review Press, 2011. Print. Hiatt, Jeffrey, and Timothy Creasey. Change Management: The People Side of Change. Prosci Learning Center Publications, 2012. Print. Holman, Peggy, Tom Devane, Steven Cady. The Change Handbook: The Definitive Resource on Today's Best Methods for Engaging Whole Systems. 2nd ed. Berrett -Koehler Publishers, 2007. Print. Hood, J H. How to book of Interpersonal Communication: Improve Your Relationships . Vol. 3. WordCraft Global Pty Limited, 2013. Print. “How to” Books. Jones, Phil. Communicating Strategy . Ashgate, 2008. Print. Kotter, John P. Leading Change . Harvard Business Review Press, 2012. Print. Locker, Kitty, and Stephen Kaczmarek. Business Communication: Building Critical Skills . 5th ed. McGraw -Hill/Irwin, 2010. Print. Luecke, Richard. Managing Change and Transition . Harvard Business Review Press, 2003. Print. Harvard Business Essentials. Rogers, Everett M. Diffusion of Innovations . 5th Ed. Free Press, 2003. Print. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "577 Acknowledgements Developing the second edition of the DAMA -DMBOK has been a labor of love for many people. The work started late in 2011 with the first revision of the Framework Paper, released in 2012. The DAMA -DMBOK Editorial Committee devoted many hours to produce the draft DMBOK2. They include: Patricia Cupoli (DAMA Philadelphia) was the editor -in-chief for the majority of this work, finding authors and helping them develop their chapters. Sadly, Pat passed away in Summer 2015, while still engaged in the project. Deborah Henderson (IRMAC – Toronto DAMA affiliate), Program Director for the DAMA -DMBOK products since their inception in 2005, was a dedicated sponsor of the project, and worked to ensure its completion after Pat’s passing. Susan Earley (DAMA Chicago), who drafted the DAMA -DMBOK2 framework, was the primary editor for the DMBOK2 draft. She edited and organized content and incorporated the extensive public comments from DAMA Members. Eva Smith (DAMA Seattle), Collaboration Tool Manager, handled logistics, including enabling DAMA members to access and comment on chapters. Elena Sykora (IRMAC – Toronto DAMA affiliate), Bibliographer Researcher, compiled the DMBOK2’s comprehensive bibliography. The Editorial Committee also appreciated the particular support of Sanjay Shirude, Cathy Nolan, Emarie Pope, and Steve Hoberman. Laura Sebastian -Coleman (DAMA New England), DAMA Publications Officer and Production Editor, shaped, polished, and finalized the manuscript for publication. In this effort, she was guided by an advisory committee that included Peter Aiken, Chris Bradley, Jan Henderyckx, Mike Jenn ings, Daragh O Brien, and myself, with lots of help from Lisa Olinda. Special thanks also go to Danette McGilvray. DMBOK2 would not have been possible without the primary contributing authors who gave substance to the vision defined in the Framework. All contributors are volunteers who shared not only their knowledge but also their time. They are credited for their con tributions below . The many DAMA Members who provided feedback on chapters are listed as well. DAMA International, the DAMA International Foundation, and the DAMA Chapter Presidents’ Council sponsored the DMBOK project. Their vision, insight, patience, and continued support enabled this project to be successful. Finally, we want to recognize the families of all the volunteers on this project, who gave of their personal time to complete this work. Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "578 • DMBOK2 Sue Geuens, President, DAMA International Primary Contributors # Chapter Primary Contributors 1 Introduction: Data Management Editorial Advisory Committee, DMBOK editors, Chris Bradley, Ken Kring 2 Data Handling Ethics 3 Data Governance and Stewardship John Ladley, Mark Cowan, Sanjay Shirude 4 Data Architecture Håkan Edvinsson 5 Data Modeling and Design Steve Hoberman 6 Data Storage and Operations Sanjay Shirude 7 Data Security David Schlesinger, CISSP 8 Data Integration and Interoperability April Reeve 9 Documents and Content Pat Cupoli 10 Reference and Master Data Gene Boomer, Mehmet Orun 11 Data Warehouse and Business Intelligence Martin Sykora, Krish Krishnan, John Ladley, Lisa Nelson 12 Metadata Saad Yacu 13 Data Quality Rossano Tavares 14 Big Data and Data Science Robert Abate, Martin Sykora 15 Data Management Maturity Assessment Mark Cowan, Deborah Henderson 16 Data Management Organizations and Roles Kelle O’Neal 17 Data Management and Organizational Change Management Micheline Casey, Andrea Thomsen, Daragh O Brien Bibliography Elena Sykora Reviewers and Commenters The following people provided valuable feedback a t various stages of the DMBOK2: Khalid Abu Shamleh Mike Beauchamp Susan Burk Gerard Adams Chan Beauvais William Burkett James Adman Glen Bellomy Beat Burtscher Afsaneh Afkari Stacie Benton Ismael Caballero Zaher Alhaj Leon Bernal Peter Campbell Shahid Ali Luciana Bicalho Betty (Elizabeth) Carpenito Suhail Ahmad AmanUllah Pawel Bober Hazbleydi Cervera Nav Amar Christiana Boehmer Indrajit Chatterjee Samuel Kofi Annan Stewart Bond Bavani Chaudhary Ivan Arroyo Gene Boomer Denise Cook Nicola Askham Taher Borsadwala Nigel Corbin Juan Azcurra Antonio Braga James Dawson Richard Back Ciaran Breen Elisio Henrique de Souza Carlos Barbieri LeRoy Broughton Patrick Derde Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "ACKNOWLEDGEMENTS • 579 Ian Batty Paul Brown Tejas Desai Steve Beaton Donna Burbank Swapnil Deshmukh Cynthia Dionisio Nicholene Kieviets Susana Navarro Shaun Dookhoo Jon King Gautham Nayak Janani Dumbleton Richard King Erkka Niemi Lee Edwards Bruno Kinoshita Andy O'Hara Jane Estrada Yasushi Kiyama Katherine O'Keefe Adrianos Evangelidis Daniel Koger Hirofumi Onozawa William Evans Katarina Kolich Mehmet Orun Mario Faria Onishi Koshi Matt Osborn Gary Flye Edwin Landale Mark Ouska Michael Fraser Teresa Lau Pamela Owens Carolyn Frey Tom LaVerdure Shailesh Paliwal Alex Friedgan Richard Leacton Mikhail Parfentev Lowell Fryman Michael Lee Melanie Parker Shu Fulai Martha Lemoine John Partyka Ketan Gadre Melody Lewin Bill Penney Oscar Galindo Chen Liu Andres Perez Alexandre Gameiro Manoel Francisco Dutra Lopes J Aparna Phal Jay Gardner Daniel Lopez Jocelyn Sedes Johnny Gay Karen Lopez Mark Segall Sue Geuens Adam Lynton Ichibori Seiji Sumit Gupta Colin Macguire Brian Phillippi Gabrielle Harrison Michael MacIntyre R. Taeza Pittman Kazuo Hashimoto Kenneth MacKinnon Edward Pok Andy Hazelwood Colin Maguire Emarie Pope Muizz Hassan Zeljko Marcan David Quan David Hay Satoshi Matsumoto K Rajeswar Rao Clifford Heath George McGeachie April Reeve Jan Henderyckx Danette McGilvray Todd Reyes Trevor Hodges R. Raymond McGirt Raul Ruggia -Frick Mark Horseman Scott McLeod Scott Sammons Joseph Howard Melanie Mecca Pushpak Sarkar Monica Howat Ben Meek John Schmidt Bill Huennekens Steve Mepham Nadine Schramm Mark Humphries Klaus Meyer Toshiya Seki Zoey Husband Josep Antoni Mira Palacios Rajamanickam Senthil Kumar Toru Ichikura Toru Miyaji Sarang Shah Thomas Ihsle Ademilson Monteiro Gaurav Sharma Gordon Irish Danielle Monteiro Vijay Sharma Fusahide Ito Subbaiah Muthu Krishnan Stephen Sherry Seokhee Jeon Mukundhan Muthukrishnan Jenny Shi Jarred Jimmerson Robert Myers Satoshi Shimada Christopher Johnson Dean Myshrall Sandeep Shinagare Wayne Johnson Krisztian Nagy Boris Shuster Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "580 • DMBOK2 Sze-Kei Jordan Kazuhiro Narita Vitaly Shusterov George Kalathoor Mohamad Naser Abi Sivasubramanian Alicia Slaughter Akira Takahashi Roy Verharen Eva Smith Steve Thomas Karel Vetrovsky Tenny Soman Noriko Watanabe Gregg Withers José Antonio Soriano Guzmán Joseph Weaver Michael Wityk Donald Soulsby Christina Weeden Marcin Wizgird Erich Stahl Alexander Titov Benjamin Wright -Jones Jerry Stembridge Steven Tolkin Teresa Wylie James Stevens Toshimitsu Tone Hitoshi Yachida Jan Stobbe Juan Pablo Torres Saad Yacu Santosh Subramaniam David Twaddell Hiroshi Yagishita Motofusa Sugaya Thijs van der Feltz Harishbabu Yelisetty Venkat Sunkara Elize van der Linde Taisei Yoshimura Alan Sweeney Peter van Nederpelt Martin Sykora Peter Vennel Acknowledgments for the Revised Edition Many people were involved in creating the revised edition of DAMA -DMBOK version 2. This initiative started in 2021 by a call to contributors to proposed corrections of errors, misprints , and inconsistencies. Two individuals were highly involved in the coordination of this project: Chris Bradley CMDP Fellow, acted as the senior editor for this release. Michele Valentini CMP Master was the coordinator to track all review committees’ deliverables and prepare the consolidated MS Word documents. The review committees were: # Chapter Contributors All context diagrams Debora Henderson, David Wiebe 1 Introduction: Data Management Geoffrey van IJzendoorn -Joshi, Maria Camilla Nørgaard 3 Data Governance and Stewardship Marcelo Malheiros 5 Data Modeling and Design Marco Wobben 13 Data Quality Lloyd Robinson, Daniel Sklar , Peter Van Nederpelt, Carl Kane, Josh Fradd Michel Hébert CDMP Master, VP Professional Development, DAMA International Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "581 Index Abstraction category, 163 Abuse Intentional, 229 Unintentional, 229 Access, 239 Access controls, 194 Access to data, 229 ACID, 174, 175 Administrative and Audit Duties Risk, 244 Adware, 233 Affiliation management, 344 Aiken, Peter, 41 Aiken’s pyramid, 42 American National Standards Institute, 143 Amsterdam Information Model, The, 36 –37, 37 Analytical model, 492 –93 ANSI Standard 859, 309 Anti -virus software, 246 Apache Mahout, 492 Application coupling, 268 Application DBA, 171 Application security requirements, 248 Architects, 102 Architectural design diagrams, 116 Architecture, 99 Architecture designs, 116 Architecture framework, 104 Architecture initiation projects, 118 –19 Archiving process, 184, 266 ARMA GARP® principles, 322, 324 ARMA International, 290 Assessments, 82 Asset, 22 Asset management software, 116 Asset tracking, 207 Asynchronous data flow, 264 Authoritarian decree, 555 Authority list, 295 Backup and recovery activities, 309 Backup files, 193 Backup software, 193 BASE, 174, 175 Basel II Accord, 237 Batch change data capture, 370 –71 Batch data integration, 263, 277 Batch interaction, 263 Belmont Principles, 54 Bias Data processing and, 60 –61 Types of, 61 Bi-Directional metadata architecture, 408 Big data, 469 –71, 474– 76 Cloud storage and, 491 Principles of, 470 Sources of, 476 Tools for, 488 –89 Big data modeling, 493 Big data strategy, 483 Black Hat hackers, 232 Blockchain databases, 173 Bot, 221 Brandeis, Louis, 55 Brewer’s Theorem, 176 Bridges, William, 540– 42 Bridges’s Transition Phases, 541 Bring -your -own -devices (BYOD), 307 Business alignment, 82 Business bias, 61 Business continuity group, 192 Business Continuity Plan, 192, 268, 310 Business data steward, 79, 323 Business glossary, 92, 95, 402, 546 Business growth, 212 Business intelligence, 59, 65, 362 Portfolio for, 375 Self-service, 384 –85 Tools for, 380 Business intelligence and analytic functions, 42 Business metadata, 398 Business performance management, 382 Business rules, 275 –76 Data integration and, 281 Business vocabulary, 153 BYOA. See Bring Your Own Apps BYOD. See Bring -your -own -devices (BYOD) C, 73 CAD / CAM. See Computer assisted design and manufacturing Canada Bill 198, 51, 299 Canadian Privacy law (PIPEDA), 56 –57, 227 Candidate Identification, 341 –42 Canonical data model, 267 CAP Theorem, 176 Capacity and growth projections, 185 CDMP, 67 CDO Organizational touch points, 83 Centralized databases, 172 Change Capacity to, 82 Checklist for managing, 542 Laws of, 540 Sustaining, 567 Change management, 539 Change agent, 540, 543, 555, 561 Change data, 185 Change data capture, 264 Change management Communication and, 555 Complacency and, 543 Errors of, 543 –44, 547 Transition and, 540 –42 Vision for, 544 Change Management Institute, 87 Change managers, 542, 543 Change vision, 568 Charts and graphs, 59 Chief data officer, 83 Chief information officer, 34 Chisholm, Malcolm, 330 Class words, 159 Classification schemes, 296 Cloud computing, 173, 251 Cloud storage, 491 Cloud -based integration, 272 CobiT. See Control objectives for information and related technology. Collaborative readiness, 82 Collection, 302 Columnar Appliance Architecture, 490 Columnar -based databases, 177 Column -oriented databases, 181 –82 Communication interrogatives, 105 Communication plan, 569 –70, 571– 72 Competitive advantage, 20, 228 Complacency, 543, 549 Completeness category, 163 Complex event processing solutions (CEP), 271, 279 Compliance activities, 244 –46 Computer assisted design and manufacturing, 184 Computer worm, 234 Confidentiality classification, 238 Confidentiality data restrictions, 226 Configuration management, 386 Configuration management tools, 402 Consistency category, 163 Content Capturing, 308 Definition of, 290 Lifecycle of, 290 Content delivery channels, 311 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "582 • DMBOK2 Content delivery methods, 292 Content handling polices, 306 Content information architecture, 308 Content lifecycle, 291 Content management, 291, 306 Content management software, 294 Content Management System (CMS), 305, 312, 314 Content metadata, 291 –92 Content modeling, 292 Context diagram, 39 Big data and data science, 471 Components of, 39 –41 Data architecture, 102 Data governance and stewardship, 71 Data modeling, 124 Data quality, 424 Data security, 211 Data warehouse/business intelligence, 360 Defined, 39 Documents and content, 288 Knowledge area, 39 Metadata, 394 Reference and master data, 328 Contractual restrictions, 228 Control activity, 40 Control objectives for information and related technology, 73 Controlled vocabulary, 292, 295, 296, 315 Coordinating data steward, 79 Corporate Information Factory (CIF), 363 –65, 365 Correctness category, 162 Covey, Stephen, 562 Critical Risk Data (CRD), 215 Cross -reference data sets, 333 –34 CRUD, 248 Cultural change, 119 Customer relationship management (CRM), 344, 346 DAMA Data Management Function Framework, 37 –41, 44 DAMA Functional Area Dependencies, 43 DAMA International, 66 DAMA International’s Certified Data Management Professional (CDMP) certification, 67 DAMA Knowledge Areas, 42 DAMA Wheel, The, 37, 42, 45, 46 DAMA’s mission, 46 DAMA- DMBOK, 37– 41, 45– 48 Data Analysis, 485 As an asset, 19, 22, 25, 54 Business acceptance of, 388 –89 Ethical approach to, 22 Ethical principles of, 55 Monetary value of, 26 Ownership of, 58 Risks and, 30, 32 Sensitive, 211 –12 Storage containers for, 195 Types of, 32 Understanding of, 21 –22 Value of, 26 –27 Data access, 191 Data access control, 239 Data acquisition, 340 Data administration, 167 Data aggregation, 62 Data and enterprise architecture, 111 Data and information relationship, 22, 36 Data architects, 103, 534 Data architecture, 47, 100, 111 Goals of, 101 Implementation guidelines and, 117 –18 Data architecture governance, 120 Data as a service (DaaS), 272 Data asset, 93 Data asset valuation, 79 –81 Data attribute, 154 Data audits, 207 Data availability, 219 Data capture Change, 264 Data category, 163 Data consistency, 239 Data consumers, 377 Data dictionary, 379, 404 Data discovery, 274 Data exchange specification design, 276 Data Exchange Standards, 272 –73 Data Federation, 271 Data flows, 109– 10 Diagram, 110 Integration, 278 Data governance, 47, 55, 67, 69, 75 Goals of, 73 –74 Guiding principles for, 73, 75, 289, 396 Implementation of, 90 Issue management, 88 Issue management for, 87 Organizational culture and, 96 Orgnizations and, 81 Procedures for, 89 Readiness assessments and, 82 Regulatory compliance and, 72 –73 Tools and techniques for, 94 Data governance and data management, 74 Data Governance Community of Interest, 94 Data governance council, 34, 86, 88, 93, 95, 238 Data governance operating framework, 85 Data governance organization parts, 76 Data governance organizations, 75, 93 Data governance program Implementation guidelines for, 95 Measurement and, 96 Data governance scorecard, 95 Data governance standards, 90 –92 Data Governance Steering Committees, 95 Data governance strategy, 34, 84 Data governance team, 419 Data handling Current state and, 63 Improvement strategies and, 64 Risk reduction and, 63 –64 Data handling ethics, 51, 53 Data insights, 472, 488 Data integration, 61 –62, 285 Near -real time, 264 Profiling and, 274 Synchronous, 265 Data integration & interoperability (DII), 47, 257 –59, 260, 273 Data integration activities, 273 –76 Data integration processes, 350, 354, 369– 72 Data integration solutions Business rules and, 281 Design of, 276 Mapping sources and, 277 Data integration tools, 380, 403 Data integrity, 218 Data issue escalation path, 89 Data lake, 476 Data lifecycle, 30 –32, 43, 273 Data lifecycle key activities, 31 Data lineage, 251, 274, 284 Data loading requirements, 196 Data loss, 204 Data management, 19, 33, 70 Challenges of, 22 Consumers for, 40 Data quality and, 27 Enterprise perspective and, 29 Goals of, 20 Initiatives and, 86 Inputs and, 40 Metadata and, 393 –95 Participants in, 40 Specialized hardware for, 177 Data management activities Control, 40 Development, 40 Operational, 40 Planning, 40 Data management framework, 35 Data management implementation roadmap, 35 Data management maturity, 82 Data management plan, 569 Data management practices, 91, 539 Assessment of, 83 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "INDEX • 583 Data management procedures Components, 91 Data management professionals, 19, 539 Data management program Human resources and, 571 Data management program charter, 34 Data management scope statement, 35 Data management strategy, 34 –35, 96 Components of, 34 Deliverables from, 34 Data map, 301, 318 Data marking, 62 Data marts, 369 Data mashups, 482 Data masking methods, 62, 219, 220, 243 Data migration, 202, 278 Data mining, 478– 79 Data mining snooping, 60 Data model Integration in, 161 Versioning of, 161 Data model management, 339 Data model repositories, 116 Data modeler, 158 Data modeling, 47, 123– 25 Goals of, 125 Standards for, 159, 160 Data modeling tools, 116, 203, 281, 404 Data models Evaluation of, 486 Data movement, 260 Data naming standards, 159 Data operations and storage activities, 188 –90 Data orchestration, 277 Data policy, 79, 86 Data privacy laws, 55 –58 Data processing errors, 187 Data producer, 160 Data producing applications, 278 Data product development lifecycle, 378 Data professionals, 65, 160 Data profiling processes, 274 Data profiling tools, 281 Data quality, 27 –28, 48 Statistics about, 27 Data quality analysis, 82 Data recovery plan, 192 Data recovery software, 193 Data regulations, 212, 238 Data remediation, 62, 374 Data replication process, 186, 196 Data retention plan, 188, 273 Data scaling, 186 Data science, 469 –74, 485 Data science tools, 488– 89 Data security, 47, 188, 211– 12 Bill of Rights, 241 Business requirements, 211 –12, 236 Goals of, 214 Monitoring of, 217, 242– 44 Outsourcing, 250 Password for, 225 Regulatory requirements for, 236 Requirements for, 217 Risk assessment and, 249 Data Security Bill of Rights, 241 Data security governance, 252 Data security management Four A’s of, 217 Guiding principles for, 214 Data security policy, 237– 38, 241 Data security practices, 249 Data security requirements, 210 Data security restrictions, 225 –28 Data security risks, 212 Data security standards, 235, 238 Data security vocabulary, 220 –23 Data services, 277 Data sharing agreements, 284, 355 Data source governance, 389 Data sources, 483 –84 Evaluation of, 349 –50 Ingesting, 483 –84 Data Standards Steering Committee, 91 Data stewards, 78 –79, 92, 237, 242, 244, 250, 335, 349 Coordinating, 79 Executive, 78 Data stewardship, 77 Committee for, 88 Team for, 88, 93 Data storage and operations, 47 Data storage areas, 369 Data storage environment, 195 Data storage goals, 168, 176 Data storage governance, 207 Data storage metrics, 206 Data storage systems, 179 –84 Data strategy, 34 Components of, 34 Ownership of, 34 Data structures, 276 Data technology requirements, 189 Data transformation, 374 Data transformation engine, 280 Data validation, 207, 340 Data valuation, 26 –27 Data vault, 370 Data virtualization, 271 Data virtualization servers, 280 Data visualization, 481 –82, 487, 491 Data warehouse, 359 –61, 362 Approaches to, 363 Batch change data capture for, 370 –71 Critical success factors for, 494 Development tracks for, 373 Goals of, 361 Governance in, 387 Historical data and, 370 Population of, 374 Requirements of, 372 Data warehousing, 48, 362 Critical success factors, 387 Database, 169 Hierarchical, 180 Multidimensional, 180 Temporal, 181 Types of, 171 Database abstraction, 169 Database access control, 194 Database administrator (DBA), 167, 170, 189, 190, 191, 195, 205– 6 Database as a service (DaaS), 174 Database availability Factors affecting, 198 Loss factors of, 198 Online criteria for, 198 Database backup, 192 Database catalogs, 403 Database execution, 199 Database log techniques, 370 Database management Organizational change and, 205 –6 Database management system (DBMS), 180 Database management technology Naming conventions and, 204 Script files and, 204 Software for, 189 Tools, 202– 3 Database management tools, 202 –3 Database monitoring tools, 203 Database Organization Spectrum, 180 Database performance Monitoring to improve, 199 Tuning to improve, 170 Database processes Archiving, 184 Capacity and growth projections of, 185 Change data within, 185 Purging, 186 Replication of, 186 Resiliency of, 187 Retention of, 188 Sharding of, 188 Database processing, 174 Database storage systems, 191 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "584 • DMBOK2 Database support, 166, 190 Database systems administration tasks, 193 Database technology Managing, 188, 189– 90 Monitoring of, 190 Support of, 166 Databases Alternative environments for, 201 Centralized, 172 Columnar -based, 177 Column -oriented, 181 –82 Data loading and, 196 Development environment and, 177 Distributed, 172 Flat file, 182 Key-value pair, 183 Multimedia, 182 Non -relational, 181 Object/Multimedia, 182 Processes of, 184 –88 Relational, 180 Spatial, 182 Specialized, 183 Triplestore, 183 Types of, 179– 84 Usage patterns of, 191 Databases process, 184 –88 Data -centric organization, 75 DBA. See Database administrator Decision support systems (DSS), 359 Default passwords, 232 Definitions category, 163 Deming, W. Edward, 53 Denormalization, 148 Design review, 161 Destination (VISION), 542 Detection and recovery risk, 244 Developer support tools, 203 Developers, 178 Development activity, 40 Development DBAs, 171 Development environment, 177, 178 Device access policies, 307 Dewey Decimal System, 296 Dice, 383 Diffie -Hellman Key Agreement, 219 Diffusion of Innovations theory, 563 Digital asset management (DAM), 300, 313 DII governance, 283 –84 DII solutions, 279 Dimensional data warehouse, 366 –67 Directory, 403, 404, 405 Disaster Recovery Plan, 310 Disasters, 187 Discovery, 82, 274, 301– 2 Disk storage, 176 Distributed databases, 172 Distributed file -based solutions technologies, 490 –91 DMBOK Pyramid, 41– 42 Document / record, 298 Audit of, 311 Management of, 310 Retention of, 310 Document and content knowledge, 289 –90 Document and content management, 47, 287 Regulatory compliance and, 288 –89 Document library system, 312 Document management, 289, 298– 300, 305, 313 Document management system, 312 Document management tool, 95 Document repository, 313 Document sanitization, 249 Drill down, 383 Dublin Core, 293 DW usage metrics, 390 DW/BI architecture, 372 Dynamic data masking method, 219 ECM. See Enterprise Content Management Systems ECM readiness assessment, 319 E-discovery, 289, 317 Key Performance Indicators (KPIs) and, 324 –25 E-discovery assessment, 320 EDM. See Enterprise Data Model EDRM. See Electronic Discovery Reference Model (EDRM) Electronic data interchange (EDI), 253 Electronic Discovery Reference Model (EDRM), 301 Electronic documents, 301 Electronic point of sale (EPOS) apps, 292 Electronic records, 289, 306 Electronic technology and business growth, 213 ELT. See Extract -Load -Transform ELT Process Flow, 263 Encryption, 218, 232, 247, 248 Enrichment, 341 Enterprise application integration model (EAI), 269 Enterprise architectural framework, 104 –5 Enterprise architecture, 100, 111, 252 Enterprise asset, 19 Enterprise Content Management (ECM), 291 Cultural change and, 321 Guidelines for, 319 Key Performance Indicators (KPIs) and, 325 Enterprise data architecture, 120 –23 Enterprise data architecture steering committee, 93 Enterprise data governance council, 76 Enterprise data model, 93, 106 –9, 107 Enterprise data warehouse, 362 Enterprise integration tool, 282 –83 Enterprise message format, 267 Enterprise perspective and data management, 29 Enterprise resource planning (ERP), 195, 258, 346 Enterprise service bus (ESB), 268, 270, 281 Enterprise standards, 276 Entity resolution, 341 Environmental factors hexagon, 38 Equivalent term relationship, 295 Ethical data handling, 51, 53, 63– 64, 64, 67 Ethical data management, 51, 59 Ethical Risk Model, 66 Ethical risks, 61 Ethics, 51 ETL data flows, 278 ETL process flow, 262 EU Basel II Accord, 237 EU Privacy Directives, 227 European Convention of Human Rights, 55 European Data Protection Supervisor, 54 Event processing method, 271, 279 Event -driven data integration, 264 Everett Rogers Diffusion of Innovations, 564 Excessive privileges, 229 Executive data stewards, 78 Extensible markup interface, 316 Extensible markup language, 315 External data, 196 Extract - Load -Transform (ELT), 262 Extract process, the, 261 Extract -Transform -Load (ETL), 199, 260, 263 Faceted taxonomies, 294, 296 Family Educational Rights and Privacy Act, 228 FASB. See Financial Accounting Standards Board Fast data integration, 265 Federal Rules of Civil Procedure, 299 Federal Trade Commission, 57 Federated architectures, 172 Federation provisions data, 172 FERPA. See Family Educational Rights and Privacy Act Financial Accounting Standards Board, 89 Financial assets, 25 Financial master data, 345 Financially sensitive data, 227 Firewalls, 221, 247 Flash memory, 177 Flat file databases, 182 Folksonomies, 293, 296 Frameworks. See Data management frameworks Freedom of speech, 59 GASB. See Government Accounting Standards Board (U.S.) General Data Protection Regulation, 56 Generally Accepted Recordkeeping Principles, 319 Geographic classification, 335 Geographic information systems (GIS), 308 Geo-statistical reference data, 335 Geuens, Sue, 42 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "INDEX • 585 Gleicher formula, 543 Glossary, 92 Godin, Seth, 563 Golden record, 337 Goodwill, 22 Governance. See also Data Governance Government Accounting Standards Board (U.S.), 89 Graphical design applications, 116 Group Think, 554 Guiding principles Data governance, 73, 75, 289, 396 Data security management, 214 Hacking/Hacker, 232 Hadoop, 490 Hash algorithms, 173 Hash encryption, 218 Health Information Protection and Portability Act (U.S.), 51 Healthcare Information Portability and Accountability Act (HIPAA), 244 Hierarchical database organization, 180 Hierarchical relationship, 295 Hierarchical Storage Management System, 182 Hierarchical taxonomy, 296 High Risk Data (HRD), 216 Historical data, 370 HOLAP. See Hybrid online analytical processing Hot backups, 193 Hub -and-spoke data pattern, 267 Hub -and-spoke interaction model, 267 –68 Hybrid online analytical processing, 384 Identity, 58 Identity management technology, 246 Identity resolution, 342 IM. See Instant messaging (IM) Image processing technology, 313 –14 Imhoff, Claudia, 364 Inconsistencies Explaining, 561 –62 In-database algorithm, 491 Indexing, 156 Industry reference data, 335 Industry -based regulations, 228 Information and Communication Technology Research, 54 Information and data relationship, 22 Information architecture, 303 Information asset, 32 Information asset tracking, 207 Information consumer, 160 Information content architecture, 311 Information Council, 323 Information economy, 19 Information gaps, 32, 93 Information governance, 322 –23 Information Governance Maturity Model (IGMM), 320 Information Governance Reference Model (IGRM), 323 Information management change initiative, 553 Information management context, 561 Information management disciplines, 54 Information quality change, 563 Information security Data classification and, 212 Techniques for managing, 247 –49 Tools used in, 246 –47 Vocabulary for, 216 Information Security and Corporate Counsel, 244 Information security team, 216 Information Systems Planning (ISP) method, 109 Information technology and data management, 33 Information technology infrastructure library (ITIL), 189, 194 In-Memory databases (IMDB), 177 Inmon, Bill, 363 Innovation, 565 Instant messaging (IM), 234 Integration cloud -based system, 272 Integration testing, 179 Intentional abuse, 229 Interaction, 267 –68, 276 Hub -and-spoke, 267– 68 point -to-point, 267 Publish and subscribe, 268 Internal integration requirements, 272 Intrusion detection system (IDS), 231, 247 Intrusion prevention system (IPS), 230, 231, 247 Islands of data, 239 ISO 15489, 299 ISO State Code, 333 Issues management, 88 –89 IT governance, 73 ITIL. See Information technology infrastructure library (ITIL) JSON. See JavaScript Object Notation (JSON) Justice/Fairness ethical principle, 60 Key exchange, 219 Key performance indicators (KPIs), 324 Key-value, 142 Key-value pair database, 183 Kimball, Ralph, 366 K-Means clustering, 485 knowledge, 20 Kohonen M, 479 Kotter, John P., 544, 546, 547, 549, 551, 557 Lambda architecture design, 176 Latency, 263 Laws of change, 540 Leader’s Data Manifesto, The, 33 Leadership, 553 Leadership alignment, 527 Legal master data, 345 Legitimate database privileges abuse, 229 –30 Licensing agreements compliance, 207 Lifecycle management, 43, 305 List, 332– 33 Litigation playbook, 318 Load processes, 262, 378 Location master data, 346 Log shipping vs. mirroring, 187 Logical data names, 159 Lone CEO scenario, 552 Loosely -coupled systems, 172 Loshin, David, 330 Low Credibility Committee, 552 Machine learning, 477 –78, 478 Machine readable catalog, 312 Malicious hacker, 232 Malware, 233 Managed database hosting, 174 Management lifecycle, 299 Managers, 551 Mapping management tools, 404 Mapping process, 263 MapQuest, 184 MapReduce, 172 MARC. See Machine Readable Catalog Market timing, 59 Martin, James, 109 Mashups, 482 Massively parallel processing (MPP), 488, 489 –90 Master data, 327 –28, 336 Business drivers and, 329 Governance policy and, 351 Master data ID management, 343 –44 Master data integration, 347 Master data management (MDM), 338 –39, 349, 350 Goals of, 329– 30 Tools and techniques of, 353 Master data sharing architecture, 348, 354 Matching. See Candidate identification Matching workflows, 343 Media monitoring, 478 Medically sensitive data, 228 Menlo Report, 54 Messaging interaction, 371 Metadata, 21, 29, 48, 213, 393 Categorization and, 306 Content, 291 –92 Data risks and, 395 Definition of, 393 Delivery mechanisms for, 414 Directory of, 404 Impact analysis of, 415 –17 Importance of, 393 –95 Integration of, 413 Managed environment for, 410 Management of, 292 Repository for, 401, 414 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "586 • DMBOK2 Repository model for, 412 Scope of, 409 Sources of, 400– 401 Types of, 397– 99 Unreliable, 62 Unstructured data and, 291, 400 Uses of, 414 Metadata architecture, 405, 408 Centralized, 406 Distributed, 407 Metadata environment, 418 Metadata governance, 419 Metadata initiatives, 418 Metadata leveraging, 395 Metadata management system, 410 Metadata management tools, 415 Metadata registries (MDR), 204 Metadata registry standard, 399 Metadata repository, 379 Metadata repository metamodel, 411 Metadata repository model, 248, 282, 411 Metadata requirements, 410 Metadata standards, 411, 420 Metadata stores, 405 Metadata strategy, 409 Phases, 409 Risk assessment and, 418 Metadata tags, 417 Metadata, 421 Metrics, 96, 253 Data protection, 255 Security, 253 –54 Security awareness, 254 Micro -controlled vocabulary, 294 Micromanagement, 555 Misleading visualizations, 59 Mission accomplished syndrome, 546 Models and diagrams Clarity of, 117 Moderate risk data (MRD), 216 MOLAP. See Multi -dimensional online analytical processing Monitoring authentication, 243 Morris, Henry, 382 Multidimensional database technologies, 180 Multidimensional eXpression, 180 Multi -dimensional online analytical processing, 384 Multi -master replication, 186 Multimedia database, 182 Multi -temporal database, 181 National Information Exchange Model (NIEM), 273 Near real -time model, 281 Near -real-time data, 371 –72 Network storage administrators (NSAs), 190 Network taxonomy, 296 Network -based audit appliance, 244 Neutral zone, 541 NIST risk management framework, 216 Node, 169 Non -relational database, 181 NoSQL, 123, 128, 130, 135, 136, 142, 143, 151, 152, 163, 183, 191, 316 Nullability, 163 Obfuscation data, 62, 219, 220 Object to processing of personal data, 56 Observability, 567 OCM. See Organizational change management (OMC) OCR software, 312 ODBC. See Open Database Connectivity OLAP. See Online analytical processing OLTP. See Online transaction processing (OLTP) Online analytical processing, 382 Online data Ethical uses of, 58 Online freedom of speech, 59 Online transaction processing (OLTP), 184 Ontology, 105, 297, 335 Open Database Connectivity, 169 Open Geospatial Consortium standard, 182 Operating framework, 73 Operational activity, 40 Operational analytics, 481 Operational data store (ODS), 369 Operational metadata, 399 Operational reports, 365, 372, 381 Orchestration data, 277 Orchestration process, 269 Organization Cultural change and, 119 Data -centric, 75 Organization for Economic Co- operation and Development (OECD), 55 Organizational behavior, 94 Organizational change management (OCM), 87 –88 Organizations and cultural change, 250, 283 Outsourcing and data security, 250 OWL. See W3C Web Ontology Language Ownership of data, 58 Party master data, 344 –45 Password, 225, 246 Payment Card Industry Data Security Standard (PCI -DSS), 90, 228, 237 PCI contractual obligations, 227 PCI-DSS. See Payment Card Industry Data Security Standard (PCI -DSS) Performance metrics, 120 –23 Performance testing, 179 Perimeter, 222 Persistent data masking method, 219 Personal data, 56 Personal health information (PHI), 228 Personal Information Protection and Electronic Documentation Act, 51 Personally private information (PPI), 227 PGP (Pretty Good Privacy), 219 Phishing, 233 Physical assets, 25 Physical data names, 159 Pick lists, 294 PIPEDA. See Personal Information Protection and Electronic Documentation Act Pivot, 384 Planning activity, 40 POC. See Proof of concept Point -to-point interaction model, 267 Policies and content handling, 306 Policy Data security, 237 Political governance, 75 Polyhierarchy, 296 Portability, 56 Predictive algorithms, 61 Predictive analytics, 479 –80, 486 Predictive Model Markup Language (PMML), 492 Predictive models, 485 Pre-production environments, 178 Prescriptive analytics, 480 Preservation, 302 PRISM, 159 Privacy law Canadian, 56 –57 Private -key encryption, 218 Privileges Legitimate database, 229 –30 Unauthorized, 230 Procedural DBAs, 171 Process controls, 269 Product data in manufacturing execution systems (MES), 346 Product data management (PDM), 346 Product lifecycle management (PLM), 346 Production DBA, 170 Production environment, 177 –78 Project management office, 86 Proof of concept, 190 Proprietary reference data, 335 Public policy and law, 55 Public -key encryption, 218 Publish and subscribe model, 268 Purging, 186 Quality assurance certification, 169 Quality assurance testing (QA), 179 Queryable audit data, 385 Query -level access control, 229 RACI. See Responsible, Accountable, Consulted, and Informed RDBMS. See Relational database management system (RDBMS) RDF. See Resource description framework (RDF) Readability category, 163 Readiness assessment, 204 Really Simple Syndication (RSS), 292 Real -time data, 371 –72 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "INDEX • 587 Real -time data integration flows, 278 Real -time data processing solution, 279 Realtime data synchronization, 265 Record quality, 323 Record system of, 337 Records, 298 Records management, 289, 300, 306, 314 Electronic documents and, 301 Key Performance Indicators (KPIs) and, 324 Maturity model for, 319 –20 Principles of, 290 Recovery types, 187 Redacting data, 62 Reference and master data, 48 Reference data, 331, 330 –36 Change and, 354 –55 Geo-statistical, 335 Industry, 335 Ontologies and, 335 Proprietary, 335 Standard, 336 Structure, 332 Taxonomies in, 334 Reference data management, 405 Reference data sets Assessment of, 351 Governance and, 353 Reference directories, 347 Regulated data, 226 Regulated information, 238 Regulatory classification, 242 Regulatory compliance, 89 Data governance and, 72 Questions on, 90 Regulatory requirements and data security, 236 Regulatory risk, 243 Reification transformations, 105 Related term relationship, 295 Relational database, 180 Relational database management system (RDBMS), 180 Relational OLAP, 384 Relative advantage, 566 Release management, 376 Remediation, 374 Replication, 186 Replication patterns Log shipping, 186 Mirroring, 186 Replication process, 196 Replication scheme, 188 Replication solutions, 266 Reporting strategies, 389 –90 Repository scanning, 413 Requirements analysis, 153 Resiliency, 187 Resource description framework (RDF), 316 –17 Resource description framework schema (RDFS), 297 Responsible, Accountable, Consulted, and Informed, 251 Retrieve response or performance metrics, 390– 93 Right to be Forgotten, 58 Risk, 215 Risk assessment, 204 Risk classifications, 215 Risk model, 66 Risk of Reliance on Inadequate Native Audit Tools, 244 Risk reduction and data security and, 212 Rivest -Shamir -Adelman (RSA), 219 Roadmap, 112 –13, 386 Rogers, Everett, 563 ROLAP. See Relational online analytical processing Role assignment grid, 240 Role assignment hierarchy, 240 Roll up, 383 SaaS applications. See also Data as a Service (SaaS) SAN. See Storage area network Sandbox, 179 Sarbanes -Oxley Act, 32, 51, 73, 243, 299 Scaling. See Data scaling Scanning repositories, 413 Schema, 169 Schema.org, 317 Scheme category, 163 Search engine optimization (SEO), 303, 306 Security Breach of Information Acts, 227 Security compliance managers, 243 Security metrics, 253 –54 Security patches, 248 Security Policy Compliance, 244 –46 Security restrictions data, 225 –28 Security risks assessment of, 240 Self-organizing maps, 479 Semantic modeling, 303 Semantic search, 304 Semi -structured data, 305 Sensitive data, 213, 307 Sentiment analysis, 478 Server administration teams, 199 Server virtualization, 174 Service accounts, 230 –31 Service level agreements (SLAs), 197, 389 Service registry, 405 Services -based architecture (SBA), 476 –77 Sharding process, 188 Shared accounts, 231 Shared -nothing database technologies, 489 Simple knowledge organization system (SKOS), 317 Single family regulations, 227 –28 SKOS. See Simple knowledge organization system SLAs. See Service level agreements Slice -and-dice, 383 SMART, 34 Smartphone, 566 Social engineering, 232 Social media policies, 307 Social networking sites, 234 Social system, 565 Social threats, 232 –33 Software as a Service (SaaS), 272 Software configuration management (SCM), 194 Software testing process, 201 Solid state drives (SSDs), 177 Solvency II, 32, 90 Source -to-target mapping, 374 Sousa, Ryan, 364 Spam, 235 Spatial database, 182 Specialized database, 183 Specialized hardware, 177 Spyware, 233 SQL injection attack, 231 Stages of adoption, 565 –66 Stakeholder analysis and communication planning, 570 Standard, 91 Standard markup languages, 315 –16 Standard reference data, 336 Standardization, 341 Standards category, 163 Standards, data security, 235, 238 Star schema, 366 Statistical ‘smoothing’, 60 Stewardship, 344, 351, 352 Storage area network (SAN), 177, 190 Storage environment management, 193 –94 Storage solutions, 266 Strategic alignment model, 35 –37 Strategic plan, 34 Strategy, 34 Streaming, 372 Structure category, 163 Structured query language (SQL), 180 Subject area coverage and data warehouse, 390 Subject area discriminator, 109 Subject area model, 108 Sustainability data governance and, 94 Sustainability Data Governance and, 96 Syndication, 292 Synonym ring, 295 System database, 195 System development lifecycle (SDLC), 126, 284 System of record, 337 System security risks, 228 Taxonomy, 295, 334, 374 Facet, 296 Faceted, 294 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers.",
        "588 • DMBOK2 Hierarchical, 296 Network, 296 TCO, 207 Team collaboration tools, 315 Team, building a, 554 Technical metadata, 398 Technology readiness, 204 Temporal database, 181 Term management, 294 –95 Terms, 294 Test data, 201 Test environment, 178 –79 Text mining, 478– 79 The four As of data security management, 217 The General Data Protection Regulation of the EU, (GDPR), 56 Third party data, 196 Threats, 215, 232– 33 Tightly -coupled systems, 172 Timing, 59 Total cost of ownership (TCO), 207 Trade secrets, 228 Transaction log Backup, 193 Dump, 193 Transform process, the, 261 Transition management. See also Change management Transition, checklist for managing, 542 Trickle feeds, 371 Triplestore database, 183 Trojan horse, 234 Trusted source, 337 Unauthorized privilege elevation, 230 Uniform resource locators (URLs), 303 Unintentional abuse, 229 Unstructured data, 304 Governance and, 323 Management of, 323 Metadata for, 291 Unstructured data analytics, 480 Urgency, 548, 549 US Federal Rules of Civil Procedure (FRCP), 301 US FTC requirements, 227 US Library of Congress Classification, 296 US Postal Service State Codes, 333 US Privacy Act, 55 User acceptance testing (UAT), 179 User data entitlement, 250 Utility computing, 174 Validation, 341 Value of data, 19 Virtual machine image, 174 Virtual machines (VMs), 179 Virtualization, 173– 74 Virus, 234 Vision Communicating the, 551 –53, 559– 60 Effective, 556 –58 Framing, 568 Importance of, 556 Visualization, 59, 481– 82, 487 Vital record, 299 Vocabulary Controlled, 292, 295, 296, 315 Micro -controlled, 294 Vocabulary management. See also Controlled vocabulary Vocabulary view, 294 Vulnerability, 214 W3C Web Ontology Language (OWL), 317 Warren, Samuel, 55 Wear -your -own -devices (WYOD), 307 Web address, 246 Websites, 94 White hat hacker, 232 Workflow content development and, 305 Workflow tools, 95, 314 Worm, computer, 234 WYOD. See Wear -your -own -devices (WYOD) XMI. See Extensible markup interface XML. See Extensible markup language XML databases, 184 Zachman Framework, 104, 105 Zachman, John A., 104 Order 75507 by Leonardo Guerreiro on November 26, 2025 For use on purchaser device only. Contains DRM with visible and invisible purchaser identifiers."
      ]
    },
    "DCAM.pdf": {
      "fingerprint": "DCAM.pdf|1771998375|1485152",
      "pages": [
        "© Enterprise Data Management Council – 2014 Data Management Capability Assessment Model (DCAM) Working Draft Version 0.7 July 30, 2014 Developed by",
        "© Enterprise Data Management Council – 2014 Published: July 30, 2014 EDM Council WORKING DRAFT – Version 0. 7 This Data Management Capability Model (“Model”) is being provided to you (“Recipient”) solely for peer review purposes . All materials are the proprietary property of EDM Council, Inc. (“EDM Council”) and all rights, titles and interests are vested therein. The Model, or any portion thereof, may not be distributed, copied or made available for the use of any other party without the prior written authorization of EDM Council. In addition, the Model, or any po rtion thereof, may not be used in any way by Recipient, its officers, employees or agents or by any other party without the prior written consent of EDM Council. By reviewing the Model, or any portion thereof, the Recipient (and each person reviewing the Model) agrees to the terms set forth above. © 2014 EDM Council, Inc. All Rights Reserved.",
        "© Enterprise Data Management Council – 2014 Page 1 INTRODUCTION The Data Management Capability Model (DCAM) was created by the Enterprise Data Management Council based on the practical experiences and hard won lessons of many of the world’s leading organizations. It is a synthesis of best practices associated with the management of data content across the horizon of interconnected processes. T he Data Management Capabilit y Model defines the scope of capabilities required to establish, enable and sustain a mature data management discipline. It addresses the strategies, organizational structures, technology and operational best practices needed t o successfully drive data management. It addresses the tenets of data management based on an understanding of business value combined with the reality of operational implementation. To manage data in today’s organizational environment starts by recogn izing that proper data management is about managing data as “meaning”. This is a relatively new concept for many organizations. It is not easy to articulate and not very well understood. Data exists everywhere within an organization and must be managed consistently within a well -defined control framework. The DCAM helps identify this framework by defining the capabilities required to make data management a critical part of a firms’ everyday operational fabric. The challenges of properly managing data a re significant. There are many legacy repositories and a plethora of functions to unravel. There are social and political barriers to overcome. There are real IT challenges and execution gaps to address. Data ownership and accountability are hard to im plement. Funding is often project based. And many firms simply don’t have the strong executive support that is needed to ensure that the organization stays the course in the face of short term measurement criteria, operational disruption and conflicting stakeholder challenges to properly address the realities of the data management challenge. We understand this reality because we’ve been there and we have the scars across our back to prove it. Data is foundational. It is the lifeblood of the organizati on. The “bad data” tax is a significant expenditure for many firms. Unraveling silos and harmonizing data is the prerequisite for eliminating redundancy, reducing reconciliation and automating business processes. Managing data is essential if we are to gain insight from analytics, feed our models with confidence, enhance our service to clients and capitalize on new (but often fleeting) business opportunities. DCAM provides the guidance needed to assess current state, and provide the objectives of targ et state , for your data program. The DCAM is organized into eight core components . 1. The Data Management Strategy discusses the elements of a sound data strategy, why it is important and how the organization needs to be organized to implement. 2. The Data Management Business Case and Funding Model addresses the creation of the business case, its accompanying funding model and the importance of engaging senior executives and key stakeholders for approval. 3. The Data Management Program discuss es what’s organizationally needed to stand up a sustainable Data Management Program. 4. Data Governance defines the operating model and the importance of policies, procedures and standards as the mechanism for alignment among (and compliance by) stakeholders. 5. Data Architecture focuses on the core concepts of “data meaning” – how data is defined,",
        "© Enterprise Data Management Council – 2014 Page 2 described and related. 6. Technology Architecture focuses on the relationship of data with the physical IT infrastructure needed for operational deployment. 7. Data Quality refers to the concept of fit -for-purpose data and the processes associated with the establishment of both data control and data supply chain management. 8. Data Operations defines the data lifecycle process and how data content management is integrated in to the overall organizational ecosystem. Each component is preceded with a definition of what it is, why it is important and how it relates to the overall data management process. These are written for business and operational executives so as to demyst ify the data management process. The components are structured into 3 5 capabilities and 109 sub-capabilities. These capabilities and sub -capabilities are the essence of the DCAM. They define the goals of data management at a practical level and establi sh the operational requirements that are needed for sustainable data management. And finally, each sub -capability has an associated set of measurement criteria to be used in the evaluation of your data management journey. Welcome to the world of data management. The EDM Council is indebted to the dozens of members who have contributed to the development of the Data Management Capability Model. We are always searching for ways to enhance and improve the model. We encourage your feedback. We are in terested in your rants, raves and alternative points of view. For more information on the DCAM and on the EDM Council, please contact us at info@edmcouncil.org Michael Atkin John Bottega Managing Dir ector Executive Advisor EDM Council EDM Council atkin@edmcouncil.org jbottega@edmcouncil.org",
        "© Enterprise Data Management Council – 2014 Page 3 Contents FOREWARD ................................ ................................ ................................ ................................ ............................... 4 1.0 DA TA MANAGEMENT STRA TEGY ................................ ................................ ................................ ......... 7 2.0 THE DA TA MANAGEMENT BUSINESS CASE AND FUNDING MODEL ................................ ........... 13 3.0 DA TA MANAGEMENT PROGRAM ................................ ................................ ................................ ........ 17 4.0 DA TA GOVERNANCE ................................ ................................ ................................ ............................. 22 5.0 DA TA ARCHITECTURE ................................ ................................ ................................ .......................... 34 6.0 TECHNOLOGY ARCHITECTURE ................................ ................................ ................................ .......... 38 7.0 DA TA QUALITY PROGRAM ................................ ................................ ................................ ................... 42 8.0 DA TA OPERA TIONS ................................ ................................ ................................ ................................ 48",
        "© Enterprise Data Management Council – 2014 Page 4 FOREW ARD The concept of data as a foundational component of business operations has arrived. It is now understood as one of the core factors of input into the full spectrum of business and organizational processes. The common theme for firms that are effective in their use of data to reduce operational costs, automate manual processes, consolidate redundant systems, minimize reconciliation and enhance business opportunities is the impl ementation and management of a data control environment . The reason why firms i mplement a control environment is to ensure trust and confidence among consumers that the data they are relying on for business processing and decision -making is precisely what they expect it to be – without the need for manual reconciliation or without re liance on data transformation processes. The core components associated with the implementation of a control environment are needed to ensure that all data elements/attributes are precisely defined, aligned to meaning, described as metadata and managed ac ross the full data lifecycle. The key to establishing a control environment however, is the achievement of “unambiguous shared meaning” across the enterprise as well as the governance of the processes related to ensuring definitional precision. Data must be consistently defined because it represents a real thing (i.e. a product, client, account, counterparty, transaction, legal entity, location, process, etc.). All other processes are built upon this foundation. In a fragmented data environment (the op posite of a control environment) applications development can result in ad hoc naming conventions which exacerbate the problem of common terms that have different meanings, common meanings that use different terms and vague definitions that don’t capture c ritical nuances. For many firms this challenge can be debilitating because there are thousands of data attributes, delivered by hundreds of internal and external sources, all stored in dozens of unconnected databases. This fragmentation results in a cont inual challenge of mapping, cross -referencing and manual reconciliation. In order to achieve a control environment, every data attribute must be understood at its “atomic level” (as a fact) that is aligned to business meaning without duplication or ambigu ity. Managing data as meaning is the key to alignment of data repositories, harmonization of business glossaries and ensuring that applications dictionaries are comparable. Achieving alignment on business meaning (including the process of how terms a re created and maintained) can be a daunting task. It is not uncommon to experience resistance from business users and IT - particularly when there are multiple existing systems linked to critical business applications. The best strategy for reconciliati on in a fragmented environment is to harmonize on the legal, contractual or business meaning rather trying to get every system to adopt the same naming convention. Nomenclature represents the structure of data and unraveling data structures/data models ar e expensive and not necessary. It is better to focus on precisely defining business concepts, documenting transformation processes and capturing real -world data relationships. Once established, existing systems, glossaries, dictionaries, repositories, e tc. can be cross -referenced to common meaning. Managing data as meaning is the cornerstone of effective data management. It needs to be managed along with other “metadata” to ensure consistency and comparability across the enterprise. The other componen ts of metadata can be organized into three core categories: descriptive metadata (i.e. information that identifies where data is located); structural metadata (i.e. information about the physical data layer and how the data is structured) and administrativ e metadata (i.e. information about when the data was created, its purpose and access rights). Data meaning and metadata management are best understood as the core of your content infrastructure and the baseline for process automation, applications integr ation and alignment across linked processes. The implementation and management of a control environment is governed by standards, policies and",
        "© Enterprise Data Management Council – 2014 Page 5 procedures. These are the essential mechanisms for establishing a sustainable data management program and for e nsuring compliance with a control environment in the face of organizational complexity. Managing meaning is the key to effective data management. Meaning is achieved through the adoption of semantic standards. Standards are governed by policy. Policy i s established by executive management, supported by data owners and enforced by Corporate A udit. Get the data infrastructure established and governed – it represents the foundation for operational efficiency and must not be compromised. Control Environm ent Capability Objectives 1. The concept of a control environment is understood by relevant stakeholders and adopted by the organization (standards -based, harmonized across lifecycle, unique identifiers, aligned to meaning). The organization recognizes the need for a control environment to meet business, operational and regulatory objectives ( see data management strategy ) 2. The components associated with a control environment have been defined, verified by stakeholders (i.e. inventoried and confirmed), aligne d with technical capability and approved by executive management. Policies, procedures and standards exist for all relevant areas including data quality, data access/distribution, authorized use/entitlement control, data privacy and data security 3. The fr amework for implementing a control environment, including reconciliation of disparate systems, have been fully resourced ( see business case and funding ) 4. The standards that are needed to implement the control environment are defined and verified by stakeho lders (for relevant products, accounts, clients, business partners, legal entities, counterparties, vendors, etc.). Business processes are identified, documented and aligned with data requirements. 5. Data attributes for relevant business processes are know n, segmented according to criticality and understood in the context of how data is compounded/how derived data elements are calculated. Existing systems, processes, repositories and consuming applications across the full data lifecycle are mapped to the c ontrol environment standards and aligned with systems of record. Rules and conversion procedures for transformation and cross -referencing are documented. Shared data attributes are identified and mapped to processes and sub -processes. 6. Standard identifie rs, metadata and taxonomies are established and integrated across the enterprise for all functions and processes. The process for new standards adoption is documented and implemented 7. Data in all repositories are aligned to “common meaning” as an ontology . The ontology is modeled and verified by SMEs. There is a common method for defining, achieving agreement, updating and promulgating the concept of “single term, single definition” based on how business processes work in the real world. All changes to the corporate ontology are synchronized and aligned to the systems of record 8. Procedures are in place to manage changes and exceptions to the control environment 9. A centralized/aligned metadata repository is implemented and maintained. The metadata repository is managed from descriptive, structural and administrative dimensions",
        "© Enterprise Data Management Council – 2014 Page 6 10. All new product development initiatives, data integration activities and data consolidation efforts use the control environment standards 11. The control environment is governed acr oss the enterprise with clear accountability. The governance process consists of a combination of IT infrastructure, program management offices, data administrators and data owners ( see governance ) 12. Compliance with the control environment is monitored, measured and audited. Results of the compliance audit is shared with executive management 13. Communications mechanisms are in place to ensure that the goals, policies and procedures of the control environment are implemented; that business and IT can commun icate with each other; that issues can be escalated as appropriate; that priorities are established; that policies and standards are implemented and that employees are in compliance with the control processes ( see governance ) 14. There is close cooperation be tween the Board of Directors, executive management, lines of business, information technology and operations on the implementation and management of the control environment. Stakeholders receive training in the policies that exist and the procedures that need to be followed to achieve organizational compliance 15. Executive management sets expectations and gives authority to implement the control environment. Expectations are translated into incentives and operational constraints. Lines of business are mana ging within established boundaries. Performance is linked to implementation of the control environment Scope of work required to achieve a data control environment",
        "© Enterprise Data Management Council – 2014 Page 7 1.0 DATA MANAGEMENT STRATEGY Definition : The Data Management Strategy determines how data management is defined, organized, funded, governed and embedded into the operations of the organization . It defines the long -term vision including a description of critical stakeholder or stakeholder functi ons that must be aligned. Data Management Strategy demonstrates the business value that the program will seek to achieve. It becomes the blueprint (or 'master plan') that describes how the organization will evaluate, define, plan, measure and execute a successful and mature data management program. Purpose: The purpose of developing a Data Management Strategy is to articulate the rational for the data management program . The strategy defines “why” the program is needed as well as the expected benefits, goals and objectives . The strategy also describes “how” to align and mobilize the organization in order to implement a successful data management program. Introduction: A Data Management Strategy defines the organizational rationale for impleme nting a data management program, explains what the overall program aims to achieve and identifies how the various components of the initiative fit together. A strategy accurately reflect s the requirements of the data consumers in order to give confidence to stakeholders that the data management program will be valuable, practical and managed in an effective manner. A data management strategy should emphasize the importance of collaboration as well as the data challenges that result due to the interconnec ted nature of business processes . A data management strategy defines the overall framework of the program. It should be structured to address the core principles of data management so that critical stakeholders can understand the value of a data manageme nt program as it relates to their functions and strategic initiatives. A Data Management Strategy needs to:  Articulate the scope of the data management program  Establish the priorities for phased implementation  Provide the guidance for establishing the data governance framework  Express the importance of developing a data quality program  Reinforce the use of data content standards.  Reflect practical implementation reality and alignment to IT and operational cap abilities  Define rational timeframes for implementation  Address the importance of establishing and staffing the data management program function  Address the importance of developing a sustainable funding model  Address the importance of developing evalua tive criteria to measure and monitor program progress and effectiveness. Central to a data management strategy is the articulation of the “target state”. An effective data management strategy describes target state objectives, identifies key stakeholders , discusses organizational structure, accountability and describes the need for discipline and governance. More importantly, a strategy identifies operational inefficiencies and gaps. It is important for a strategy to compare target state to current stat e in order to show the organizational, functional and technological gaps, and then demonstrate the how these gaps will be closed.",
        "© Enterprise Data Management Council – 2014 Page 8 Data management strategy is not static and must be able to evolve as the need of the organization change. The most effective and successful data management strategies are those that are visibly endorsed by executive management and are supported by mandatory organizational policy. Goals:  Define a strategy that is aligned with the goals and objectives of the organization and ensure this strategy is approved by all relevant business, technology, operational and executive stakeholders  Explain the importance of establishing a recognized and sustainable data management \"program\". Define the need for metrics to assess the program and to ensure alignment with established cost/benefit evaluation methodologies.  Capture high level data requirements. Ensure all relevant corporate audit and regulatory issues have been identified and that key stakeholders understand and agree to the high level requirements.  Define the process for determining scope and priorities of the Data Management Program. Ensure that the scope of the Program is aligned with defined business value and organizational priorities.  Make sure the Program can be practically implemented from both a technical and architectural perspective .  Identify high -level immediate; transitional and long -term deliverables as well as associated resource and funding requirements necessary to implement and sustain the data manageme nt program.  Ensure the data management strategy is clearly articulated and communicated across the organization and is reflected in architectural technology planning. Core Questions:  Does the Data Management Strategy clearly articulate the reason and th e importance of implementing a separate Enterprise Data Management Program?  Is there executive, operational, technology and business buy -in? Do stakeholders agree to support and sustain such a program?  Has the Data Management Strategy sufficiently frame d the immediate, medium and long -term goals and objectives of the Data Management Program in line with organizational priorities?  Has the Data Management Strategy effectively identified the critical areas of focus including how priorities are established a nd verified?  Has the Data Management Strategy identified staffing resources, operating model and the funding approach needed to establish, lead and maintain the Data Management Program?",
        "© Enterprise Data Management Council – 2014 Page 9 Capabilities and Assessment Criteria: 1. DATA MANAGEMENT STRATEGY 1.1. A Data Management Strategy (DMS) is Specified and Shared with Relevant Stakeholders 1.1.1. DMS is developed The data management strategy needs to be documented in collaboration with the full spectrum of business, technology and operations management. Capability Objectives  DMS has been documented .  DMS has been aligned with business, technology and operations .  DMS has been published to all relevant stakeholders . 1.1.2. The DMS is aligned with the high-level organizational objectives. High level organizational objectives are those identified by executive management as organizational goals (i.e.: the organizational objective is to improve customer support and services). Capability Objectives  DMS is fully mapped to and aligned with the high-level organizational objectives .  DMS is approved by the executive committee and relevant stakeholders .  Process is established to ensure the future alignment of the DMS to organizational objectives . 1.1.3. The DMS includes an established mechanism for approval Capability Objectives  Mechanism for capturing feedback from relevant stakeholders exists .  Feedback has been collected and incorporated into the DMS .  DMS has been reviewed and approved by relevant (named) stakeholders . 1.1.4. The DMS is enforced and audited for compliance Capability Objectives  Audit has reviewed and approved the DMS .  Audit has determined that its implementation can be enforced via existing corporate audit examinations .  Audit is actively conducting examinations of data programs to ensure adherence to the DMS . 1.2. High Level Business Requirements are Captured, Prioritized, and Integrated into the DMS 1.2.1. High level business requirements have been documented and used to create the DMS Capability Objectives  High level business requirements for critical business lines and corporate functions have been documented .  High level business requirements for critical business lines/corporate functions have been incorporated into the DMS .",
        "© Enterprise Data Management Council – 2014 Page 10 1.2.2. Requirements incorporated into the data management strategy have been prioritized and approved by identified stakeho lders. Capability Objectives  Business requirements (incorporated into the data management strategy ) have been revie wed, prioritized and approved by identified stakeholders .  Regular requirements review cycles have been established . 1.3. The DMS Defines the Importance of Identifying , Prioritizing and Assuring the Appropriate Use of Authorized Data Domains. 1.3.1. The DMS defines the requirement to identify, inventory and prioritize authorized data domains Data Domains are logical categories of data that are designated as prioritized factors o f input into critical business functions. For example, “trade data” or “regional sales data” could be designated as authorized data domains . Data domains are identified based on the strategic understanding of business requirements and an awareness of the organizational priorities. Capability Objectives  DMS defines the importance of data domain designation .  DMS defines the need to create and govern the data domain inventory . 1.3.2. The DMS articulates the importance of establishing enterprise policy to enforce appropriate use of authorized data domains. Capability Objectives  DMS defines the need for established policy about the use of authorized data domains .  DMS defines the need for governance over the use of authorized data domains . 1.4. The DMS is Aligned with and Mapped to Architectural, IT and Operational Capabilities 1.4.1. Data architecture concepts hav e been incorporated into the DMS . Data Architecture focuses on the design, definition, management and control of data content. This includes giving data business meaning, describing its metadata, and designing and managing taxonomies and ontologies (See Information Architecture) Capability Objectives  Data architecture concepts are defin ed and incorporated into the DMS .  Data architecture concepts are aligned with stakeholder plans and roadmaps .  Data architecture concepts are approved by relevant stakeholders . 1.4.2. Technology concepts have been incorporated into the DMS . Technology concepts refer to the strategy, design and implementation of the physical infrastructure (platforms and tools) in support of the DMS . (See Information Architecture) Capability Objectives  Technology concep ts are incorporated into the DMS .  Technol ogy concepts are aligned with stakeholder plans and roadmaps .  Technology concepts are approved by relevant stakeholders .",
        "© Enterprise Data Management Council – 2014 Page 11 1.4.3. Operational concepts have been incorporated into the DMS . Operational concepts include such areas as uptime requirements; business continuity planning; retention and archiving guidelines; defensible destruction requirements; privacy standards; etc. Capability Objectives  Operational concepts are incorporated into the DMS .  Operational concepts are aligned with operational goals and obj ectives.  Operational concepts have been approved by relevant operations groups . 1.5. The DMS Requires the Creation of an Effective Governance Program. 1.5.1. The DMS defines the purpose and objectives for establishing data governance Capability Objectives  The DMS specifies the need for the creation of a data governance program .  The DMS articulates the purpose, objectives and expected outcomes of the data governance program. 1.5.2. The DMS describes the data governance target state organizational structure Capability Objectives  The target -state governance program organizational structure is defined in the DMS . 1.5.3. The DMS describes the governance roles and responsibilities Roles and responsibilities of the data management organization as well as the roles and responsibilities of the business -line data executives and data s tewards are addressed in the DMS . Capability Objectives  The DMS identifies the relevant governance stakeholders .  The DMS describes the roles, responsibilities and relationships of the stakehol ders. 1.6. The DMS Defines How the Data Management Program will be Measured and Evaluated. 1.6.1. The DMS defines how the data management program itself will be measured Program categories include areas such as governance, policies, standards implementation, stakeholder buy -in, etc. Capability Objectives  The DMS calls for the development of metrics to track program progress .  Metric plans are socialized with relevant stakeholders .  Feedback is received and incorporated into the data management strategy.  Stakeholders review and approve the metric plans and approach . 1.6.2. The DMS defines the importance of developing outcome metrics to determine the effectiveness of the data management program. Outcome metrics areas include improved data quality, reduction in op erational fails, improved discovery, access to critical data, etc.) Capability Objectives  The DMS calls for the development of outcome metrics.  Outcome metric plans are socialized with relevant stakeholders  Feedback is received and incorporated into the DMS  Stakeholders review and approve the outcome metrics plans and approach",
        "© Enterprise Data Management Council – 2014 Page 12 1.6.3. The DMS defines the approach needed to evaluate the adherence to the DMS implementation. The DMS defines how adherence to the strategy will be measured and tracked. This would include items such as appointment of required resources to support the data management program; adoption of standards; compliance with policy; etc. Capability Objectives  The DMS identifies the approach to measuring adherence to the Data Management Strategy .  The approach is socialized with relevant stakeholders .  Feedback is captured and incorporated into the DMS.  Stakeholders review and approve . 1.7. The Data Management Strategy Calls for the Creation of a Communication and Training Program 1.7.1. The DMS describes the importance of data management program awareness through the communication of goals and objectives, scope, priorities, policies and standards. Capability Objectives  DMS identifies the need for a communication strategy .  DMS defines the core components, scope and reach of the communications and training program. 1.7.2. The DMS details the need for an education and training program to ensure stakeholder understanding, buy -in and compliance to the data management program Capability Objectives  DMS defines the need for training on the purpose and objectives of the data management program .  The DMS addresses the approaches and methodologies for a comprehensive data management training program.",
        "© Enterprise Data Management Council – 2014 Page 13 2.0 THE DATA MANAGEMENT BUSINESS CASE AND FUNDING MODEL Definition : The Data Management Business Case is the justification for creating and funding a data management program. The DM Business Case answers the \"why\" questions. It addresses the \"so w hat\" challenges. It articulates the major data and data related issues facing a firm or business function and describes the expected outcomes and benefits that can be achieved through the implementation of a successful data management program. Data Management Funding Model provides th e rationale for the investment in data management, the mechanism to ensure the allocation of sufficient capital needed for implementation and the methodologies used to measure both the costs and contributions derived from the data management program. Toge ther, the Data Management Business Case and the Data Management Funding Model are critical steps needed to ensure program stakeholder commitment and agreement to the overall objectives of the program. Purpose: Data Management is no different than any oth er established business process. It needs to be justified, funded, measured and evaluated. The Data Management Business Case provides the rationale for the investment in data management. It provides clarity of purpose, enabling agreement and support of program objectives from senior executives as well as program stakeholders. The Data Management Funding Model describes the overall framework used to ensure that the objectives and processes of data management become a sustainable activity within the organ ization. Introduction: The Data Management Business Case articulates the benefits of the data management function, in alignment with the objectives defined, communicated and agreed upon in the Data Management Strategy. It discusses both the defensive benefits of the program (satisfying regulatory requirements, improved risk management, improved data governance and control, improved data quality), as well as the offensive benefits of the program (improved customer service, innovative product development , increased revenues, improved market penetration). The business case is the cost/benefit realization of the set of activities and deliverables expected from data management and should include both the strategic dimensions (i.e. the establishment of trust and confidence in the end -to-end flow of data) as well as the tactical components (i.e. number of transformation processes, the inventory of data -intensive applications) related to program implementation. The strategic aspects of the business case answer s the “why the firm is focusing on data management” questions, helps achieve alignment across the stakeholders and is instrumental in reducing disagreements over ownership of the data management program. The tactical business case is easier to translate i nto “spreadsheet metrics,” is specific to the individual firm and helps management understand the costs, benefits and risks associated with the maturity of the data management program. In all cases, it is essential to link the business case with realisti c strategic and tactical measurement criteria and align both of them with the long term sequence plan for the data management program. This enables the organization to understand the total costs associated with implementation as well as maintenance of the data management program and helps ensure that it is sufficiently funded to meet both near term and long range objectives. The funding model defines the mechanism used to generate and maintain capital needed for the data management program throughout its lifecycle. It establishes the methodology used for cost allocation",
        "© Enterprise Data Management Council – 2014 Page 14 among business lines and can be used to help align stakeholders on funding -related issues. In mature organizations, the funding model reflects the individual requirements of the various c omponents of the organization and is integrated with governance to ensure that appropriate oversight and accountability is applied to data management. Verifiable metrics are essential and the metrics must be aligned with tangible business objectives. A well-structured funding model can help avoid debates over business priorities, mitigate internal competition and facilitate open discussions among relevant stakeholders. There is no single model for funding data management initiatives. The specific mode l implemented will depend on the dynamics and operational culture of the individual firm. However, t he fundamental components of the funding model should always include areas such as: investment criteria/priorities, budget management, delivered versus exp ected benefits, allocation methodology and capital needed for ongoing management of the program. It is important to recognize that there will be both fixed and variable components to the funding model for data management. Strong consideration should b e given to allocating initial funding as a fixed corporate expenditure. Data management programs that are funded as a capital investment have a much greater rate of long -term success as compared to ‘grass roots’ funding. Grass roots funding can become mi red by competition among business units, is often aligned with a tactical view of data management and frequently reinforces short -term evaluation cycles. An organization can expect its funding model to evolve along with the maturity of their data managem ent program. Goals:  Create a data management business case based on verified input from stakeholders across lines of business that incorporate both strategic and tactical objectives of the data management program  Align the business case to the agreed -upon business drivers and organizational objectives  Ensure the business case has been socialized and agree to by program stakeholders to ensure commitment and support of the data management objectives  Establish a funding model that supports the agreed upon Dat a Management Business Case and fits with the culture of the organization in order to ensure buy -in from program stakeholders and commitment to sustainability.  Create a mechanism to ensure that the business case remains aligned with business objectives as the organization evolves and matures. Core Questions:  Are the strategic goals of the organization reflected in and aligned with the Data Management Business Case and Funding Model?  Is the Funding Model sufficient to support the implementation the data management program.  Have the funding requirements been translated into the business case and aligned with the objectives, sequence priorities and implementation roadmap of the data man agement strategy  Does the funding model cover all aspects of data management (e.g.: tangible, intangible, special requests, urgent requirements, unique applications, etc.)  Is there a defined process with established criteria for determining and verifying t he investment required for data management and is it aligned with the business structure, priorities and governance process organization.",
        "© Enterprise Data Management Council – 2014 Page 15 Capabilities and Assessment Criteria: 2. DATA MANAGEMENT BUSINESS CASE AND FUNDING MODEL 2.1. The Data Management Business Case is Aligned to Strategic Drivers and Tangible Business Outcomes in Collaboration with Defined Stakeholders. 2.1.1. The Data Management Business Case is mapped to and aligned with drivers, requirements and strategy Capability Object ives  The DM Business Case is aligned with the business objectives and strategic priorities of the line of business.  The DM Business Case is mapped and aligned with organizational priorities and objectives. 2.1.2. High level business outcomes are defined and sequenced. A primary function of the business case is to define the challenges of the current state and to define the pathway to improvement . Capability Objectives  Expected outcomes are defined and sequenced.  Current -to-Target State is defined and articulated . 2.1.3. DM Business Case is socialized and validated by program stakeholders Buy-in is predicated on stakeholder validation of the viability of the proposed program Capability Objectives  The DM Business Case has been socialized to program stakeholders .  Target objectives have been reviewed and validated.  Outcomes, benefits, timelines and target thresholds have been reviewed and approved . 2.2. The Data Management Funding Model has been established, approved and adopted by the organization. 2.2.1. The DM Funding Model is matched to business requirements, implementation timelines and operational capabilities. Capability Objectives  DM Funding Model is proposed and socialized with program stakeholders.  Feedback is being collected and incorporated into the model.  DM Funding Model has been approved by program stakeholders.  DM Funding Model is reviewed and enhanced as part of the annual funding process to reflect evolving requirements. 2.2.2. The DM Funding M odel is aligned with the business process of the organization Capability Objectives  DM Funding addresses current year budget cycle.  DM Funding is mapped to a multi -year implementation plan.  Data management funding is integrated as a sustainable corporate function.",
        "© Enterprise Data Management Council – 2014 Page 16 2.2.3. Implementation of the DM Funding Model is enforced. Capability Objectives  Funding is a llocated and approved by the lines of business .  All budgets are reviewed and approved by the Data Management Organization .  Data Management Organization is empowered to enforce the line of business data management funding allocation in accordance with DM Program objectives. 2.3. The Funding Model can be measured and evaluated against tangible business objectives 2.3.1. Total expense for the Data Management Program is captured, maintained and analyzed Capability Objectives  Total expense is captured, maintained and analyzed at both the line of business and organizational levels . 2.3.2. A standard methodology for calculating ROI is established Capability Objectives  Methodology for ROI is established by individual lines of business as well as at the organizational level for aggregate evaluation. 2.3.3. ROI is measured, monitored and used for making Data Management Program decisions Capability Objectives  ROI is measured, monitored and used for LOB decision making.  ROI is aggregated at the organizational level and used to influence data management program priorities .",
        "© Enterprise Data Management Council – 2014 Page 17 3.0 DATA MANAGEMENT PROGRAM Definition : A Data Management Program is an organizational function dedicated to the management of data as an asset throughout an organization. It illustrates how the management of data quality, definition and content supports strategic, busin ess and operational obje ctives. It reinforces the necessity of orchestration, active collaboration and alignment among diverse stakeholders in order to instill confidence in data as a trusted factor of input into business and operational processes . Purpose: The purpose of a Da ta Management Program is to embed the concepts of data management into the operational framework of an organization on a sustainable basis. T he creation of the data management program elevates the importance of data content management and integrates it as a core component of organizational operations. It establishes data management as a sustainable activity and reinforce s the importance of managing “data as meaning” across the organization. Introduction: The concept of managing “data as meaning” is not always well understood. For many organizations, data is understood as something to process. It is acquired, normalized, stored, processed and integrated into applications. And while data processing is a critical function, data is also designed to be an accurate representation of real and meaningful things ( i.e. an obligation associated with a business deal, ingredients into a production process, identifying credentials of a customer, role performed in an organizational relationship, etc. ). The establishment of the data management program within organizations is designed to ensure the management of data as meaning and help orchestrate the alignment of data precision with data processing capabilities. These are complementary activities t hat should be viewed as the “factors of production” for information intensive organizations. The data management program defines the key components that are needed to ensure trust and confidence in data content and provides guidance for its interaction ac ross the organization. The function of data management is derived from an understanding of business objectives and organizational priorities as well as knowledge of how data flows from initiation through validation through enrichment through transformati on and into consuming applications. In many environments, data content is understood as a manufactured product and flows through organization as part of a linked process. It is this linked nature of the process that highlights the collaborative componen ts of data management. The goal is to instill a sense of collective ownership of data quality among all relevant stakeholders. The data management program should be established as a formal, independent and sustainable part of the organization. The lin es of responsibility and accountability need to be established. An inventory should be created to ensure the Office of Data Management has access to the appropriate staff resources and functional capabilities in order to deliver the data needed to support organizational objectives. An effective data management program has the strong support of executive management, appropriate governance authority to ensure to implementation of a control environment for data and a well -structured model of how stakeholder s will engage on data -related issues. An effectively designed data management program that is flexible enough to accommodate to changing circumstances will help embed the importance of data content management into the culture of the organization.",
        "© Enterprise Data Management Council – 2014 Page 18 Goals:  Ensure that the Data Management Program is established , communicated and institutionalized as an independent and sustainable activity  Ensure that the Data Management Program is staffed to provide sustainable operation  Establish the role, responsibil ity, accountability and authority of the program stakeholders  Establish the stakeholder engagement model to ensure consistency in day -to-day operations, interactions and decision making.  Establish the structure and process to ensure that executive management support is institutionalized  Define the organizational structure and process to ensure stakeholder’s program adherence and adoption as well as conflict escalation and resolution. Core Questions:  Is the data management function aligned with the data management strategy and organizational objectives?  Does our organization have the right mixture of skills, resources and capabilities to effectively implement and govern the data management function?  Does the Data Management Program have the appropr iate support from executive management?",
        "© Enterprise Data Management Council – 2014 Page 19 Capabilities and Assessment Criteria: 3. DATA MANAGEMENT PROGRAM 3.1. The Data Management Program is Established and Empowered. 3.1.1. The Data Management Program is established. Data Management Program is established and communicated to all relevant stakeholders Capability Objectives  Data Management Program is sanctioned by executive management .  The role of the Data Management Program is communicated across the firm through formal organizational channels . 3.1.2. The Data Management Program has the authority to enforce adherence and compliance. Data Management Program must be formally empowered by senior management and its role communicated to all relevant stakeholders . Capability Objectives  Data Management Program is opera ting collaboratively with program stakeholders.  Data Management Program has the authority to enforce adherence and compliance through policy and documented procedure. 3.2. The roadmaps for the Data Management Program are developed, socialized and approved . 3.2.1. Program roadmaps are defined, developed and aligned with the Data Management Strategy Program roadmaps define “target state ”, and describe the steps required to attain. Roadmap topics include, but are not limited to governance structure; content managemen t strategy; infrastructure design; data architecture; etc. Capability Objectives  Program roadmaps are developed.  Program roadmaps are aligned to all components of the Data Management Strategy. 3.2.2. Program roadmaps are socialized and agreed to by Program S takeholders It is essential that roadmaps are shared with relevant stakeholders. Working with stakeholders during the development phases invites collaborative feedback and buy -in. Capability Objectives  Data Management Program roadmaps are shared with and aligned to the roadmaps of the program stakeholders (i.e.: architecture; technology; operational roadmaps etc.) .  Stakeholders verify and approve Data Management Program roadmap alignment. 3.2.3. Project plans are developed detailing deliverables, timelines and milestones Once roadmaps are agreed to and approved, they must be translated into tangible mechanisms for delivery. The Data Management Program Office is responsible for the creation, coordination and management of the data management project plans. Capability Objectives  Project plans are developed and aligned to program implementation roadmaps .  Routine program review procedures are in place to track progress of development plans .",
        "© Enterprise Data Management Council – 2014 Page 20 3.3. Stakeholder Engagement Established and Confirmed 3.3.1. Identified stakeholders commit and are held accountable to the Data Management Program deliverables. Data Management often requires participation and cooperation from staff and resources outside the data management program organizational structure. Those identified as relevant stakeholders must be held accountable for on time and on budget project delivery. To strengthen that commitment, performance in support of the data management program should reflect in stakeholder reviews and/or compensation. Capability Object ives  Roadmaps and program milestones have been communicated to the program stakeholders.  Program stakeholders have reviewed program deliverables .  Stakeholders are in agreement with ( and are being held accountable ) to program deliverables.  Program stakeholders are committed to the program deliverables through job description modification and/or through compensation/bonus. 3.3.2. Funds are allocated and aligned to program roadmaps and workstreams Sufficient funding dedicated to the data management program must be committed to by business, technology and operations. In a mature data management program, the Data Management Office is granted authority to review and approve committed budgets. Capability Objectives  Funding has been allocated and aligned to the program roadmaps and workstreams.  Funding allocations have be en reviewed by the data management PMO .  Funding c hallenges have been discussed and reconciled .  Funding levels have been approved and allocated. 3.3.3. Resource plans are ali gned with and verified against p rogram requirements. Proper resource levels with appropriate skillsets must be secured by relevant stakeholders Capability Objectives  Resource planning is complete  Resource plans have been reviewed by the data management PMO .  Resource challenges have been discussed and re conciled .  Resources plans have been approved. 3.4. Communication Program is Designed and Operational 3.4.1. Communication Plan s have been created, published and implemented Capability Objectives  Communication Plans have been developed and shared with relevant stakeholders .  Program communications are operational and implemented . 3.4.2. Communications channels have been established. The full spectrum of communications channels (i.e. websites, access portals, reference libraries, documents, training materials, town hall meetings, etc.) need to be used to ensure that stakeholders understand the goals, objectives and processes associated with the data management program . Capability Objectives  Communication channels are established .  Communications are actively being updated and delivered to relevant stakeholders .",
        "© Enterprise Data Management Council – 2014 Page 21 3.4.3. Active engagement with industry regulatory bodies is established to ensure communication of program management and development Capability Objectives  Communication to Regulators occurs during routine Regulator meetings.  Proactive communications are taking place with Regulatory bodies. 3.4.4. Active engagement with external industry and standards bodies are in place Engagement with industry trade organizations, research consortia and standards bodies ensure that the organization is aware of and aligned with the latest trends associated with data management and new developments related to the data management best pract ice Capability Objectives  Stakeholders are kept abreast of changes and events in the data management industry.  A formal function is established with dedicated resources to actively participate in data management industry activities and events. 3.5. Data Management Routines are Established, Operational and Measured 3.5.1. Routines for support of the data management program have been established Routines for steady -state operations of the data management program are taking place. Routines include but are not limited to regular stakeholder meetings, planning sessions, status reporting, etc. Capability Objectives  Program routines required for operational support have been identified and scheduled.  Program routines, meetings and working sessions are taking place 3.5.2. Issue identification, prioritization, escalation and conflict resolution are defined and operational An established escalation process is necessary to resolve conflicts, reconcile priorities and ensure efficient operations. It demonstrates improved service to the organization, promotes the benefits of an established data management program, and is an important operational routine expected of audit and regulatory reviews Capability Objectives  Issue Management routines (meetings; check points; etc.) ar e defined .  Issue Management routines are documented and operational . 3.5.3. Metrics (i.e.: KPIs, KRIs) are defined and used to track Program progress. Capability Objectives  Program tracking metrics have been designed.  Program tracking metrics are being captured and reported.  Program metrics are being analyzed and incorporated into program modifications.",
        "© Enterprise Data Management Council – 2014 Page 22 4.0 DATA GOVERNANCE Definition : Data Governance is the backbone of a successful Data Management Program. Data Governance is the process of setting standards, defining rules, establishing policy and implementing oversight to ensure adherence to data management best practices. Governance is the formalization and empowerment of the Program to ensure propagation and sustainability throughout the organization. Purpose: The purpose of Data Governance is to formalize Data Management as an established business function . Data Governance establish es the rules of engagement, drive s funding and prioritization, enforce s compliance. Data Governance defines the guidelines for data movement , which defines how data will be acquired, persisted, distributed, appropriately used, archived and/or defen sibly destroyed. Data Governance defines oversight by establishing control guidelines, approval processes and evaluation of adherence to policies and procedures. Data Governance ensure s that data management principles are fully defined, stakeholders are identified and empowered and adoption is achieved . Governance also ensures that technology, business and operations functions are held responsible and accountable for the maintenance, quality and proper use of data throughout the organization. Introduction: Governance is the key to successful data management. It establishes lines of authority and ensures that the principles of data management can and will be implemented. It establishes the mechanisms for stakeholder collaboration and defines the organizati onal structure by which the data program will be managed. The governance infrastructure determines where the program resides in the corporate hierarchy, helps manage stakeholder expectations, ensures the adoption of policies and standards, articulates the mechanism for conflict resolution, ensures adequate funding and sets the methodology for measuring data management progress. Governance over the data management program is multidimensional and includes activities related to strategy, operations, data ar chitecture, IT implementation, data quality and procurement. It is not created as a steady state activity but will mature and evolve over time. And while the most appropriate structure will vary across organizations, a clear mission with links to tangib le business objectives as well as a mechanism for realignment is essential for long term success. For example, domain councils might exist to oversee the intersection of business, technology, and operations . Governing boards might be created to establish business data priorities and resolve conflicts. Tactical groups might exist to manage workflow, perform data reconciliation, address quality of critical data attributes, perform business analysis and provide triage to resolve pressing business challenges with data. All of these components need to be linked into an overall framework if governance is going to successfully embed data management concepts into the culture of the organization as well as manage implementation. The organizational model for da ta governance establishes the mechanism by which the data management program is managed, funded and implemented. It defines the management hierarchy and accountability structures for the data program including how people and processes interact. The key o bjectives are to ensure that the principles of data management are defined and adopted across the organization; that the mechanisms are in place to ensure sustainable funding; and that stakeholders are aligned on the collaborative nature of data management . Executive sponsors are essential to ensure that data governance is successful . Sponsors need to be engaged in both the objectives and structure of the data management program from its inception. The executive mandate helps establish shared expectation s and promotes confidence that program objectives are a high priority despite any disruption created to business priorities",
        "© Enterprise Data Management Council – 2014 Page 23 and operational structures. Executive sponsors have a critical and active role in managing expectations and in establishing a funct ional mechanism for addressing competing priorities. In order to implement governance, the organization needs to ensure that the deployment plan will be effective within their business environment. The governance structure can be used to prevent attempts to “boil the ocean,” provide a mechanism to limit selling of obscure technical concepts that don’t mean anything to business users, help avoid finger pointing, and minimize environments where stakeholders are put on the defensive. After the initial imple mentation, the governance framework itself needs to be evaluated, measured and adjusted based on business reality and to ensure that it is fully integrated into operations. One of the core functions of data governance is to manage the staffing requirement s needed to implement the processes and technologies associated with sustainable data management. This should be accompanied by a formal inventory of resource requirements and aligned with the data lifecycle. And since it is not always necessary (or poss ible) for all essential staff resources to be “owned” by the data management organization, governance is needed to implement the strategy for resource sharing. Resourcing for data management combines IT knowledge, business experience, and data management expertise. The goal is to align skill sets with resources to identify “natural” candidates for new data roles. The components of data management governance also need to be closely aligned with criteria for measuring the value of data management again st defined objectives. Measurement criteria can be used to evaluate the gap between actual and expected value (disparity); the relationship between data management variables (correlation) and the measurement of the data program against objectives (perfor mance). These can be translated into practical measurement criteria such as the cost of correcting mismatches, the time spent on data reconciliation, opportunities for systems consolidation, reduction in the number of transformations, responsiveness to cu stomers, acceleration of business, reduction in operational risk, etc. Standing Up Your Governance Program Although this may differ from organization to organization, there are generally 4 steps that are needed in order to establish an effective data management governance program. 1. Establish the Governance Structure: The aim of the governance structure is to identify and organize the critical stakeholders and link them to the necessary data management support components. In order to implement governan ce, the organization needs a formal deployment plan to ensure that the governance structure, organizational model, and oversight mechanism will work within the business environment. Interacting with executive management to ensure that adequate funding for data management is in place is critical to ensure that governance is successful. 2. Implement Policy Formalizing policy is the foundation for Data Governance. Policy addresses how data is gathered, maintained, delivered and utilized. For policy to be effe ctive, it must be enforced and made auditable across the enterprise. 3. Develop the Operate Model: The operate model must be implemented and deployed to ensure that the data manageme nt principles are fully defined, adopted and adhered . The model provides guidance for managing the structure and activities of the data governance program . The model defines the controls, checkpoints and tollgates required, and establishes formal approval processes for the program.",
        "© Enterprise Data Management Council – 2014 Page 24 4. Monitor and Measure: Formal process for adequatel y monitoring and measuring the effectiveness of the data management program must be deployed to ensure the program is meeting its stated objectives . The program must be evaluated to ensure ongoing consistency with organization policy, and alignme nt with business strategy. Continuously measuring the program is essential. Metrics - based measurement criteria should be developed and used to track the progress and health of the program. Measurement criteria can include areas such as: measurement of c ompliance to policy and standards; the cost of correcting mismatches on trade repairs, the time spent on reconciliation, consolidation and better use of existing data sources, reduction in the number of transformations, consolidation of redundant systems, responsiveness to customers, acceleration of business, operational risk, etc. Goals:  Establish executive sponsorship for the program. C ommunicate purpose and objectives.  Establish a functional data management organizational structure with clear role definitions, responsibilities, and accountabilities for data management resources .  Establish governance implementation procedures to ensure compliance with policies, processes, standards and resources. Ensure that the structure provides for program oversight, policy enforcement , and issue escalation.  Develop comprehensive and achievable policies and procedures.  Define clear lines of authority and responsibility for decision -making as well as mechanisms for enforcement of data management based on operatio nal constraints .  Ensure that appropriate resources have been allocated to ensure that data governance is effectively implemented .  Develop and implement a uniform process for establishing a comprehensive set of metrics . Ensure stakeholder collaboration in the development and use of metrics for meeting data management measurement criteria .  Formalize consistent reporting of metrics to identify the progress, health and benefits of the data management program. Core Questions:  Have the data management policies been defined, developed and validated with key stakeholders?  Has a governance structure been established? ( Stakeholders identified; charters written; responsibilities assigned, etc.)  Are there mechanisms in place for issue escalation and resoluti on?  Are there mechanisms in place for establishing and resolving prioritization issues among stakeholders?  Are the appropriate executives identified and engaged?  Has the methodology to ensure compliance with established policies, processes and standards across the full data lifecycle been defined?",
        "© Enterprise Data Management Council – 2014 Page 25  Is the funding model and resource strategy sufficient to support the objectives of the data management program?  Have the metrics been validated by stakeholder criteria, aligned with business objectives and colle cted in a timely manner?  Are the metrics specific and achievable (actionable) within your organization to improve data management and meet objectives?",
        "© Enterprise Data Management Council – 2014 Page 26 Capabilities and Assessment Criteria: 4. DATA GOVERNANCE 4.1. Data Governance Structure is Created. 4.1.1. Data Management Office (DMO) is created. The Data Management Office refers to the centralized organization responsible for championing the data program Capability Objectives  DMO is designed and planned .  DMO is approved and charted .  DMO is created. 4.1.2. The DMO has an executive owner. A senior executive (ex: Chief Data Officer) must be appointed and given full authority to run the DMO Capability Objectives  Need for executive owner is recognized, socialized and communicated.  Executive owner is hired or appointed.  Duties and authority of the executive owner have been communicated to all relevant stakeholders. 4.1.3. The data governance plan is created A comprehensive governance plan needs to be built in collaboration with critical stakeholders Capability Objectives  Data g overnance plan is drafted and aligned to operational objectives, priorities and culture .  Data governance p lan has been shared with relevant stakeholders .  Data governance p lan has been reviewed and feedback has been incorporated into the final version.  Data governance plan is approved . 4.1.4. Program Office (PMO) is established and staffed with required skill sets. The data program will require the coordination of many projects across a firm or division. Resources may be shared. It is important that a PMO is est ablished and appropriately staffed with adequate resources to manage the required workload. Capability Objectives  PMO is approved and chartered.  PMO is staffed.  PMO is authorized to ensure and enforce alignment of projects to data management policy and standards.",
        "© Enterprise Data Management Council – 2014 Page 27 4.1.5. Enterprise governance structure is designed and implemented Enterprise governance structure refers to the organizational construct across the enterprise. Individuals must be appointed in business lines and control functions and given the res ponsibility of data management within those verticals, preferably, reporting into the COO or business leader within that group Capability Objectives  Govern ance structure has been defined, documented and shared with relevant stakeholders.  Organizational gov ernance structures have been implemented.  Working committees are established with written and approved charters.  Stakeholders have been appointed .  Stakeholder roles and responsibilities have been communicated.  Stakeholders are held accountable for their participation in the data management program (i.e. via performance reviews and compensation considerations. 4.2. Policy and Standards are Documented, Shared and Approved 4.2.1. Policy and s tandards are written and complete Policy and Standards define how business, technology and operations control data including how data is acquired, managed, maintained and delivered throughout an organization. P&S must be developed in partnership with stakeholders to ensure buy -in as well as alignment with existing strategies and controls. Although P&S can vary, most will contain rules and guidelines pertaining to data ownership; data definition, data lineage, metadata, data quality; data access; permissible use; data sourcing and controls . Capability Objectives  Policy and standar ds are developed in collaboration with (business, technology and operations) stakeholders .  Policy and standards are complete and verified  Policy and standards are in alignment with Data Management Strategy. 4.2.2. Policy and s tandards have been rev iewed and approved by relevant p rogram stakeholders Policy and standards must be shared and reviewed by relevant stakeholders to ensure agreem ent, alignment and buy - in. Policy and standards are critical elements and should be subjected to a rigorous challenge proce ss by stakeholders Capability Objectives  Policy and standards have been shared and reviewed by relevant stakeholders.  Feedback from stakeholders has been incorporated into the final version of the policy and standards.  Policy and standards have been validated and approved. 4.2.3. Policy and standards have been reviewed and approved by senior executive governing bodies Policy and standards must be recognized and supported by senior executive management. Data governance must be aligned with (and become a component of) the existing governance structures of the enterprise. Capability Objectives  Policy and Standards have been submitted to the organizational governance mechanism for evaluation.  Policy and Standards have been approved.",
        "© Enterprise Data Management Council – 2014 Page 28 4.3. Program Controls are Established 4.3.1. Project “Review and Approval” processes are established Policy and standards must be enforced in a controlled manner via checkpoints, formal review mechanisms and organizational approval boards. Controlled enforcement must be created to ensur e that all new development as well as data access, usage and transmission of data adhere to established policy and standards . Capability Objectives  Review and approve processes and responsibilities for data -related projects have been communicated to relevant stakeholders.  Review and approval processes are operational (includes areas such as “Approval to Build ”, “Approval to Access ”, “Approval to Use ”, “Approval to Send ”, etc. ).  Review and approval processes are aligned with the control mechanisms of o ther existing cross - organizational processes (i.e. Change Management policy must reference d, and be harmonized with, data management policy) . 4.3.2. Policy and s tandards are enforceable and auditable Policy and standards must be supported by established audit pr ocesses and routines. Lack of adherence to policy and standards must be elevated as a formal audit issue that must be resolved. Capability Objectives  The Data Management Office has the authority to examine and enforce adherence to data management policy and standards.  Corporate Audit examines and enforces adherence to the data management policy and standards. 4.3.3. Metrics are in place to track program adherence, progress and outcomes Metrics constitute the empirical evidence required to determine the effective ness of the data management program. Metrics development is ongoing as new business processes are developed and aligned to the data governance P&S. A successful metrics program will not only capture, aggregate and report metrics, but will also affect pro gram change based on metric evaluations. Capability Objectives  Metrics and thresholds are established.  Metrics are tracked and reported to relevant stakeholders.  Metrics are tracked and reported to executive management.  Metrics inform and drive program decisions and modifications. 4.3.4. Formal training programs have been designed and implemented Behavior and culture change are required for effective data management. Formal training is needed to ensure those with data responsibility are operating in accordanc e with established P&S. Capability Objectives  Training programs are designed and operational.  Training is mandated as an operational requirement.",
        "© Enterprise Data Management Council – 2014 Page 29 4.4. Program Governance is Operational 4.4.1. Meeting routines are established Governance and control routines (meetings, minutes, actions items, etc.), must be documented and incorporated into practice Capability Objectives  Meeting routines have been defined and scheduled.  Meeting routines are operational.  Group s are meeting and functioning in accordance with their individual charters. 4.4.2. Requirements are captured and prioritized Business requirements are an output of the established routines. Sustainable processes are in place to capture, review and verify busines s requirements. Capability Objectives  Routines to capture requirements are established.  Prioritization process is established and adhered to by relevant stakeholders .  Prioritization processes for business requirements are designed, rationalized and approve d by stakeholders .  Priorities are reviewed and enhanced on a regular schedule . 4.4.3. Funding Model is Operational Funding model is implemented across the data management program. The funding model is likely to be multi -year and must be built into the firm's annual funding review cycle. (see business case & funding model process area) Capability Objectives  Funding model is operational .  Funding model is repeatable and aligned to organizational funding cycles . 4.4.4. Escalation Procedures are developed and documented Formal escalation procedures must be agreed to and documented. Escalation procedures are the mechanism used by the organization to address critical decisions and resolve conflicts. Capability Objectives  Escalation procedures have been defined and document ed.  Procedures have been reviewed and approved by relevant executive management and organizational governance bodies . 4.5. Content Governance is Established 4.5.1. Authorized data domains have been identified and inventoried Authorized data domains are logical representation of a category of data that supports a business function (e.g. “trades” is a data domain that supports capital markets). It is imperative that these strategic data assets are identified and inventoried to ensure their proper use in critical applications Capability Objectives  Authorized data domains are identified.  Authorized data domains are declared.  Authorized data domains are inventoried.",
        "© Enterprise Data Management Council – 2014 Page 30 4.5.2. Critical Data Elements (CDEs) have been identified and inventoried CDEs refer to the individual data attributes that are used to support critical business functions. CDEs must be identified and catalogued to ensure evidence of proper sourcing, lineage and usage. Capability Objectives  CDEs have been identified and inventoried.  CDE sources have been documented.  Approved business definitions have been assigned.  Data lineage has been documented and validated.  CDEs (and their lineage) are maintained in accordance with data management p olicy and standards. 4.5.3. Data domain taxonomies have been developed and are actively maintained Taxonomies define how things relate. Data taxonomies define relationship of elements within a data domain. Taxonomies are critical to establishing common definition and language of data across an enterprise and are required to ens ure data's proper use. Capability Objectives  Data domain taxonomies are defined .  Authorized data domains are verified by business subject matter experts .  Authorized data domain taxonomies are been published and are being used by upstream/downstream systems as data is shared across business processes .  All new business development use established authorized data domain taxonomies .  Internal taxonomies are aligned with (and cross referenced to) global standards . 4.5.4. Unique and precise data identification schemes and methodologies have been defined, applied and are in use. Data Identification schemes and methodologies are used to ensure precise identification of data factors of input. Customer ID; Legal Entity ID; Product ID are examples of unique identific ation. Establishing ID methodologies are critical for data aggregation, classification and analysis. Unique identification is a foundational concept and is emerging as a required component for regulatory reporting and risk analysis. Capability Objectives  Identifiers have been defined for critical business entities (e.g. product; customer; account; etc.) .  Internal entity IDs have been assigned , published and are being used across business processes .  Internal IDs are aligned (and cross referenced) to industry standard identifiers . 4.5.5. Data c lassifications are defined and assigned Data classifications are critical for control and analysis of data. Data classifications are critical to establishing standard treatment of data across an enterprise and for aggregating data for analytical purposes. Capability Objectives  Data classifications have been established, assigned to data domains and verified by stakeholders .  Data classifications are adopted and implemented in systems .  Data classifications dictate how data is to be handled throughout the business process (e.g.: classification of data denotes privacy treatment, info -security treatment, masking, encryption, risk analysis, etc.) . 4.6. Technology Governance is Established. 4.6.1. Platform governance is established Technology defines and governs how databases and data warehouses are approved, develop ed and deployed . Technology approach needs to be aligned with the Data Management Strategy. Capability Objectives  Procedures for platform governance are defined and developed by IT and are aligned to the Data Management Strategy .  Platform governance is implemented and operational .",
        "© Enterprise Data Management Council – 2014 Page 31 4.6.2. Data distribution governance is established Technology defines and governs how data is distributed across the network. Capability Objectives  A data distribution strategy and governance is defined by IT .  The data distribution strategy is aligned with the objectives of the Data Management Strategy .  Data distribution governance is implemented and operational . 4.6.3. Data storage governance is established Technology defines and governs how data is persisted, archived, restored and defensively destroyed, in alignment with business objectives, the Data Management Strategy, and Legal and Compliance considerations. Storage includes online, archive , cloud and other 3rd party storage medium. Capability Objectives  Data storage strategy and governance is developed by IT .  Data storage strategy and governance is aligned with business, data management and legal and compliance objectives .  Data storage strategy and governance has been reviewed and approved by relevant stakeholders .  Data storage governance (people, process, technology) has been implemented and is operational . 4.6.4. BI, ETL and data tool governance is established Technology defines and governs the technology stack for BI (Business Intelligence), ETL and other data related tools. Data tools include but are not limited to discovery tools, data quality tools, data profiling tools, metadata tools, lineage tools, etc.) Capability Object ives  Technology defines the permissible technology stack for ETL and related data tools .  ETL and data tool governance is implemented and operational across all technology development teams . 4.7. Cross -Organizational Enterprise Data Governance is Established 4.7.1. Data Governance is aligned with Info rmation Security Policy Capability Objectives  Data management policy and standards are aligned with Info -Security policy and standards .  Cross -organizational dependencies are formally recognized and reflected in each groups’ policy and standards . 4.7.2. Data governance is aligned with Privacy and Cross -Border Policy Capability Objectives  Data management policy and standards are aligned with Privacy and Cross -Border policy and standards .  Cross -organizational dependencies are formally recognized and reflected in each groups’ policy and standards .",
        "© Enterprise Data Management Council – 2014 Page 32 4.7.3. Data governance is aligned with external (3rd Party) data usage policy and standards Firms routinely share their data with 3rd party entities (vendors, service providers, reporting agencies, etc.). Policies and standards are required to govern what data can (and cannot) be shared, what approvals are required to permit external use of data and how data delivered to 3rd parties will be protected (in alignment with corporate information security standards). Capability Objectives  Data management policies and procedures for 3rd party data usage have been developed and aligned with business objectives, data management strate gy, privacy policies, information security policies, and permissible data usage policies .  3rd Party data governance policies and standards are implemented and operational.  Cross -organizational dependencies are formally recognized and are reflected in relev ant groups’ policies and standards . 4.7.4. Data governance is aligned with Legal and Compliance Data Policy Data Management strategy and governance must be aligned with legal and compliance data policies not already discussed. Capability Objectives  Data Management policies and standards are aligned with Legal and Compliance data policy and standards.  Cross -organizational dependencies are formally recognized and reflected in relevant groups’ policy and standards.",
        "© Enterprise Data Management Council – 2014 Page 33 INFORMA TION ARCHITECTURE There are three architectural disciplines that are important for firms to understand in order to operate cohesively and achieve their organizational data management objective s. Business Architecture, which define business processes; Data Architecture, whi ch defines the meaning and relationships of data within those business processes and Technology Architecture, which defines how technology infrastructure (platforms, tools and applications) work together to enable access and use of data. Information Arc hitecture is the result of combining the disciplines of Data Architecture and Technology Architecture to support the goals and objectives of the business. There are four elements that need to be understood in the context of Information Architecture.  Semantics : Semantics refers to the adoption of precise, shared and consistent meaning. At the heart of the Data Management challenge has been the inability to harmonize disparate data across an enterprise or a business function because the meaning of data h as not been standardized. Semantics is the discipline of assigning unambiguous meaning to data throughout the data lifecycle.  Conceptual Data Model : Conceptual data models describe data at its highest level, ide ntifying the critical data objects needed t o satisfy a business objective as well as defining their relationships to one another.  Logical Data Model: The Logical Data models is a fully -attributed conceptual model that has been abstracted from any physical implementation. The logical model repre sents the business requirements in terms of what is needed to satisfy the objectives of the business function.  Physical Model: The physical model is the instantiation of the meaning, relationships and attributes of data into a physical implementation. The following section will look at Information Architecture through the lens of its component parts – Data Architecture with its focus on content meaning and Technology Architecture with its focus on platform, tools and applications.",
        "© Enterprise Data Management Council – 2014 Page 34 5.0 DATA ARCHITECTURE Definition : Data Architecture speaks to the design, definition, management and control of information “content”. Data Architecture identifies data domains, documents metadata, defines critical data elements, establishes taxonomies and models ontologies that are critical to ensuring that the meaning of data is precise and unambiguous and that the usage of data is consistent and transparent. Purpose: A Data Architecture function establishes consistency in definition and use of data throughout an organization. Adhering to a prescribed data architecture forces business and technology to take the necessary steps to define and document data meaning, defin e the appropriate use of the data, and to ensure that proper governance is in place to consistently manage “data as meaning” on a sustainable basis. Introduction: Data exists throughout an organization across all facets of business operations. The design of a firm’s Data Architecture is based on a comprehensive understanding of business requirements. Unraveling the business process informs how data should be identified, defined, modeled and related. Technology Architecture then dictates how the data architecture design is instantiated into physical repositories in order to provide optimized access, security, efficient storage management and speed of process ing. In order to establish a successful Data Architecture program, there are a number of specific architectural ‘steps’ that must be developed and adhered to. First is to understand the scope of data needed to satisfy the business requirements. The scope of data generally falls into two categories: (1) identif ication of logical domains and (2) identification of the physical repositories.  Identification of Logical Domains of Data Logical domains of data represent the data (not the databases) that are needed to satisfy the business requirements. Logical data dom ains are grouped into three categories: 1. Reference Data Domains (“nouns”): T hese describe the formal or contractual attributes of a business object (i.e.: products, instruments ; customers, legal entities; counterparties; etc. ) 2. Transaction Data Domains (“ve rbs”): these describe the actions associated with an event (sale; trade; deal; payment; etc.) 3. Derived Data Domains (“adjective”): these describe the newly created, quantitative values that result from an aggregation or analytical operation of reference and /or transactional attrib utes (i.e.: calculated balances; exposure metrics; demographic calculations; etc.)  Identification of Physical Repositories Underlying the logical data domains are multitudes of physical (often overlapping ) repositories of data that will map into the logical data domains. Identification of these underlying physical repositories is a critical step towards minimizing the complexity of legacy environments, reducing replication, better understanding data lineage, assigning data ownershi p and assessing data quality . Once the domains (and their underlying physical sources of data) have been identified, precise busine ss definitions (common semantic language ) for the identified data entities must be assigned and agreed upon",
        "© Enterprise Data Management Council – 2014 Page 35 by critical stakeholders. Data Architecture is about managing meaning. The importance of assigning precise definitions in the context of business reality (relationships) , the creation of a shared ‘data dictionary’ and getting the buy -in from both upstream and downstream users cannot be minimized. Without this common understanding of data attributes (aligned to business meaning) , Data Architecture will struggle to succeed, t he risk of inappropriate use of data will increases and the ability to ‘sha re’ data across an enterprise with confidence will be hindered. The next step in addressing data architecture is to define data taxonomies and business ontologies . Data taxonomies define how data entities are structurally aligned and related. For each officially designated data domain that is identified , inventoried and deemed critical, a taxonomy must be defined , maintained and mandated for all systems using this data as input into their business functions. With critical business function taxonomies defined and in place, the organization needs to model the relationships between taxonomies into a business ontology . Ontologies represent the relationships and knowledge of multiple related taxonomies across functional domains. Semantics, taxonomies an d ontologies define and relate the content of data in order to enable the organization to realize its maximum value in a consistent and controlled manner. Once the content is defined, it needs to be precisely described as metadata . Metadata falls into three categories: descriptive metadata, structured metadata and administrative metadata.  Descriptive metadata describes attributes used for discovery and identification (i.e.: author; title; source).  Structural metadata describes how attributes are created or derived (ex: a derived attribute would describe what attributes that were used to derive its value).  Administrative metadata provides information related to the creation, classification and/or appropriate use of data. Admi nistrative metadata would include information like “NPI” -Non Public Information; data access entitlements; archive an d retention requirements, etc. Goals:  Data Architecture defines common meaning of data  Common meaning is driven by business stakeholders  Relationships between and among data attributes is based on business requirements  Data is designed logically, abstracted from physical implementation  Data Architecture informs physical implementation Core Questions:  Are business stakeholders driving cont ent definition ?  Are policies in place to govern the creation and maintenance of data attributes and relationships?  Are governance procedures in place to ensure adherence to established data architecture standards?  Are design reviews in place and required to ensure enhancements and new development are utilizing standard data architecture definitions?  Is adherence to data architecture standards auditable?",
        "© Enterprise Data Management Council – 2014 Page 36 Capabilities and Assessment Criteria: 5. DATA ARCHITECTURE 5.1. Identify the data (logically and physically) 5.1.1. Logical domains of data have been identified, documented and inventoried. Logical domains of data represent the data (not the “databases”) that are needed to satisfy the business requirements. Logical data domains fall into three categories - reference data; transactional data; and derived data. Identification of these domains must be driven by the Business from the perspective of “what data is needed to perform the required business functions?” Capability Objectives  Business stakeholders have been selected to drive the identification of the logical data domains.  Logical data d omains have been identified and prioritized. 5.1.2. Underlying physical repositories of data have been identified, documented and inventoried Underlying the logical data domains are physical (often legacy) repositories of data that will feed the logical domains. Capability Objectives  Underlying physical repositories linked to the logical data domains have been identified  Identified repositori es have been inventoried and the inventory is actively maintained. 5.2. Define the Data (semantically and structurally ) 5.2.1. Attribute level “business” definitions are defined, documented and approved by relevant stakeholders Business definitions are non -technical descriptions of data attributes that are based in contractual, legal and/or business facts Capability Objectives  Business definitions are documented, verified by users and assigned to physical repositories .  Business definitions are assigned to fully -attributed conceptual models.  Authorized d ata domains are fully populated with business definitions.  The conceptual mapping is agreed to by relevant stakeholders . 5.2.2. Taxonomies and o ntologies are created, documented, maintained and governed Data taxonomies define how data entities are structurally aligned and related. For each officially designated data domain that is identified, inventoried and deemed critical, a taxonomy must be defined, maintained and mandated for all systems using this data as input int o their business functions Capability Objectives  Taxonomies are defined for fully -attributed conceptual models and agreed to by relevant stakeholders.  Taxonomy relations are captured and documented into domain ontologies .",
        "© Enterprise Data Management Council – 2014 Page 37 5.2.3. Metadata is defined Metadata must be captured and inventoried into a metadata repository so it is usable by all relevant development teams Capability Objectives  Metadata from the physical repositories is captured and inventoried.  Metadata for fully -attributed conceptual models is ca ptured and inventoried into a metadata repository, and is being used by relevant stakeholders  Metadata is rationalized across taxonomies and ontologies 5.3. Govern the Data (establish sustainable data architecture governance) 5.3.1. Data architecture governance procedures are in place and aligned with business governance processes Alignment to Business processes include: Business Process Definition; Operations procedures; 3rd party contract specifications; etc. Capability Objectives  Data governance is aligned with business processes to ensure semantic definitions, taxonomies and CDEs are properly assigned, maintained. 5.3.2. Data architecture governance procedures are in place and aligned with technology Alignment to Technology processes include: design reviews, approvals to build approvals, validation of appropriate usage approvals, permit to deliver approvals, etc. Capability Objectives  All technology development is required by governance policy to follow data architecture standards  All technology development use established data architecture elements.",
        "© Enterprise Data Management Council – 2014 Page 38 6.0 TECHNOLOGY ARCHITECTURE Definition : Technology Architecture refers to the strategy, design and implementation of the physical architecture in support of the defined data ar chitecture. Technology architecture defines the platforms and the tools and how they need to be designed for maximum efficiency in support of the data management strategy. The purpose of technology architecture is to define h ow data is physically acquire d, moved, persisted and distributed in a streamlined and efficient manner. Physical data proximity, bandwidth, processing time, backup and recovery, archiving, etc. are all important elements of a mature technology architecture. Purpose: The efficient and effective movement of data is critical to business operations. Technology architecture determines how data, tools and platforms operate in collaboration to satisfy business requirements. The proper alignment of these components dictate s application efficiency and system processing speed . This enable s firms to control costs and achieve infrastructure scalability and elasticity which are the characteristics of an enterprise infrastructure that is designed for long -term implementation success. Technol ogy Architecture is articulated in the technology architecture roadmap. The technology architecture roadmap defines the target state infrastructure and provides guidelines for implementation. The roadmap further defines the technology governance and cont rols that are needed to ensure compliance across the enterprise . Introduction: Information Architecture is the combination of both data architecture (content) with technology architecture (implementation) . Data architecture should not dictate technology. Technology is the responsibility of the technology department. However, Data Architecture does inform technology. Data Architecture captures the information requirements of the business and translates them into the “what, where a nd when” of data – what data is needed; where is it to be delivered and by when. Technology Architecture is the enabler and defines the plan and roadmap for implementation. There are four areas of technology architecture that are critical to a successful data management program. 1. Database Platforms : Technology Architecture defines acceptable data platforms for enterprise use. E nterprise -class database platforms, appliance technologies, distributed computing, and in- memory solutions all need to be defined, communicated and governed by technology architecture. 2. Tools : Often one of the biggest expenses and source of inconsistent handling of data is the proliferation of multiple, disparate data management technology tools within an organization. Technology Ar chitecture must define the a llowable tool stacks – what BI (Business Intelligence) tools, ETL (extract, transform, load) tools and various discovery tools are permitted for use within the organization. 3. Storage Strategy : Technology architecture must defin e how firm s will stor e and maintain its data. A component of the target -state storage strategy is the determin ation of how data and data costs will be maintained, how and what data will be stored (including decisions about the use of internal versus exter nal cloud technology ), how data will be archived and retained, and how data will be d efensibly destroyed/removed from the firm’s infrastructure . 4. Operational Risk Planning : A sound technology architecture addresses operational risk, business continuity and disaster recovery strategies. Data is the ‘life -blood’ of a firm and needs proper planning to ensure that data flows to all parts of a firm even in the face of events th at interrupt business continuity.",
        "© Enterprise Data Management Council – 2014 Page 39 And finally, all of the above elements of a sound technology architecture must be supported by a strong technology governance operate model. Policies must be in place, agreed to by all technology and business stakeholde rs, supported by executive management, and subject to internal audit scrutiny and adherence. Without governance, technology will grow and develop uncontrolled and lead to inefficiencies and security issues putting data quality at risk. Goals:  Data Ar chitecture defines target -state infrastructure in support of data management  Tool selection and implementation is simplified, reducing complexity and cost  Storage strategy is developed consistent with the objectives of business while controlling cost and risk  Operational risk architecture is implemented to ensure continuous flow of data to critical business functions in the event of an outage incident Core Questions:  Is technology architecture being driven by business requirements?  Are policies in place to govern the selection and use of technologies throughout the organization?  Are governance procedures in place to ensure adherence?  Are design reviews in place and required to ensure enhancements and new development are utilizing standard technology archi tecture definitions?  Is adherence to data architecture standards auditable?",
        "© Enterprise Data Management Council – 2014 Page 40 Capabilities and Assessment Criteria: 6. TECHNOLOGY ARCHITECTURE 6.1. Data P latform Strategy Defined and Governed 6.1.1. Technology architecture strategy is defined and agreed to by relevant stakeholders . Capability Objectives  An integrated technology architecture strategy is designed, socialized and agreed by relevant technology, business and senior executive stakeholders.  An integrated architecture strategy is support ed and enforced by corporate audit policy . 6.1.2. An actionable roadmap is developed and adopted for implementation of the technology architecture For a technology roadmap to be sustainable, it must have a budget commitment over the life of the designed roadmap. Capability Objectives  A multi -year technology architecture roadmap has been developed.  The roadmap adheres to the approved technology architecture strategy.  Budgets have been developed and a pproved as well as b uilt into the firm’s budget cycle processes. 6.1.3. Platform governance structure and processes are in place . Capability Objectives  Integrated governance structure and policies are in place, operational and in alignment with the data management strategy.  All enhancements and new development are subject to architectural platform design review and approval. 6.2. Data Technology Tool Stack Defined and Governed 6.2.1. Technology tool selection strategy is defined and verified by relevant stakeholders Capability Objectives  Integrated technology tool strategy has been designed, socialized and agreed to by relevant technology, business and senior executive stakeholders.  The tool strategy is supported by corporate policy and enforced by Corporate A udit. 6.2.2. Technology tool roadmap is developed and implemented Capability Objectives  Integrated technology tool roadmap has been developed in adherence to the technology tool strategy (including guidelines for new development as well as decommission plans for non -standardized legacy tool implementations).  Budgets have been developed and approved and have been built into a firm’s budget cycle processes. 6.2.3. Tool selection governance structure and process is in place and operational Capability Objectives  Integrated tool governance structure (with associated policies) are in place, operationa l and in alignment with the data management strategy.  All enhancements and new development are subject to tool selection review and approval.",
        "© Enterprise Data Management Council – 2014 Page 41 6.3. Data Storage Management Strategy Defined and Governed 6.3.1. Data storage management strategy is defined and agreed to by relevant stakeholders Capability Objectives  Integrated storage management strategy has been designed, socialized and agreed to by relevant technology, business and senior executive stakeholders.  The storage management strategy is backed by Corpora te Audit. 6.3.2. Data storage management roadmap is developed and implemented Capability Objectives  Integrated data storage roadmap has been developed.  Budgets have been developed, approved and integrated into the firm’s budget processes. 6.3.3. Storage governan ce structure and processes are in place and operational Capability Objectives  Integrated storage management governance structure and policies are in place and operational.  All enhancements and new development are subject to a review and approval consistent with the defined storage management strategy and roadmap. 6.4. IT Operational Risk Planning in Place 6.4.1. Data infrastructure contingency planning is defined and in place Capability Objectives  Integrated IT operational risk management strategy has been developed, socialized and agreed to by relevant technology, business and senior executive stakeholders.  Integrated IT operational risk management strategy is backed by Corporate Audit. 6.4.2. Operational Risk Governance Structure and processes are in place and operational Capability Objectives  Integrated operational risk governance structure and policies are in place and in operation.  All enhancements and new development are subject to a review and approval of their operational risk plans.  Operational risk planning is subject to Corporate Audit.",
        "© Enterprise Data Management Council – 2014 Page 42 7.0 DATA QUALITY PROGRAM Definition : Data Quality describes the degree in which data is fit for purpose for a given business process or operation. Terms such as accuracy, completeness and timeliness are all components of data quality. Data Quality is not a process itself, but is the net res ult of a chain of processes across the full data supply chain to ensure that data delivered meets the needs of its intended consumers. Data Quality requires an understanding of how data is sourced, defined, transformed, delivered and lastly, consumed. Purpose: The Data Quality Program defines the goals, approaches and plans of action to ensure that data content is of sufficient quality to support defined business and strategic goals of the organization. The Data Quality Program should be developed in alignment with business objectives, measured against defined data quality dimensions and based on an analysis of the current state of data quality. Introduction: Data quality is a broad conceptual term that needs to be understood in the context of how d ata is intended to be used. Perfect data is not always a viable objective. The quality of the data needs to be defined in terms that are relevant to the data consumers to ensure that it is fit for its intended purpose. The overall goal of data managemen t is to ensure that users have confidence that the data they are using for decision making accurately reflects the facts the data is designed to represent - without the need for reconciliation or manual transformation. The organization needs to develop a data quality strategy and establish the overall plans for managing the integrity and relevance of data. One of the essential objectives is to create a shared culture of data quality stemming from executive management and integrated throughout the opera tions of the organization. In order to achieve this cultural shift, the organization must agree on both requirements and the measurement of data quality that can be applied across multiple business units and applications. This will enable business sponso rs, data consumers, and IT to link data quality management processes with objectives such as better risk management, enhanced analytics, better client service and improved operational efficiencies. Data quality can be segmented into a number of core dimen sions including: completeness ( the availability of required data attributes ), coverage ( the availability of required data records ), conformity ( alignment of content with required standards ), consistency ( how well the data complies with required formats/def initions ), accuracy ( the relationship of the content with original intent ), duplication ( the redundancy of records and or attributes ), and timeliness ( the currency of content representation as well as whether the data is available/can be used when needed ). The identification and prioritization of the dimensions of data quality fosters effective communication about data quality expectations and are an essential pre -requisite of the data management program. Creating a profile of the current state of data qu ality is an important initial component of the overall data quality initiative and should be performed periodically (i.e. whenever data is transformed). The goal is to assess patterns in the data as well as to identify anomalies and commonalities as a bas eline of what is currently stored in databases and how actual values may differ from expected values. Once the data profile is established, the organization needs to evaluate the current state against data quality requirements (i.e. tolerances and thresh olds) as well as against business requirements to determine whether the data is fit-for-purpose.",
        "© Enterprise Data Management Council – 2014 Page 43 The underlying purpose of this assessment process is to measure the quality of the most important business attributes and to determine what content is in n eed of remediation. A robust Data Quality Program (in partnership with business) identifies and declares what data is most important. It is this identification of critical data elements , or CDEs that helps inform business and technology in terms of which attributes are prioritized for key business functions. The designation of CDEs helps ensure that the highest level of accuracy and data quality treatment is applied. This is the data t hat needs to be “cleansed” to meet data consumer requirements. Data cleansing should be performed against a pre - defined set of business rules to identify anomalies that can be linked to operational processes. Data cleansing should be performed as close to the point of capture as possible. There should be a clearly defined strategy (with owners) for data cleansing to ensure that cleansing rules are known and to avoid duplicate cleansing processes at multiple points in the information management cycle. The overall goal is to clean data once at the point of data capture based on verifiable documentation and business rules as well as to reconcile the processes that allow defective data content into the system. Data corrections must be communicated to (and aligned with) all downstream repositories and upstream systems. It is important to have a consistent and documented process for issue escalation and change verification for both internal originators and data vendors. It is also important to ensure that data meets quality standards throughout the lifecycle so that it can be integrated into operational data stores. This component of the data quality process is about the identification of content that is missing, determination of data that needs to be enriched and the validation of data against internal standards to prevent data errors before data is propagated into production environments. In order for Data Quality to be sustained, a strong governance structure must be in place to support the data quality activities, ensure compliance to data quality processes and ensure the highest level of organizational support (senior executive management). Data quality processes need to be documented, operationalized and routinely validated via data management review s and formal audit processes. Data quality cannot be achieved centrally or monolithically. Enterprise Data Quality requires the commitment and participation of a broad set of stakeholders. Since data quality is the result of a chain of business proces ses, stakeholders along that chain must be in place, authorized and held responsible for the quality of data as it flows through their respective areas. Data Quality requires coordinated organizational support. Data quality processes and objectives must be part of the operational culture of a firm for it to be sustained and successful. Goals:  Data quality strategy is aligned with business plans and target operating models.  Standard dimensions associated with data quality are defined and prioritized by stakeholders.  Data quality processes (profiling, assessment, cleansing, and integration) are established and used for all data initiatives across the full systems lifecycle.  Data profiling methodologies are standardized, documented and implemented across all critical data stores and repositories.  Data quality metrics (tolerances, logic checks, thresholds, duplications, null sets, padding/meaningless spaces, all/no capit als, string length) are defined, documented, aligned with",
        "© Enterprise Data Management Council – 2014 Page 44 business requirements.  The root causes of data errors are researched. Data quality cleansing and remediation is prioritized based on organizational requirements and business criticality.  There is a bi -directional communication mechanism in place with suppliers to improve overall data quality. Data quality processes between data vendors and the organization are documented via SLA’s and synchronized. Core Questions:  Is it understood that Da ta Quality is not an objective unto itself, but an indication of an inefficient business process or broken technology?  Is it understood that Data Quality is an cultural shift? Improved data quality touches all aspects of business and technology processes .  For a Data Quality Program to be sustainable, training is required. Are the necessary resources (dollars and people) earmarked to implement and operate an Enterprise Data Quality Program?",
        "© Enterprise Data Management Council – 2014 Page 45 Capabilities and Assessment Criteria: 7. DATA QUALITY 7.1. Data Quality Program is Established. 7.1.1. The data quality strategy and a pproach is defined and socialized Data Quality strategy and approach encompasses the “what/how/who” of data quality. It needs to address the scope of the data to be scrutinized and reviewed; how the DQ assessments will be performed (metrics defined) and who will be responsible (defined roles and responsibilities) . Data Quality involves cultural change. It is critical that a documented DQ strategy and approach is socialized with relevant stak eholders (technology, business and operations), to ensure awareness, support and commitment . Capability Objectives  DQ strategy and approach has been designed and developed  DQ strategy and approach has been communicated to relevant stakeholders and  Feedback from stakeholders has been incorporated into the final version of the DQ strategy .  Stakeholders and Senior Management endorse and support the DQ program and strategy. 7.1.2. Accountable parties have been identified and roles and responsibilities have bee n assigned. A Data Quality program requires a network of data stewards and subject matter experts to ensure data is properly captured, processed and delivered. Accountable parties must be identified and their roles and responsibilities must be clearly an d unambiguously communicated. Capability Objectives  Accountable parties have been identified .  Accountable parties have been aligned to the organizational data management governance structure.  Data quality responsibilities have been assigned.  Individuals are held accountable for the performance of their data quality function via annual reviews and compensation considerations. 7.1.3. The Data Quality roles and responsibilities have been communicated. The organization needs to be aware of the assign ed roles, responsibilities, and authorities for the DQ program. Capability Objectives  DQ Governance roles and responsibilities have been defined .  DQ Governance roles and responsibilities have been aligned with overall governance strategy.  Roles and resp onsibilities have been communicated to relevant stakeholders - feedback has been incorporated into the final role definitions.  Stakeholders and Senior Management endorse and support the defined roles and responsibilities.",
        "© Enterprise Data Management Council – 2014 Page 46 7.2. The Current State of Data Quality is Assessed and Remediation Plans are Developed 7.2.1. All relevant data have been identified and prioritized. In performing a current state data quality analysis, it is important to include all relevant data in order to determine the true state of data quality. Data must be profiled, analyzed and graded. Data remediation plans need to be developed and prioritized. Data Architecture defines the domains of data as “reference data, transactional data, and derived data”. Across these domains, two states e xist – current and historical. All aspects and all states of data must be considered when performing a current state analysis. Across this scope of data, data can be prioritized based on their relevancy to critical business function. Data elements that are important to prioritized business functions are identified and designated as Critical Data Elements (CDEs). Designated CDSs receive the highest levels of monitoring to ensure the quality of these attributes is maintained. CDE designation is a control led process. Changes (additions or deletions) to the list of CDEs must be reviewed and approved by business stakeholders Capability Objectives  The scope of data subject to the data quality program has been identified (current and historical)  The scope of data has been prioritized in alignment with the data management strategy and business priorities.  CDEs are defined, verified, designated and actively maintained. 7.2.2. Data is profiled, analyzed and graded The scope of data under consideration must be profiled to determine the full spectrum of data quality dimensions (e.g. completeness, timeliness, coverage, conformity, referential integrity, consistency and levels of duplication and redundancy ). This analysis must include both a row -based analysis (accuracy of the record) and a column -based analysis (statistical columnar analysis). Metadata must also be reviewed to ensure the description and intended use of data is properly defined. Capability Objectives  In-scope data has been profiled and statistically analyz ed.  Metadata has been reviewed and gap analysis has been performed.  In-scope data has been graded and catalogued. 7.2.3. Data remediation has been planned, prioritized and actioned. Based on the current state analysis performed, remediation plans must be developed to address the most pressing data quality issues, as well as timelines established for ongoing DQ evaluation and maintenance Capability Objectives  Data remediation plans are developed and prioritized  High priority data remediation is being action ed.  Timelines have been established for ongoing remediation",
        "© Enterprise Data Management Council – 2014 Page 47 7.3. Data Quality Program is Operational 7.3.1. Data Quality ‘control points’ are in place along the full spec trum of the data supply chain. A Data Quality Program is developed to quantitatively measure the quality of data as it flows across business and technology processes. Data Quality is governed by setting goals and objectives, establishing control points, determining root -cause, remediat ing data gaps, and holding the business, data and technology teams accountable for achieving and sustaining the highest data quality standards possible. Data quality control points validate data along the supply chain. Data Quality Controls include the i mplementation of business rules, establishing workflows, setting data quality tolerances, defining exception handling processes and defining escalation procedures as data moves from data provider to data consumer. Capability Objectives  Data Quality control points are in place and fully operational along the data supply chain  Control remediation procedures are documented and evidenced. 7.3.2. Data Quality Metrics are captured , reported and used to drive data remediation. Control points capture data quality metrics and produce routine reports for executive management. Metrics are used to track data quality progress and stability and drive data remediation efforts. Capability Objectives  Data Quality metrics are captured on a routine basis.  Data Quality metrics are b eing reported to executive , business and technology management  Metrics are being used to drive and prioritize remediation efforts. 7.3.3. Root -Cause analysis is performed Data remediation is not only about correcting existing data, it is also about determining t he root -cause of the data quality deterioration at the source to avoid damaging data in the future. Capability Objectives  Root -cause is determined  Corrective measures to business and/or technology processes are identified and implemented. 7.3.4. Data Quality processes are audited Data Quality auditing occurs on 3 levels : Quality Assurance (QA) Assessment: Business performs self -assessments based on defined data quality processes and objectives . Quality Control (QC): The Data Management Function preforms a fa cilitated audit of a business -line’s data quality processes and is empowered to enforce the business lines to remediate any gaps found to ensure adhere to data quality best practices. Corporate Audit: Business line data quality processes are subject to co rporate audits. Failure to satisfy this review may result in formal escalated audits written against a business line or function. Capability Objectives  Data Stewards have performed self -assessment of the business -line data quality processes (QA).  The Data Management Organization has performed facilitated assessments of business -line data quality operations (QC).  The Data Management Organization is empowered to force operational teams to remediate gaps found in their operational data quality processes.  Corporate Audit performs routine examinations of business -line Data Quality procedures.  Formal Audit Issues are generated if operational gaps be uncovered.",
        "© Enterprise Data Management Council – 2014 Page 48 8.0 DATA OPERA TIONS Definition : Data Operations is the organizational process by which the data assets of a firm are managed in order to realize their maximum value. There are three elements of Data Operations: 1. The orchestration of data management capabilities within a controlled operational model. 2. The stewardship of the Data Management Lifecycle – from source to consumption to disposition 3. The integration of data management into the “information eco -system” (how data management coordinates with other control functions within an organization). Purpos e: The purpose of Data Operations is to coordinate the people, process and technology of data management into a cohesive operational model. Data Operations defines the mechanisms used to capture requirements, unravel data flows and linked processes and d etermine how data is to be delivered to the end-consumer. Data Operations supports the Data Management Lifecycle. It ensures that proper resources and controls are in place as data moves throughout its lifecycle journey. And Data Operations ensures coll aboration and alignment to cross -organizational control functions. Areas such as Information Security, Data Privacy and Change Management must operate in sync with Data Management in order to ensure data is properly managed across all business functions. Introduction: One of the first functions within Data Operations is the orchestration of the data management capabilities and component disciplines. These disciplines have to be aligned to effectively manage data across an organization. Data Operations i s the process of aligning all of the capabilities discussed in this model into a consistent operational flow. Each capability has to be properly resourced and prioritized as well as supported by business, technology and senior management. The successful coordination of these elements is a determining factor in the success of the data management program. It is the responsibility of the data management organization and the Chief Data Officer to structure and coordinate the operational model in order to properly define data meaning , ensure data quality, and deliver data in a timely and efficiently manner. And this must all be evidenced through demonstration of organizational structures, charters, policies and senior management directives. Data is a core fa ctor of input into business functio ns and operational processes. The Data Management Lifecycle tracks the progress of data from source … to storage … to maintenance … through distribution … to c onsumption … to reuse … to archiving … and finally to defensible destruction. The mechanisms used to identify, align and validate the data as factors of input into business functions are derived by reverse engineering existing processes into their individual data attributes and by unraveling the “data assemb ly” processes used to create the required data sets. This reverse engineering (or data requirements definition) process needs to be managed with precision to avoid confusion and miscommunication between what the business users truly need for their intended application and what IT professionals need for techn ical implementation. Data requirements should be modeled, aligned with business meaning, prioritized in terms of how critical it is to the application, verified by all relevant stakeholders and re -verified by stakeholders to ensure that essential concepts are not “lost in translation.” This is particularly critical for data that is shared among multiple users and for core data attributes that are used as a baseline for onward expression in operational calculations or business formulas.",
        "© Enterprise Data Management Council – 2014 Page 49 For complex app lications and for all aggregation -related processes, it is essential to understand and document how the data moves from system -to-system; how the data is transformed or mapped; and how the data is aligned to business definition/standard meaning. Gaining a greement on this “lineage” process is fundamental for ensuring that the results of decentralized or linked processing can be trusted to be consistent and comparable. The final element of Data Operations is the integration of data management into the “I nformation Ecosystem” of an organization. The Information Ecosystem is a concept that describes how data is managed collaboratively across all enterprise control functions. Control functions such as Information Security, Storage Management, Legal and Com pliance, Privacy, and Vendor Management all have responsibilities on how data is managed. It is imperative that the policies of data management are integrated and aligned with the policies of the cross -organizational control functions to ensure data is being managed consistently and holistically across the organization. Additionally, Data Operations must align to technology development policies and best practices. Data Management capabilities such as Architecture, Governance and Data Quality should all be integrated into a firm’s SDLC (Software Development Lifecycle) processes to ensure that data management considerations are being adequately addressed at the appropriate stages of the development cycle. Nothing should operate in a silo. Operating within an ecosystem recognizes interdependencies and ensures collaboration. Goals:  Data Operations are functional and aligned with Data Management capabilities  Data Operations are accountable to the data management organization (or Chief Data Officer) to ensure consistency in how data is managed along the data supply chain.  Data Operations follow defined data management capability best practices and are routinely subject to review and audit  Data Operations supports all facets of the Data Management Lifecycle  The Information Ecosystem is properly governed and controlled. All data entering the system is subject to the collective policies across all control functions. Core Questions:  Are Data Operations units functioning independently within business silos?  Does the Data Management Function have accountability for how Data Operations are run?  Do Data Operations units support the Data Management Lifecycle?  Are Data Operations units subject to routine audits? Have operational gaps been identified and are remediation plans in progress?  Does Data Operations collaborate with cross -organizational control function teams?  Is data managed differently across control functions?",
        "© Enterprise Data Management Council – 2014 Page 50 Capabilities and Assessment Criteria: 8. DATA OPERATIONS PROGRAM 8.1. Data Operations are aligned with Enterprise Data Management Capabilities and Strategy. 8.1.1. Data Operations are aligned to the Data Management Capabilities and Strategy Data Operations can operate centrally, or federated across an organization. In either model, Data Operations must be aligned with the enterprise data management organization, capabilities and strategy Data Operations must function consistent with the overall data management capabilities and strategy. Data Operations must adhere to operational standards and are subject to routine audit Capability Objectives  Data Operations functions in alignment with enterprise data management capabilities and strategy.  Data Operations is subject to enterprise data management policies and standards.  Data Operations are subject to Corporate Audit to ensure alignment to policy and standards 8.1.2. Data Operations are accountable to the Data Management Organization It is essential that Data Operations functions across the Enterprise are aligned and accountable to the enterprise data management function (Data Management Office) to ensure consistency in approach, coordinated prioritization and assurance of proper resource allocation a nd funding. Capability Objectives  Data Operations teams across the enterprise are accountable to the enterprise Data Management Office. 8.1.3. Data Operations follow industry best practices Regardless of w hether Data Operations is centralized or federated, the p rocess and methodology for capturing requirements and designing data solutions must be consistent with and aligned to internal and industry best practices. Capability Objectives  Data Operations teams capture data requirements and design data solutions that are consistent across businesses and in alignment with enterprise data management best practices.  Data Operations teams capture data requirements and design data solutions that are consistent across businesses and in alignment with industry best practices 8.2. Data Operations Support the Data Management Lifecycle 8.2.1. Data Operations ensure data sourcing and procurement are performed in alignment with business requirements Data procurement should be consistent and in alignment with Data Management policies and standards as well as with the policies and standards of other business control functions (i.e.: vendor management, legal, compliance, etc.) Capability Objectives  Data lifecycle management is performed consistently across the organization.  Data lifecycle is aligned with and accountable to Enterprise Data Management organization policy and standards.",
        "© Enterprise Data Management Council – 2014 Page 51 8.2.2. Critical end -to-end data flows and essential attributes for i n-scope business processes are defined and mapped. Data Operations is responsible for the stewardship of the Critical Data Elements (CDEs) and the how these elements flow (lineage) across linked processes. This must be done in coordination with Enterprise Data Management office and aligned to EDM policy and standards. Capability Objectives  CDE and lineage have been identified and mapped across business lines in coordination with the Enterprise Data Management office  CDE and lineage mappings are in alignmen t with EDM Policy and Standards. 8.2.3. The compounding processes and calculations for derived and transformed data are identified, documented and mapped Capability Objectives  Data transformation processes and calculations have been identified and documented across business functions in coordination with Enterprise Data Management Office.  Data Transformation Processes are in alignment with data management p olicy and standards. 8.3. Data Management is aligned with cross -organizational Control Functions. 8.3.1. Control Fu nction policies and standards reflect interdependencies with Data Management policies and standards Data Management controls and best practices are formally included in cross -organizational control function policies and standards to ensure collaboration an d alignment Capability Objectives  Cross -organizational references are formally included in each enterprise policy and standards  Control teams are h eld accountable and subject to Corporate A udit to ensure formal coordination of each groups policy and standards . 8.3.2. Regular routines are established with cross -organizational control teams. Cross-organizational teams meet regularly to keep abreast of evolving issues related to data and data operations . Capability Objectives  Enterprise Control functions forma lly coordinates with Enterprise Data Management via regular engagements, meetings and routines. 8.3.3. All data entered into the Information Ecosystem is subject to cross -organizational controls. All new data introduced into, or delivered out of the information ecosystem is subject to cross -organizational control standards to ensure enterprise -wide compliance. Capability Objectives  Data introduced into or delivered out of the ecosystem is subject to design review and approval.  Data introduced into or delivered out of the ecosystem is subject to all cross -organizational data control policy and standards."
      ]
    },
    "GDPR.pdf": {
      "fingerprint": "GDPR.pdf|1771999125|561371",
      "pages": [
        "General Data Protection Regulation (GDPR) - full text REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA relevance) View: Oﬃcial source THEEUROPEANPARLIAMENTANDTHECOUNCILOFTHEEUROPEAN UNION, Having regard to the Treaty on the Functioning of the European Union, and in particular Article 16 thereof, Having regard to the proposal from the European Commission, After transmission of the draft legislative act to the national parliaments, Having regard to the opinion of the European Economic and Social Committee1, Having regard to the opinion of the Committee of the Regions2, Acting in accordance with the ordinary legislative procedure3, Whereas: (1)The protection of natural persons in relation to the processing of personal data is a fundamental right. Article 8(1) of the Charter of Fundamental Rights of the European Union (the ‘Charter’) and Article 16(1) of the Treaty on the Functioning of the European Union (TFEU) provide that everyone has the right to the protection of personal data concerning him or her. (2)The principles of, and rules on the protection of natural persons with regard to the processing of their personal data should, whatever their nationality 1OJ C 229, 31.7.2012, p. 90. 2OJ C 391, 18.12.2012, p. 127. 3Position of the European Parliament of 12 March 2014 (not yet published in the Oﬃcial Journal) and position of the Council at ﬁrst reading of 8 April 2016 (not yet published in the Oﬃcial Journal). Position of the European Parliament of 14 April 2016. 1",
        "or residence, respect their fundamental rights and freedoms, in particular their right to the protection of personal data. This Regulation is intended to contribute to the accomplishment of an area of freedom, security and justice and of an economic union, to economic and social progress, to the strengthening and the convergence of the economies within the internal market, and to the well-being of natural persons. (3)Directive 95/46/EC of the European Parliament and of the Council4seeks to harmonise the protection of fundamental rights and freedoms of natural persons in respect of processing activities and to ensure the free ﬂow of personal data between Member States. (4)The processing of personal data should be designed to serve mankind. The right to the protection of personal data is not an absolute right; it must be consideredinrelationtoitsfunctioninsocietyandbebalancedagainstother fundamental rights, in accordance with the principle of proportionality. This Regulation respects all fundamental rights and observes the freedoms and principles recognised in the Charter as enshrined in the Treaties, in particular the respect for private and family life, home and communications, the protection of personal data, freedom of thought, conscience and religion, freedom of expression and information, freedom to conduct a business, the right to an eﬀective remedy and to a fair trial, and cultural, religious and linguistic diversity. (5)The economic and social integration resulting from the functioning of the internal market has led to a substantial increase in cross-border ﬂows of personal data. The exchange of personal data between public and private actors, including natural persons, associations and undertakings across the Union has increased. National authorities in the Member States are being called upon by Union law to cooperate and exchange personal data so as to be able to perform their duties or carry out tasks on behalf of an authority in another Member State. (6)Rapid technological developments and globalisation have brought new challenges for the protection of personal data. The scale of the collection and sharing of personal data has increased signiﬁcantly. Technology allows both private companies and public authorities to make use of personal data on an unprecedented scale in order to pursue their activities. Natural persons increasingly make personal information available publicly and globally. Technology has transformed both the economy and social life, and should further facilitate the free ﬂow of personal data within the Union and the transfer to third countries and international organisations, while ensuring a high level of the protection of personal data. (7)Those developments require a strong and more coherent data protection framework in the Union, backed by strong enforcement, given the impor- tance of creating the trust that will allow the digital economy to develop across the internal market. Natural persons should have control of their own personal data. Legal and practical certainty for natural persons, 4Directive 95/46/EC of the European Parliament and of the Council of 24 October 1995 on the protection of individuals with regard to the processing of personal data and on the free movement of such data (OJ L 281, 23.11.1995, p. 31). 2",
        "economic operators and public authorities should be enhanced. (8)Where this Regulation provides for speciﬁcations or restrictions of its rules by Member State law, Member States may, as far as necessary for coherence and for making the national provisions comprehensible to the persons to whom they apply, incorporate elements of this Regulation into their national law. (9)The objectives and principles of Directive 95/46/EC remain sound, but it has not prevented fragmentation in the implementation of data protection across the Union, legal uncertainty or a widespread public perception that there are signiﬁcant risks to the protection of natural persons, in particular with regard to online activity. Diﬀerences in the level of protection of the rights and freedoms of natural persons, in particular the right to the protection of personal data, with regard to the processing of personal data in the Member States may prevent the free ﬂow of personal data throughout the Union. Those diﬀerences may therefore constitute an obstacle to the pursuit of economic activities at the level of the Union, distort competition and impede authorities in the discharge of their responsibilities under Union law. Such a diﬀerence in levels of protection is due to the existence of diﬀerences in the implementation and application of Directive 95/46/EC. (10)Inordertoensureaconsistentandhighlevelofprotectionofnaturalpersons and to remove the obstacles to ﬂows of personal data within the Union, the level of protection of the rights and freedoms of natural persons with regard to the processing of such data should be equivalent in all Member States. Consistent and homogenous application of the rules for the protection of the fundamental rights and freedoms of natural persons with regard to the processing of personal data should be ensured throughout the Union. Regarding the processing of personal data for compliance with a legal obligation, for the performance of a task carried out in the public interest orintheexerciseofoﬃcialauthorityvestedinthecontroller, MemberStates should be allowed to maintain or introduce national provisions to further specify the application of the rules of this Regulation. In conjunction with the general and horizontal law on data protection implementing Directive 95/46/EC, Member States have several sector-speciﬁc laws in areas that need more speciﬁc provisions. This Regulation also provides a margin of manoeuvre for Member States to specify its rules, including for the processing of special categories of personal data (‘sensitive data’). To that extent, this Regulation does not exclude Member State law that sets out the circumstances for speciﬁc processing situations, including determining more precisely the conditions under which the processing of personal data is lawful. (11)Eﬀective protection of personal data throughout the Union requires the strengthening and setting out in detail of the rights of data subjects and the obligations of those who process and determine the processing of personal data, as well as equivalent powers for monitoring and ensuring compliance with the rules for the protection of personal data and equivalent sanctions for infringements in the Member States. (12)Article 16(2) TFEU mandates the European Parliament and the Council 3",
        "to lay down the rules relating to the protection of natural persons with regard to the processing of personal data and the rules relating to the free movement of personal data. (13)In order to ensure a consistent level of protection for natural persons throughout the Union and to prevent divergences hampering the free move- ment of personal data within the internal market, a Regulation is necessary to provide legal certainty and transparency for economic operators, in- cluding micro, small and medium-sized enterprises, and to provide natural persons in all Member States with the same level of legally enforceable rights and obligations and responsibilities for controllers and processors, to ensure consistent monitoring of the processing of personal data, and equiva- lent sanctions in all Member States as well as eﬀective cooperation between the supervisory authorities of diﬀerent Member States. The proper func- tioning of the internal market requires that the free movement of personal data within the Union is not restricted or prohibited for reasons connected with the protection of natural persons with regard to the processing of personal data. To take account of the speciﬁc situation of micro, small and medium-sized enterprises, this Regulation includes a derogation for organisations with fewer than 250 employees with regard to record-keeping. In addition, the Union institutions and bodies, and Member States and their supervisory authorities, are encouraged to take account of the speciﬁc needs of micro, small and medium-sized enterprises in the application of this Regulation. The notion of micro, small and medium-sized enterprises should draw from Article 2 of the Annex to Commission Recommendation 2003/361/EC5 (14)The protection aﬀorded by this Regulation should apply to natural persons, whatever their nationality or place of residence, in relation to the processing of their personal data. This Regulation does not cover the processing of personal data which concerns legal persons and in particular undertakings established as legal persons, including the name and the form of the legal person and the contact details of the legal person. (15)In order to prevent creating a serious risk of circumvention, the protection of natural persons should be technologically neutral and should not depend on the techniques used. The protection of natural persons should apply to the processing of personal data by automated means, as well as to manual processing, if the personal data are contained or are intended to be contained in a ﬁling system. Files or sets of ﬁles, as well as their cover pages, which are not structured according to speciﬁc criteria should not fall within the scope of this Regulation. (16)This Regulation does not apply to issues of protection of fundamental rights and freedoms or the free ﬂow of personal data related to activities which fall outside the scope of Union law, such as activities concerning national security. This Regulation does not apply to the processing of personal data by the Member States when carrying out activities in relation to the common foreign and security policy of the Union. 5Commission Recommendation of 6 May 2003 concerning the deﬁnition of micro, small and medium-sized enterprises (C(2003) 1422) (OJ L 124, 20.5.2003, p. 36). 4",
        "(17)Regulation(EC)No45/2001oftheEuropeanParliamentandoftheCouncil (6) applies to the processing of personal data by the Union institutions, bodies, oﬃces and agencies. Regulation (EC) No 45/2001 and other Union legal acts applicable to such processing of personal data should be adapted to the principles and rules established in this Regulation and applied in the light of this Regulation. In order to provide a strong and coherent data protection framework in the Union, the necessary adaptations of Regulation (EC) No 45/2001 should follow after the adoption of this Regulation, in order to allow application at the same time as this Regulation. (18)This Regulation does not apply to the processing of personal data by a natural person in the course of a purely personal or household activity and thus with no connection to a professional or commercial activity. Personal or household activities could include correspondence and the holding of addresses, or social networking and online activity undertaken within the context of such activities. However, this Regulation applies to controllers or processors which provide the means for processing personal data for such personal or household activities. (19)The protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investi- gation, detection or prosecution of criminal oﬀences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security and the free movement of such data, is the subject of a speciﬁc Union legal act. This Regulation should not, therefore, apply to processing activities for those purposes. However, personal data processed by public authorities under this Regulation should, when used for those purposes, be governed by a more speciﬁc Union legal act, namely Directive (EU) 2016/680 of the European Parliament and of the Council (7). Member States may entrust competent authorities within the meaning of Directive (EU) 2016/680 with tasks which are not necessarily carried out for the purposes of the prevention, investigation, detection or prosecution of criminal oﬀences or the execution of criminal penalties, including the safeguarding against and prevention of threats to public security, so that the processing of personal data for those other purposes, in so far as it is within the scope of Union law, falls within the scope of this Regulation. With regard to the processing of personal data by those competent au- thorities for purposes falling within scope of this Regulation, Member States should be able to maintain or introduce more speciﬁc provisions to adapt the application of the rules of this Regulation. Such provisions may determine more precisely speciﬁc requirements for the processing of personal data by those competent authorities for those other purposes, taking into account the constitutional, organisational and administrative structure of the respective Member State. When the processing of per- sonal data by private bodies falls within the scope of this Regulation, this Regulation should provide for the possibility for Member States under speciﬁc conditions to restrict by law certain obligations and rights when such a restriction constitutes a necessary and proportionate measure in a democratic society to safeguard speciﬁc important interests including public security and the prevention, investigation, detection or prosecution 5",
        "of criminal oﬀences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security. This is relevant for instance in the framework of anti-money laundering or the activities of forensic laboratories. (20)WhilethisRegulationapplies, interalia, totheactivitiesofcourtsandother judicialauthorities, UnionorMemberStatelawcouldspecifytheprocessing operations and processing procedures in relation to the processing of personal data by courts and other judicial authorities. The competence of the supervisory authorities should not cover the processing of personal data when courts are acting in their judicial capacity, in order to safeguard the independence of the judiciary in the performance of its judicial tasks, including decision-making. It should be possible to entrust supervision of such data processing operations to speciﬁc bodies within the judicial system of the Member State, which should, in particular ensure compliance with the rules of this Regulation, enhance awareness among members of the judiciary of their obligations under this Regulation and handle complaints in relation to such data processing operations. (21)This Regulation is without prejudice to the application of Directive 2000/31/EC of the European Parliament and of the Council (8), in particular of the liability rules of intermediary service providers in Articles 12 to 15 of that Directive. That Directive seeks to contribute to the proper functioning of the internal market by ensuring the free movement of information society services between Member States. (22)Any processing of personal data in the context of the activities of an establishment of a controller or a processor in the Union should be carried out in accordance with this Regulation, regardless of whether the processing itself takes place within the Union. Establishment implies the eﬀective and real exercise of activity through stable arrangements. The legal form of such arrangements, whether through a branch or a subsidiary with a legal personality, is not the determining factor in that respect. (23)In order to ensure that natural persons are not deprived of the protection to which they are entitled under this Regulation, the processing of personal data of data subjects who are in the Union by a controller or a processor not established in the Union should be subject to this Regulation where the processing activities are related to oﬀering goods or services to such data subjects irrespective of whether connected to a payment. In order to determinewhethersuchacontrollerorprocessorisoﬀeringgoodsorservices to data subjects who are in the Union, it should be ascertained whether it is apparent that the controller or processor envisages oﬀering services to data subjects in one or more Member States in the Union. Whereas the mere accessibility of the controller’s, processor’s or an intermediary’s website in the Union, of an email address or of other contact details, or the use of a language generally used in the third country where the controller is established, is insuﬃcient to ascertain such intention, factors such as the use of a language or a currency generally used in one or more Member States with the possibility of ordering goods and services in that other language, or the mentioning of customers or users who are in the Union, may make it apparent that the controller envisages oﬀering goods or services to data 6",
        "subjects in the Union. (24)The processing of personal data of data subjects who are in the Union by a controller or processor not established in the Union should also be subject to this Regulation when it is related to the monitoring of the behaviour of such data subjects in so far as their behaviour takes place within the Union. In order to determine whether a processing activity can be considered to monitor the behaviour of data subjects, it should be ascertained whether natural persons are tracked on the internet including potential subsequent use of personal data processing techniques which consist of proﬁling a natural person, particularly in order to take decisions concerning her or him or for analysing or predicting her or his personal preferences, behaviours and attitudes. (25)Where Member State law applies by virtue of public international law, this Regulation should also apply to a controller not established in the Union, such as in a Member State’s diplomatic mission or consular post. (26)The principles of data protection should apply to any information con- cerning an identiﬁed or identiﬁable natural person. Personal data which have undergone pseudonymisation, which could be attributed to a natural person by the use of additional information should be considered to be information on an identiﬁable natural person. To determine whether a natural person is identiﬁable, account should be taken of all the means reasonably likely to be used, such as singling out, either by the controller or by another person to identify the natural person directly or indirectly. To ascertain whether means are reasonably likely to be used to identify the natural person, account should be taken of all objective factors, such as the costs of and the amount of time required for identiﬁcation, taking into consideration the available technology at the time of the processing and technological developments. The principles of data protection should therefore not apply to anonymous information, namely information which does not relate to an identiﬁed or identiﬁable natural person or to personal data rendered anonymous in such a manner that the data subject is not or no longer identiﬁable. This Regulation does not therefore concern the processing of such anonymous information, including for statistical or research purposes. (27)This Regulation does not apply to the personal data of deceased persons. Member States may provide for rules regarding the processing of personal data of deceased persons. (28)The application of pseudonymisation to personal data can reduce the risks to the data subjects concerned and help controllers and processors to meet their data-protection obligations. The explicit introduction of ‘pseudonymisation’ in this Regulation is not intended to preclude any other measures of data protection. (29)In order to create incentives to apply pseudonymisation when processing personal data, measures of pseudonymisation should, whilst allowing gen- eral analysis, be possible within the same controller when that controller has taken technical and organisational measures necessary to ensure, for the processing concerned, that this Regulation is implemented, and that 7",
        "additional information for attributing the personal data to a speciﬁc data subject is kept separately. The controller processing the personal data should indicate the authorised persons within the same controller. (30)Natural persons may be associated with online identiﬁers provided by their devices, applications, tools and protocols, such as internet protocol addresses, cookie identiﬁers or other identiﬁers such as radio frequency identiﬁcation tags. This may leave traces which, in particular when com- bined with unique identiﬁers and other information received by the servers, may be used to create proﬁles of the natural persons and identify them. (31)Public authorities to which personal data are disclosed in accordance with a legal obligation for the exercise of their oﬃcial mission, such as tax and customs authorities, ﬁnancial investigation units, independent administrative authorities, or ﬁnancial market authorities responsible for the regulation and supervision of securities markets should not be regarded as recipients if they receive personal data which are necessary to carry out a particular inquiry in the general interest, in accordance with Union or Member State law. The requests for disclosure sent by the public authorities should always be in writing, reasoned and occasional and should not concern the entirety of a ﬁling system or lead to the interconnection of ﬁling systems. The processing of personal data by those public authorities should comply with the applicable data-protection rules according to the purposes of the processing. (32)Consent should be given by a clear aﬃrmative act establishing a freely given, speciﬁc, informed and unambiguous indication of the data subject’s agreement to the processing of personal data relating to him or her, such as by a written statement, including by electronic means, or an oral statement. This could include ticking a box when visiting an internet website, choosing technical settings for information society services or another statement or conductwhichclearlyindicatesinthiscontextthedatasubject’sacceptance of the proposed processing of his or her personal data. Silence, pre-ticked boxes or inactivity should not therefore constitute consent. Consent should cover all processing activities carried out for the same purpose or purposes. When the processing has multiple purposes, consent should be given for all of them. If the data subject’s consent is to be given following a request by electronic means, the request must be clear, concise and not unnecessarily disruptive to the use of the service for which it is provided. (33)It is often not possible to fully identify the purpose of personal data processing for scientiﬁc research purposes at the time of data collection. Therefore, data subjects should be allowed to give their consent to certain areasofscientiﬁcresearchwheninkeepingwithrecognisedethicalstandards for scientiﬁc research. Data subjects should have the opportunity to give their consent only to certain areas of research or parts of research projects to the extent allowed by the intended purpose. (34)Genetic data should be deﬁned as personal data relating to the inherited or acquired genetic characteristics of a natural person which result from the analysis of a biological sample from the natural person in question, in particular chromosomal, deoxyribonucleic acid (DNA) or ribonucleic 8",
        "acid (RNA) analysis, or from the analysis of another element enabling equivalent information to be obtained. (35)Personal data concerning health should include all data pertaining to the health status of a data subject which reveal information relating to the past, current or future physical or mental health status of the data subject. This includes information about the natural person collected in the course of the registration for, or the provision of, health care services as referred to in Directive 2011/24/EU of the European Parliament and of the Council (9) to that natural person; a number, symbol or particular assigned to a natural person to uniquely identify the natural person for health purposes; information derived from the testing or examination of a body part or bodily substance, including from genetic data and biological samples; and any information on, for example, a disease, disability, disease risk, medical history, clinical treatment or the physiological or biomedical state of the data subject independent of its source, for example from a physician or other health professional, a hospital, a medical device or an in vitro diagnostic test. (36)The main establishment of a controller in the Union should be the place of its central administration in the Union, unless the decisions on the purposes and means of the processing of personal data are taken in another establishment of the controller in the Union, in which case that other establishmentshouldbeconsideredtobethemainestablishment. Themain establishment of a controller in the Union should be determined according to objective criteria and should imply the eﬀective and real exercise of management activities determining the main decisions as to the purposes and means of processing through stable arrangements. That criterion should not depend on whether the processing of personal data is carried out at that location. The presence and use of technical means and technologies for processing personal data or processing activities do not, in themselves, constitute a main establishment and are therefore not determining criteria for a main establishment. The main establishment of the processor should be the place of its central administration in the Union or, if it has no central administration in the Union, the place where the main processing activities take place in the Union. In cases involving both the controller and the processor, the competent lead supervisory authority should remain the supervisory authority of the Member State where the controller has its main establishment, but the supervisory authority of the processor should be considered to be a supervisory authority concerned and that supervisory authority should participate in the cooperation procedure provided for by this Regulation. In any case, the supervisory authorities of the Member State or Member States where the processor has one or more establishments should not be considered to be supervisory authorities concerned where the draft decision concerns only the controller. Where the processing is carried out by a group of undertakings, the main establishment of the controlling undertaking should be considered to be the main establishment of the group of undertakings, except where the purposes and means of processing are determined by another undertaking. (37)A group of undertakings should cover a controlling undertaking and its 9",
        "controlled undertakings, whereby the controlling undertaking should be the undertaking which can exert a dominant inﬂuence over the other undertakings by virtue, for example, of ownership, ﬁnancial participation or the rules which govern it or the power to have personal data protection rules implemented. An undertaking which controls the processing of personal data in undertakings aﬃliated to it should be regarded, together with those undertakings, as a group of undertakings. (38)Children merit speciﬁc protection with regard to their personal data, as they may be less aware of the risks, consequences and safeguards concerned and their rights in relation to the processing of personal data. Such speciﬁc protection should, in particular, apply to the use of personal data of children for the purposes of marketing or creating personality or user proﬁles and the collection of personal data with regard to children when using services oﬀered directly to a child. The consent of the holder of parental responsibility should not be necessary in the context of preventive or counselling services oﬀered directly to a child. (39)Any processing of personal data should be lawful and fair. It should be transparent to natural persons that personal data concerning them are collected, used, consulted or otherwise processed and to what extent the personal data are or will be processed. The principle of transparency requires that any information and communication relating to the processing of those personal data be easily accessible and easy to understand, and that clear and plain language be used. That principle concerns, in particular, information to the data subjects on the identity of the controller and the purposes of the processing and further information to ensure fair and transparent processing in respect of the natural persons concerned and their right to obtain conﬁrmation and communication of personal data concerning them which are being processed. Natural persons should be made aware of risks, rules, safeguards and rights in relation to the processing of personal data and how to exercise their rights in relation to such processing. In particular, the speciﬁc purposes for which personal data are processed should be explicit and legitimate and determined at the time of the collection of the personal data. The personal data should be adequate, relevant and limited to what is necessary for the purposes for which they are processed. This requires, in particular, ensuring that the period for which the personal data are stored is limited to a strict minimum. Personal data should be processed only if the purpose of the processing could not reasonably be fulﬁlled by other means. In order to ensure that the personal data are not kept longer than necessary, time limits should be established by the controller for erasure or for a periodic review. Every reasonable step should be taken to ensure that personal data which are inaccurate are rectiﬁed or deleted. Personal data should be processed in a manner that ensures appropriate security and conﬁdentiality of the personal data, including for preventing unauthorised access to or use of personal data and the equipment used for the processing. (40)In order for processing to be lawful, personal data should be processed on the basis of the consent of the data subject concerned or some other legitimate basis, laid down by law, either in this Regulation or in other 10",
        "Union or Member State law as referred to in this Regulation, including the necessity for compliance with the legal obligation to which the controller is subject or the necessity for the performance of a contract to which the data subject is party or in order to take steps at the request of the data subject prior to entering into a contract. (41)Where this Regulation refers to a legal basis or a legislative measure, this does not necessarily require a legislative act adopted by a parliament, without prejudice to requirements pursuant to the constitutional order of the Member State concerned. However, such a legal basis or legislative measureshouldbeclearandpreciseanditsapplicationshouldbeforeseeable to persons subject to it, in accordance with the case-law of the Court of Justice of the European Union (the ‘Court of Justice’) and the European Court of Human Rights. (42)Where processing is based on the data subject’s consent, the controller should be able to demonstrate that the data subject has given consent to the processing operation. In particular in the context of a written declaration on another matter, safeguards should ensure that the data subject is aware of the fact that and the extent to which consent is given. In accordance with Council Directive 93/13/EEC (10) a declaration of consent pre-formulated by the controller should be provided in an intelligible and easily accessible form, using clear and plain language and it should not contain unfair terms. For consent to be informed, the data subject should be aware at least of the identity of the controller and the purposes of the processing for which the personal data are intended. Consent should not be regarded as freely given if the data subject has no genuine or free choice or is unable to refuse or withdraw consent without detriment. (43)In order to ensure that consent is freely given, consent should not provide a valid legal ground for the processing of personal data in a speciﬁc case where there is a clear imbalance between the data subject and the controller, in particular where the controller is a public authority and it is therefore unlikely that consent was freely given in all the circumstances of that speciﬁc situation. Consent is presumed not to be freely given if it does not allow separate consent to be given to diﬀerent personal data processing operations despite it being appropriate in the individual case, or if the performance of a contract, including the provision of a service, is dependent on the consent despite such consent not being necessary for such performance. (44)Processing should be lawful where it is necessary in the context of a contract or the intention to enter into a contract. (45)Where processing is carried out in accordance with a legal obligation to which the controller is subject or where processing is necessary for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority, the processing should have a basis in Union or Member State law. This Regulation does not require a speciﬁc law for each individual processing. A law as a basis for several processing operations based on a legal obligation to which the controller is subject or where processing is necessary for the performance of a task carried out 11",
        "in the public interest or in the exercise of an oﬃcial authority may be suﬃcient. It should also be for Union or Member State law to determine the purpose of processing. Furthermore, that law could specify the general conditions of this Regulation governing the lawfulness of personal data processing, establish speciﬁcations for determining the controller, the type of personal data which are subject to the processing, the data subjects concerned, the entities to which the personal data may be disclosed, the purpose limitations, the storage period and other measures to ensure lawful and fair processing. It should also be for Union or Member State law to determine whether the controller performing a task carried out in the public interest or in the exercise of oﬃcial authority should be a public authority or another natural or legal person governed by public law, or, where it is in the public interest to do so, including for health purposes such as public health and social protection and the management of health care services, by private law, such as a professional association. (46)The processing of personal data should also be regarded to be lawful where it is necessary to protect an interest which is essential for the life of the data subject or that of another natural person. Processing of personal data based on the vital interest of another natural person should in principle take place only where the processing cannot be manifestly based on another legal basis. Some types of processing may serve both important grounds of public interest and the vital interests of the data subject as for instance when processing is necessary for humanitarian purposes, including for monitoring epidemics and their spread or in situations of humanitarian emergencies, in particular in situations of natural and man-made disasters. (47)The legitimate interests of a controller, including those of a controller to which the personal data may be disclosed, or of a third party, may provide a legal basis for processing, provided that the interests or the fundamental rights and freedoms of the data subject are not overriding, taking into consideration the reasonable expectations of data subjects based on their relationship with the controller. Such legitimate interest could exist for example where there is a relevant and appropriate relationship between the data subject and the controller in situations such as where the data subject is a client or in the service of the controller. At any rate the existence of a legitimate interest would need careful assessment including whether a data subject can reasonably expect at the time and in the context of the collection of the personal data that processing for that purpose may take place. The interests and fundamental rights of the data subject could in particular override the interest of the data controller where personal data are processed in circumstances where data subjects do not reasonably expect further processing. Given that it is for the legislator to provide by law for the legal basis for public authorities to process personal data, that legal basis should not apply to the processing by public authorities in the performance of their tasks. The processing of personal data strictly necessary for the purposes of preventing fraud also constitutes a legitimate interest of the data controller concerned. The processing of personal data for direct marketing purposes may be regarded as carried out for a legitimate interest. 12",
        "(48)Controllers that are part of a group of undertakings or institutions aﬃliated to a central body may have a legitimate interest in transmitting personal data within the group of undertakings for internal administrative purposes, including the processing of clients’ or employees’ personal data. The general principles for the transfer of personal data, within a group of undertakings, to an undertaking located in a third country remain unaﬀected. (49)The processing of personal data to the extent strictly necessary and pro- portionate for the purposes of ensuring network and information security, i.e. the ability of a network or an information system to resist, at a given level of conﬁdence, accidental events or unlawful or malicious actions that compromise the availability, authenticity, integrity and conﬁdentiality of stored or transmitted personal data, and the security of the related services oﬀered by, or accessible via, those networks and systems, by public author- ities, by computer emergency response teams (CERTs), computer security incident response teams (CSIRTs), by providers of electronic communi- cations networks and services and by providers of security technologies and services, constitutes a legitimate interest of the data controller con- cerned. This could, for example, include preventing unauthorised access to electronic communications networks and malicious code distribution and stopping ‘denial of service’ attacks and damage to computer and electronic communication systems. (50)The processing of personal data for purposes other than those for which the personal data were initially collected should be allowed only where the processing is compatible with the purposes for which the personal data were initially collected. In such a case, no legal basis separate from that which allowed the collection of the personal data is required. If the processing is necessary for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller, Union or Member State law may determine and specify the tasks and purposes for which the further processing should be regarded as compatible and lawful. Further processing for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes should be considered to be compatible lawful processing operations. The legal basis provided by Union or Member State law for the processing of personal data may also provide a legal basis for further processing. In order to ascertain whether a purpose of further processing is compatible with the purpose for which the personal data are initially collected, the controller, after having met all the requirements for the lawfulness of the original processing, should take into account, inter alia: any link between those purposes and the purposes of the intended further processing; the context in which the personal data have been collected, in particular the reasonable expectations of data subjects based on their relationship with the controller as to their further use; the nature of the personal data; the consequences of the intended further processing for data subjects; and the existence of appropriate safeguards in both the original and intended further processing operations. Where the data subject has given consent or the processing is based on UnionorMemberStatelawwhichconstitutesanecessaryandproportionate 13",
        "measure in a democratic society to safeguard, in particular, important objectives of general public interest, the controller should be allowed to further process the personal data irrespective of the compatibility of the purposes. In any case, the application of the principles set out in this Regulation and in particular the information of the data subject on those other purposes and on his or her rights including the right to object, should be ensured. Indicating possible criminal acts or threats to public security by the controller and transmitting the relevant personal data in individual cases or in several cases relating to the same criminal act or threats to public security to a competent authority should be regarded as being in the legitimate interest pursued by the controller. However, such transmission in the legitimate interest of the controller or further processing of personal data should be prohibited if the processing is not compatible with a legal, professional or other binding obligation of secrecy. (51)Personal data which are, by their nature, particularly sensitive in relation to fundamental rights and freedoms merit speciﬁc protection as the context of their processing could create signiﬁcant risks to the fundamental rights and freedoms. Those personal data should include personal data revealing racial or ethnic origin, whereby the use of the term ‘racial origin’ in this Regulation does not imply an acceptance by the Union of theories which attempttodeterminetheexistenceofseparatehumanraces. Theprocessing of photographs should not systematically be considered to be processing of special categories of personal data as they are covered by the deﬁnition of biometric data only when processed through a speciﬁc technical means allowing the unique identiﬁcation or authentication of a natural person. Such personal data should not be processed, unless processing is allowed in speciﬁc cases set out in this Regulation, taking into account that Member States law may lay down speciﬁc provisions on data protection in order to adapt the application of the rules of this Regulation for compliance with a legal obligation or for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller. In addition to the speciﬁc requirements for such processing, the general principles and other rules of this Regulation should apply, in particular as regards the conditions for lawful processing. Derogations from the general prohibition for processing such special categories of personal data should be explicitly provided, inter alia, where the data subject gives his or her explicit consent or in respect of speciﬁc needs in particular where the processing is carried out in the course of legitimate activities by certain associations or foundations the purpose of which is to permit the exercise of fundamental freedoms. (52)Derogating from the prohibition on processing special categories of personal data should also be allowed when provided for in Union or Member State law and subject to suitable safeguards, so as to protect personal data and other fundamental rights, where it is in the public interest to do so, in particular processing personal data in the ﬁeld of employment law, social protection law including pensions and for health security, monitoring and alert purposes, the prevention or control of communicable diseases and other serious threats to health. Such a derogation may be made for health purposes, including public health and the management of health-care ser- 14",
        "vices, especially in order to ensure the quality and cost-eﬀectiveness of the procedures used for settling claims for beneﬁts and services in the health insurance system, or for archiving purposes in the public interest, scien- tiﬁc or historical research purposes or statistical purposes. A derogation should also allow the processing of such personal data where necessary for the establishment, exercise or defence of legal claims, whether in court proceedings or in an administrative or out-of-court procedure. (53)Special categories of personal data which merit higher protection should be processed for health-related purposes only where necessary to achieve those purposes for the beneﬁt of natural persons and society as a whole, in particular in the context of the management of health or social care services and systems, including processing by the management and central national health authorities of such data for the purpose of quality control, management information and the general national and local supervision of the health or social care system, and ensuring continuity of health or social care and cross-border healthcare or health security, monitoring and alert purposes, or for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes, based on Union or Member State law which has to meet an objective of public interest, as well as for studies conducted in the public interest in the area of public health. Therefore, this Regulation should provide for harmonised conditions for the processing of special categories of personal data concerning health, in respect of speciﬁc needs, in particular where the processing of such data is carried out for certain health-related purposes by persons subject to a legal obligation of professional secrecy. Union or Member State law should provide for speciﬁc and suitable measures so as to protect the fundamental rights and the personal data of natural persons. Member States should be allowed to maintain or introduce further conditions, including limitations, with regard to the processing of genetic data, biometric data or data concerning health. However, this should not hamper the free ﬂow of personal data within the Union when those conditions apply to cross- border processing of such data. (54)The processing of special categories of personal data may be necessary for reasons of public interest in the areas of public health without consent of the data subject. Such processing should be subject to suitable and speciﬁc measures so as to protect the rights and freedoms of natural persons. In that context, ‘public health’ should be interpreted as deﬁned in Regulation (EC) No 1338/2008 of the European Parliament and of the Council (11), namely all elements related to health, namely health status, including morbidity and disability, the determinants having an eﬀect on that health status, health care needs, resources allocated to health care, the provision of, and universal access to, health care as well as health care expenditure and ﬁnancing, and the causes of mortality. Such processing of data concerning health for reasons of public interest should not result in personal data being processed for other purposes by third parties such as employers or insurance and banking companies. (55)Moreover, the processing of personal data by oﬃcial authorities for the purpose of achieving the aims, laid down by constitutional law or by 15",
        "international public law, of oﬃcially recognised religious associations, is carried out on grounds of public interest. (56)Where in the course of electoral activities, the operation of the democratic system in a Member State requires that political parties compile personal data on people’s political opinions, the processing of such data may be per- mitted for reasons of public interest, provided that appropriate safeguards are established. (57)If the personal data processed by a controller do not permit the controller to identify a natural person, the data controller should not be obliged to acquire additional information in order to identify the data subject for the sole purpose of complying with any provision of this Regulation. However, the controller should not refuse to take additional information provided by the data subject in order to support the exercise of his or her rights. Identiﬁcation should include the digital identiﬁcation of a data subject, for example through authentication mechanism such as the same credentials, used by the data subject to log-in to the on-line service oﬀered by the data controller. (58) The principle of transparency requires that any information addressed to the public or to the data subject be concise, easily accessible and easy to understand, and that clear and plain language and, additionally, where appropriate, visualisation be used. Such information could be provided in electronic form, for example, when addressed to the public, through a website. This is of particular relevance in situations where the proliferation of actors and the technological complexity of practice make it diﬃcult for the data subject to know and understand whether, by whom and for what purpose personal data relating to him or her are being collected, such as in the case of online advertising. Given that children merit speciﬁc protection, any information and communication, where processing is addressed to a child, should be in such a clear and plain language that the child can easily understand. (59)Modalities should be provided for facilitating the exercise of the data subject’s rights under this Regulation, including mechanisms to request and, if applicable, obtain, free of charge, in particular, access to and rectiﬁcation or erasure of personal data and the exercise of the right to object. The controller should also provide means for requests to be made electronically, especially where personal data are processed by electronic means. The controller should be obliged to respond to requests from the data subject without undue delay and at the latest within one month and to give reasons where the controller does not intend to comply with any such requests. (60)The principles of fair and transparent processing require that the data subject be informed of the existence of the processing operation and its purposes. The controller should provide the data subject with any further information necessary to ensure fair and transparent processing taking into account the speciﬁc circumstances and context in which the personal data are processed. Furthermore, the data subject should be informed of the existence of proﬁling and the consequences of such proﬁling. Where the 16",
        "personal data are collected from the data subject, the data subject should also be informed whether he or she is obliged to provide the personal data and of the consequences, where he or she does not provide such data. That information may be provided in combination with standardised icons in order to give in an easily visible, intelligible and clearly legible manner, a meaningful overview of the intended processing. Where the icons are presented electronically, they should be machine-readable. (61)Theinformationinrelationtotheprocessingofpersonaldatarelatingtothe data subject should be given to him or her at the time of collection from the data subject, or, where the personal data are obtained from another source, within a reasonable period, depending on the circumstances of the case. Where personal data can be legitimately disclosed to another recipient, the data subject should be informed when the personal data are ﬁrst disclosed to the recipient. Where the controller intends to process the personal data for a purpose other than that for which they were collected, the controller should provide the data subject prior to that further processing with information on that other purpose and other necessary information. Where the origin of the personal data cannot be provided to the data subject because various sources have been used, general information should be provided. (62)However, it is not necessary to impose the obligation to provide infor- mation where the data subject already possesses the information, where the recording or disclosure of the personal data is expressly laid down by law or where the provision of information to the data subject proves to be impossible or would involve a disproportionate eﬀort. The latter could in particular be the case where processing is carried out for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes. In that regard, the number of data subjects, the age of the data and any appropriate safeguards adopted should be taken into consideration. (63)A data subject should have the right of access to personal data which have been collected concerning him or her, and to exercise that right easily and at reasonable intervals, in order to be aware of, and verify, the lawfulness of the processing. This includes the right for data subjects to have access to data concerning their health, for example the data in their medical records containing information such as diagnoses, examination results, assessments by treating physicians and any treatment or interventions provided. Every data subject should therefore have the right to know and obtain communication in particular with regard to the purposes for which the personal data are processed, where possible the period for which the personal data are processed, the recipients of the personal data, the logic involved in any automatic personal data processing and, at least when based on proﬁling, the consequences of such processing. Where possible, the controller should be able to provide remote access to a secure system which would provide the data subject with direct access to his or her personal data. That right should not adversely aﬀect the rights or freedoms of others, including trade secrets or intellectual property and in particular the copyright protecting the software. However, the result 17",
        "of those considerations should not be a refusal to provide all information to the data subject. Where the controller processes a large quantity of information concerning the data subject, the controller should be able to request that, before the information is delivered, the data subject specify the information or processing activities to which the request relates. (64)The controller should use all reasonable measures to verify the identity of a data subject who requests access, in particular in the context of online services and online identiﬁers. A controller should not retain personal data for the sole purpose of being able to react to potential requests. (65)A data subject should have the right to have personal data concerning him or her rectiﬁed and a ‘right to be forgotten’ where the retention of such data infringes this Regulation or Union or Member State law to which the controller is subject. In particular, a data subject should have the right to have his or her personal data erased and no longer processed where the personal data are no longer necessary in relation to the purposes for which they are collected or otherwise processed, where a data subject has withdrawn his or her consent or objects to the processing of personal data concerning him or her, or where the processing of his or her personal data does not otherwise comply with this Regulation. That right is relevant in particular where the data subject has given his or her consent as a child and is not fully aware of the risks involved by the processing, and later wants to remove such personal data, especially on the internet. The data subject should be able to exercise that right notwithstanding the fact that he or she is no longer a child. However, the further retention of the personal data should be lawful where it is necessary, for exercising the right of freedom of expression and information, for compliance with a legal obligation, for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller, on the grounds of public interest in the area of public health, for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes, or for the establishment, exercise or defence of legal claims. (66)To strengthen the right to be forgotten in the online environment, the right to erasure should also be extended in such a way that a controller who has made the personal data public should be obliged to inform the controllers which are processing such personal data to erase any links to, or copies or replications of those personal data. In doing so, that controller should take reasonable steps, taking into account available technology and the means available to the controller, including technical measures, to inform the controllers which are processing the personal data of the data subject’s request. (67)Methods by which to restrict the processing of personal data could include, inter alia, temporarily moving the selected data to another processing sys- tem, making the selected personal data unavailable to users, or temporarily removing published data from a website. In automated ﬁling systems, the restriction of processing should in principle be ensured by technical means in such a manner that the personal data are not subject to further processing operations and cannot be changed. The fact that the processing 18",
        "of personal data is restricted should be clearly indicated in the system. (68)To further strengthen the control over his or her own data, where the processing of personal data is carried out by automated means, the data subject should also be allowed to receive personal data concerning him or her which he or she has provided to a controller in a structured, commonly used, machine-readable and interoperable format, and to transmit it to another controller. Data controllers should be encouraged to develop interoperable formats that enable data portability. That right should apply where the data subject provided the personal data on the basis of his or her consent or the processing is necessary for the performance of a contract. It should not apply where processing is based on a legal ground other than consent or contract. By its very nature, that right should not be exercised against controllers processing personal data in the exercise of their public duties. It should therefore not apply where the processing of the personal data is necessary for compliance with a legal obligation to which the controller is subject or for the performance of a task carried out in the public interest or in the exercise of an oﬃcial authority vested in the controller. The data subject’s right to transmit or receive personal data concerning him or her should not create an obligation for the controllers to adopt or maintain processing systems which are technically compatible. Where, in a certain set of personal data, more than one data subject is concerned, the right to receive the personal data should be without prejudice to the rights and freedoms of other data subjects in accordance with this Regulation. Furthermore, that right should not prejudice the right of the data subject to obtain the erasure of personal data and the limitations of that right as set out in this Regulation and should, in particular, not imply the erasure of personal data concerning the data subject which have been provided by him or her for the performance of a contract to the extent that and for as long as the personal data are necessary for the performance of that contract. Where technically feasible, the data subject should have the right to have the personal data transmitted directly from one controller to another. (69)Where personal data might lawfully be processed because processing is necessary for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller, or on grounds of thelegitimateinterestsofacontrollerorathirdparty, adatasubjectshould, nevertheless, be entitled to object to the processing of any personal data relating to his or her particular situation. It should be for the controller to demonstrate that its compelling legitimate interest overrides the interests or the fundamental rights and freedoms of the data subject. (70)Where personal data are processed for the purposes of direct marketing, the data subject should have the right to object to such processing, including proﬁling to the extent that it is related to such direct marketing, whether with regard to initial or further processing, at any time and free of charge. That right should be explicitly brought to the attention of the data subject and presented clearly and separately from any other information. (71)The data subject should have the right not to be subject to a decision, which may include a measure, evaluating personal aspects relating to him 19",
        "or her which is based solely on automated processing and which produces legal eﬀects concerning him or her or similarly signiﬁcantly aﬀects him or her, such as automatic refusal of an online credit application or e- recruiting practices without any human intervention. Such processing includes ‘proﬁling’ that consists of any form of automated processing of personal data evaluating the personal aspects relating to a natural person, in particular to analyse or predict aspects concerning the data subject’s performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements, where it produces legal eﬀects concerning him or her or similarly signiﬁcantly aﬀects him or her. However, decision-making based on such processing, including proﬁling, should be allowed where expressly authorised by Union or Member State law to which the controller is subject, including for fraud and tax-evasion monitoring and prevention purposes conducted in accordance with the regulations, standards and recommendations of Union institutions or national oversight bodies and to ensure the security and reliability of a service provided by the controller, or necessary for the entering or performance of a contract between the data subject and a controller, or when the data subject has given his or her explicit consent. In any case, such processing should be subject to suitable safeguards, which should include speciﬁc information to the data subject and the right to obtain human intervention, to express his or her point of view, to obtain an explanation of the decision reached after such assessment and to challenge the decision. Such measure should not concern a child. In order to ensure fair and transparent processing in respect of the data subject, taking into account the speciﬁc circumstances and context in which the personal data are processed, the controller should use appropriate mathematicalorstatisticalproceduresfortheproﬁling, implementtechnical and organisational measures appropriate to ensure, in particular, that factors which result in inaccuracies in personal data are corrected and the risk of errors is minimised, secure personal data in a manner that takes account of the potential risks involved for the interests and rights of the data subject and that prevents, inter alia, discriminatory eﬀects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an eﬀect. Automated decision-making and proﬁling based on special categories of personal data should be allowed only under speciﬁc conditions. (72)Proﬁling is subject to the rules of this Regulation governing the processing of personal data, such as the legal grounds for processing or data protection principles. The European Data Protection Board established by this Regulation (the ‘Board’) should be able to issue guidance in that context. (73)Restrictions concerning speciﬁc principles and the rights of information, access to and rectiﬁcation or erasure of personal data, the right to data portability, the right to object, decisions based on proﬁling, as well as the communication of a personal data breach to a data subject and certain related obligations of the controllers may be imposed by Union or Member State law, as far as necessary and proportionate in a democratic society to 20",
        "safeguard public security, including the protection of human life especially in response to natural or manmade disasters, the prevention, investigation and prosecution of criminal oﬀences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security, or of breaches of ethics for regulated professions, other important objectives of general public interest of the Union or of a Member State, in particular an important economic or ﬁnancial interest of the Union or of a Member State, the keeping of public registers kept for reasons of general public interest, further processing of archived personal data to provide speciﬁc information related to the political behaviour under former totalitarian state regimes or the protection of the data subject or the rights and freedoms of others, including social protection, public health and humanitarian purposes. Those restrictions should be in accordance with the requirements set out in the Charter and in the European Convention for the Protection of Human Rights and Fundamental Freedoms. (74)The responsibility and liability of the controller for any processing of personal data carried out by the controller or on the controller’s behalf should be established. In particular, the controller should be obliged to implement appropriate and eﬀective measures and be able to demonstrate the compliance of processing activities with this Regulation, including the eﬀectiveness of the measures. Those measures should take into account the nature, scope, context and purposes of the processing and the risk to the rights and freedoms of natural persons. (75)The risk to the rights and freedoms of natural persons, of varying likelihood and severity, may result from personal data processing which could lead to physical, material or non-material damage, in particular: where the processing may give rise to discrimination, identity theft or fraud, ﬁnancial loss, damage to the reputation, loss of conﬁdentiality of personal data protected by professional secrecy, unauthorised reversal of pseudonymi- sation, or any other signiﬁcant economic or social disadvantage; where data subjects might be deprived of their rights and freedoms or prevented from exercising control over their personal data; where personal data are processed which reveal racial or ethnic origin, political opinions, religion or philosophical beliefs, trade union membership, and the processing of genetic data, data concerning health or data concerning sex life or criminal convictions and oﬀences or related security measures; where personal as- pects are evaluated, in particular analysing or predicting aspects concerning performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements, in order to create or use personal proﬁles; where personal data of vulnerable natural persons, in particular of children, are processed; or where processing involves a large amount of personal data and aﬀects a large number of data subjects. (76)The likelihood and severity of the risk to the rights and freedoms of the data subject should be determined by reference to the nature, scope, context and purposes of the processing. Risk should be evaluated on the basis of an objective assessment, by which it is established whether data processing operations involve a risk or a high risk. (77)Guidance on the implementation of appropriate measures and on the 21",
        "demonstration of compliance by the controller or the processor, especially as regards the identiﬁcation of the risk related to the processing, their assessment in terms of origin, nature, likelihood and severity, and the identiﬁcation of best practices to mitigate the risk, could be provided in particular by means of approved codes of conduct, approved certiﬁcations, guidelines provided by the Board or indications provided by a data protec- tion oﬃcer. The Board may also issue guidelines on processing operations that are considered to be unlikely to result in a high risk to the rights and freedoms of natural persons and indicate what measures may be suﬃcient in such cases to address such risk. (78)The protection of the rights and freedoms of natural persons with regard to the processing of personal data require that appropriate technical and organisational measures be taken to ensure that the requirements of this Regulation are met. In order to be able to demonstrate compliance with this Regulation, the controller should adopt internal policies and implement measures which meet in particular the principles of data protection by design and data protection by default. Such measures could consist, inter alia, ofminimisingtheprocessingofpersonaldata, pseudonymisingpersonal data as soon as possible, transparency with regard to the functions and processing of personal data, enabling the data subject to monitor the data processing, enabling the controller to create and improve security features. When developing, designing, selecting and using applications, services and products that are based on the processing of personal data or process personal data to fulﬁl their task, producers of the products, services and applications should be encouraged to take into account the right to data protection when developing and designing such products, services and applications and, with due regard to the state of the art, to make sure that controllers and processors are able to fulﬁl their data protection obligations. The principles of data protection by design and by default should also be taken into consideration in the context of public tenders. (79)The protection of the rights and freedoms of data subjects as well as the responsibility and liability of controllers and processors, also in relation to the monitoring by and measures of supervisory authorities, requires a clear allocation of the responsibilities under this Regulation, including where a controller determines the purposes and means of the processing jointly with other controllers or where a processing operation is carried out on behalf of a controller. (80)Where a controller or a processor not established in the Union is processing personal data of data subjects who are in the Union whose processing activities are related to the oﬀering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union, or to the monitoring of their behaviour as far as their behaviour takes place within the Union, the controller or the processor should designate a representative, unless the processing is occasional, does not include processing, on a large scale, of special categories of personal data or the processing of personal data relating to criminal convictions and oﬀences, and is unlikely to result in a risk to the rights and freedoms of natural persons, taking into account the nature, context, scope and 22",
        "purposes of the processing or if the controller is a public authority or body. The representative should act on behalf of the controller or the processor and may be addressed by any supervisory authority. The representative should be explicitly designated by a written mandate of the controller or of the processor to act on its behalf with regard to its obligations under this Regulation. The designation of such a representative does not aﬀect the responsibility or liability of the controller or of the processor under this Regulation. Such a representative should perform its tasks according to the mandate received from the controller or processor, including cooperating with the competent supervisory authorities with regard to any action taken to ensure compliance with this Regulation. The designated representative shouldbesubjecttoenforcementproceedingsintheeventofnon-compliance by the controller or processor. (81)To ensure compliance with the requirements of this Regulation in respect of the processing to be carried out by the processor on behalf of the controller, when entrusting a processor with processing activities, the controller should use only processors providing suﬃcient guarantees, in particular in terms of expert knowledge, reliability and resources, to implement technical and organisational measures which will meet the requirements of this Regulation, including for the security of processing. The adherence of the processor to an approved code of conduct or an approved certiﬁcation mechanism may be used as an element to demonstrate compliance with the obligations of the controller. The carrying-out of processing by a processor should be governed by a contract or other legal act under Union or Member State law, binding the processor to the controller, setting out the subject-matter and duration of the processing, the nature and purposes of the processing, the type of personal data and categories of data subjects, taking into account the speciﬁc tasks and responsibilities of the processor in the context of the processing to be carried out and the risk to the rights and freedoms of the data subject. The controller and processor may choose to use an individual contract or standard contractual clauses which are adopted either directly by the Commission or by a supervisory authority in accordance with the consistency mechanism and then adopted by the Commission. After the completion of the processing on behalf of the controller, the processor should, at the choice of the controller, return or delete the personal data, unless there is a requirement to store the personal data under Union or Member State law to which the processor is subject. (82)In order to demonstrate compliance with this Regulation, the controller or processor should maintain records of processing activities under its responsibility. Each controller and processor should be obliged to cooperate withthesupervisoryauthorityandmakethoserecords, onrequest, available to it, so that it might serve for monitoring those processing operations. (83)In order to maintain security and to prevent processing in infringement of this Regulation, the controller or processor should evaluate the risks inherent in the processing and implement measures to mitigate those risks, such as encryption. Those measures should ensure an appropriate level of security, including conﬁdentiality, taking into account the state of the art and the costs of implementation in relation to the risks and the nature 23",
        "of the personal data to be protected. In assessing data security risk, consideration should be given to the risks that are presented by personal data processing, such as accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to, personal data transmitted, stored or otherwise processed which may in particular lead to physical, material or non-material damage. (84)In order to enhance compliance with this Regulation where processing operations are likely to result in a high risk to the rights and freedoms of natural persons, the controller should be responsible for the carrying-out of a data protection impact assessment to evaluate, in particular, the origin, nature, particularity and severity of that risk. The outcome of the assessment should be taken into account when determining the appropriate measures to be taken in order to demonstrate that the processing of personal data complies with this Regulation. Where a data-protection impact assessment indicates that processing operations involve a high risk which the controller cannot mitigate by appropriate measures in terms of available technology and costs of implementation, a consultation of the supervisory authority should take place prior to the processing. (85)A personal data breach may, if not addressed in an appropriate and timely manner, result in physical, material or non-material damage to natural persons such as loss of control over their personal data or limitation of their rights, discrimination, identity theft or fraud, ﬁnancial loss, unauthorised reversal of pseudonymisation, damage to reputation, loss of conﬁdentiality of personal data protected by professional secrecy or any other signiﬁcant economic or social disadvantage to the natural person concerned. Therefore, as soon as the controller becomes aware that a personal data breach has occurred, the controller should notify the personal data breach to the supervisory authority without undue delay and, where feasible, not later than 72 hours after having become aware of it, unless the controller is able to demonstrate, in accordance with the accountability principle, that the personal data breach is unlikely to result in a risk to the rights and freedoms of natural persons. Where such notiﬁcation cannot be achieved within 72 hours, the reasons for the delay should accompany the notiﬁcation and information may be provided in phases without undue further delay. (86)The controller should communicate to the data subject a personal data breach, without undue delay, where that personal data breach is likely to result in a high risk to the rights and freedoms of the natural person in order to allow him or her to take the necessary precautions. The commu- nication should describe the nature of the personal data breach as well as recommendations for the natural person concerned to mitigate potential adverse eﬀects. Such communications to data subjects should be made as soon as reasonably feasible and in close cooperation with the supervisory authority, respecting guidance provided by it or by other relevant authori- ties such as law-enforcement authorities. For example, the need to mitigate an immediate risk of damage would call for prompt communication with data subjects whereas the need to implement appropriate measures against continuing or similar personal data breaches may justify more time for communication. 24",
        "(87)It should be ascertained whether all appropriate technological protection and organisational measures have been implemented to establish imme- diately whether a personal data breach has taken place and to inform promptly the supervisory authority and the data subject. The fact that the notiﬁcation was made without undue delay should be established tak- ing into account in particular the nature and gravity of the personal data breach and its consequences and adverse eﬀects for the data subject. Such notiﬁcation may result in an intervention of the supervisory authority in accordance with its tasks and powers laid down in this Regulation. (88)In setting detailed rules concerning the format and procedures applicable to the notiﬁcation of personal data breaches, due consideration should be given to the circumstances of that breach, including whether or not personal data had been protected by appropriate technical protection measures, eﬀectively limiting the likelihood of identity fraud or other forms ofmisuse. Moreover, suchrulesandproceduresshouldtakeintoaccountthe legitimate interests of law-enforcement authorities where early disclosure could unnecessarily hamper the investigation of the circumstances of a personal data breach. (89)Directive 95/46/EC provided for a general obligation to notify the process- ing of personal data to the supervisory authorities. While that obligation produces administrative and ﬁnancial burdens, it did not in all cases con- tribute to improving the protection of personal data. Such indiscriminate general notiﬁcation obligations should therefore be abolished, and replaced by eﬀective procedures and mechanisms which focus instead on those types of processing operations which are likely to result in a high risk to the rights and freedoms of natural persons by virtue of their nature, scope, context and purposes. Such types of processing operations may be those which in, particular, involve using new technologies, or are of a new kind and where no data protection impact assessment has been carried out before by the controller, or where they become necessary in the light of the time that has elapsed since the initial processing. (90)In such cases, a data protection impact assessment should be carried out by the controller prior to the processing in order to assess the particular likelihood and severity of the high risk, taking into account the nature, scope, context and purposes of the processing and the sources of the risk. That impact assessment should include, in particular, the measures, safeguards and mechanisms envisaged for mitigating that risk, ensuring the protection of personal data and demonstrating compliance with this Regulation. (91)This should in particular apply to large-scale processing operations which aim to process a considerable amount of personal data at regional, national or supranational level and which could aﬀect a large number of data subjects and which are likely to result in a high risk, for example, on account of their sensitivity, where in accordance with the achieved state of technological knowledge a new technology is used on a large scale as well as to other processing operations which result in a high risk to the rights and freedoms of data subjects, in particular where those operations render it more diﬃcult for data subjects to exercise their rights. A data 25",
        "protection impact assessment should also be made where personal data are processed for taking decisions regarding speciﬁc natural persons following any systematic and extensive evaluation of personal aspects relating to natural persons based on proﬁling those data or following the processing of special categories of personal data, biometric data, or data on criminal convictions and oﬀences or related security measures. A data protection impact assessment is equally required for monitoring publicly accessible areas on a large scale, especially when using optic-electronic devices or for any other operations where the competent supervisory authority considers that the processing is likely to result in a high risk to the rights and freedoms of data subjects, in particular because they prevent data subjects from exercising a right or using a service or a contract, or because they are carried out systematically on a large scale. The processing of personal data should not be considered to be on a large scale if the processing concerns personal data from patients or clients by an individual physician, other health care professional or lawyer. In such cases, a data protection impact assessment should not be mandatory. (92)There are circumstances under which it may be reasonable and economical for the subject of a data protection impact assessment to be broader than a single project, for example where public authorities or bodies intend to establish a common application or processing platform or where several controllers plan to introduce a common application or processing environment across an industry sector or segment or for a widely used horizontal activity. (93)In the context of the adoption of the Member State law on which the performance of the tasks of the public authority or public body is based and which regulates the speciﬁc processing operation or set of operations in question, Member States may deem it necessary to carry out such assessment prior to the processing activities. (94)Where a data protection impact assessment indicates that the processing would, in the absence of safeguards, security measures and mechanisms to mitigate the risk, result in a high risk to the rights and freedoms of natural persons and the controller is of the opinion that the risk cannot be mitigated by reasonable means in terms of available technologies and costs of implementation, the supervisory authority should be consulted prior to the start of processing activities. Such high risk is likely to result from certain types of processing and the extent and frequency of processing, which may result also in a realisation of damage or interference with the rights and freedoms of the natural person. The supervisory authority should respond to the request for consultation within a speciﬁed period. However, the absence of a reaction of the supervisory authority within that period should be without prejudice to any intervention of the supervisory authority in accordance with its tasks and powers laid down in this Regulation, including the power to prohibit processing operations. As part of that consultation process, the outcome of a data protection impact assessment carried out with regard to the processing at issue may be submitted to the supervisory authority, in particular the measures envisaged to mitigate the risk to the rights and freedoms of natural persons. 26",
        "(95)The processor should assist the controller, where necessary and upon request, in ensuring compliance with the obligations deriving from the car- rying out of data protection impact assessments and from prior consultation of the supervisory authority. (96)A consultation of the supervisory authority should also take place in the course of the preparation of a legislative or regulatory measure which provides for the processing of personal data, in order to ensure compliance oftheintendedprocessingwiththisRegulationandinparticulartomitigate the risk involved for the data subject. (97)Where the processing is carried out by a public authority, except for courts or independent judicial authorities when acting in their judicial capacity, where, in the private sector, processing is carried out by a controller whose core activities consist of processing operations that require regular and systematic monitoring of the data subjects on a large scale, or where the core activities of the controller or the processor consist of processing on a large scale of special categories of personal data and data relating to criminal convictions and oﬀences, a person with expert knowledge of data protection law and practices should assist the controller or processor to monitor internal compliance with this Regulation. In the private sector, the core activities of a controller relate to its primary activities and do not relate to the processing of personal data as ancillary activities. The necessary level of expert knowledge should be determined in particular according to the data processing operations carried out and the protection required for the personal data processed by the controller or the processor. Such data protection oﬃcers, whether or not they are an employee of the controller, should be in a position to perform their duties and tasks in an independent manner. (98)Associations or other bodies representing categories of controllers or pro- cessors should be encouraged to draw up codes of conduct, within the limits of this Regulation, so as to facilitate the eﬀective application of this Regulation, taking account of the speciﬁc characteristics of the processing carried out in certain sectors and the speciﬁc needs of micro, small and medium enterprises. In particular, such codes of conduct could calibrate the obligations of controllers and processors, taking into account the risk likely to result from the processing for the rights and freedoms of natural persons. (99)When drawing up a code of conduct, or when amending or extending such a code, associations and other bodies representing categories of controllers or processors should consult relevant stakeholders, including data subjects where feasible, and have regard to submissions received and views expressed in response to such consultations. (100)In order to enhance transparency and compliance with this Regulation, the establishment of certiﬁcation mechanisms and data protection seals and marks should be encouraged, allowing data subjects to quickly assess the level of data protection of relevant products and services. (101)Flows of personal data to and from countries outside the Union and in- ternational organisations are necessary for the expansion of international 27",
        "trade and international cooperation. The increase in such ﬂows has raised new challenges and concerns with regard to the protection of personal data. However, when personal data are transferred from the Union to controllers, processors or other recipients in third countries or to inter- national organisations, the level of protection of natural persons ensured in the Union by this Regulation should not be undermined, including in cases of onward transfers of personal data from the third country or international organisation to controllers, processors in the same or another third country or international organisation. In any event, transfers to third countries and international organisations may only be carried out in full compliance with this Regulation. A transfer could take place only if, subject to the other provisions of this Regulation, the conditions laid down in the provisions of this Regulation relating to the transfer of personal data to third countries or international organisations are complied with by the controller or processor. (102)This Regulation is without prejudice to international agreements concluded between the Union and third countries regulating the transfer of personal data including appropriate safeguards for the data subjects. Member States may conclude international agreements which involve the transfer of personaldatatothirdcountriesorinternationalorganisations, asfarassuch agreements do not aﬀect this Regulation or any other provisions of Union law and include an appropriate level of protection for the fundamental rights of the data subjects. (103)The Commission may decide with eﬀect for the entire Union that a third country, a territory or speciﬁed sector within a third country, or an in- ternational organisation, oﬀers an adequate level of data protection, thus providing legal certainty and uniformity throughout the Union as regards the third country or international organisation which is considered to provide such level of protection. In such cases, transfers of personal data to that third country or international organisation may take place without the need to obtain any further authorisation. The Commission may also decide, having given notice and a full statement setting out the reasons to the third country or international organisation, to revoke such a decision. (104)In line with the fundamental values on which the Union is founded, in particular the protection of human rights, the Commission should, in its assessment of the third country, or of a territory or speciﬁed sector within a third country, take into account how a particular third country respects the rule of law, access to justice as well as international human rights norms and standards and its general and sectoral law, including legislation concerning public security, defence and national security as well as public order and criminal law. The adoption of an adequacy decision with regard to a territory or a speciﬁed sector in a third country should take into account clear and objective criteria, such as speciﬁc processing activities and the scope of applicable legal standards and legislation in force in the third country. The third country should oﬀer guarantees ensuring an adequate level of protection essentially equivalent to that ensured within the Union, in particular where personal data are processed in one or several speciﬁc sectors. In particular, the third country should ensure eﬀective 28",
        "independent data protection supervision and should provide for cooperation mechanisms with the Member States’ data protection authorities, and the data subjects should be provided with eﬀective and enforceable rights and eﬀective administrative and judicial redress. (105)Apart from the international commitments the third country or interna- tional organisation has entered into, the Commission should take account of obligations arising from the third country’s or international organisa- tion’s participation in multilateral or regional systems in particular in relation to the protection of personal data, as well as the implementation of such obligations. In particular, the third country’s accession to the Council of Europe Convention of 28 January 1981 for the Protection of Individuals with regard to the Automatic Processing of Personal Data and its Additional Protocol should be taken into account. The Commission should consult the Board when assessing the level of protection in third countries or international organisations. (106)The Commission should monitor the functioning of decisions on the level of protection in a third country, a territory or speciﬁed sector within a third country, or an international organisation, and monitor the functioning of decisions adopted on the basis of Article 25(6) or Article 26(4) of Directive 95/46/EC. In its adequacy decisions, the Commission should provide for a periodic review mechanism of their functioning. That periodic review should be conducted in consultation with the third country or international organisation in question and take into account all relevant developments in the third country or international organisation. For the purposes of monitoring and of carrying out the periodic reviews, the Commission should take into consideration the views and ﬁndings of the European Parliament and of the Council as well as of other relevant bodies and sources. The Commission should evaluate, within a reasonable time, the functioning of the latter decisions and report any relevant ﬁndings to the Committee within the meaning of Regulation (EU) No 182/2011 of the European Parliament and of the Council (12) as established under this Regulation, to the European Parliament and to the Council. (107)The Commission may recognise that a third country, a territory or a speciﬁed sector within a third country, or an international organisation no longer ensures an adequate level of data protection. Consequently the transfer of personal data to that third country or international organisation should be prohibited, unless the requirements in this Regulation relating to transfers subject to appropriate safeguards, including binding corporate rules, and derogations for speciﬁc situations are fulﬁlled. In that case, provision should be made for consultations between the Commission and suchthirdcountriesorinternationalorganisations. TheCommissionshould, in a timely manner, inform the third country or international organisation of the reasons and enter into consultations with it in order to remedy the situation. (108)In the absence of an adequacy decision, the controller or processor should take measures to compensate for the lack of data protection in a third country by way of appropriate safeguards for the data subject. Such appropriatesafeguardsmayconsistofmakinguseofbindingcorporaterules, 29",
        "standard data protection clauses adopted by the Commission, standard data protection clauses adopted by a supervisory authority or contractual clauses authorised by a supervisory authority. Those safeguards should ensure compliance with data protection requirements and the rights of the data subjects appropriate to processing within the Union, including the availability of enforceable data subject rights and of eﬀective legal remedies, including to obtain eﬀective administrative or judicial redress and to claim compensation, in the Union or in a third country. They should relate in particular to compliance with the general principles relating to personal data processing, the principles of data protection by design and by default. Transfers may also be carried out by public authorities or bodies with public authorities or bodies in third countries or with international organisations with corresponding duties or functions, including on the basis of provisions to be inserted into administrative arrangements, such as a memorandum of understanding, providing for enforceable and eﬀective rights for data subjects. Authorisation by the competent supervisory authority should be obtained when the safeguards are provided for in administrative arrangements that are not legally binding. (109)The possibility for the controller or processor to use standard data- protection clauses adopted by the Commission or by a supervisory authority should prevent controllers or processors neither from including the standard data-protection clauses in a wider contract, such as a contract between the processor and another processor, nor from adding other clauses or additional safeguards provided that they do not contradict, directly or indi- rectly, the standard contractual clauses adopted by the Commission or by a supervisory authority or prejudice the fundamental rights or freedoms of the data subjects. Controllers and processors should be encouraged to pro- vide additional safeguards via contractual commitments that supplement standard protection clauses. (110)A group of undertakings, or a group of enterprises engaged in a joint economicactivity, shouldbeabletomakeuseofapprovedbindingcorporate rules for its international transfers from the Union to organisations within the same group of undertakings, or group of enterprises engaged in a joint economic activity, provided that such corporate rules include all essential principles and enforceable rights to ensure appropriate safeguards for transfers or categories of transfers of personal data. (111)Provisions should be made for the possibility for transfers in certain circumstances where the data subject has given his or her explicit consent, where the transfer is occasional and necessary in relation to a contract or a legal claim, regardless of whether in a judicial procedure or whether in an administrative or any out-of-court procedure, including procedures before regulatory bodies. Provision should also be made for the possibility for transfers where important grounds of public interest laid down by Union or Member State law so require or where the transfer is made from a register established by law and intended for consultation by the public or persons having a legitimate interest. In the latter case, such a transfer should not involve the entirety of the personal data or entire categories of the data contained in the register and, when the register is intended for consultation 30",
        "by persons having a legitimate interest, the transfer should be made only at the request of those persons or, if they are to be the recipients, taking into full account the interests and fundamental rights of the data subject. (112)Those derogations should in particular apply to data transfers required and necessary for important reasons of public interest, for example in cases of international data exchange between competition authorities, tax or cus- toms administrations, between ﬁnancial supervisory authorities, between services competent for social security matters, or for public health, for example in the case of contact tracing for contagious diseases or in order to reduce and/or eliminate doping in sport. A transfer of personal data should also be regarded as lawful where it is necessary to protect an interest which is essential for the data subject’s or another person’s vital interests, including physical integrity or life, if the data subject is incapable of giving consent. In the absence of an adequacy decision, Union or Member State law may, for important reasons of public interest, expressly set limits to the transfer of speciﬁc categories of data to a third country or an interna- tional organisation. Member States should notify such provisions to the Commission. Any transfer to an international humanitarian organisation of personal data of a data subject who is physically or legally incapable of giving consent, with a view to accomplishing a task incumbent under the Geneva Conventions or to complying with international humanitarian law applicable in armed conﬂicts, could be considered to be necessary for an important reason of public interest or because it is in the vital interest of the data subject. (113)Transfers which can be qualiﬁed as not repetitive and that only concern a limited number of data subjects, could also be possible for the purposes of the compelling legitimate interests pursued by the controller, when those interests are not overridden by the interests or rights and freedoms of the data subject and when the controller has assessed all the circumstances surrounding the data transfer. The controller should give particular consid- eration to the nature of the personal data, the purpose and duration of the proposed processing operation or operations, as well as the situation in the country of origin, the third country and the country of ﬁnal destination, and should provide suitable safeguards to protect fundamental rights and freedoms of natural persons with regard to the processing of their personal data. Such transfers should be possible only in residual cases where none of the other grounds for transfer are applicable. For scientiﬁc or historical research purposes or statistical purposes, the legitimate expectations of society for an increase of knowledge should be taken into consideration. The controller should inform the supervisory authority and the data subject about the transfer. (114)In any case, where the Commission has taken no decision on the adequate level of data protection in a third country, the controller or processor should make use of solutions that provide data subjects with enforceable and eﬀective rights as regards the processing of their data in the Union once those data have been transferred so that that they will continue to beneﬁt from fundamental rights and safeguards. (115)Some third countries adopt laws, regulations and other legal acts which 31",
        "purport to directly regulate the processing activities of natural and legal persons under the jurisdiction of the Member States. This may include judgments of courts or tribunals or decisions of administrative authorities in third countries requiring a controller or processor to transfer or disclose personal data, and which are not based on an international agreement, such as a mutual legal assistance treaty, in force between the requesting third country and the Union or a Member State. The extraterritorial application of those laws, regulations and other legal acts may be in breach of international law and may impede the attainment of the protection of natural persons ensured in the Union by this Regulation. Transfers should only be allowed where the conditions of this Regulation for a transfer to third countries are met. This may be the case, inter alia, where disclosure is necessary for an important ground of public interest recognised in Union or Member State law to which the controller is subject. (116)When personal data moves across borders outside the Union it may put at increased risk the ability of natural persons to exercise data protection rightsinparticulartoprotectthemselvesfromtheunlawfuluseordisclosure of that information. At the same time, supervisory authorities may ﬁnd that they are unable to pursue complaints or conduct investigations relating to the activities outside their borders. Their eﬀorts to work together in the cross-border context may also be hampered by insuﬃcient preventative or remedial powers, inconsistent legal regimes, and practical obstacles like resource constraints. Therefore, there is a need to promote closer cooperation among data protection supervisory authorities to help them exchange information and carry out investigations with their international counterparts. For the purposes of developing international cooperation mechanisms to facilitate and provide international mutual assistance for the enforcement of legislation for the protection of personal data, the Commission and the supervisory authorities should exchange information and cooperate in activities related to the exercise of their powers with competent authorities in third countries, based on reciprocity and in accordance with this Regulation. (117)The establishment of supervisory authorities in Member States, empowered to perform their tasks and exercise their powers with complete indepen- dence, is an essential component of the protection of natural persons with regard to the processing of their personal data. Member States should be able to establish more than one supervisory authority, to reﬂect their constitutional, organisational and administrative structure. (118)The independence of supervisory authorities should not mean that the supervisory authorities cannot be subject to control or monitoring mecha- nisms regarding their ﬁnancial expenditure or to judicial review. (119)Where a Member State establishes several supervisory authorities, it should establish by law mechanisms for ensuring the eﬀective participation of those supervisory authorities in the consistency mechanism. That Member State should in particular designate the supervisory authority which functions as a single contact point for the eﬀective participation of those authorities in the mechanism, to ensure swift and smooth cooperation with other supervisory authorities, the Board and the Commission. 32",
        "(120)Each supervisory authority should be provided with the ﬁnancial and human resources, premises and infrastructure necessary for the eﬀective performance of their tasks, including those related to mutual assistance and cooperation with other supervisory authorities throughout the Union. Each supervisory authority should have a separate, public annual budget, which may be part of the overall state or national budget. (121)The general conditions for the member or members of the supervisory authority should be laid down by law in each Member State and should in particular provide that those members are to be appointed, by means of a transparent procedure, either by the parliament, government or the head of State of the Member State on the basis of a proposal from the government, a member of the government, the parliament or a chamber of the parliament, or by an independent body entrusted under Member State law. In order to ensure the independence of the supervisory authority, the member or members should act with integrity, refrain from any action that is incompatible with their duties and should not, during their term of oﬃce, engage in any incompatible occupation, whether gainful or not. The supervisory authority should have its own staﬀ, chosen by the supervisory authority or an independent body established by Member State law, which should be subject to the exclusive direction of the member or members of the supervisory authority. (122)Each supervisory authority should be competent on the territory of its own Member State to exercise the powers and to perform the tasks conferred on it in accordance with this Regulation. This should cover in particular the processing in the context of the activities of an establishment of the controller or processor on the territory of its own Member State, the processing of personal data carried out by public authorities or private bodies acting in the public interest, processing aﬀecting data subjects on its territory or processing carried out by a controller or processor not established in the Union when targeting data subjects residing on its territory. This should include handling complaints lodged by a data subject, conducting investigations on the application of this Regulation and promoting public awareness of the risks, rules, safeguards and rights in relation to the processing of personal data. (123)Thesupervisoryauthoritiesshouldmonitortheapplicationoftheprovisions pursuant to this Regulation and contribute to its consistent application throughout the Union, in order to protect natural persons in relation to the processing of their personal data and to facilitate the free ﬂow of personal data within the internal market. For that purpose, the supervisory authorities should cooperate with each other and with the Commission, withouttheneedforanyagreementbetweenMemberStatesontheprovision of mutual assistance or on such cooperation. (124)Where the processing of personal data takes place in the context of the activities of an establishment of a controller or a processor in the Union and the controller or processor is established in more than one Member State, or where processing taking place in the context of the activities of a single establishment of a controller or processor in the Union substantially aﬀects or is likely to substantially aﬀect data subjects in more than one 33",
        "Member State, the supervisory authority for the main establishment of the controller or processor or for the single establishment of the controller or processor should act as lead authority. It should cooperate with the other authorities concerned, because the controller or processor has an establishment on the territory of their Member State, because data subjects residing on their territory are substantially aﬀected, or because a complaint has been lodged with them. Also where a data subject not residing in that Member State has lodged a complaint, the supervisory authority with which such complaint has been lodged should also be a supervisory authority concerned. Within its tasks to issue guidelines on any question covering the application of this Regulation, the Board should be able to issue guidelines in particular on the criteria to be taken into account in order to ascertain whether the processing in question substantially aﬀects data subjects in more than one Member State and on what constitutes a relevant and reasoned objection. (125)The lead authority should be competent to adopt binding decisions regard- ing measures applying the powers conferred on it in accordance with this Regulation. In its capacity as lead authority, the supervisory authority should closely involve and coordinate the supervisory authorities concerned in the decision-making process. Where the decision is to reject the com- plaint by the data subject in whole or in part, that decision should be adopted by the supervisory authority with which the complaint has been lodged. (126)The decision should be agreed jointly by the lead supervisory authority and the supervisory authorities concerned and should be directed towards the main or single establishment of the controller or processor and be binding on the controller and processor. The controller or processor should take the necessary measures to ensure compliance with this Regulation and the implementation of the decision notiﬁed by the lead supervisory authority to the main establishment of the controller or processor as regards the processing activities in the Union. (127)Each supervisory authority not acting as the lead supervisory authority should be competent to handle local cases where the controller or processor is established in more than one Member State, but the subject matter of the speciﬁc processing concerns only processing carried out in a single Member State and involves only data subjects in that single Member State, for example, where the subject matter concerns the processing of employees’ personal data in the speciﬁc employment context of a Member State. In such cases, the supervisory authority should inform the lead supervisory authority without delay about the matter. After being informed, the lead supervisory authority should decide, whether it will handle the case pursuant to the provision on cooperation between the lead supervisory authority and other supervisory authorities concerned (‘one-stop-shop mechanism’), or whether the supervisory authority which informed it should handle the case at local level. When deciding whether it will handle the case, the lead supervisory authority should take into account whether there is an establishment of the controller or processor in the Member State of the supervisory authority which informed it in order to ensure eﬀective 34",
        "enforcement of a decision vis-à-vis the controller or processor. Where the lead supervisory authority decides to handle the case, the supervisory authority which informed it should have the possibility to submit a draft for a decision, of which the lead supervisory authority should take utmost account when preparing its draft decision in that one-stop-shop mechanism. (128)The rules on the lead supervisory authority and the one-stop-shop mech- anism should not apply where the processing is carried out by public authorities or private bodies in the public interest. In such cases the only supervisory authority competent to exercise the powers conferred to it in accordance with this Regulation should be the supervisory authority of the Member State where the public authority or private body is established. (129)In order to ensure consistent monitoring and enforcement of this Regulation throughout the Union, the supervisory authorities should have in each Member State the same tasks and eﬀective powers, including powers of investigation, corrective powers and sanctions, and authorisation and advisory powers, in particular in cases of complaints from natural persons, and without prejudice to the powers of prosecutorial authorities under MemberStatelaw, tobringinfringementsofthisRegulationtotheattention of the judicial authorities and engage in legal proceedings. Such powers should also include the power to impose a temporary or deﬁnitive limitation, including a ban, on processing. Member States may specify other tasks related to the protection of personal data under this Regulation. The powers of supervisory authorities should be exercised in accordance with appropriate procedural safeguards set out in Union and Member State law, impartially, fairly and within a reasonable time. In particular each measure should be appropriate, necessary and proportionate in view of ensuring compliance with this Regulation, taking into account the circumstances of each individual case, respect the right of every person to be heard before any individual measure which would aﬀect him or her adversely is taken and avoid superﬂuous costs and excessive inconveniences for the persons concerned. Investigatory powers as regards access to premises should be exercised in accordance with speciﬁc requirements in Member State procedural law, such as the requirement to obtain a prior judicial authorisation. Each legally binding measure of the supervisory authority should be in writing, be clear and unambiguous, indicate the supervisory authority which has issued the measure, the date of issue of the measure, bear the signature of the head, or a member of the supervisory authority authorised by him or her, give the reasons for the measure, and refer to the right of an eﬀective remedy. This should not preclude additional requirements pursuant to Member State procedural law. The adoption of a legally binding decision implies that it may give rise to judicial review in the Member State of the supervisory authority that adopted the decision. (130)Where the supervisory authority with which the complaint has been lodged is not the lead supervisory authority, the lead supervisory authority should closely cooperate with the supervisory authority with which the complaint has been lodged in accordance with the provisions on cooperation and consistencylaiddown inthisRegulation. Insuchcases, the leadsupervisory authority should, when taking measures intended to produce legal eﬀects, 35",
        "including the imposition of administrative ﬁnes, take utmost account of the view of the supervisory authority with which the complaint has been lodged and which should remain competent to carry out any investigation on the territory of its own Member State in liaison with the competent supervisory authority. (131)Where another supervisory authority should act as a lead supervisory authority for the processing activities of the controller or processor but the concretesubjectmatterofacomplaintorthepossibleinfringementconcerns only processing activities of the controller or processor in the Member State where the complaint has been lodged or the possible infringement detected and the matter does not substantially aﬀect or is not likely to substantially aﬀect data subjects in other Member States, the supervisory authority receiving a complaint or detecting or being informed otherwise of situations that entail possible infringements of this Regulation should seek an amicable settlement with the controller and, if this proves unsuccessful, exercise its full range of powers. This should include: speciﬁc processing carried out in the territory of the Member State of the supervisory authority or with regard to data subjects on the territory of that Member State; processing that is carried out in the context of an oﬀer of goods or services speciﬁcally aimed at data subjects in the territory of the Member State of the supervisory authority; or processing that has to be assessed taking into account relevant legal obligations under Member State law. (132)Awareness-raising activities by supervisory authorities addressed to the public should include speciﬁc measures directed at controllers and pro- cessors, including micro, small and medium-sized enterprises, as well as natural persons in particular in the educational context. (133)The supervisory authorities should assist each other in performing their tasks and provide mutual assistance, so as to ensure the consistent ap- plication and enforcement of this Regulation in the internal market. A supervisory authority requesting mutual assistance may adopt a provisional measure if it receives no response to a request for mutual assistance within one month of the receipt of that request by the other supervisory authority. (134)Each supervisory authority should, where appropriate, participate in joint operations with other supervisory authorities. The requested supervisory authority should be obliged to respond to the request within a speciﬁed time period. (135)In order to ensure the consistent application of this Regulation through- out the Union, a consistency mechanism for cooperation between the supervisory authorities should be established. That mechanism should in particular apply where a supervisory authority intends to adopt a measure intended to produce legal eﬀects as regards processing operations which substantially aﬀect a signiﬁcant number of data subjects in several Member States. It should also apply where any supervisory authority concerned or the Commission requests that such matter should be handled in the consistency mechanism. That mechanism should be without prejudice to any measures that the Commission may take in the exercise of its powers under the Treaties. 36",
        "(136)In applying the consistency mechanism, the Board should, within a de- termined period of time, issue an opinion, if a majority of its members so decides or if so requested by any supervisory authority concerned or the Commission. The Board should also be empowered to adopt legally binding decisions where there are disputes between supervisory authorities. For that purpose, it should issue, in principle by a two-thirds majority of its members, legally binding decisions in clearly speciﬁed cases where there are conﬂicting views among supervisory authorities, in particular in the cooperation mechanism between the lead supervisory authority and supervisory authorities concerned on the merits of the case, in particular whether there is an infringement of this Regulation. (137)There may be an urgent need to act in order to protect the rights and freedoms of data subjects, in particular when the danger exists that the enforcement of a right of a data subject could be considerably impeded. A supervisory authority should therefore be able to adopt duly justiﬁed provisional measures on its territory with a speciﬁed period of validity which should not exceed three months. (138)The application of such mechanism should be a condition for the lawfulness of a measure intended to produce legal eﬀects by a supervisory authority in those cases where its application is mandatory. In other cases of cross- border relevance, the cooperation mechanism between the lead supervisory authority and supervisory authorities concerned should be applied and mutual assistance and joint operations might be carried out between the supervisory authorities concerned on a bilateral or multilateral basis without triggering the consistency mechanism. (139)In order to promote the consistent application of this Regulation, the Board should be set up as an independent body of the Union. To fulﬁl its objectives, the Board should have legal personality. The Board should be represented by its Chair. It should replace the Working Party on the Protection of Individuals with Regard to the Processing of Personal Data established by Directive 95/46/EC. It should consist of the head of a supervisory authority of each Member State and the European Data Protection Supervisor or their respective representatives. The Commission should participate in the Board’s activities without voting rights and the European Data Protection Supervisor should have speciﬁc voting rights. TheBoardshouldcontributetotheconsistentapplicationofthisRegulation throughout the Union, including by advising the Commission, in particular on the level of protection in third countries or international organisations, and promoting cooperation of the supervisory authorities throughout the Union. The Board should act independently when performing its tasks. (140)The Board should be assisted by a secretariat provided by the European Data Protection Supervisor. The staﬀ of the European Data Protection Supervisor involved in carrying out the tasks conferred on the Board by this Regulation should perform its tasks exclusively under the instructions of, and report to, the Chair of the Board. (141)Every data subject should have the right to lodge a complaint with a single supervisory authority, in particular in the Member State of his or 37",
        "her habitual residence, and the right to an eﬀective judicial remedy in accordance with Article 47 of the Charter if the data subject considers that his or her rights under this Regulation are infringed or where the su- pervisory authority does not act on a complaint, partially or wholly rejects or dismisses a complaint or does not act where such action is necessary to protect the rights of the data subject. The investigation following a complaint should be carried out, subject to judicial review, to the extent that is appropriate in the speciﬁc case. The supervisory authority should inform the data subject of the progress and the outcome of the complaint within a reasonable period. If the case requires further investigation or coordination with another supervisory authority, intermediate information should be given to the data subject. In order to facilitate the submis- sion of complaints, each supervisory authority should take measures such as providing a complaint submission form which can also be completed electronically, without excluding other means of communication. (142)Where a data subject considers that his or her rights under this Regulation are infringed, he or she should have the right to mandate a not-for-proﬁt body, organisation or association which is constituted in accordance with the law of a Member State, has statutory objectives which are in the public interest and is active in the ﬁeld of the protection of personal data to lodge a complaint on his or her behalf with a supervisory authority, exercise the right to a judicial remedy on behalf of data subjects or, if provided for in Member State law, exercise the right to receive compensation on behalf of data subjects. A Member State may provide for such a body, organisation or association to have the right to lodge a complaint in that Member State, independently of a data subject’s mandate, and the right to an eﬀective judicial remedy where it has reasons to consider that the rights of a data subject have been infringed as a result of the processing of personal data which infringes this Regulation. That body, organisation or association may not be allowed to claim compensation on a data subject’s behalf independently of the data subject’s mandate. (143)Any natural or legal person has the right to bring an action for annulment of decisions of the Board before the Court of Justice under the conditions provided for in Article 263 TFEU. As addressees of such decisions, the supervisory authorities concerned which wish to challenge them have to bring action within two months of being notiﬁed of them, in accordance with Article 263 TFEU. Where decisions of the Board are of direct and individual concern to a controller, processor or complainant, the latter may bring an action for annulment against those decisions within two months of their publication on the website of the Board, in accordance with Article 263 TFEU. Without prejudice to this right under Article 263 TFEU, each natural or legal person should have an eﬀective judicial remedy before the competent national court against a decision of a supervisory authority which produces legal eﬀects concerning that person. Such a decision concerns in particular the exercise of investigative, corrective and authorisation powers by the supervisory authority or the dismissal or rejection of complaints. However, the right to an eﬀective judicial remedy does not encompass measures taken by supervisory authorities which are not legally binding, such as opinions issued by or advice provided by the 38",
        "supervisory authority. Proceedings against a supervisory authority should be brought before the courts of the Member State where the supervisory authority is established and should be conducted in accordance with that Member State’s procedural law. Those courts should exercise full jurisdiction, which should include jurisdiction to examine all questions of fact and law relevant to the dispute before them. Where a complaint has been rejected or dismissed by a supervisory au- thority, the complainant may bring proceedings before the courts in the same Member State. In the context of judicial remedies relating to the application of this Regulation, national courts which consider a decision on the question necessary to enable them to give judgment, may, or in the case provided for in Article 267 TFEU, must, request the Court of Justice to give a preliminary ruling on the interpretation of Union law, including this Regulation. Furthermore, where a decision of a supervisory authority implementing a decision of the Board is challenged before a national court and the validity of the decision of the Board is at issue, that national court does not have the power to declare the Board’s decision invalid but must refer the question of validity to the Court of Justice in accordance with Article 267 TFEU as interpreted by the Court of Justice, where it considers the decision invalid. However, a national court may not refer a question on the validity of the decision of the Board at the request of a natural or legal person which had the opportunity to bring an action for annulment of that decision, in particular if it was directly and individually concerned by that decision, but had not done so within the period laid down in Article 263 TFEU. (144)Where a court seized of proceedings against a decision by a supervisory authority has reason to believe that proceedings concerning the same processing, such as the same subject matter as regards processing by the same controller or processor, or the same cause of action, are brought before a competent court in another Member State, it should contact that court in order to conﬁrm the existence of such related proceedings. If related proceedings are pending before a court in another Member State, any court other than the court ﬁrst seized may stay its proceedings or may, on request of one of the parties, decline jurisdiction in favour of the court ﬁrst seized if that court has jurisdiction over the proceedings in question and its law permits the consolidation of such related proceedings. Proceedings are deemed to be related where they are so closely connected that it is expedient to hear and determine them together in order to avoid the risk of irreconcilable judgments resulting from separate proceedings. (145)For proceedings against a controller or processor, the plaintiﬀ should have the choice to bring the action before the courts of the Member States where the controller or processor has an establishment or where the data subject resides, unless the controller is a public authority of a Member State acting in the exercise of its public powers. (146)The controller or processor should compensate any damage which a person may suﬀer as a result of processing that infringes this Regulation. The controller or processor should be exempt from liability if it proves that it is not in any way responsible for the damage. The concept of damage 39",
        "should be broadly interpreted in the light of the case-law of the Court of Justice in a manner which fully reﬂects the objectives of this Regulation. This is without prejudice to any claims for damage deriving from the violation of other rules in Union or Member State law. Processing that infringes this Regulation also includes processing that infringes delegated and implementing acts adopted in accordance with this Regulation and Member State law specifying rules of this Regulation. Data subjects should receive full and eﬀective compensation for the damage they have suﬀered. Where controllers or processors are involved in the same processing, each controller or processor should be held liable for the entire damage. However, where they are joined to the same judicial proceedings, in accordance with Member State law, compensation may be apportioned according to the responsibility of each controller or processor for the damage caused by the processing, provided that full and eﬀective compensation of the data subject who suﬀered the damage is ensured. Any controller or processor which has paid full compensation may subsequently institute recourse proceedings against other controllers or processors involved in the same processing. (147)Where speciﬁc rules on jurisdiction are contained in this Regulation, in particular as regards proceedings seeking a judicial remedy including compensation, against a controller or processor, general jurisdiction rules such as those of Regulation (EU) No 1215/2012 of the European Parliament and of the Council (13) should not prejudice the application of such speciﬁc rules. (148)In order to strengthen the enforcement of the rules of this Regulation, penalties including administrative ﬁnes should be imposed for any infringe- ment of this Regulation, in addition to, or instead of appropriate measures imposed by the supervisory authority pursuant to this Regulation. In a case of a minor infringement or if the ﬁne likely to be imposed would con- stitute a disproportionate burden to a natural person, a reprimand may be issued instead of a ﬁne. Due regard should however be given to the nature, gravity and duration of the infringement, the intentional character of the infringement, actions taken to mitigate the damage suﬀered, degree of responsibility or any relevant previous infringements, the manner in which the infringement became known to the supervisory authority, compliance with measures ordered against the controller or processor, adherence to a code of conduct and any other aggravating or mitigating factor. The imposition of penalties including administrative ﬁnes should be subject to appropriate procedural safeguards in accordance with the general principles of Union law and the Charter, including eﬀective judicial protection and due process. (149)Member States should be able to lay down the rules on criminal penalties for infringements of this Regulation, including for infringements of national rules adopted pursuant to and within the limits of this Regulation. Those criminal penalties may also allow for the deprivation of the proﬁts obtained through infringements of this Regulation. However, the imposition of crim- inal penalties for infringements of such national rules and of administrative penalties should not lead to a breach of the principle of ne bis in idem, as 40",
        "interpreted by the Court of Justice. (150)In order to strengthen and harmonise administrative penalties for infringe- ments of this Regulation, each supervisory authority should have the power to impose administrative ﬁnes. This Regulation should indicate infringe- ments and the upper limit and criteria for setting the related administrative ﬁnes, which should be determined by the competent supervisory authority in each individual case, taking into account all relevant circumstances of the speciﬁc situation, with due regard in particular to the nature, gravity and duration of the infringement and of its consequences and the measures taken to ensure compliance with the obligations under this Regulation and to prevent or mitigate the consequences of the infringement. Where administrative ﬁnes are imposed on an undertaking, an undertaking should be understood to be an undertaking in accordance with Articles 101 and 102 TFEU for those purposes. Where administrative ﬁnes are imposed on persons that are not an undertaking, the supervisory authority should take account of the general level of income in the Member State as well as the economic situation of the person in considering the appropriate amount of the ﬁne. The consistency mechanism may also be used to promote a consistent application of administrative ﬁnes. It should be for the Member States to determine whether and to which extent public authorities should be subject to administrative ﬁnes. Imposing an administrative ﬁne or giving a warning does not aﬀect the application of other powers of the supervisory authorities or of other penalties under this Regulation. (151)The legal systems of Denmark and Estonia do not allow for administrative ﬁnes as set out in this Regulation. The rules on administrative ﬁnes may be applied in such a manner that in Denmark the ﬁne is imposed by competent national courts as a criminal penalty and in Estonia the ﬁne is imposed by the supervisory authority in the framework of a misdemeanour procedure, provided that such an application of the rules in those Member States has an equivalent eﬀect to administrative ﬁnes imposed by supervisory authorities. Therefore the competent national courts should take into account the recommendation by the supervisory authority initiating the ﬁne. In any event, the ﬁnes imposed should be eﬀective, proportionate and dissuasive. (152)Where this Regulation does not harmonise administrative penalties or where necessary in other cases, for example in cases of serious infringements of this Regulation, Member States should implement a system which provides for eﬀective, proportionate and dissuasive penalties. The nature of such penalties, criminal or administrative, should be determined by Member State law. (153)Member States law should reconcile the rules governing freedom of ex- pression and information, including journalistic, academic, artistic and or literary expression with the right to the protection of personal data pursuant to this Regulation. The processing of personal data solely for journalistic purposes, or for the purposes of academic, artistic or literary expression should be subject to derogations or exemptions from certain provisions of this Regulation if necessary to reconcile the right to the protection of personal data with the right to freedom of expression and 41",
        "information, as enshrined in Article 11 of the Charter. This should apply in particular to the processing of personal data in the audiovisual ﬁeld and in news archives and press libraries. Therefore, Member States should adopt legislative measures which lay down the exemptions and derogations necessary for the purpose of balancing those fundamental rights. Member States should adopt such exemptions and derogations on general princi- ples, the rights of the data subject, the controller and the processor, the transfer of personal data to third countries or international organisations, the independent supervisory authorities, cooperation and consistency, and speciﬁc data-processing situations. Where such exemptions or derogations diﬀer from one Member State to another, the law of the Member State to which the controller is subject should apply. In order to take account of the importance of the right to freedom of expression in every democratic society, it is necessary to interpret notions relating to that freedom, such as journalism, broadly. (154)This Regulation allows the principle of public access to oﬃcial documents to be taken into account when applying this Regulation. Public access to oﬃcial documents may be considered to be in the public interest. Personal data in documents held by a public authority or a public body should be able to be publicly disclosed by that authority or body if the disclosure is provided for by Union or Member State law to which the public authority or public body is subject. Such laws should reconcile public access to oﬃcial documents and the reuse of public sector information with the right to the protection of personal data and may therefore provide for the necessary reconciliation with the right to the protection of personal data pursuant to this Regulation. The reference to public authorities and bodies should in that context include all authorities or other bodies covered by Member State law on public access to documents. Directive 2003/98/EC of the European Parliament and of the Council (14) leaves intact and in no way aﬀects the level of protection of natural persons with regard to the processing of personal data under the provisions of Union and Member State law, and in particular does not alter the obligations and rights set out in this Regulation. In particular, that Directive should not apply to documents to which access is excluded or restricted by virtue of the access regimes on the grounds of protection of personal data, and parts of documents accessible by virtue of those regimes which contain personal data the re-use of which has been provided for by law as being incompatible with the law concerning the protection of natural persons with regard to the processing of personal data. (155)Member State law or collective agreements, including ‘works agreements’, may provide for speciﬁc rules on the processing of employees’ personal data in the employment context, in particular for the conditions under which personal data in the employment context may be processed on the basis of the consent of the employee, the purposes of the recruitment, the perfor- mance of the contract of employment, including discharge of obligations laid down by law or by collective agreements, management, planning and organisation of work, equality and diversity in the workplace, health and safety at work, and for the purposes of the exercise and enjoyment, on an individual or collective basis, of rights and beneﬁts related to employment, 42",
        "and for the purpose of the termination of the employment relationship. (156)Theprocessing ofpersonal datafor archiving purposesin the publicinterest, scientiﬁc or historical research purposes or statistical purposes should be subject to appropriate safeguards for the rights and freedoms of the data subject pursuant to this Regulation. Those safeguards should ensure that technical and organisational measures are in place in order to ensure, in particular, the principle of data minimisation. The further processing of personal data for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes is to be carried out when the controller has assessed the feasibility to fulﬁl those purposes by processing data which do not permit or no longer permit the identiﬁcation of data subjects, provided that appropriate safeguards exist (such as, for instance, pseudonymisation of the data). Member States should provide for appropriate safeguards for the processing of personal data for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes. Member States should be authorised to provide, under speciﬁc conditions and subject to appropriate safeguards for data subjects, speciﬁcations and derogations with regard to the information requirements and rights to rectiﬁcation, to erasure, to be forgotten, to restriction of processing, to data portability, and to object when processing personal data for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes. The conditions and safeguards in question may entail speciﬁc procedures for data subjects to exercise those rights if this is appropriate in the light of the purposes sought by the speciﬁc processing along with technical and organisational measures aimed at minimising the processing of personal data in pursuance of the proportionality and necessity principles. The processing of personal data for scientiﬁc purposes should also comply with other relevant legislation such as on clinical trials. (157)By coupling information from registries, researchers can obtain new knowl- edge of great value with regard to widespread medical conditions such as cardiovascular disease, cancer and depression. On the basis of registries, research results can be enhanced, as they draw on a larger population. Within social science, research on the basis of registries enables researchers to obtain essential knowledge about the long-term correlation of a number of social conditions such as unemployment and education with other life conditions. Research results obtained through registries provide solid, high-quality knowledge which can provide the basis for the formulation and implementation of knowledge-based policy, improve the quality of life for a number of people and improve the eﬃciency of social services. In order to facilitate scientiﬁc research, personal data can be processed for scientiﬁc research purposes, subject to appropriate conditions and safeguards set out in Union or Member State law. (158)Where personal data are processed for archiving purposes, this Regulation should also apply to that processing, bearing in mind that this Regulation should not apply to deceased persons. Public authorities or public or private bodies that hold records of public interest should be services which, pursuant to Union or Member State law, have a legal obligation 43",
        "to acquire, preserve, appraise, arrange, describe, communicate, promote, disseminate and provide access to records of enduring value for general public interest. Member States should also be authorised to provide for the further processing of personal data for archiving purposes, for example with a view to providing speciﬁc information related to the political behaviour under former totalitarian state regimes, genocide, crimes against humanity, in particular the Holocaust, or war crimes. (159)Where personal data are processed for scientiﬁc research purposes, this Regulation should also apply to that processing. For the purposes of this Regulation, the processing of personal data for scientiﬁc research purposes shouldbeinterpretedinabroadmannerincludingforexampletechnological development and demonstration, fundamental research, applied research and privately funded research. In addition, it should take into account the Union’s objective under Article 179(1) TFEU of achieving a European Research Area. Scientiﬁc research purposes should also include studies conducted in the public interest in the area of public health. To meet the speciﬁcities of processing personal data for scientiﬁc research purposes, speciﬁc conditions should apply in particular as regards the publication or otherwise disclosure of personal data in the context of scientiﬁc research purposes. If the result of scientiﬁc research in particular in the health context gives reason for further measures in the interest of the data subject, the general rules of this Regulation should apply in view of those measures. (160)Where personal data are processed for historical research purposes, this Regulation should also apply to that processing. This should also include historical research and research for genealogical purposes, bearing in mind that this Regulation should not apply to deceased persons. (161)For the purpose of consenting to the participation in scientiﬁc research activities in clinical trials, the relevant provisions of Regulation (EU) No 536/2014 of the European Parliament and of the Council (15) should apply. (162)Where personal data are processed for statistical purposes, this Regulation should apply to that processing. Union or Member State law should, within the limits of this Regulation, determine statistical content, control of access, speciﬁcations for the processing of personal data for statistical purposes and appropriate measures to safeguard the rights and freedoms of the data subject and for ensuring statistical conﬁdentiality. Statistical purposes mean any operation of collection and the processing of personal data necessary for statistical surveys or for the production of statistical results. Those statistical results may further be used for diﬀerent purposes, including a scientiﬁc research purpose. The statistical purpose implies that the result of processing for statistical purposes is not personal data, but aggregate data, and that this result or the personal data are not used in support of measures or decisions regarding any particular natural person. (163)The conﬁdential information which the Union and national statistical authorities collect for the production of oﬃcial European and oﬃcial national statistics should be protected. European statistics should be developed, produced and disseminated in accordance with the statistical principles as set out in Article 338(2) TFEU, while national statistics 44",
        "should also comply with Member State law. Regulation (EC) No 223/2009 of the European Parliament and of the Council (16) provides further speciﬁcations on statistical conﬁdentiality for European statistics. (164)As regards the powers of the supervisory authorities to obtain from the controller or processor access to personal data and access to their premises, Member States may adopt by law, within the limits of this Regulation, speciﬁc rules in order to safeguard the professional or other equivalent secrecy obligations, in so far as necessary to reconcile the right to the protection of personal data with an obligation of professional secrecy. This is without prejudice to existing Member State obligations to adopt rules on professional secrecy where required by Union law. (165)This Regulation respects and does not prejudice the status under existing constitutional law of churches and religious associations or communities in the Member States, as recognised in Article 17 TFEU. (166)In order to fulﬁl the objectives of this Regulation, namely to protect the fundamental rights and freedoms of natural persons and in particular their right to the protection of personal data and to ensure the free movement of personal data within the Union, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission. In particular, delegated acts should be adopted in respect of criteria and requirements for certiﬁcation mechanisms, information to be presented by standardised icons and procedures for providing such icons. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level. The Commission, when preparing and drawing-up delegated acts, should ensure a simultaneous, timely and appropriatetransmissionofrelevantdocumentstotheEuropeanParliament and to the Council. (167)In order to ensure uniform conditions for the implementation of this Regulation, implementing powers should be conferred on the Commission when provided for by this Regulation. Those powers should be exercised in accordance with Regulation (EU) No 182/2011. In that context, the Commission should consider speciﬁc measures for micro, small and medium- sized enterprises. (168)Theexaminationprocedureshouldbeusedfortheadoptionofimplementing acts on standard contractual clauses between controllers and processors and between processors; codes of conduct; technical standards and mechanisms for certiﬁcation; the adequate level of protection aﬀorded by a third country, aterritoryoraspeciﬁedsectorwithinthatthirdcountry, oraninternational organisation; standard protection clauses; formats and procedures for the exchange of information by electronic means between controllers, processors and supervisory authorities for binding corporate rules; mutual assistance; and arrangements for the exchange of information by electronic means between supervisory authorities, and between supervisory authorities and the Board. (169)The Commission should adopt immediately applicable implementing acts where available evidence reveals that a third country, a territory or a speciﬁed sector within that third country, or an international organisation 45",
        "does not ensure an adequate level of protection, and imperative grounds of urgency so require. (170)Since the objective of this Regulation, namely to ensure an equivalent level of protection of natural persons and the free ﬂow of personal data throughout the Union, cannot be suﬃciently achieved by the Member States and can rather, by reason of the scale or eﬀects of the action, be better achieved at Union level, the Union may adopt measures, in accordance with the principle of subsidiarity as set out in Article 5 of the Treaty on European Union (TEU). In accordance with the principle of proportionality as set out in that Article, this Regulation does not go beyond what is necessary in order to achieve that objective. (171)Directive 95/46/EC should be repealed by this Regulation. Processing already under way on the date of application of this Regulation should be brought into conformity with this Regulation within the period of two years after which this Regulation enters into force. Where processing is based on consent pursuant to Directive 95/46/EC, it is not necessary for the data subject to give his or her consent again if the manner in which the consent has been given is in line with the conditions of this Regulation, so as to allow the controller to continue such processing after the date of application of this Regulation. Commission decisions adopted and authorisations by supervisory authorities based on Directive 95/46/EC remain in force until amended, replaced or repealed. (172)The European Data Protection Supervisor was consulted in accordance with Article 28(2) of Regulation (EC) No 45/2001 and delivered an opinion on 7 March 2012 (17). (173)This Regulation should apply to all matters concerning the protection of fundamental rights and freedoms vis-à-vis the processing of personal data which are not subject to speciﬁc obligations with the same objective set out in Directive 2002/58/EC of the European Parliament and of the Council (18), including the obligations on the controller and the rights of natural persons. In order to clarify the relationship between this Regulation and Directive 2002/58/EC, that Directive should be amended accordingly. Once this Regulation is adopted, Directive 2002/58/EC should be reviewed in particular in order to ensure consistency with this Regulation, HAVE ADOPTED THIS REGULATION: CHAPTER I General provisions Article 1 Subject-matter and objectives 1.This Regulation lays down rules relating to the protection of natural persons with regard to the processing of personal data and rules relating to the free movement of personal data. 2.This Regulation protects fundamental rights and freedoms of natural persons and in particular their right to the protection of personal data. 3.The free movement of personal data within the Union shall be neither restricted nor prohibited for reasons connected with the protection of 46",
        "natural persons with regard to the processing of personal data. Article 2 Material scope 1.This Regulation applies to the processing of personal data wholly or partly by automated means and to the processing other than by automated means of personal data which form part of a ﬁling system or are intended to form part of a ﬁling system. 2. This Regulation does not apply to the processing of personal data: (a)in the course of an activity which falls outside the scope of Union law; (b)by the Member States when carrying out activities which fall within the scope of Chapter 2 of Title V of the TEU; (c)by a natural person in the course of a purely personal or household activity; (d)by competent authorities for the purposes of the prevention, investi- gation, detection or prosecution of criminal oﬀences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security. 3.For the processing of personal data by the Union institutions, bodies, oﬃces and agencies, Regulation (EC) No 45/2001 applies. Regulation (EC) No 45/2001 and other Union legal acts applicable to such processing of personal data shall be adapted to the principles and rules of this Regulation in accordance with Article 98. 4.This Regulation shall be without prejudice to the application of Directive 2000/31/EC, in particular of the liability rules of intermediary service providers in Articles 12 to 15 of that Directive. Article 3 Territorial scope 1.This Regulation applies to the processing of personal data in the context of the activities of an establishment of a controller or a processor in the Union, regardless of whether the processing takes place in the Union or not. 2.This Regulation applies to the processing of personal data of data subjects who are in the Union by a controller or processor not established in the Union, where the processing activities are related to: (a)the oﬀering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union; or (b)the monitoring of their behaviour as far as their behaviour takes place within the Union. 3.This Regulation applies to the processing of personal data by a controller not established in the Union, but in a place where Member State law applies by virtue of public international law. Article 4 Deﬁnitions For the purposes of this Regulation: 47",
        "1.‘personal data’ means any information relating to an identiﬁed or identiﬁ- able natural person (‘data subject’); an identiﬁable natural person is one who can be identiﬁed, directly or indirectly, in particular by reference to an identiﬁer such as a name, an identiﬁcation number, location data, an online identiﬁer or to one or more factors speciﬁc to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person; 2.‘processing’ means any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by auto- mated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction; 3.‘restriction of processing’ means the marking of stored personal data with the aim of limiting their processing in the future; 4.‘proﬁling’ means any form of automated processing of personal data con- sisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person’s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements; 5.‘pseudonymisation’ means the processing of personal data in such a manner that the personal data can no longer be attributed to a speciﬁc data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identiﬁed or identiﬁable natural person; 6.‘ﬁling system’ means any structured set of personal data which are acces- sible according to speciﬁc criteria, whether centralised, decentralised or dispersed on a functional or geographical basis; 7.‘controller’ means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the speciﬁc criteria for its nomination may be provided for by Union or Member State law; 8.‘processor’ means a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller; 9.‘recipient’ means a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing; 10.‘third party’ means a natural or legal person, public authority, agency or 48",
        "body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data; 11.‘consent’ of the data subject means any freely given, speciﬁc, informed and unambiguous indication of the data subject’s wishes by which he or she, by a statement or by a clear aﬃrmative action, signiﬁes agreement to the processing of personal data relating to him or her; 12.‘personal data breach’ means a breach of security leading to the accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to, personal data transmitted, stored or otherwise processed; 13.‘genetic data’ means personal data relating to the inherited or acquired genetic characteristics of a natural person which give unique information about the physiology or the health of that natural person and which result, in particular, from an analysis of a biological sample from the natural person in question; 14.‘biometric data’ means personal data resulting from speciﬁc technical pro- cessing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or conﬁrm the unique identiﬁcation of that natural person, such as facial images or dactyloscopic data; 15.‘data concerning health’ means personal data related to the physical or mental health of a natural person, including the provision of health care services, which reveal information about his or her health status; 16. ‘main establishment’ means: a.as regards a controller with establishments in more than one Member State, the place of its central administration in the Union, unless the decisions on the purposes and means of the processing of personal data are taken in another establishment of the controller in the Union and the latter establishment has the power to have such decisions implemented, in which case the establishment having taken such decisions is to be considered to be the main establishment; b.as regards a processor with establishments in more than one Member State, the place of its central administration in the Union, or, if the processor has no central administration in the Union, the estab- lishment of the processor in the Union where the main processing activities in the context of the activities of an establishment of the processor take place to the extent that the processor is subject to speciﬁc obligations under this Regulation; 17.‘representative’ means a natural or legal person established in the Union who, designated by the controller or processor in writing pursuant to Article 27, represents the controller or processor with regard to their respective obligations under this Regulation; 18.‘enterprise’ means a natural or legal person engaged in an economic activity, irrespectiveofitslegalform, includingpartnershipsorassociationsregularly engaged in an economic activity; 49",
        "19.‘group of undertakings’ means a controlling undertaking and its controlled undertakings; 20.‘binding corporate rules’ means personal data protection policies which are adhered to by a controller or processor established on the territory of a Member State for transfers or a set of transfers of personal data to a controller or processor in one or more third countries within a group of undertakings, or group of enterprises engaged in a joint economic activity; 21.‘supervisory authority’ means an independent public authority which is established by a Member State pursuant to Article 51; 22.‘supervisory authority concerned’ means a supervisory authority which is concerned by the processing of personal data because: a.thecontrollerorprocessorisestablishedontheterritoryoftheMember State of that supervisory authority; b.data subjects residing in the Member State of that supervisory au- thority are substantially aﬀected or likely to be substantially aﬀected by the processing; or c. a complaint has been lodged with that supervisory authority; 23. ‘cross-border processing’ means either: a.processing of personal data which takes place in the context of the activities of establishments in more than one Member State of a controller or processor in the Union where the controller or processor is established in more than one Member State; or b.processing of personal data which takes place in the context of the activities of a single establishment of a controller or processor in the Union but which substantially aﬀects or is likely to substantially aﬀect data subjects in more than one Member State. 24.‘relevant and reasoned objection’ means an objection to a draft decision as to whether there is an infringement of this Regulation, or whether envisaged action in relation to the controller or processor complies with this Regulation, which clearly demonstrates the signiﬁcance of the risks posed by the draft decision as regards the fundamental rights and freedoms of data subjects and, where applicable, the free ﬂow of personal data within the Union; 25.‘information society service’ means a service as deﬁned in point (b) of Article 1(1) of Directive (EU) 2015/1535 of the European Parliament and of the Council (19); 26.‘international organisation’ means an organisation and its subordinate bodies governed by public international law, or any other body which is set up by, or on the basis of, an agreement between two or more countries. CHAPTER II Principles Article 5 Principles relating to processing of personal data 50",
        "1. Personal data shall be: (a)processed lawfully, fairly and in a transparent manner in relation to the data subject (‘lawfulness, fairness and transparency’); (b)collected for speciﬁed, explicit and legitimate purposes and not fur- ther processed in a manner that is incompatible with those purposes; further processing for archiving purposes in the public interest, sci- entiﬁc or historical research purposes or statistical purposes shall, in accordance with Article 89(1), not be considered to be incompatible with the initial purposes (‘purpose limitation’); (c)adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’); (d)accurate and, where necessary, kept up to date; every reasonable step must be taken to ensure that personal data that are inaccurate, having regard to the purposes for which they are processed, are erased or rectiﬁed without delay (‘accuracy’); (e)kept in a form which permits identiﬁcation of data subjects for no longer than is necessary for the purposes for which the personal data areprocessed; personaldatamaybestoredforlongerperiodsinsofaras the personal data will be processed solely for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes in accordance with Article 89(1) subject to implementation of the appropriate technical and organisational measures required by this Regulation in order to safeguard the rights and freedoms of the data subject (‘storage limitation’); (f)processedinamannerthatensuresappropriatesecurityofthepersonal data, including protection against unauthorised or unlawful processing and against accidental loss, destruction or damage, using appropriate technical or organisational measures (‘integrity and conﬁdentiality’). 2.The controller shall be responsible for, and be able to demonstrate compli- ance with, paragraph 1 (‘accountability’). Article 6 Lawfulness of processing 1.Processing shall be lawful only if and to the extent that at least one of the following applies: (a)the data subject has given consent to the processing of his or her personal data for one or more speciﬁc purposes; (b)processing is necessary for the performance of a contract to which the data subject is party or in order to take steps at the request of the data subject prior to entering into a contract; (c)processing is necessary for compliance with a legal obligation to which the controller is subject; (d)processing is necessary in order to protect the vital interests of the data subject or of another natural person; 51",
        "(e)processing is necessary for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller; (f)processing is necessary for the purposes of the legitimate interests pursued by the controller or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data, in particular where the data subject is a child. Point (f) of the ﬁrst subparagraph shall not apply to processing carried out by public authorities in the performance of their tasks. 2.Member States may maintain or introduce more speciﬁc provisions to adapt the application of the rules of this Regulation with regard to processing for compliance with points (c) and (e) of paragraph 1 by determining more precisely speciﬁc requirements for the processing and other measures to ensure lawful and fair processing including for other speciﬁc processing situations as provided for in Chapter IX. 3.The basis for the processing referred to in point (c) and (e) of paragraph 1 shall be laid down by: (a) Union law; or (b) Member State law to which the controller is subject. The purpose of the processing shall be determined in that legal basis or, as regards the processing referred to in point (e) of paragraph 1, shall be necessary for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller. That legal basis may contain speciﬁc provisions to adapt the application of rules of this Regulation, inter alia: the general conditions governing the lawfulness of processing by the controller; the types of data which are subject to the processing; the data subjects concerned; the entities to, and the purposes for which, the personal data may be disclosed; the purpose limitation; storage periods; and processing operations and processing procedures, including measures to ensure lawful and fair processing such as those for other speciﬁc processing situations as provided for in Chapter IX. The Union or the Member State law shall meet an objective of public interest and be proportionate to the legitimate aim pursued. 4.Where the processing for a purpose other than that for which the personal data have been collected is not based on the data subject’s consent or on a UnionorMemberStatelawwhichconstitutesanecessaryandproportionate measure in a democratic society to safeguard the objectives referred to in Article 23(1), the controller shall, in order to ascertain whether processing for another purpose is compatible with the purpose for which the personal data are initially collected, take into account, inter alia: (a)any link between the purposes for which the personal data have been collected and the purposes of the intended further processing; (b)the context in which the personal data have been collected, in par- ticular regarding the relationship between data subjects and the 52",
        "controller; (c)the nature of the personal data, in particular whether special cat- egories of personal data are processed, pursuant to Article 9, or whether personal data related to criminal convictions and oﬀences are processed, pursuant to Article 10; (d)the possible consequences of the intended further processing for data subjects; (e)the existence of appropriate safeguards, which may include encryption or pseudonymisation. Article 7 Conditions for consent 1.Where processing is based on consent, the controller shall be able to demonstrate that the data subject has consented to processing of his or her personal data. 2.If the data subject’s consent is given in the context of a written declaration whichalsoconcernsothermatters, therequestforconsentshallbepresented in a manner which is clearly distinguishable from the other matters, in an intelligible and easily accessible form, using clear and plain language. Any part of such a declaration which constitutes an infringement of this Regulation shall not be binding. 3.The data subject shall have the right to withdraw his or her consent at any time. The withdrawal of consent shall not aﬀect the lawfulness of processing based on consent before its withdrawal. Prior to giving consent, the data subject shall be informed thereof. It shall be as easy to withdraw as to give consent. 4.When assessing whether consent is freely given, utmost account shall be taken of whether, inter alia, the performance of a contract, including the provision of a service, is conditional on consent to the processing of personal data that is not necessary for the performance of that contract. Article 8 Conditions applicable to child’s consent in relation to information society services 1.Wherepoint(a)ofArticle6(1)applies, inrelationtotheoﬀerofinformation society services directly to a child, the processing of the personal data of a child shall be lawful where the child is at least 16 years old. Where the child is below the age of 16 years, such processing shall be lawful only if and to the extent that consent is given or authorised by the holder of parental responsibility over the child. Member States may provide by law for a lower age for those purposes provided that such lower age is not below 13 years. 2.The controller shall make reasonable eﬀorts to verify in such cases that consent is given or authorised by the holder of parental responsibility over the child, taking into consideration available technology. 3.Paragraph 1 shall not aﬀect the general contract law of Member States such as the rules on the validity, formation or eﬀect of a contract in relation 53",
        "to a child. Article 9 Processing of special categories of personal data 1.Processing of personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person’s sex life or sexual orientation shall be prohibited. 2. Paragraph 1 shall not apply if one of the following applies: (a)the data subject has given explicit consent to the processing of those personal data for one or more speciﬁed purposes, except where Union or Member State law provide that the prohibition referred to in paragraph 1 may not be lifted by the data subject; (b)processing is necessary for the purposes of carrying out the obligations and exercising speciﬁc rights of the controller or of the data subject in the ﬁeld of employment and social security and social protection law in so far as it is authorised by Union or Member State law or a collective agreement pursuant to Member State law providing for appropriate safeguards for the fundamental rights and the interests of the data subject; (c)processing is necessary to protect the vital interests of the data subject or of another natural person where the data subject is physically or legally incapable of giving consent; (d)processing is carried out in the course of its legitimate activities with appropriate safeguards by a foundation, association or any other not-for-proﬁt body with a political, philosophical, religious or trade union aim and on condition that the processing relates solely to the members or to former members of the body or to persons who have regular contact with it in connection with its purposes and that the personal data are not disclosed outside that body without the consent of the data subjects; (e)processing relates to personal data which are manifestly made public by the data subject; (f)processing is necessary for the establishment, exercise or defence of legal claims or whenever courts are acting in their judicial capacity; (g)processing is necessary for reasons of substantial public interest, on the basis of Union or Member State law which shall be proportionate to the aim pursued, respect the essence of the right to data protec- tion and provide for suitable and speciﬁc measures to safeguard the fundamental rights and the interests of the data subject; (h)processing is necessary for the purposes of preventive or occupational medicine, for the assessment of the working capacity of the employee, medical diagnosis, the provision of health or social care or treatment or the management of health or social care systems and services on the basis of Union or Member State law or pursuant to contract with 54",
        "a health professional and subject to the conditions and safeguards referred to in paragraph 3; (i)processing is necessary for reasons of public interest in the area of public health, such as protecting against serious cross-border threats to health or ensuring high standards of quality and safety of health care and of medicinal products or medical devices, on the basis of Union or Member State law which provides for suitable and speciﬁc measures to safeguard the rights and freedoms of the data subject, in particular professional secrecy; (j)processing is necessary for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes in accordance with Article 89(1) based on Union or Member State law which shall be proportionate to the aim pursued, respect the essence of the right to data protection and provide for suitable and speciﬁc measures to safeguard the fundamental rights and the interests of the data subject. 3.Personal data referred to in paragraph 1 may be processed for the purposes referred to in point (h) of paragraph 2 when those data are processed by or under the responsibility of a professional subject to the obligation of professional secrecy under Union or Member State law or rules established by national competent bodies or by another person also subject to an obligation of secrecy under Union or Member State law or rules established by national competent bodies. 4.Member States may maintain or introduce further conditions, including limitations, with regard to the processing of genetic data, biometric data or data concerning health. Article 10 Processing of personal data relating to criminal convictions and oﬀences Processing of personal data relating to criminal convictions and oﬀences or related security measures based on Article 6(1) shall be carried out only under the control of oﬃcial authority or when the processing is authorised by Union or Member State law providing for appropriate safeguards for the rights and freedoms of data subjects. Any comprehensive register of criminal convictions shall be kept only under the control of oﬃcial authority. Article 11 Processing which does not require identiﬁcation 1.If the purposes for which a controller processes personal data do not or do no longer require the identiﬁcation of a data subject by the controller, the controller shall not be obliged to maintain, acquire or process additional information in order to identify the data subject for the sole purpose of complying with this Regulation. 2.Where, in cases referred to in paragraph 1 of this Article, the controller is able to demonstrate that it is not in a position to identify the data subject, the controller shall inform the data subject accordingly, if possible. In such cases, Articles 15 to 20 shall not apply except where the data subject, for 55",
        "the purpose of exercising his or her rights under those articles, provides additional information enabling his or her identiﬁcation. CHAPTER III Rights of the data subject Section 1 Transparency and modalities Article 12 Transparent information, communication and modalities for the exercise of the rights of the data subject 1.The controller shall take appropriate measures to provide any information referred to in Articles 13 and 14 and any communication under Articles 15 to 22 and 34 relating to processing to the data subject in a concise, transparent, intelligible and easily accessible form, using clear and plain language, in particular for any information addressed speciﬁcally to a child. The information shall be provided in writing, or by other means, including, where appropriate, by electronic means. When requested by the data subject, the information may be provided orally, provided that the identity of the data subject is proven by other means. 2.The controller shall facilitate the exercise of data subject rights under Articles 15 to 22. In the cases referred to in Article 11(2), the controller shall not refuse to act on the request of the data subject for exercising his or her rights under Articles 15 to 22, unless the controller demonstrates that it is not in a position to identify the data subject. 3.The controller shall provide information on action taken on a request under Articles 15 to 22 to the data subject without undue delay and in any event within one month of receipt of the request. That period may be extended by two further months where necessary, taking into account the complexity and number of the requests. The controller shall inform the data subject of any such extension within one month of receipt of the request, together with the reasons for the delay. Where the data subject makes the request by electronic form means, the information shall be provided by electronic means where possible, unless otherwise requested by the data subject. 4.If the controller does not take action on the request of the data subject, the controller shall inform the data subject without delay and at the latest within one month of receipt of the request of the reasons for not taking action and on the possibility of lodging a complaint with a supervisory authority and seeking a judicial remedy. 5.Information provided under Articles 13 and 14 and any communication and any actions taken under Articles 15 to 22 and 34 shall be provided free of charge. Where requests from a data subject are manifestly unfounded or excessive, in particular because of their repetitive character, the controller may either: (a)charge a reasonable fee taking into account the administrative costs of providing the information or communication or taking the action requested; or (b) refuse to act on the request. 56",
        "The controller shall bear the burden of demonstrating the manifestly unfounded or excessive character of the request. 6.Without prejudice to Article 11, where the controller has reasonable doubts concerning the identity of the natural person making the request referred to in Articles 15 to 21, the controller may request the provision of additional information necessary to conﬁrm the identity of the data subject. 7.The information to be provided to data subjects pursuant to Articles 13 and 14 may be provided in combination with standardised icons in order to give in an easily visible, intelligible and clearly legible manner a meaningful overview of the intended processing. Where the icons are presented electronically they shall be machine-readable. 8.The Commission shall be empowered to adopt delegated acts in accordance with Article 92 for the purpose of determining the information to be presented by the icons and the procedures for providing standardised icons. Section 2 Information and access to personal data Article 13 Information to be provided where personal data are collected from the data subject 1.Where personal data relating to a data subject are collected from the data subject, the controller shall, at the time when personal data are obtained, provide the data subject with all of the following information: (a)the identity and the contact details of the controller and, where applicable, of the controller’s representative; (b) the contact details of the data protection oﬃcer, where applicable; (c)thepurposesoftheprocessingforwhichthepersonaldataareintended as well as the legal basis for the processing; (d)wheretheprocessingisbasedonpoint(f)ofArticle6(1), thelegitimate interests pursued by the controller or by a third party; (e) the recipients or categories of recipients of the personal data, if any; (f)where applicable, the fact that the controller intends to transfer personal data to a third country or international organisation and the existence or absence of an adequacy decision by the Commission, or in the case of transfers referred to in Article 46 or 47, or the second subparagraph of Article 49(1), reference to the appropriate or suitable safeguards and the means by which to obtain a copy of them or where they have been made available. 2.In addition to the information referred to in paragraph 1, the controller shall, at the time when personal data are obtained, provide the data subject with the following further information necessary to ensure fair and transparent processing: (a)the period for which the personal data will be stored, or if that is not possible, the criteria used to determine that period; 57",
        "(b)the existence of the right to request from the controller access to and rectiﬁcation or erasure of personal data or restriction of processing concerning the data subject or to object to processing as well as the right to data portability; (c)where the processing is based on point (a) of Article 6(1) or point (a) of Article 9(2), the existence of the right to withdraw consent at any time, without aﬀecting the lawfulness of processing based on consent before its withdrawal; (d) the right to lodge a complaint with a supervisory authority; (e)whether the provision of personal data is a statutory or contractual requirement, or a requirement necessary to enter into a contract, as well as whether the data subject is obliged to provide the personal data and of the possible consequences of failure to provide such data; (f)the existence of automated decision-making, including proﬁling, re- ferred to in Article 22(1) and (4) and, at least in those cases, meaning- ful information about the logic involved, as well as the signiﬁcance and the envisaged consequences of such processing for the data subject. 3.Where the controller intends to further process the personal data for a purpose other than that for which the personal data were collected, the controller shall provide the data subject prior to that further processing with information on that other purpose and with any relevant further information as referred to in paragraph 2. 4.Paragraphs 1, 2 and 3 shall not apply where and insofar as the data subject already has the information. Article 14 Information to be provided where personal data have not been obtained from the data subject 1.Where personal data have not been obtained from the data subject, the controller shall provide the data subject with the following information: (a)the identity and the contact details of the controller and, where applicable, of the controller’s representative; (b) the contact details of the data protection oﬃcer, where applicable; (c)thepurposesoftheprocessingforwhichthepersonaldataareintended as well as the legal basis for the processing; (d) the categories of personal data concerned; (e) the recipients or categories of recipients of the personal data, if any; (f)where applicable, that the controller intends to transfer personal data to a recipient in a third country or international organisation and the existence or absence of an adequacy decision by the Commission, or in the case of transfers referred to in Article 46 or 47, or the second subparagraph of Article 49(1), reference to the appropriate or suitable safeguards and the means to obtain a copy of them or where they have been made available. 58",
        "2.In addition to the information referred to in paragraph 1, the controller shall provide the data subject with the following information necessary to ensure fair and transparent processing in respect of the data subject: (a)the period for which the personal data will be stored, or if that is not possible, the criteria used to determine that period; (b)wheretheprocessingisbasedonpoint(f)ofArticle6(1), thelegitimate interests pursued by the controller or by a third party; (c)the existence of the right to request from the controller access to and rectiﬁcation or erasure of personal data or restriction of processing concerning the data subject and to object to processing as well as the right to data portability; (d)where processing is based on point (a) of Article 6(1) or point (a) of Article 9(2), the existence of the right to withdraw consent at any time, without aﬀecting the lawfulness of processing based on consent before its withdrawal; (e) the right to lodge a complaint with a supervisory authority; (f)from which source the personal data originate, and if applicable, whether it came from publicly accessible sources; (g)the existence of automated decision-making, including proﬁling, re- ferred to in Article 22(1) and (4) and, at least in those cases, meaning- ful information about the logic involved, as well as the signiﬁcance and the envisaged consequences of such processing for the data subject. 3.The controller shall provide the information referred to in paragraphs 1 and 2: (a)withinareasonableperiodafterobtainingthepersonaldata, butatthe latest within one month, having regard to the speciﬁc circumstances in which the personal data are processed; (b)if the personal data are to be used for communication with the data subject, at the latest at the time of the ﬁrst communication to that data subject; or (c)if a disclosure to another recipient is envisaged, at the latest when the personal data are ﬁrst disclosed. 4.Where the controller intends to further process the personal data for a purpose other than that for which the personal data were obtained, the controller shall provide the data subject prior to that further processing with information on that other purpose and with any relevant further information as referred to in paragraph 2. 5. Paragraphs 1 to 4 shall not apply where and insofar as: (a) the data subject already has the information; (b)the provision of such information proves impossible or would involve a disproportionate eﬀort, in particular for processing for archiving 59",
        "purposes in the public interest, scientiﬁc or historical research pur- poses or statistical purposes, subject to the conditions and safeguards referred to in Article 89(1) or in so far as the obligation referred to in paragraph 1 of this Article is likely to render impossible or seriously impair the achievement of the objectives of that processing. In such cases the controller shall take appropriate measures to protect the data subject’s rights and freedoms and legitimate interests, including making the information publicly available; (c)obtaining or disclosure is expressly laid down by Union or Member State law to which the controller is subject and which provides ap- propriate measures to protect the data subject’s legitimate interests; or (d)where the personal data must remain conﬁdential subject to an obli- gation of professional secrecy regulated by Union or Member State law, including a statutory obligation of secrecy. Article 15 Right of access by the data subject 1.The data subject shall have the right to obtain from the controller con- ﬁrmation as to whether or not personal data concerning him or her are being processed, and, where that is the case, access to the personal data and the following information: (a) the purposes of the processing; (b) the categories of personal data concerned; (c)therecipientsorcategoriesofrecipienttowhomthepersonaldatahave been or will be disclosed, in particular recipients in third countries or international organisations; (d)where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; (e)the existence of the right to request from the controller rectiﬁcation or erasure of personal data or restriction of processing of personal data concerning the data subject or to object to such processing; (f) the right to lodge a complaint with a supervisory authority; (g)where the personal data are not collected from the data subject, any available information as to their source; (h)the existence of automated decision-making, including proﬁling, re- ferred to in Article 22(1) and (4) and, at least in those cases, meaning- ful information about the logic involved, as well as the signiﬁcance and the envisaged consequences of such processing for the data subject. 2.Wherepersonaldataaretransferredtoathirdcountryortoaninternational organisation, the data subject shall have the right to be informed of the appropriate safeguards pursuant to Article 46 relating to the transfer. 3.The controller shall provide a copy of the personal data undergoing process- ing. For any further copies requested by the data subject, the controller 60",
        "may charge a reasonable fee based on administrative costs. Where the data subject makes the request by electronic means, and unless otherwise requested by the data subject, the information shall be provided in a commonly used electronic form. 4.The right to obtain a copy referred to in paragraph 3 shall not adversely aﬀect the rights and freedoms of others. Section 3 Rectiﬁcation and erasure Article 16 Right to rectiﬁcation The data subject shall have the right to obtain from the controller without undue delay the rectiﬁcation of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement. Article 17 Right to erasure (‘right to be forgotten’) 1.The data subject shall have the right to obtain from the controller the erasure of personal data concerning him or her without undue delay and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies: (a)the personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed; (b)the data subject withdraws consent on which the processing is based according to point (a) of Article 6(1), or point (a) of Article 9(2), and where there is no other legal ground for the processing; (c)the data subject objects to the processing pursuant to Article 21(1) and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2); (d) the personal data have been unlawfully processed; (e)the personal data have to be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject; (f)the personal data have been collected in relation to the oﬀer of information society services referred to in Article 8(1). 2.Where the controller has made the personal data public and is obliged pursuant to paragraph 1 to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform controllers which are processing the personal data that the data subject has requested the erasure by such controllers of any links to, or copy or replication of, those personal data. 3.Paragraphs 1 and 2 shall not apply to the extent that processing is neces- sary: 61",
        "(a) for exercising the right of freedom of expression and information; (b)for compliance with a legal obligation which requires processing by Union or Member State law to which the controller is subject or for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller; (c)for reasons of public interest in the area of public health in accordance with points (h) and (i) of Article 9(2) as well as Article 9(3); (d)for archiving purposes in the public interest, scientiﬁc or historical research purposes or statistical purposes in accordance with Article 89(1) in so far as the right referred to in paragraph 1 is likely to render impossible or seriously impair the achievement of the objectives of that processing; or (e) for the establishment, exercise or defence of legal claims. Article 18 Right to restriction of processing 1.Thedatasubjectshallhavetherighttoobtainfromthecontrollerrestriction of processing where one of the following applies: (a)the accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data; (b)the processing is unlawful and the data subject opposes the erasure of the personal data and requests the restriction of their use instead; (c)the controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims; (d)the data subject has objected to processing pursuant to Article 21(1) pending the veriﬁcation whether the legitimate grounds of the con- troller override those of the data subject. 2.Where processing has been restricted under paragraph 1, such personal data shall, with the exception of storage, only be processed with the data subject’s consent or for the establishment, exercise or defence of legal claims or for the protection of the rights of another natural or legal person or for reasons of important public interest of the Union or of a Member State. 3.A data subject who has obtained restriction of processing pursuant to paragraph 1 shall be informed by the controller before the restriction of processing is lifted. Article 19 Notiﬁcation obligation regarding rectiﬁcation or erasure of personal data or restriction of processing The controller shall communicate any rectiﬁcation or erasure of personal data or restriction of processing carried out in accordance with Article 16, Article 17(1) and Article 18 to each recipient to whom the personal data have been disclosed, unless this proves impossible or involves disproportionate eﬀort. The controller 62",
        "shall inform the data subject about those recipients if the data subject requests it. Article 20 Right to data portability 1.Thedatasubjectshallhavetherighttoreceivethepersonaldataconcerning him or her, which he or she has provided to a controller, in a structured, commonlyusedandmachine-readableformatandhavetherighttotransmit those data to another controller without hindrance from the controller to which the personal data have been provided, where: (a)the processing is based on consent pursuant to point (a) of Article 6(1) or point (a) of Article 9(2) or on a contract pursuant to point (b) of Article 6(1); and (b) the processing is carried out by automated means. 2.In exercising his or her right to data portability pursuant to paragraph 1, the data subject shall have the right to have the personal data transmitted directly from one controller to another, where technically feasible. 3.The exercise of the right referred to in paragraph 1 of this Article shall be without prejudice to Article 17. That right shall not apply to processing necessary for the performance of a task carried out in the public interest or in the exercise of oﬃcial authority vested in the controller. 4.The right referred to in paragraph 1 shall not adversely aﬀect the rights and freedoms of others. Section 4 Right to object and automated individual decision-making Article 21 Right to object 1.The data subject shall have the right to object, on grounds relating to his or her particular situation, at any time to processing of personal data concerning him or her which is based on point (e) or (f) of Article 6(1), including proﬁling based on those provisions. The controller shall no longer process the personal data unless the controller demonstrates compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject or for the establishment, exercise or defence of legal claims. 2.Where personal data are processed for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing, which includes proﬁling to the extent that it is related to such direct marketing. 3.Where the data subject objects to processing for direct marketing purposes, the personal data shall no longer be processed for such purposes. 4.At the latest at the time of the ﬁrst communication with the data subject, the right referred to in paragraphs 1 and 2 shall be explicitly brought to the attention of the data subject and shall be presented clearly and separately from any other information. 63",
        "5.In the context of the use of information society services, and notwithstand- ing Directive 2002/58/EC, the data subject may exercise his or her right to object by automated means using technical speciﬁcations. 6.Where personal data are processed for scientiﬁc or historical research purposes or statistical purposes pursuant to Article 89(1), the data subject, on grounds relating to his or her particular situation, shall have the right to object to processing of personal data concerning him or her, unless the processing is necessary for the performance of a task carried out for reasons of public interest. Article 22 Automated individual decision-making, including proﬁling 1.The data subject shall have the right not to be subject to a decision based solely on automated processing, including proﬁling, which produces legal eﬀects concerning him or her or similarly signiﬁcantly aﬀects him or her. 2. Paragraph 1 shall not apply if the decision: (a)is necessary for entering into, or performance of, a contract between the data subject and a data controller; (b)is authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject’s rights and freedoms and legitimate interests; or (c) is based on the data subject’s explicit consent. 3.In the cases referred to in points (a) and (c) of paragraph 2, the data controller shall implement suitable measures to safeguard the data subject’s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and to contest the decision. 4.Decisions referred to in paragraph 2 shall not be based on special categories of personal data referred to in Article 9(1), unless point (a) or (g) of Article 9(2) applies and suitable measures to safeguard the data subject’s rights and freedoms and legitimate interests are in place. Section 5 Restrictions Article 23 Restrictions 1.Union or Member State law to which the data controller or processor is subject may restrict by way of a legislative measure the scope of the obligations and rights provided for in Articles 12 to 22 and Article 34, as well as Article 5 in so far as its provisions correspond to the rights and obligations provided for in Articles 12 to 22, when such a restriction respects the essence of the fundamental rights and freedoms and is a necessary and proportionate measure in a democratic society to safeguard: (a) national security; (b) defence; (c) public security; 64",
        "(d)the prevention, investigation, detection or prosecution of criminal oﬀences or the execution of criminal penalties, including the safe- guarding against and the prevention of threats to public security; (e)other important objectives of general public interest of the Union or of a Member State, in particular an important economic or ﬁnancial interest of the Union or of a Member State, including monetary, budgetary and taxation a matters, public health and social security; (f) the protection of judicial independence and judicial proceedings; (g)the prevention, investigation, detection and prosecution of breaches of ethics for regulated professions; (h)a monitoring, inspection or regulatory function connected, even occa- sionally, to the exercise of oﬃcial authority in the cases referred to in points (a) to (e) and (g); (i)the protection of the data subject or the rights and freedoms of others; (j) the enforcement of civil law claims. 2.In particular, any legislative measure referred to in paragraph 1 shall contain speciﬁc provisions at least, where relevant, as to: (a) the purposes of the processing or categories of processing; (b) the categories of personal data; (c) the scope of the restrictions introduced; (d) the safeguards to prevent abuse or unlawful access or transfer; (e) the speciﬁcation of the controller or categories of controllers; (f)the storage periods and the applicable safeguards taking into account the nature, scope and purposes of the processing or categories of processing; (g) the risks to the rights and freedoms of data subjects; and (h)the right of data subjects to be informed about the restriction, unless that may be prejudicial to the purpose of the restriction. CHAPTER IV Controller and processor Section 1 General obligations Article 24 Responsibility of the controller 1.Takingintoaccountthenature, scope, contextandpurposesofprocessingas wellastherisksofvaryinglikelihoodandseverityfortherightsandfreedoms of natural persons, the controller shall implement appropriate technical and organisational measures to ensure and to be able to demonstrate that processing is performed in accordance with this Regulation. Those measures shall be reviewed and updated where necessary. 65",
        "2.Where proportionate in relation to processing activities, the measures referred to in paragraph 1 shall include the implementation of appropriate data protection policies by the controller. 3.Adherence to approved codes of conduct as referred to in Article 40 or approved certiﬁcation mechanisms as referred to in Article 42 may be used as an element by which to demonstrate compliance with the obligations of the controller. Article 25 Data protection by design and by default 1.Taking into account the state of the art, the cost of implementation and the nature, scope, context and purposes of processing as well as the risks of varying likelihood and severity for rights and freedoms of natural persons posed by the processing, the controller shall, both at the time of the determination of the means for processing and at the time of the processing itself, implement appropriate technical and organisational measures, such as pseudonymisation, which are designed to implement data- protection principles, such as data minimisation, in an eﬀective manner and to integrate the necessary safeguards into the processing in order to meet the requirements of this Regulation and protect the rights of data subjects. 2.The controller shall implement appropriate technical and organisational measures for ensuring that, by default, only personal data which are necessary for each speciﬁc purpose of the processing are processed. That obligation applies to the amount of personal data collected, the extent of their processing, the period of their storage and their accessibility. In particular, such measures shall ensure that by default personal data are not made accessible without the individual’s intervention to an indeﬁnite number of natural persons. 3.An approved certiﬁcation mechanism pursuant to Article 42 may be used as an element to demonstrate compliance with the requirements set out in paragraphs 1 and 2 of this Article. Article 26 Joint controllers 1.Where two or more controllers jointly determine the purposes and means of processing, they shall be joint controllers. They shall in a transparent manner determine their respective responsibilities for compliance with the obligations under this Regulation, in particular as regards the exercising of the rights of the data subject and their respective duties to provide the information referred to in Articles 13 and 14, by means of an arrangement between them unless, and in so far as, the respective responsibilities of the controllers are determined by Union or Member State law to which the controllers are subject. The arrangement may designate a contact point for data subjects. 2.The arrangement referred to in paragraph 1 shall duly reﬂect the respective roles and relationships of the joint controllers vis-à-vis the data subjects. The essence of the arrangement shall be made available to the data subject. 3.Irrespective of the terms of the arrangement referred to in paragraph 1, 66",
        "the data subject may exercise his or her rights under this Regulation in respect of and against each of the controllers. Article 27 Representatives of controllers or processors not established in the Union 1.Where Article 3(2) applies, the controller or the processor shall designate in writing a representative in the Union. 2.The obligation laid down in paragraph 1 of this Article shall not apply to: (a)processing which is occasional, does not include, on a large scale, processing of special categories of data as referred to in Article 9(1) or processing of personal data relating to criminal convictions and oﬀences referred to in Article 10, and is unlikely to result in a risk to the rights and freedoms of natural persons, taking into account the nature, context, scope and purposes of the processing; or (b) a public authority or body. 3.The representative shall be established in one of the Member States where the data subjects, whose personal data are processed in relation to the oﬀering of goods or services to them, or whose behaviour is monitored, are. 4.The representative shall be mandated by the controller or processor to be addressed in addition to or instead of the controller or the processor by, in particular, supervisory authorities and data subjects, on all issues related to processing, for the purposes of ensuring compliance with this Regulation. 5.The designation of a representative by the controller or processor shall be without prejudice to legal actions which could be initiated against the controller or the processor themselves. Article 28 Processor 1.Where processing is to be carried out on behalf of a controller, the controller shall use only processors providing suﬃcient guarantees to implement appropriate technical and organisational measures in such a manner that processing will meet the requirements of this Regulation and ensure the protection of the rights of the data subject. 2.The processor shall not engage another processor without prior speciﬁc or generalwrittenauthorisationofthecontroller. Inthecaseofgeneralwritten authorisation, the processor shall inform the controller of any intended changes concerning the addition or replacement of other processors, thereby giving the controller the opportunity to object to such changes. 3.Processing by a processor shall be governed by a contract or other legal act under Union or Member State law, that is binding on the processor with regard to the controller and that sets out the subject-matter and duration of the processing, the nature and purpose of the processing, the type of personal data and categories of data subjects and the obligations and rights of the controller. That contract or other legal act shall stipulate, in particular, that the processor: 67",
        "(a)processes the personal data only on documented instructions from the controller, including with regard to transfers of personal data to a third country or an international organisation, unless required to do so by Union or Member State law to which the processor is subject; in such a case, the processor shall inform the controller of that legal requirement before processing, unless that law prohibits such information on important grounds of public interest; (b)ensures that persons authorised to process the personal data have committed themselves to conﬁdentiality or are under an appropriate statutory obligation of conﬁdentiality; (c) takes all measures required pursuant to Article 32; (d)respects the conditions referred to in paragraphs 2 and 4 for engaging another processor; (e)taking into account the nature of the processing, assists the controller by appropriate technical and organisational measures, insofar as this is possible, for the fulﬁlment of the controller’s obligation to respond to requests for exercising the data subject’s rights laid down in Chapter III; (f)assists the controller in ensuring compliance with the obligations pursuant to Articles 32 to 36 taking into account the nature of processing and the information available to the processor; (g)at the choice of the controller, deletes or returns all the personal data to the controller after the end of the provision of services relating to processing, and deletes existing copies unless Union or Member State law requires storage of the personal data; (h)makes available to the controller all information necessary to demon- strate compliance with the obligations laid down in this Article and allow for and contribute to audits, including inspections, conducted by the controller or another auditor mandated by the controller. With regard to point (h) of the ﬁrst subparagraph, the processor shall immediately inform the controller if, in its opinion, an instruction infringes this Regulation or other Union or Member State data protection provisions. 4.Where a processor engages another processor for carrying out speciﬁc processing activities on behalf of the controller, the same data protection obligations as set out in the contract or other legal act between the controller and the processor as referred to in paragraph 3 shall be imposed on that other processor by way of a contract or other legal act under Union or Member State law, in particular providing suﬃcient guarantees to implement appropriate technical and organisational measures in such a manner that the processing will meet the requirements of this Regulation. Where that other processor fails to fulﬁl its data protection obligations, the initial processor shall remain fully liable to the controller for the performance of that other processor’s obligations. 5.Adherence of a processor to an approved code of conduct as referred to in Article 40 or an approved certiﬁcation mechanism as referred to in 68",
        "Article 42 may be used as an element by which to demonstrate suﬃcient guarantees as referred to in paragraphs 1 and 4 of this Article. 6.Without prejudice to an individual contract between the controller and the processor, the contract or the other legal act referred to in paragraphs 3 and 4 of this Article may be based, in whole or in part, on standard contractual clausesreferred toin paragraphs7 and8 ofthis Article, including whenthey are part of a certiﬁcation granted to the controller or processor pursuant to Articles 42 and 43. 7.The Commission may lay down standard contractual clauses for the matters referred to in paragraph 3 and 4 of this Article and in accordance with the examination procedure referred to in Article 93(2). 8.A supervisory authority may adopt standard contractual clauses for the matters referred to in paragraph 3 and 4 of this Article and in accordance with the consistency mechanism referred to in Article 63. 9.The contract or the other legal act referred to in paragraphs 3 and 4 shall be in writing, including in electronic form. 10.Without prejudice to Articles 82, 83 and 84, if a processor infringes this Regulation by determining the purposes and means of processing, the processor shall be considered to be a controller in respect of that processing. Article 29 Processing under the authority of the controller or processor The processor and any person acting under the authority of the controller or of the processor, who has access to personal data, shall not process those data except on instructions from the controller, unless required to do so by Union or Member State law. Article 30 Records of processing activities 1.Each controller and, where applicable, the controller’s representative, shall maintain a record of processing activities under its responsibility. That record shall contain all of the following information: (a)the name and contact details of the controller and, where applica- ble, the joint controller, the controller’s representative and the data protection oﬃcer; (b) the purposes of the processing; (c)a description of the categories of data subjects and of the categories of personal data; (d)the categories of recipients to whom the personal data have been or will be disclosed including recipients in third countries or international organisations; (e)where applicable, transfers of personal data to a third country or an international organisation, including the identiﬁcation of that third country or international organisation and, in the case of transfers referred to in the second subparagraph of Article 49(1), the documen- tation of suitable safeguards; 69",
        "(f)where possible, the envisaged time limits for erasure of the diﬀerent categories of data; (g)where possible, a general description of the technical and organisa- tional security measures referred to in Article 32(1). 2.Each processor and, where applicable, the processor’s representative shall maintain a record of all categories of processing activities carried out on behalf of a controller, containing: (a)the name and contact details of the processor or processors and of each controller on behalf of which the processor is acting, and, where applicable, of the controller’s or the processor’s representative, and the data protection oﬃcer; (b) the categories of processing carried out on behalf of each controller; (c)where applicable, transfers of personal data to a third country or an international organisation, including the identiﬁcation of that third country or international organisation and, in the case of transfers referred to in the second subparagraph of Article 49(1), the documen- tation of suitable safeguards; (d)where possible, a general description of the technical and organisa- tional security measures referred to in Article 32(1). 3.The records referred to in paragraphs 1 and 2 shall be in writing, including in electronic form. 4.The controller or the processor and, where applicable, the controller’s or the processor’s representative, shall make the record available to the supervisory authority on request. 5.The obligations referred to in paragraphs 1 and 2 shall not apply to an enterprise or an organisation employing fewer than 250 persons unless the processing it carries out is likely to result in a risk to the rights and freedoms ofdata subjects, the processingis not occasional, or theprocessing includes special categories of data as referred to in Article 9(1) or personal data relating to criminal convictions and oﬀences referred to in Article 10. Article 31 Cooperation with the supervisory authority The controller and the processor and, where applicable, their representatives, shall cooperate, on request, with the supervisory authority in the performance of its tasks. Section 2 Security of personal data Article 32 Security of processing 1.Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, the controller and the processor shall implement appropriate tech- nical and organisational measures to ensure a level of security appropriate to the risk, including inter alia as appropriate: 70",
        "(a) the pseudonymisation and encryption of personal data; (b)the ability to ensure the ongoing conﬁdentiality, integrity, availability and resilience of processing systems and services; (c)the ability to restore the availability and access to personal data in a timely manner in the event of a physical or technical incident; (d)a process for regularly testing, assessing and evaluating the eﬀective- ness of technical and organisational measures for ensuring the security of the processing. 2.In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. 3.Adherence to an approved code of conduct as referred to in Article 40 or an approved certiﬁcation mechanism as referred to in Article 42 may be used as an element by which to demonstrate compliance with the requirements set out in paragraph 1 of this Article. 4.The controller and processor shall take steps to ensure that any natural person acting under the authority of the controller or the processor who has access to personal data does not process them except on instructions from the controller, unless he or she is required to do so by Union or Member State law. Article 33 Notiﬁcation of a personal data breach to the supervisory authority 1.In the case of a personal data breach, the controller shall without undue delay and, where feasible, not later than 72 hours after having become aware of it, notify the personal data breach to the supervisory authority competent in accordance with Article 55, unless the personal data breach is unlikely to result in a risk to the rights and freedoms of natural persons. Where the notiﬁcation to the supervisory authority is not made within 72 hours, it shall be accompanied by reasons for the delay. 2.Theprocessorshallnotifythecontrollerwithoutunduedelayafterbecoming aware of a personal data breach. 3. The notiﬁcation referred to in paragraph 1 shall at least: (a)describe the nature of the personal data breach including where possible, the categories and approximate number of data subjects concerned and the categories and approximate number of personal data records concerned; (b)communicate the name and contact details of the data protection oﬃcer or other contact point where more information can be obtained; (c) describe the likely consequences of the personal data breach; (d)describe the measures taken or proposed to be taken by the controller to address the personal data breach, including, where appropriate, measures to mitigate its possible adverse eﬀects. 71",
        "4.Where, and in so far as, it is not possible to provide the information at the same time, the information may be provided in phases without undue further delay. 5.The controller shall document any personal data breaches, comprising the facts relating to the personal data breach, its eﬀects and the remedial action taken. That documentation shall enable the supervisory authority to verify compliance with this Article. Article 34 Communication of a personal data breach to the data subject 1.When the personal data breach is likely to result in a high risk to the rights and freedoms of natural persons, the controller shall communicate the personal data breach to the data subject without undue delay. 2.The communication to the data subject referred to in paragraph 1 of this Article shall describe in clear and plain language the nature of the personal data breach and contain at least the information and measures referred to in points (b), (c) and (d) of Article 33(3). 3.The communication to the data subject referred to in paragraph 1 shall not be required if any of the following conditions are met: (a)the controller has implemented appropriate technical and organisa- tional protection measures, and those measures were applied to the personal data aﬀected by the personal data breach, in particular those that render the personal data unintelligible to any person who is not authorised to access it, such as encryption; (b)the controller has taken subsequent measures which ensure that the high risk to the rights and freedoms of data subjects referred to in paragraph 1 is no longer likely to materialise; (c)it would involve disproportionate eﬀort. In such a case, there shall instead be a public communication or similar measure whereby the data subjects are informed in an equally eﬀective manner. 4.If the controller has not already communicated the personal data breach to thedatasubject, thesupervisoryauthority, havingconsideredthelikelihood of the personal data breach resulting in a high risk, may require it to do so or may decide that any of the conditions referred to in paragraph 3 are met. Section 3 Data protection impact assessment and prior consultation Article 35 Data protection impact assessment 1.Where a type of processing in particular using new technologies, and taking into account the nature, scope, context and purposes of the processing, is likely to result in a high risk to the rights and freedoms of natural persons, the controller shall, prior to the processing, carry out an assessment of the impact of the envisaged processing operations on the protection of personal data. A single assessment may address a set of similar processing operations that present similar high risks. 72",
        "2.The controller shall seek the advice of the data protection oﬃcer, where designated, when carrying out a data protection impact assessment. 3.A data protection impact assessment referred to in paragraph 1 shall in particular be required in the case of: (a)a systematic and extensive evaluation of personal aspects relating to natural persons which is based on automated processing, including proﬁling, and on which decisions are based that produce legal eﬀects concerning the natural person or similarly signiﬁcantly aﬀect the natural person; (b)processing on a large scale of special categories of data referred to in Article 9(1), or of personal data relating to criminal convictions and oﬀences referred to in Article 10; or (c)a systematic monitoring of a publicly accessible area on a large scale. 4.The supervisory authority shall establish and make public a list of the kind of processing operations which are subject to the requirement for a data protection impact assessment pursuant to paragraph 1. The supervisory authority shall communicate those lists to the Board referred to in Article 68. 5.The supervisory authority may also establish and make public a list of the kind of processing operations for which no data protection impact assessment is required. The supervisory authority shall communicate those lists to the Board. 6.Prior to the adoption of the lists referred to in paragraphs 4 and 5, the competent supervisory authority shall apply the consistency mechanism referred to in Article 63 where such lists involve processing activities which are related to the oﬀering of goods or services to data subjects or to the monitoring of their behaviour in several Member States, or may substantially aﬀect the free movement of personal data within the Union. 7. The assessment shall contain at least: (a)asystematicdescriptionoftheenvisagedprocessingoperationsandthe purposes of the processing, including, where applicable, the legitimate interest pursued by the controller; (b)an assessment of the necessity and proportionality of the processing operations in relation to the purposes; (c)an assessment of the risks to the rights and freedoms of data subjects referred to in paragraph 1; and (d)the measures envisaged to address the risks, including safeguards, security measures and mechanisms to ensure the protection of personal data and to demonstrate compliance with this Regulation taking into account the rights and legitimate interests of data subjects and other persons concerned. 8.Compliance with approved codes of conduct referred to in Article 40 by the relevant controllers or processors shall be taken into due account 73",
        "in assessing the impact of the processing operations performed by such controllers or processors, in particular for the purposes of a data protection impact assessment. 9.Where appropriate, the controller shall seek the views of data subjects or their representatives on the intended processing, without prejudice to the protection of commercial or public interests or the security of processing operations. 10.Where processing pursuant to point (c) or (e) of Article 6(1) has a legal basis in Union law or in the law of the Member State to which the controller is subject, that law regulates the speciﬁc processing operation or set of operations in question, and a data protection impact assessment has already been carried out as part of a general impact assessment in the context of the adoption of that legal basis, paragraphs 1 to 7 shall not apply unless Member States deem it to be necessary to carry out such an assessment prior to processing activities. 11.Where necessary, the controller shall carry out a review to assess if process- ing is performed in accordance with the data protection impact assessment at least when there is a change of the risk represented by processing operations. Article 36 Prior consultation 1.The controller shall consult the supervisory authority prior to processing where a data protection impact assessment under Article 35 indicates that the processing would result in a high risk in the absence of measures taken by the controller to mitigate the risk. 2.Where the supervisory authority is of the opinion that the intended process- ing referred to in paragraph 1 would infringe this Regulation, in particular where the controller has insuﬃciently identiﬁed or mitigated the risk, the supervisory authority shall, within period of up to eight weeks of receipt of the request for consultation, provide written advice to the controller and, where applicable to the processor, and may use any of its powers referred to in Article 58. That period may be extended by six weeks, taking into account the complexity of the intended processing. The supervisory author- ity shall inform the controller and, where applicable, the processor, of any such extension within one month of receipt of the request for consultation together with the reasons for the delay. Those periods may be suspended until the supervisory authority has obtained information it has requested for the purposes of the consultation. 3.When consulting the supervisory authority pursuant to paragraph 1, the controller shall provide the supervisory authority with: (a)where applicable, the respective responsibilities of the controller, joint controllers and processors involved in the processing, in particular for processing within a group of undertakings; (b) the purposes and means of the intended processing; (c)the measures and safeguards provided to protect the rights and free- doms of data subjects pursuant to this Regulation; 74",
        "(d) where applicable, the contact details of the data protection oﬃcer; (e)the data protection impact assessment provided for in Article 35; and (f) any other information requested by the supervisory authority. 4.Member States shall consult the supervisory authority during the prepara- tion of a proposal for a legislative measure to be adopted by a national parliament, or of a regulatory measure based on such a legislative measure, which relates to processing. 5.Notwithstanding paragraph 1, Member State law may require controllers to consult with, and obtain prior authorisation from, the supervisory authority in relation to processing by a controller for the performance of a task carried out by the controller in the public interest, including processing in relation to social protection and public health. Section 4 Data protection oﬃcer Article 37 Designation of the data protection oﬃcer 1.The controller and the processor shall designate a data protection oﬃcer in any case where: (a)the processing is carried out by a public authority or body, except for courts acting in their judicial capacity; (b)the core activities of the controller or the processor consist of process- ing operations which, by virtue of their nature, their scope and/or their purposes, require regular and systematic monitoring of data subjects on a large scale; or (c)the core activities of the controller or the processor consist of process- ing on a large scale of special categories of data pursuant to Article 9 and personal data relating to criminal convictions and oﬀences referred to in Article 10. 2.A group of undertakings may appoint a single data protection oﬃcer provided that a data protection oﬃcer is easily accessible from each estab- lishment. 3.Where the controller or the processor is a public authority or body, a single data protection oﬃcer may be designated for several such authorities or bodies, taking account of their organisational structure and size. 4.In cases other than those referred to in paragraph 1, the controller or processor or associations and other bodies representing categories of con- trollers or processors may or, where required by Union or Member State law shall, designate a data protection oﬃcer. The data protection oﬃcer may act for such associations and other bodies representing controllers or processors. 5.The data protection oﬃcer shall be designated on the basis of professional qualities and, in particular, expert knowledge of data protection law and practices and the ability to fulﬁl the tasks referred to in Article 39. 75",
        "6.The data protection oﬃcer may be a staﬀ member of the controller or processor, or fulﬁl the tasks on the basis of a service contract. 7.The controller or the processor shall publish the contact details of the data protection oﬃcer and communicate them to the supervisory authority. Article 38 Position of the data protection oﬃcer 1.The controller and the processor shall ensure that the data protection oﬃcer is involved, properly and in a timely manner, in all issues which relate to the protection of personal data. 2.The controller and processor shall support the data protection oﬃcer in performing the tasks referred to in Article 39 by providing resources neces- sary to carry out those tasks and access to personal data and processing operations, and to maintain his or her expert knowledge. 3.The controller and processor shall ensure that the data protection oﬃcer does not receive any instructions regarding the exercise of those tasks. He or she shall not be dismissed or penalised by the controller or the processor for performing his tasks. The data protection oﬃcer shall directly report to the highest management level of the controller or the processor. 4.Data subjects may contact the data protection oﬃcer with regard to all issues related to processing of their personal data and to the exercise of their rights under this Regulation. 5.The data protection oﬃcer shall be bound by secrecy or conﬁdentiality concerning the performance of his or her tasks, in accordance with Union or Member State law. 6.The data protection oﬃcer may fulﬁl other tasks and duties. The controller or processor shall ensure that any such tasks and duties do not result in a conﬂict of interests. Article 39 Tasks of the data protection oﬃcer 1. The data protection oﬃcer shall have at least the following tasks: (a)to inform and advise the controller or the processor and the employ- ees who carry out processing of their obligations pursuant to this Regulation and to other Union or Member State data protection provisions; (b)to monitor compliance with this Regulation, with other Union or Member State data protection provisions and with the policies of the controller or processor in relation to the protection of personal data, including the assignment of responsibilities, awareness-raising and training of staﬀ involved in processing operations, and the related audits; (c)to provide advice where requested as regards the data protection impact assessment and monitor its performance pursuant to Article 35; (d) to cooperate with the supervisory authority; 76",
        "(e)to act as the contact point for the supervisory authority on issues relating to processing, including the prior consultation referred to in Article 36, and to consult, where appropriate, with regard to any other matter. 2.The data protection oﬃcer shall in the performance of his or her tasks have due regard to the risk associated with processing operations, taking into account the nature, scope, context and purposes of processing. Section 5 Codes of conduct and certiﬁcation Article 40 Codes of conduct 1.The Member States, the supervisory authorities, the Board and the Com- mission shall encourage the drawing up of codes of conduct intended to contribute to the proper application of this Regulation, taking account of the speciﬁc features of the various processing sectors and the speciﬁc needs of micro, small and medium-sized enterprises. 2.Associations and other bodies representing categories of controllers or processors may prepare codes of conduct, or amend or extend such codes, for the purpose of specifying the application of this Regulation, such as with regard to: (a) fair and transparent processing; (b) the legitimate interests pursued by controllers in speciﬁc contexts; (c) the collection of personal data; (d) the pseudonymisation of personal data; (e) the information provided to the public and to data subjects; (f) the exercise of the rights of data subjects; (g)the information provided to, and the protection of, children, and the manner in which the consent of the holders of parental responsibility over children is to be obtained; (h)the measures and procedures referred to in Articles 24 and 25 and the measures to ensure security of processing referred to in Article 32; (i)the notiﬁcation of personal data breaches to supervisory authori- ties and the communication of such personal data breaches to data subjects; (j)the transfer of personal data to third countries or international or- ganisations; or (k)out-of-court proceedings and other dispute resolution procedures for resolving disputes between controllers and data subjects with regard to processing, without prejudice to the rights of data subjects pursuant to Articles 77 and 79. 3.In addition to adherence by controllers or processors subject to this Regu- lation, codes of conduct approved pursuant to paragraph 5 of this Article 77",
        "and having general validity pursuant to paragraph 9 of this Article may also be adhered to by controllers or processors that are not subject to this Regulation pursuant to Article 3 in order to provide appropriate safeguards within the framework of personal data transfers to third countries or inter- national organisations under the terms referred to in point (e) of Article 46(2). Such controllers or processors shall make binding and enforceable commitments, via contractual or other legally binding instruments, to apply those appropriate safeguards including with regard to the rights of data subjects. 4.A code of conduct referred to in paragraph 2 of this Article shall contain mechanisms which enable the body referred to in Article 41(1) to carry out the mandatory monitoring of compliance with its provisions by the controllers or processors which undertake to apply it, without prejudice to the tasks and powers of supervisory authorities competent pursuant to Article 55 or 56. 5.Associations and other bodies referred to in paragraph 2 of this Article which intend to prepare a code of conduct or to amend or extend an existing code shall submit the draft code, amendment or extension to the supervisory authority which is competent pursuant to Article 55. The supervisory authority shall provide an opinion on whether the draft code, amendment or extension complies with this Regulation and shall approve that draft code, amendment or extension if it ﬁnds that it provides suﬃcient appropriate safeguards. 6.Where the draft code, or amendment or extension is approved in accordance with paragraph 5, and where the code of conduct concerned does not relate to processing activities in several Member States, the supervisory authority shall register and publish the code. 7.Where a draft code of conduct relates to processing activities in several Member States, the supervisory authority which is competent pursuant to Article 55 shall, before approving the draft code, amendment or extension, submit it in the procedure referred to in Article 63 to the Board which shall provide an opinion on whether the draft code, amendment or extension complies with this Regulation or, in the situation referred to in paragraph 3 of this Article, provides appropriate safeguards. 8.Where the opinion referred to in paragraph 7 conﬁrms that the draft code, amendment or extension complies with this Regulation, or, in the situation referred to in paragraph 3, provides appropriate safeguards, the Board shall submit its opinion to the Commission. 9.The Commission may, by way of implementing acts, decide that the approvedcodeofconduct, amendmentorextensionsubmittedtoitpursuant to paragraph 8 of this Article have general validity within the Union. Those implementing acts shall be adopted in accordance with the examination procedure set out in Article 93(2). 10.The Commission shall ensure appropriate publicity for the approved codes which have been decided as having general validity in accordance with paragraph 9. 78",
        "11.The Board shall collate all approved codes of conduct, amendments and extensions in a register and shall make them publicly available by way of appropriate means. Article 41 Monitoring of approved codes of conduct 1.Without prejudice to the tasks and powers of the competent supervisory authority under Articles 57 and 58, the monitoring of compliance with a code of conduct pursuant to Article 40 may be carried out by a body which has an appropriate level of expertise in relation to the subject-matter of the code and is accredited for that purpose by the competent supervisory authority. 2.A body as referred to in paragraph 1 may be accredited to monitor compliance with a code of conduct where that body has: (a)demonstrated its independence and expertise in relation to the subject- matter of the code to the satisfaction of the competent supervisory authority; (b)established procedures which allow it to assess the eligibility of con- trollers and processors concerned to apply the code, to monitor their compliance with its provisions and to periodically review its operation; (c)established procedures and structures to handle complaints about infringements of the code or the manner in which the code has been, or is being, implemented by a controller or processor, and to make those procedures and structures transparent to data subjects and the public; and (d)demonstrated to the satisfaction of the competent supervisory author- ity that its tasks and duties do not result in a conﬂict of interests. 3.The competent supervisory authority shall submit the draft criteria for accreditation of a body as referred to in paragraph 1 of this Article to the Board pursuant to the consistency mechanism referred to in Article 63. 4.Without prejudice to the tasks and powers of the competent supervisory authority and the provisions of Chapter VIII, a body as referred to in paragraph 1 of this Article shall, subject to appropriate safeguards, take appropriate action in cases of infringement of the code by a controller or processor, including suspension or exclusion of the controller or processor concerned from the code. It shall inform the competent supervisory authority of such actions and the reasons for taking them. 5.The competent supervisory authority shall revoke the accreditation of a body as referred to in paragraph 1 if the conditions for accreditation are not, or are no longer, met or where actions taken by the body infringe this Regulation. 6.This Article shall not apply to processing carried out by public authorities and bodies. Article 42 Certiﬁcation 79",
        "1.The Member States, the supervisory authorities, the Board and the Com- mission shall encourage, in particular at Union level, the establishment of data protection certiﬁcation mechanisms and of data protection seals and marks, for the purpose of demonstrating compliance with this Regulation of processing operations by controllers and processors. The speciﬁc needs of micro, small and medium-sized enterprises shall be taken into account. 2.In addition to adherence by controllers or processors subject to this Regu- lation, data protection certiﬁcation mechanisms, seals or marks approved pursuant to paragraph 5 of this Article may be established for the purpose of demonstrating the existence of appropriate safeguards provided by con- trollers or processors that are not subject to this Regulation pursuant to Article 3 within the framework of personal data transfers to third coun- tries or international organisations under the terms referred to in point (f) of Article 46(2). Such controllers or processors shall make binding and enforceable commitments, via contractual or other legally binding instruments, to apply those appropriate safeguards, including with regard to the rights of data subjects. 3.The certiﬁcation shall be voluntary and available via a process that is transparent. 4.A certiﬁcation pursuant to this Article does not reduce the responsibility of the controller or the processor for compliance with this Regulation and is without prejudice to the tasks and powers of the supervisory authorities which are competent pursuant to Article 55 or 56. 5.A certiﬁcation pursuant to this Article shall be issued by the certiﬁcation bodies referred to in Article 43 or by the competent supervisory authority, on the basis of criteria approved by that competent supervisory authority pursuant to Article 58(3) or by the Board pursuant to Article 63. Where the criteria are approved by the Board, this may result in a common certiﬁcation, the European Data Protection Seal. 6.The controller or processor which submits its processing to the certiﬁcation mechanism shall provide the certiﬁcation body referred to in Article 43, or where applicable, the competent supervisory authority, with all information and access to its processing activities which are necessary to conduct the certiﬁcation procedure. 7.Certiﬁcation shall be issued to a controller or processor for a maximum period of three years and may be renewed, under the same conditions, provided that the relevant requirements continue to be met. Certiﬁcation shall be withdrawn, as applicable, by the certiﬁcation bodies referred to in Article 43 or by the competent supervisory authority where the requirements for the certiﬁcation are not or are no longer met. 8.The Board shall collate all certiﬁcation mechanisms and data protection seals and marks in a register and shall make them publicly available by any appropriate means. Article 43 Certiﬁcation bodies 1.Without prejudice to the tasks and powers of the competent supervisory 80",
        "authority under Articles 57 and 58, certiﬁcation bodies which have an appropriate level of expertise in relation to data protection shall, after informing the supervisory authority in order to allow it to exercise its powers pursuant to point (h) of Article 58(2) where necessary, issue and renew certiﬁcation. Member States shall ensure that those certiﬁcation bodies are accredited by one or both of the following: (a)the supervisory authority which is competent pursuant to Article 55 or 56; (b)the national accreditation body named in accordance with Regulation (EC) No 765/2008 of the European Parliament and of the Council (20) in accordance with EN-ISO/IEC 17065/2012 and with the addi- tional requirements established by the supervisory authority which is competent pursuant to Article 55 or 56. 2.Certiﬁcation bodies referred to in paragraph 1 shall be accredited in accordance with that paragraph only where they have: (a)demonstrated their independence and expertise in relation to the subject-matter of the certiﬁcation to the satisfaction of the competent supervisory authority; (b)undertaken to respect the criteria referred to in Article 42(5) and approved by the supervisory authority which is competent pursuant to Article 55 or 56 or by the Board pursuant to Article 63; (c)established procedures for the issuing, periodic review and withdrawal of data protection certiﬁcation, seals and marks; (d)established procedures and structures to handle complaints about infringements of the certiﬁcation or the manner in which the certiﬁca- tion has been, or is being, implemented by the controller or processor, and to make those procedures and structures transparent to data subjects and the public; and (e)demonstrated, to the satisfaction of the competent supervisory au- thority, that their tasks and duties do not result in a conﬂict of interests. 3.The accreditation of certiﬁcation bodies as referred to in paragraphs 1 and 2 of this Article shall take place on the basis of criteria approved by the supervisory authority which is competent pursuant to Article 55 or 56 or by the Board pursuant to Article 63. In the case of accreditation pursuant to point (b) of paragraph 1 of this Article, those requirements shall complement those envisaged in Regulation (EC) No 765/2008 and the technical rules that describe the methods and procedures of the certiﬁcation bodies. 4.The certiﬁcation bodies referred to in paragraph 1 shall be responsible for the proper assessment leading to the certiﬁcation or the withdrawal of such certiﬁcation without prejudice to the responsibility of the controller or processor for compliance with this Regulation. The accreditation shall be issued for a maximum period of ﬁve years and may be renewed on the same 81",
        "conditions provided that the certiﬁcation body meets the requirements set out in this Article. 5.The certiﬁcation bodies referred to in paragraph 1 shall provide the compe- tent supervisory authorities with the reasons for granting or withdrawing the requested certiﬁcation. 6.The requirements referred to in paragraph 3 of this Article and the criteria referredtoinArticle42(5)shallbemadepublicbythesupervisoryauthority in an easily accessible form. The supervisory authorities shall also transmit those requirements and criteria to the Board. The Board shall collate all certiﬁcation mechanisms and data protection seals in a register and shall make them publicly available by any appropriate means. 7.Without prejudice to Chapter VIII, the competent supervisory authority or the national accreditation body shall revoke an accreditation of a certi- ﬁcation body pursuant to paragraph 1 of this Article where the conditions for the accreditation are not, or are no longer, met or where actions taken by a certiﬁcation body infringe this Regulation. 8.The Commission shall be empowered to adopt delegated acts in accordance with Article 92 for the purpose of specifying the requirements to be taken into account for the data protection certiﬁcation mechanisms referred to in Article 42(1). 9.The Commission may adopt implementing acts laying down technical standards for certiﬁcation mechanisms and data protection seals and marks, and mechanisms to promote and recognise those certiﬁcation mechanisms, seals and marks. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 93(2). CHAPTER V Transfers of personal data to third countries or international organisations Article 44 General principle for transfers Any transfer of personal data which are undergoing processing or are intended for processing after transfer to a third country or to an international organisation shall take place only if, subject to the other provisions of this Regulation, the conditions laid down in this Chapter are complied with by the controller and processor, including for onward transfers of personal data from the third country or an international organisation to another third country or to another international organisation. All provisions in this Chapter shall be applied in order to ensure that the level of protection of natural persons guaranteed by this Regulation is not undermined. Article 45 Transfers on the basis of an adequacy decision 1.A transfer of personal data to a third country or an international organi- sation may take place where the Commission has decided that the third country, a territory or one or more speciﬁed sectors within that third coun- try, or the international organisation in question ensures an adequate level of protection. Such a transfer shall not require any speciﬁc authorisation. 82",
        "2.When assessing the adequacy of the level of protection, the Commission shall, in particular, take account of the following elements: (a)the rule of law, respect for human rights and fundamental freedoms, relevant legislation, both general and sectoral, including concerning public security, defence, national security and criminal law and the access of public authorities to personal data, as well as the implemen- tation of such legislation, data protection rules, professional rules and security measures, including rules for the onward transfer of personal data to another third country or international organisation which are complied with in that country or international organisation, case-law, as well as eﬀective and enforceable data subject rights and eﬀective ad- ministrative and judicial redress for the data subjects whose personal data are being transferred; (b)the existence and eﬀective functioning of one or more independent su- pervisory authorities in the third country or to which an international organisation is subject, with responsibility for ensuring and enforcing compliance with the data protection rules, including adequate enforce- ment powers, for assisting and advising the data subjects in exercising their rights and for cooperation with the supervisory authorities of the Member States; and (c)the international commitments the third country or international organisation concerned has entered into, or other obligations arising from legally binding conventions or instruments as well as from its participation in multilateral or regional systems, in particular in relation to the protection of personal data. 3.The Commission, after assessing the adequacy of the level of protection, may decide, by means of implementing act, that a third country, a territory or one or more speciﬁed sectors within a third country, or an international organisation ensures an adequate level of protection within the meaning of paragraph 2 of this Article. The implementing act shall provide for a mechanism for a periodic review, at least every four years, which shall take into account all relevant developments in the third country or international organisation. The implementing act shall specify its territorial and sectoral application and, where applicable, identify the supervisory authority or authorities referred to in point (b) of paragraph 2 of this Article. The implementing act shall be adopted in accordance with the examination procedure referred to in Article 93(2). 4.The Commission shall, on an ongoing basis, monitor developments in third countries and international organisations that could aﬀect the functioning of decisions adopted pursuant to paragraph 3 of this Article and decisions adopted on the basis of Article 25(6) of Directive 95/46/EC. 5.The Commission shall, where available information reveals, in particular following the review referred to in paragraph 3 of this Article, that a third country, a territory or one or more speciﬁed sectors within a third country, or an international organisation no longer ensures an adequate level of protection within the meaning of paragraph 2 of this Article, to the extent necessary, repeal, amend or suspend the decision referred to in 83",
        "paragraph 3 of this Article by means of implementing acts without retro- active eﬀect. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 93(2). On duly justiﬁed imperative grounds of urgency, the Commission shall adopt immediately applicable implementing acts in accordance with the procedure referred to in Article 93(3). 6.The Commission shall enter into consultations with the third country or international organisation with a view to remedying the situation giving rise to the decision made pursuant to paragraph 5. 7.A decision pursuant to paragraph 5 of this Article is without prejudice to transfers of personal data to the third country, a territory or one or more speciﬁed sectors within that third country, or the international organisation in question pursuant to Articles 46 to 49. 8.The Commission shall publish in the Oﬃcial Journal of the European Union and on its website a list of the third countries, territories and speciﬁed sectors within a third country and international organisations for which it has decided that an adequate level of protection is or is no longer ensured. 9.Decisions adopted by the Commission on the basis of Article 25(6) of Directive 95/46/EC shall remain in force until amended, replaced or repealed by a Commission Decision adopted in accordance with paragraph 3 or 5 of this Article. Article 46 Transfers subject to appropriate safeguards 1.In the absence of a decision pursuant to Article 45(3), a controller or processor may transfer personal data to a third country or an international organisation only if the controller or processor has provided appropriate safeguards, and on condition that enforceable data subject rights and eﬀective legal remedies for data subjects are available. 2.The appropriate safeguards referred to in paragraph 1 may be provided for, without requiring any speciﬁc authorisation from a supervisory authority, by: (a)a legally binding and enforceable instrument between public authori- ties or bodies; (b) binding corporate rules in accordance with Article 47; (c)standard data protection clauses adopted by the Commission in accordance with the examination procedure referred to in Article 93(2); (d)standard data protection clauses adopted by a supervisory author- ity and approved by the Commission pursuant to the examination procedure referred to in Article 93(2); (e)an approved code of conduct pursuant to Article 40 together with binding and enforceable commitments of the controller or processor in the third country to apply the appropriate safeguards, including as regards data subjects’ rights; or 84",
        "(f)an approved certiﬁcation mechanism pursuant to Article 42 together with binding and enforceable commitments of the controller or proces- sor in the third country to apply the appropriate safeguards, including as regards data subjects’ rights. 3.Subject to the authorisation from the competent supervisory authority, the appropriate safeguards referred to in paragraph 1 may also be provided for, in particular, by: (a)contractual clauses between the controller or processor and the con- troller, processor or the recipient of the personal data in the third country or international organisation; or (b)provisions to be inserted into administrative arrangements between public authorities or bodies which include enforceable and eﬀective data subject rights. 4.The supervisory authority shall apply the consistency mechanism referred to in Article 63 in the cases referred to in paragraph 3 of this Article. 5.Authorisations by a Member State or supervisory authority on the basis of Article 26(2) of Directive 95/46/EC shall remain valid until amended, replaced or repealed, if necessary, by that supervisory authority. Decisions adopted by the Commission on the basis of Article 26(4) of Directive 95/46/EC shall remain in force until amended, replaced or repealed, if necessary, by a Commission Decision adopted in accordance with paragraph 2 of this Article. Article 47 Binding corporate rules 1.The competent supervisory authority shall approve binding corporate rules in accordance with the consistency mechanism set out in Article 63, provided that they: (a)are legally binding and apply to and are enforced by every mem- ber concerned of the group of undertakings, or group of enterprises engaged in a joint economic activity, including their employees; (b)expressly confer enforceable rights on data subjects with regard to the processing of their personal data; and (c) fulﬁl the requirements laid down in paragraph 2. 2.The binding corporate rules referred to in paragraph 1 shall specify at least: (a)the structure and contact details of the group of undertakings, or group of enterprises engaged in a joint economic activity and of each of its members; (b)the data transfers or set of transfers, including the categories of personal data, the type of processing and its purposes, the type of data subjects aﬀected and the identiﬁcation of the third country or countries in question; (c) their legally binding nature, both internally and externally; 85",
        "(d)the application of the general data protection principles, in particular purpose limitation, data minimisation, limited storage periods, data quality, data protection by design and by default, legal basis for processing, processing of special categories of personal data, measures to ensure data security, and the requirements in respect of onward transfers to bodies not bound by the binding corporate rules; (e)the rights of data subjects in regard to processing and the means to exercise those rights, including the right not to be subject to decisions based solely on automated processing, including proﬁling in accordance with Article 22, the right to lodge a complaint with the competent supervisory authority and before the competent courts of the Member States in accordance with Article 79, and to obtain redress and, where appropriate, compensation for a breach of the binding corporate rules; (f)the acceptance by the controller or processor established on the territory of a Member State of liability for any breaches of the binding corporate rules by any member concerned not established in the Union; the controller or the processor shall be exempt from that liability, in whole or in part, only if it proves that that member is not responsible for the event giving rise to the damage; (g)how the information on the binding corporate rules, in particular on the provisions referred to in points (d), (e) and (f) of this paragraph is provided to the data subjects in addition to Articles 13 and 14; (h)the tasks of any data protection oﬃcer designated in accordance with Article 37 or any other person or entity in charge of the monitoring compliance with the binding corporate rules within the group of undertakings, or group of enterprises engaged in a joint economic activity, as well as monitoring training and complaint-handling; (i) the complaint procedures; (j)the mechanisms within the group of undertakings, or group of enter- prises engaged in a joint economic activity for ensuring the veriﬁcation of compliance with the binding corporate rules. Such mechanisms shall include data protection audits and methods for ensuring correc- tive actions to protect the rights of the data subject. Results of such veriﬁcation should be communicated to the person or entity referred to in point (h) and to the board of the controlling undertaking of a group of undertakings, or of the group of enterprises engaged in a joint economic activity, and should be available upon request to the competent supervisory authority; (k)the mechanisms for reporting and recording changes to the rules and reporting those changes to the supervisory authority; (l)the cooperation mechanism with the supervisory authority to ensure compliance by any member of the group of undertakings, or group of enterprises engaged in a joint economic activity, in particular by mak- ing available to the supervisory authority the results of veriﬁcations of the measures referred to in point (j); 86",
        "(m)the mechanisms for reporting to the competent supervisory authority anylegalrequirementstowhichamemberofthegroupofundertakings, or group of enterprises engaged in a joint economic activity is subject in a third country which are likely to have a substantial adverse eﬀect on the guarantees provided by the binding corporate rules; and (n) the appropriate data protection training to personnel having perma- nent or regular access to personal data. 3.The Commission may specify the format and procedures for the exchange of information between controllers, processors and supervisory authorities for binding corporate rules within the meaning of this Article. Those implementing acts shall be adopted in accordance with the examination procedure set out in Article 93(2). Article 48 Transfers or disclosures not authorised by Union law Any judgment of a court or tribunal and any decision of an administrative authority of a third country requiring a controller or processor to transfer or disclose personal data may only be recognised or enforceable in any manner if based on an international agreement, such as a mutual legal assistance treaty, in force between the requesting third country and the Union or a Member State, without prejudice to other grounds for transfer pursuant to this Chapter. Article 49 Derogations for speciﬁc situations 1.In the absence of an adequacy decision pursuant to Article 45(3), or of appropriate safeguards pursuant to Article 46, including binding corporate rules, a transfer or a set of transfers of personal data to a third country or an international organisation shall take place only on one of the following conditions: (a)the data subject has explicitly consented to the proposed transfer, after having been informed of the possible risks of such transfers for the data subject due to the absence of an adequacy decision and appropriate safeguards; (b)the transfer is necessary for the performance of a contract between the data subject and the controller or the implementation of pre- contractual measures taken at the data subject’s request; (c)thetransferisnecessaryfortheconclusionorperformanceofacontract concluded in the interest of the data subject between the controller and another natural or legal person; (d) the transfer is necessary for important reasons of public interest; (e)the transfer is necessary for the establishment, exercise or defence of legal claims; (f)the transfer is necessary in order to protect the vital interests of the data subject or of other persons, where the data subject is physically or legally incapable of giving consent; (g)the transfer is made from a register which according to Union or Member State law is intended to provide information to the public 87",
        "and which is open to consultation either by the public in general or by any person who can demonstrate a legitimate interest, but only to the extent that the conditions laid down by Union or Member State law for consultation are fulﬁlled in the particular case. Where a transfer could not be based on a provision in Article 45 or 46, including the provisions on binding corporate rules, and none of the derogations for a speciﬁc situation referred to in the ﬁrst subparagraph of thisparagraphisapplicable, atransfertoathirdcountryoraninternational organisation may take place only if the transfer is not repetitive, concerns only a limited number of data subjects, is necessary for the purposes of compelling legitimate interests pursued by the controller which are not overridden by the interests or rights and freedoms of the data subject, and the controller has assessed all the circumstances surrounding the data transfer and has on the basis of that assessment provided suitable safeguards with regard to the protection of personal data. The controller shall inform the supervisory authority of the transfer. The controller shall, in addition to providing the information referred to in Articles 13 and 14, inform the data subject of the transfer and on the compelling legitimate interests pursued. 2.A transfer pursuant to point (g) of the ﬁrst subparagraph of paragraph 1 shall not involve the entirety of the personal data or entire categories of the personal data contained in the register. Where the register is intended for consultation by persons having a legitimate interest, the transfer shall be made only at the request of those persons or if they are to be the recipients. 3.Points (a), (b) and (c) of the ﬁrst subparagraph of paragraph 1 and the second subparagraph thereof shall not apply to activities carried out by public authorities in the exercise of their public powers. 4.The public interest referred to in point (d) of the ﬁrst subparagraph of paragraph 1 shall be recognised in Union law or in the law of the Member State to which the controller is subject. 5.In the absence of an adequacy decision, Union or Member State law may, for important reasons of public interest, expressly set limits to the transfer of speciﬁc categories of personal data to a third country or an international organisation. Member States shall notify such provisions to the Commission. 6.The controller or processor shall document the assessment as well as the suitable safeguards referred to in the second subparagraph of paragraph 1 of this Article in the records referred to in Article 30. Article 50 International cooperation for the protection of personal data In relation to third countries and international organisations, the Commission and supervisory authorities shall take appropriate steps to: (a)develop international cooperation mechanisms to facilitate the eﬀective enforcement of legislation for the protection of personal data; (b)provide international mutual assistance in the enforcement of legislation for the protection of personal data, including through notiﬁcation, com- 88",
        "plaint referral, investigative assistance and information exchange, subject to appropriate safeguards for the protection of personal data and other fundamental rights and freedoms; (c)engage relevant stakeholders in discussion and activities aimed at furthering internationalcooperationintheenforcementoflegislationfortheprotection of personal data; (d)promote the exchange and documentation of personal data protection legislation and practice, including on jurisdictional conﬂicts with third countries. CHAPTER VI Independent supervisory authorities Section 1 Independent status Article 51 Supervisory authority 1.Each Member State shall provide for one or more independent public au- thorities to be responsible for monitoring the application of this Regulation, in order to protect the fundamental rights and freedoms of natural persons in relation to processing and to facilitate the free ﬂow of personal data within the Union (‘supervisory authority’). 2.Each supervisory authority shall contribute to the consistent application of this Regulation throughout the Union. For that purpose, the supervi- sory authorities shall cooperate with each other and the Commission in accordance with Chapter VII. 3.Where more than one supervisory authority is established in a Member State, that Member State shall designate the supervisory authority which is to represent those authorities in the Board and shall set out the mechanism to ensure compliance by the other authorities with the rules relating to the consistency mechanism referred to in Article 63. 4.Each Member State shall notify to the Commission the provisions of its law which it adopts pursuant to this Chapter, by 25 May 2018 and, without delay, any subsequent amendment aﬀecting them. Article 52 Independence 1.Each supervisory authority shall act with complete independence in per- forming its tasks and exercising its powers in accordance with this Regula- tion. 2.The member or members of each supervisory authority shall, in the perfor- mance of their tasks and exercise of their powers in accordance with this Regulation, remain free from external inﬂuence, whether direct or indirect, and shall neither seek nor take instructions from anybody. 3.Member or members of each supervisory authority shall refrain from any action incompatible with their duties and shall not, during their term of oﬃce, engage in any incompatible occupation, whether gainful or not. 4.Each Member State shall ensure that each supervisory authority is provided with the human, technical and ﬁnancial resources, premises and infras- 89",
        "tructure necessary for the eﬀective performance of its tasks and exercise of its powers, including those to be carried out in the context of mutual assistance, cooperation and participation in the Board. 5.Each Member State shall ensure that each supervisory authority chooses and has its own staﬀ which shall be subject to the exclusive direction of the member or members of the supervisory authority concerned. 6.Each Member State shall ensure that each supervisory authority is subject to ﬁnancial control which does not aﬀect its independence and that it has separate, public annual budgets, which may be part of the overall state or national budget. Article 53 General conditions for the members of the supervisory authority 1.Member States shall provide for each member of their supervisory authori- ties to be appointed by means of a transparent procedure by: •their parliament; •their government; •their head of State; or •an independent body entrusted with the appointment under Member State law. 2.Each member shall have the qualiﬁcations, experience and skills, in partic- ular in the area of the protection of personal data, required to perform its duties and exercise its powers. 3.The duties of a member shall end in the event of the expiry of the term of oﬃce, resignation or compulsory retirement, in accordance with the law of the Member State concerned. 4.A member shall be dismissed only in cases of serious misconduct or if the member no longer fulﬁls the conditions required for the performance of the duties. Article 54 Rules on the establishment of the supervisory authority 1. Each Member State shall provide by law for all of the following: (a) the establishment of each supervisory authority; (b)the qualiﬁcations and eligibility conditions required to be appointed as member of each supervisory authority; (c)the rules and procedures for the appointment of the member or members of each supervisory authority; (d)thedurationofthetermofthememberormembersofeachsupervisory authority of no less than four years, except for the ﬁrst appointment after 24 May 2016, part of which may take place for a shorter period where that is necessary to protect the independence of the supervisory authority by means of a staggered appointment procedure; (e)whether and, if so, for how many terms the member or members of each supervisory authority is eligible for reappointment; 90",
        "(f)the conditions governing the obligations of the member or members and staﬀ of each supervisory authority, prohibitions on actions, oc- cupations and beneﬁts incompatible therewith during and after the term of oﬃce and rules governing the cessation of employment. 2.The member or members and the staﬀ of each supervisory authority shall, in accordance with Union or Member State law, be subject to a duty of professional secrecy both during and after their term of oﬃce, with regard to any conﬁdential information which has come to their knowledge in the course of the performance of their tasks or exercise of their powers. During their term of oﬃce, that duty of professional secrecy shall in particular apply to reporting by natural persons of infringements of this Regulation. Section 2 Competence, tasks and powers Article 55 Competence 1.Each supervisory authority shall be competent for the performance of the tasks assigned to and the exercise of the powers conferred on it in accordance with this Regulation on the territory of its own Member State. 2.Where processing is carried out by public authorities or private bodies acting on the basis of point (c) or (e) of Article 6(1), the supervisory authority of the Member State concerned shall be competent. In such cases Article 56 does not apply. 3.Supervisory authorities shall not be competent to supervise processing operations of courts acting in their judicial capacity. Article 56 Competence of the lead supervisory authority 1.Without prejudice to Article 55, the supervisory authority of the main establishment or of the single establishment of the controller or processor shall be competent to act as lead supervisory authority for the cross-border processing carried out by that controller or processor in accordance with the procedure provided in Article 60. 2.By derogation from paragraph 1, each supervisory authority shall be competent to handle a complaint lodged with it or a possible infringement of this Regulation, if the subject matter relates only to an establishment in its Member State or substantially aﬀects data subjects only in its Member State. 3.In the cases referred to in paragraph 2 of this Article, the supervisory authority shall inform the lead supervisory authority without delay on that matter. Within a period of three weeks after being informed the lead supervisory authority shall decide whether or not it will handle the case in accordance with the procedure provided in Article 60, taking into account whether or not there is an establishment of the controller or processor in the Member State of which the supervisory authority informed it. 4.Where the lead supervisory authority decides to handle the case, the procedure provided in Article 60 shall apply. The supervisory authority which informed the lead supervisory authority may submit to the lead 91",
        "supervisory authority a draft for a decision. The lead supervisory authority shall take utmost account of that draft when preparing the draft decision referred to in Article 60(3). 5.Where the lead supervisory authority decides not to handle the case, the supervisory authority which informed the lead supervisory authority shall handle it according to Articles 61 and 62. 6.Theleadsupervisoryauthorityshallbethesoleinterlocutorofthecontroller or processor for the cross-border processing carried out by that controller or processor. Article 57 Tasks 1.Without prejudice to other tasks set out under this Regulation, each supervisory authority shall on its territory: (a) monitor and enforce the application of this Regulation; (b)promote public awareness and understanding of the risks, rules, safe- guards and rights in relation to processing. Activities addressed speciﬁcally to children shall receive speciﬁc attention; (c)advise, in accordance with Member State law, the national parliament, the government, and other institutions and bodies on legislative and administrative measures relating to the protection of natural persons’ rights and freedoms with regard to processing; (d)promotetheawarenessofcontrollersandprocessorsoftheirobligations under this Regulation; (e)upon request, provide information to any data subject concerning the exercise of their rights under this Regulation and, if appropriate, cooperate with the supervisory authorities in other Member States to that end; (f)handle complaints lodged by a data subject, or by a body, organisation or association in accordance with Article 80, and investigate, to the extent appropriate, the subject matter of the complaint and inform the complainant of the progress and the outcome of the investigation within a reasonable period, in particular if further investigation or coordination with another supervisory authority is necessary; (g)cooperate with, including sharing information and provide mutual assistance to, other supervisory authorities with a view to ensuring the consistency of application and enforcement of this Regulation; (h)conduct investigations on the application of this Regulation, including onthebasisofinformationreceivedfromanothersupervisoryauthority or other public authority; (i)monitor relevant developments, insofar as they have an impact on the protection of personal data, in particular the development of informa- tion and communication technologies and commercial practices; (j)adopt standard contractual clauses referred to in Article 28(8) and in point (d) of Article 46(2); 92",
        "(k)establish and maintain a list in relation to the requirement for data protection impact assessment pursuant to Article 35(4); (l) give advice on the processing operations referred to in Article 36(2); (m)encourage the drawing up of codes of conduct pursuant to Article 40(1) and provide an opinion and approve such codes of conduct which provide suﬃcient safeguards, pursuant to Article 40(5); (n)encourage the establishment of data protection certiﬁcation mech- anisms and of data protection seals and marks pursuant to Article 42(1), and approve the criteria of certiﬁcation pursuant to Article 42(5); (o)where applicable, carry out a periodic review of certiﬁcations issued in accordance with Article 42(7); (p)draftandpublishthecriteriaforaccreditationofabodyformonitoring codes of conduct pursuant to Article 41 and of a certiﬁcation body pursuant to Article 43; (q)conduct the accreditation of a body for monitoring codes of conduct pursuant to Article 41 and of a certiﬁcation body pursuant to Article 43; (r)authorise contractual clauses and provisions referred to in Article 46(3); (s) approve binding corporate rules pursuant to Article 47; (t) contribute to the activities of the Board; (u)keep internal records of infringements of this Regulation and of mea- sures taken in accordance with Article 58(2); and (v) fulﬁl any other tasks related to the protection of personal data. 2.Each supervisory authority shall facilitate the submission of complaints referred to in point (f) of paragraph 1 by measures such as a complaint sub- mission form which can also be completed electronically, without excluding other means of communication. 3.The performance of the tasks of each supervisory authority shall be free of charge for the data subject and, where applicable, for the data protection oﬃcer. 4.Where requests are manifestly unfounded or excessive, in particular be- cause of their repetitive character, the supervisory authority may charge a reasonable fee based on administrative costs, or refuse to act on the request. The supervisory authority shall bear the burden of demonstrating the manifestly unfounded or excessive character of the request. Article 58 Powers 1.Each supervisory authority shall have all of the following investigative powers: 93",
        "(a)to order the controller and the processor, and, where applicable, the controller’s or the processor’s representative to provide any informa- tion it requires for the performance of its tasks; (b) to carry out investigations in the form of data protection audits; (c)to carry out a review on certiﬁcations issued pursuant to Article 42(7); (d)to notify the controller or the processor of an alleged infringement of this Regulation; (e)to obtain, from the controller and the processor, access to all personal data and to all information necessary for the performance of its tasks; (f)to obtain access to any premises of the controller and the processor, including to any data processing equipment and means, in accordance with Union or Member State procedural law. 2.Each supervisory authority shall have all of the following corrective powers: (a)to issue warnings to a controller or processor that intended processing operations are likely to infringe provisions of this Regulation; (b)to issue reprimands to a controller or a processor where processing operations have infringed provisions of this Regulation; (c)to order the controller or the processor to comply with the data sub- ject’s requests to exercise his or her rights pursuant to this Regulation; (d)to order the controller or processor to bring processing operations into compliance with the provisions of this Regulation, where appropriate, in a speciﬁed manner and within a speciﬁed period; (e)to order the controller to communicate a personal data breach to the data subject; (f)to impose a temporary or deﬁnitive limitation including a ban on processing; (g)to order the rectiﬁcation or erasure of personal data or restriction of processing pursuant to Articles 16, 17 and 18 and the notiﬁcation of such actions to recipients to whom the personal data have been disclosed pursuant to Article 17(2) and Article 19; (h)to withdraw a certiﬁcation or to order the certiﬁcation body to with- draw a certiﬁcation issued pursuant to Articles 42 and 43, or to order the certiﬁcation body not to issue certiﬁcation if the requirements for the certiﬁcation are not or are no longer met; (i)to impose an administrative ﬁne pursuant to Article 83, in addition to, or instead of measures referred to in this paragraph, depending on the circumstances of each individual case; (j)to order the suspension of data ﬂows to a recipient in a third country or to an international organisation. 3.Each supervisory authority shall have all of the following authorisation and advisory powers: 94",
        "(a)to advise the controller in accordance with the prior consultation procedure referred to in Article 36; (b) to issue, on its own initiative or on request, opinions to the national parliament, the Member State government or, in accordance with Member State law, to other institutions and bodies as well as to the public on any issue related to the protection of personal data; (c)to authorise processing referred to in Article 36(5), if the law of the Member State requires such prior authorisation; (d)to issue an opinion and approve draft codes of conduct pursuant to Article 40(5); (e) to accredit certiﬁcation bodies pursuant to Article 43; (f)toissuecertiﬁcationsandapprovecriteriaofcertiﬁcationinaccordance with Article 42(5); (g)to adopt standard data protection clauses referred to in Article 28(8) and in point (d) of Article 46(2); (h)to authorise contractual clauses referred to in point (a) of Article 46(3); (i)to authorise administrative arrangements referred to in point (b) of Article 46(3); (j) to approve binding corporate rules pursuant to Article 47. 4.The exercise of the powers conferred on the supervisory authority pursuant to this Article shall be subject to appropriate safeguards, including eﬀective judicial remedy and due process, set out in Union and Member State law in accordance with the Charter. 5.Each Member State shall provide by law that its supervisory authority shall have the power to bring infringements of this Regulation to the attention of the judicial authorities and where appropriate, to commence or engage otherwise in legal proceedings, in order to enforce the provisions of this Regulation. 6.Each Member State may provide by law that its supervisory authority shall have additional powers to those referred to in paragraphs 1, 2 and 3. The exercise of those powers shall not impair the eﬀective operation of Chapter VII. Article 59 Activity reports Each supervisory authority shall draw up an annual report on its activities, which may include a list of types of infringement notiﬁed and types of measures taken in accordance with Article 58(2). Those reports shall be transmitted to the national parliament, the government and other authorities as designated by Member State law. They shall be made available to the public, to the Commission and to the Board. 95",
        "CHAPTER VII Cooperation and consistency Section 1 Cooperation Article 60 Cooperation between the lead supervisory authority and the other supervisory authorities concerned 1.The lead supervisory authority shall cooperate with the other supervisory authorities concerned in accordance with this Article in an endeavour to reach consensus. The lead supervisory authority and the supervisory authorities concerned shall exchange all relevant information with each other. 2.The lead supervisory authority may request at any time other supervisory authorities concerned to provide mutual assistance pursuant to Article 61 and may conduct joint operations pursuant to Article 62, in particular for carrying out investigations or for monitoring the implementation of a measure concerning a controller or processor established in another Member State. 3.The lead supervisory authority shall, without delay, communicate the relevant information on the matter to the other supervisory authorities concerned. It shall without delay submit a draft decision to the other supervisory authorities concerned for their opinion and take due account of their views. 4.Where any of the other supervisory authorities concerned within a period of four weeks after having been consulted in accordance with paragraph 3 of this Article, expresses a relevant and reasoned objection to the draft decision, the lead supervisory authority shall, if it does not follow the relevant and reasoned objection or is of the opinion that the objection is not relevant or reasoned, submit the matter to the consistency mechanism referred to in Article 63. 5.Where the lead supervisory authority intends to follow the relevant and reasonedobjectionmade, itshallsubmittotheothersupervisoryauthorities concerned a revised draft decision for their opinion. That revised draft decision shall be subject to the procedure referred to in paragraph 4 within a period of two weeks. 6.Where none of the other supervisory authorities concerned has objected to the draft decision submitted by the lead supervisory authority within the period referred to in paragraphs 4 and 5, the lead supervisory authority and the supervisory authorities concerned shall be deemed to be in agreement with that draft decision and shall be bound by it. 7. The lead supervisory authority shall adopt and notify the decision to the main establishment or single establishment of the controller or processor, as the case may be and inform the other supervisory authorities concerned and the Board of the decision in question, including a summary of the relevant facts and grounds. The supervisory authority with which a complaint has been lodged shall inform the complainant on the decision. 8.By derogation from paragraph 7, where a complaint is dismissed or rejected, 96",
        "the supervisory authority with which the complaint was lodged shall adopt the decision and notify it to the complainant and shall inform the controller thereof. 9.Where the lead supervisory authority and the supervisory authorities concerned agree to dismiss or reject parts of a complaint and to act on other parts of that complaint, a separate decision shall be adopted for each of those parts of the matter. The lead supervisory authority shall adopt the decision for the part concerning actions in relation to the controller, shall notify it to the main establishment or single establishment of the controller or processor on the territory of its Member State and shall inform the complainant thereof, while the supervisory authority of the complainant shall adopt the decision for the part concerning dismissal or rejection of that complaint, and shall notify it to that complainant and shall inform the controller or processor thereof. 10.After being notiﬁed of the decision of the lead supervisory authority pursuant to paragraphs 7 and 9, the controller or processor shall take the necessary measures to ensure compliance with the decision as regards processing activities in the context of all its establishments in the Union. The controller or processor shall notify the measures taken for complying with the decision to the lead supervisory authority, which shall inform the other supervisory authorities concerned. 11.Where, in exceptional circumstances, a supervisory authority concerned has reasons to consider that there is an urgent need to act in order to protect the interests of data subjects, the urgency procedure referred to in Article 66 shall apply. 12.The lead supervisory authority and the other supervisory authorities concerned shall supply the information required under this Article to each other by electronic means, using a standardised format. Article 61 Mutual assistance 1.Supervisory authorities shall provide each other with relevant information and mutual assistance in order to implement and apply this Regulation in a consistent manner, and shall put in place measures for eﬀective cooperation with one another. Mutual assistance shall cover, in particular, information requests and supervisory measures, such as requests to carry out prior authorisations and consultations, inspections and investigations. 2.Each supervisory authority shall take all appropriate measures required to reply to a request of another supervisory authority without undue delay and no later than one month after receiving the request. Such measures may include, in particular, the transmission of relevant information on the conduct of an investigation. 3.Requestsforassistanceshallcontainallthenecessaryinformation, including the purpose of and reasons for the request. Information exchanged shall be used only for the purpose for which it was requested. 4.The requested supervisory authority shall not refuse to comply with the request unless: 97",
        "(a)it is not competent for the subject-matter of the request or for the measures it is requested to execute; or (b)compliance with the request would infringe this Regulation or Union or Member State law to which the supervisory authority receiving the request is subject. 5.The requested supervisory authority shall inform the requesting supervi- sory authority of the results or, as the case may be, of the progress of the measures taken in order to respond to the request. The requested supervisory authority shall provide reasons for any refusal to comply with a request pursuant to paragraph 4. 6.Requested supervisory authorities shall, as a rule, supply the information requested by other supervisory authorities by electronic means, using a standardised format. 7.Requested supervisory authorities shall not charge a fee for any action taken by them pursuant to a request for mutual assistance. Supervisory authorities may agree on rules to indemnify each other for speciﬁc ex- penditure arising from the provision of mutual assistance in exceptional circumstances. 8.Where a supervisory authority does not provide the information referred to in paragraph 5 of this Article within one month of receiving the request of another supervisory authority, the requesting supervisory authority may adopt a provisional measure on the territory of its Member State in accordance with Article 55(1). In that case, the urgent need to act under Article 66(1) shall be presumed to be met and require an urgent binding decision from the Board pursuant to Article 66(2). 9.The Commission may, by means of implementing acts, specify the format and procedures for mutual assistance referred to in this Article and the arrangements for the exchange of information by electronic means between supervisory authorities, and between supervisory authorities and the Board, in particular the standardised format referred to in paragraph 6 of this Article. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 93(2). Article 62 Joint operations of supervisory authorities 1.The supervisory authorities shall, where appropriate, conduct joint op- erations including joint investigations and joint enforcement measures in which members or staﬀ of the supervisory authorities of other Member States are involved. 2.Where the controller or processor has establishments in several Member States or where a signiﬁcant number of data subjects in more than one Member State are likely to be substantially aﬀected by processing opera- tions, a supervisory authority of each of those Member States shall have the right to participate in joint operations. The supervisory authority which is competent pursuant to Article 56(1) or (4) shall invite the super- visory authority of each of those Member States to take part in the joint 98",
        "operations and shall respond without delay to the request of a supervisory authority to participate. 3.A supervisory authority may, in accordance with Member State law, and with the seconding supervisory authority’s authorisation, confer powers, including investigative powers on the seconding supervisory authority’s members or staﬀ involved in joint operations or, in so far as the law of the Member State of the host supervisory authority permits, allow the seconding supervisory authority’s members or staﬀ to exercise their investigative powers in accordance with the law of the Member State of the seconding supervisory authority. Such investigative powers may be exercised only under the guidance and in the presence of members or staﬀ of the host supervisory authority. The seconding supervisory authority’s members or staﬀ shall be subject to the Member State law of the host supervisory authority. 4.Where, in accordance with paragraph 1, staﬀ of a seconding supervisory authority operate in another Member State, the Member State of the host supervisory authority shall assume responsibility for their actions, including liability, for any damage caused by them during their operations, in accordance with the law of the Member State in whose territory they are operating. 5.The Member State in whose territory the damage was caused shall make good such damage under the conditions applicable to damage caused by its own staﬀ. The Member State of the seconding supervisory authority whose staﬀ has caused damage to any person in the territory of another Member State shall reimburse that other Member State in full any sums it has paid to the persons entitled on their behalf. 6.Without prejudice to the exercise of its rights vis-à-vis third parties and with the exception of paragraph 5, each Member State shall refrain, in the case provided for in paragraph 1, from requesting reimbursement from another Member State in relation to damage referred to in paragraph 4. 7.Where a joint operation is intended and a supervisory authority does not, within one month, comply with the obligation laid down in the second sentence of paragraph 2 of this Article, the other supervisory authorities may adopt a provisional measure on the territory of its Member State in accordance with Article 55. In that case, the urgent need to act under Article 66(1) shall be presumed to be met and require an opinion or an urgent binding decision from the Board pursuant to Article 66(2). Section 2 Consistency Article 63 Consistency mechanism In order to contribute to the consistent application of this Regulation throughout the Union, the supervisory authorities shall cooperate with each other and, where relevant, with the Commission, through the consistency mechanism as set out in this Section. Article 64 Opinion of the Board 99",
        "1.The Board shall issue an opinion where a competent supervisory authority intends to adopt any of the measures below. To that end, the competent supervisory authority shall communicate the draft decision to the Board, when it: (a)aims to adopt a list of the processing operations subject to the requirement for a data protection impact assessment pursuant to Article 35(4); (b)concerns a matter pursuant to Article 40(7) whether a draft code of conduct or an amendment or extension to a code of conduct complies with this Regulation; (c)aims to approve the criteria for accreditation of a body pursuant to Article 41(3) or a certiﬁcation body pursuant to Article 43(3); (d)aims to determine standard data protection clauses referred to in point (d) of Article 46(2) and in Article 28(8); (e)aims to authorise contractual clauses referred to in point (a) of Article 46(3); or (f)aims to approve binding corporate rules within the meaning of Article 47. 2.Any supervisory authority, the Chair of the Board or the Commission may request that any matter of general application or producing eﬀects in more than one Member State be examined by the Board with a view to obtaining an opinion, in particular where a competent supervisory authority does not comply with the obligations for mutual assistance in accordance with Article 61 or for joint operations in accordance with Article 62. 3.In the cases referred to in paragraphs 1 and 2, the Board shall issue an opinion on the matter submitted to it provided that it has not already issued an opinion on the same matter. That opinion shall be adopted within eight weeks by simple majority of the members of the Board. That period may be extended by a further six weeks, taking into account the complexity of the subject matter. Regarding the draft decision referred to in paragraph 1 circulated to the members of the Board in accordance with paragraph 5, a member which has not objected within a reasonable period indicated by the Chair, shall be deemed to be in agreement with the draft decision. 4.Supervisory authorities and the Commission shall, without undue delay, communicate by electronic means to the Board, using a standardised format any relevant information, including as the case may be a summary of the facts, the draft decision, the grounds which make the enactment of such measure necessary, and the views of other supervisory authorities concerned. 5.The Chair of the Board shall, without undue, delay inform by electronic means: (a)the members of the Board and the Commission of any relevant in- formation which has been communicated to it using a standardised 100",
        "format. The secretariat of the Board shall, where necessary, provide translations of relevant information; and (b)thesupervisoryauthorityreferredto, asthecasemaybe, inparagraphs 1 and 2, and the Commission of the opinion and make it public. 6.The competent supervisory authority shall not adopt its draft decision referred to in paragraph 1 within the period referred to in paragraph 3. 7.The supervisory authority referred to in paragraph 1 shall take utmost account of the opinion of the Board and shall, within two weeks after receiving the opinion, communicate to the Chair of the Board by electronic means whether it will maintain or amend its draft decision and, if any, the amended draft decision, using a standardised format. 8.Where the supervisory authority concerned informs the Chair of the Board within the period referred to in paragraph 7 of this Article that it does not intend to follow the opinion of the Board, in whole or in part, providing the relevant grounds, Article 65(1) shall apply. Article 65 Dispute resolution by the Board 1.In order to ensure the correct and consistent application of this Regulation in individual cases, the Board shall adopt a binding decision in the following cases: (a)where, in a case referred to in Article 60(4), a supervisory authority concerned has raised a relevant and reasoned objection to a draft decision of the lead authority or the lead authority has rejected such an objection as being not relevant or reasoned. The binding decision shall concern all the matters which are the subject of the relevant and reasoned objection, in particular whether there is an infringement of this Regulation; (b)where there are conﬂicting views on which of the supervisory authori- ties concerned is competent for the main establishment; (c)where a competent supervisory authority does not request the opinion of the Board in the cases referred to in Article 64(1), or does not follow the opinion of the Board issued under Article 64. In that case, any supervisory authority concerned or the Commission may communicate the matter to the Board. 2.The decision referred to in paragraph 1 shall be adopted within one month from the referral of the subject-matter by a two-thirds majority of the members of the Board. That period may be extended by a further month on account of the complexity of the subject-matter. The decision referred to in paragraph 1 shall be reasoned and addressed to the lead supervisory authority and all the supervisory authorities concerned and binding on them. 3.Where the Board has been unable to adopt a decision within the periods referred to in paragraph 2, it shall adopt its decision within two weeks following the expiration of the second month referred to in paragraph 2 by 101",
        "a simple majority of the members of the Board. Where the members of the Board are split, the decision shall by adopted by the vote of its Chair. 4.The supervisory authorities concerned shall not adopt a decision on the subject matter submitted to the Board under paragraph 1 during the periods referred to in paragraphs 2 and 3. 5.The Chair of the Board shall notify, without undue delay, the decision referred to in paragraph 1 to the supervisory authorities concerned. It shall inform the Commission thereof. The decision shall be published on the website of the Board without delay after the supervisory authority has notiﬁed the ﬁnal decision referred to in paragraph 6. 6.The lead supervisory authority or, as the case may be, the supervisory authority with which the complaint has been lodged shall adopt its ﬁnal decision on the basis of the decision referred to in paragraph 1 of this Article, without undue delay and at the latest by one month after the Board has notiﬁed its decision. The lead supervisory authority or, as the case may be, the supervisory authority with which the complaint has been lodged, shall inform the Board of the date when its ﬁnal decision is notiﬁed respectively to the controller or the processor and to the data subject. The ﬁnal decision of the supervisory authorities concerned shall be adopted under the terms of Article 60(7), (8) and (9). The ﬁnal decision shall refer to the decision referred to in paragraph 1 of this Article and shall specify that the decision referred to in that paragraph will be published on the website of the Board in accordance with paragraph 5 of this Article. The ﬁnal decision shall attach the decision referred to in paragraph 1 of this Article. Article 66 Urgency procedure 1.In exceptional circumstances, where a supervisory authority concerned considers that there is an urgent need to act in order to protect the rights and freedoms of data subjects, it may, by way of derogation from the consistency mechanism referred to in Articles 63, 64 and 65 or the procedure referred to in Article 60, immediately adopt provisional measures intended to produce legal eﬀects on its own territory with a speciﬁed period of validity which shall not exceed three months. The supervisory authority shall, without delay, communicate those measures and the reasons for adopting them to the other supervisory authorities concerned, to the Board and to the Commission. 2.Where a supervisory authority has taken a measure pursuant to paragraph 1 and considers that ﬁnal measures need urgently be adopted, it may request an urgent opinion or an urgent binding decision from the Board, giving reasons for requesting such opinion or decision. 3.Any supervisory authority may request an urgent opinion or an urgent binding decision, as the case may be, from the Board where a competent supervisory authority has not taken an appropriate measure in a situation where there is an urgent need to act, in order to protect the rights and freedoms of data subjects, giving reasons for requesting such opinion or decision, including for the urgent need to act. 102",
        "4.By derogation from Article 64(3) and Article 65(2), an urgent opinion or an urgent binding decision referred to in paragraphs 2 and 3 of this Article shall be adopted within two weeks by simple majority of the members of the Board. Article 67 Exchange of information The Commission may adopt implementing acts of general scope in order to specify the arrangements for the exchange of information by electronic means between supervisory authorities, and between supervisory authorities and the Board, in particular the standardised format referred to in Article 64. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 93(2). Section 3 European data protection board Article 68 European Data Protection Board 1.The European Data Protection Board (the ‘Board’) is hereby established as a body of the Union and shall have legal personality. 2. The Board shall be represented by its Chair. 3.The Board shall be composed of the head of one supervisory authority of each Member State and of the European Data Protection Supervisor, or their respective representatives. 4.Where in a Member State more than one supervisory authority is respon- sible for monitoring the application of the provisions pursuant to this Regulation, a joint representative shall be appointed in accordance with that Member State’s law. 5.The Commission shall have the right to participate in the activities and meetings of the Board without voting right. The Commission shall desig- nate a representative. The Chair of the Board shall communicate to the Commission the activities of the Board. 6.In the cases referred to in Article 65, the European Data Protection Super- visor shall have voting rights only on decisions which concern principles and rules applicable to the Union institutions, bodies, oﬃces and agencies which correspond in substance to those of this Regulation. Article 69 Independence 1.The Board shall act independently when performing its tasks or exercising its powers pursuant to Articles 70 and 71. 2.Without prejudice to requests by the Commission referred to in point (b) of Article 70(1) and in Article 70(2), the Board shall, in the performance of its tasks or the exercise of its powers, neither seek nor take instructions from anybody. Article 70 Tasks of the Board 103",
        "1.The Board shall ensure the consistent application of this Regulation. To that end, the Board shall, on its own initiative or, where relevant, at the request of the Commission, in particular: (a)monitor and ensure the correct application of this Regulation in the cases provided for in Articles 64 and 65 without prejudice to the tasks of national supervisory authorities; (b)advise the Commission on any issue related to the protection of personal data in the Union, including on any proposed amendment of this Regulation; (c)advise the Commission on the format and procedures for the ex- change of information between controllers, processors and supervisory authorities for binding corporate rules; (d)issue guidelines, recommendations, and best practices on procedures for erasing links, copies or replications of personal data from publicly available communication services as referred to in Article 17(2); (e)examine, on its own initiative, on request of one of its members or on request of the Commission, any question covering the applica- tion of this Regulation and issue guidelines, recommendations and best practices in order to encourage consistent application of this Regulation; (f)issue guidelines, recommendations and best practices in accordance with point (e) of this paragraph for further specifying the criteria and conditions for decisions based on proﬁling pursuant to Article 22(2); (g)issue guidelines, recommendations and best practices in accordance with point (e) of this paragraph for establishing the personal data breaches and determining the undue delay referred to in Article 33(1) and (2) and for the particular circumstances in which a controller or a processor is required to notify the personal data breach; (h)issue guidelines, recommendations and best practices in accordance with point (e) of this paragraph as to the circumstances in which a personal data breach is likely to result in a high risk to the rights and freedoms of the natural persons referred to in Article 34(1). (i)issue guidelines, recommendations and best practices in accordance with point (e) of this paragraph for the purpose of further specify- ing the criteria and requirements for personal data transfers based on binding corporate rules adhered to by controllers and binding corporate rules adhered to by processors and on further necessary requirements to ensure the protection of personal data of the data subjects concerned referred to in Article 47; (j)issue guidelines, recommendations and best practices in accordance with point (e) of this paragraph for the purpose of further specifying the criteria and requirements for the personal data transfers on the basis of Article 49(1); 104",
        "(k)draw up guidelines for supervisory authorities concerning the appli- cation of measures referred to in Article 58(1), (2) and (3) and the setting of administrative ﬁnes pursuant to Article 83; (l)review the practical application of the guidelines, recommendations and best practices referred to in points (e) and (f); (m)issue guidelines, recommendations and best practices in accordance with point (e) of this paragraph for establishing common procedures for reporting by natural persons of infringements of this Regulation pursuant to Article 54(2); (n)encourage the drawing-up of codes of conduct and the establishment of data protection certiﬁcation mechanisms and data protection seals and marks pursuant to Articles 40 and 42; (o)carry out the accreditation of certiﬁcation bodies and its periodic review pursuant to Article 43 and maintain a public register of accred- ited bodies pursuant to Article 43(6) and of the accredited controllers or processors established in third countries pursuant to Article 42(7); (p)specify the requirements referred to in Article 43(3) with a view to the accreditation of certiﬁcation bodies under Article 42; (q)provide the Commission with an opinion on the certiﬁcation require- ments referred to in Article 43(8); (r)provide the Commission with an opinion on the icons referred to in Article 12(7); (s)provide the Commission with an opinion for the assessment of the adequacy of the level of protection in a third country or international organisation, including for the assessment whether a third country, a territory or one or more speciﬁed sectors within that third country, or an international organisation no longer ensures an adequate level of protection. To that end, the Commission shall provide the Board with all necessary documentation, including correspondence with the government of the third country, with regard to that third country, territory or speciﬁed sector, or with the international organisation. (t)issue opinions on draft decisions of supervisory authorities pursuant to the consistency mechanism referred to in Article 64(1), on matters submitted pursuant to Article 64(2) and to issue binding decisions pursuant to Article 65, including in cases referred to in Article 66; (u)promote the cooperation and the eﬀective bilateral and multilateral exchange of information and best practices between the supervisory authorities; (v)promote common training programmes and facilitate personnel ex- changes between the supervisory authorities and, where appropriate, withthesupervisoryauthoritiesofthirdcountriesorwithinternational organisations; (w)promote the exchange of knowledge and documentation on data protection legislation and practice with data protection supervisory 105",
        "authorities worldwide. (x)issue opinions on codes of conduct drawn up at Union level pursuant to Article 40(9); and (y)maintain a publicly accessible electronic register of decisions taken by supervisory authorities and courts on issues handled in the consistency mechanism. 2.Where the Commission requests advice from the Board, it may indicate a time limit, taking into account the urgency of the matter. 3.The Board shall forward its opinions, guidelines, recommendations, and best practices to the Commission and to the committee referred to in Article 93 and make them public. 4.The Board shall, where appropriate, consult interested parties and give them the opportunity to comment within a reasonable period. The Board shall, without prejudice to Article 76, make the results of the consultation procedure publicly available. Article 71 Reports 1.The Board shall draw up an annual report regarding the protection of natural persons with regard to processing in the Union and, where relevant, in third countries and international organisations. The report shall be made public and be transmitted to the European Parliament, to the Council and to the Commission. 2.The annual report shall include a review of the practical application of the guidelines, recommendations and best practices referred to in point (l) of Article 70(1) as well as of the binding decisions referred to in Article 65. Article 72 Procedure 1.The Board shall take decisions by a simple majority of its members, unless otherwise provided for in this Regulation. 2.The Board shall adopt its own rules of procedure by a two-thirds majority of its members and organise its own operational arrangements. Article 73 Chair 1.The Board shall elect a chair and two deputy chairs from amongst its members by simple majority. 2.The term of oﬃce of the Chair and of the deputy chairs shall be ﬁve years and be renewable once. Article 74 Tasks of the Chair 1. The Chair shall have the following tasks: (a) to convene the meetings of the Board and prepare its agenda; (b)to notify decisions adopted by the Board pursuant to Article 65 to the lead supervisory authority and the supervisory authorities concerned; 106",
        "(c)to ensure the timely performance of the tasks of the Board, in partic- ular in relation to the consistency mechanism referred to in Article 63. 2.The Board shall lay down the allocation of tasks between the Chair and the deputy chairs in its rules of procedure. Article 75 Secretariat 1.TheBoardshallhaveasecretariat, whichshallbeprovidedbytheEuropean Data Protection Supervisor. 2.The secretariat shall perform its tasks exclusively under the instructions of the Chair of the Board. 3.The staﬀ of the European Data Protection Supervisor involved in carrying out the tasks conferred on the Board by this Regulation shall be subject to separate reporting lines from the staﬀ involved in carrying out tasks conferred on the European Data Protection Supervisor. 4.Where appropriate, the Board and the European Data Protection Su- pervisor shall establish and publish a Memorandum of Understanding implementing this Article, determining the terms of their cooperation, and applicable to the staﬀ of the European Data Protection Supervisor involved in carrying out the tasks conferred on the Board by this Regulation. 5.The secretariat shall provide analytical, administrative and logistical sup- port to the Board. 6. The secretariat shall be responsible in particular for: (a) the day-to-day business of the Board; (b)communication between the members of the Board, its Chair and the Commission; (c) communication with other institutions and the public; (d)the use of electronic means for the internal and external communica- tion; (e) the translation of relevant information; (f) the preparation and follow-up of the meetings of the Board; (g)the preparation, drafting and publication of opinions, decisions on the settlement of disputes between supervisory authorities and other texts adopted by the Board. Article 76 Conﬁdentiality 1.The discussions of the Board shall be conﬁdential where the Board deems it necessary, as provided for in its rules of procedure. 2.Access to documents submitted to members of the Board, experts and representatives of third parties shall be governed by Regulation (EC) No 1049/2001 of the European Parliament and of the Council (21). 107",
        "CHAPTER VIII Remedies, liability and penalties Article 77 Right to lodge a complaint with a supervisory authority 1.Without prejudice to any other administrative or judicial remedy, every data subject shall have the right to lodge a complaint with a supervisory authority, in particular in the Member State of his or her habitual residence, place of work or place of the alleged infringement if the data subject considers that the processing of personal data relating to him or her infringes this Regulation. 2.The supervisory authority with which the complaint has been lodged shall inform the complainant on the progress and the outcome of the complaint including the possibility of a judicial remedy pursuant to Article 78. Article 78 Right to an eﬀective judicial remedy against a supervisory authority 1.Without prejudice to any other administrative or non-judicial remedy, each natural or legal person shall have the right to an eﬀective judicial remedy against a legally binding decision of a supervisory authority concerning them. 2.Without prejudice to any other administrative or non-judicial remedy, each data subject shall have the right to a an eﬀective judicial remedy where the supervisory authority which is competent pursuant to Articles 55 and 56 does not handle a complaint or does not inform the data subject within three months on the progress or outcome of the complaint lodged pursuant to Article 77. 3.Proceedings against a supervisory authority shall be brought before the courts of the Member State where the supervisory authority is established. 4.Where proceedings are brought against a decision of a supervisory au- thority which was preceded by an opinion or a decision of the Board in the consistency mechanism, the supervisory authority shall forward that opinion or decision to the court. Article 79 Right to an eﬀective judicial remedy against a controller or processor 1.Without prejudice to any available administrative or non-judicial remedy, including the right to lodge a complaint with a supervisory authority pursuant to Article 77, each data subject shall have the right to an eﬀective judicial remedy where he or she considers that his or her rights under this Regulation have been infringed as a result of the processing of his or her personal data in non-compliance with this Regulation. 2.Proceedings against a controller or a processor shall be brought before the courts of the Member State where the controller or processor has an establishment. Alternatively, such proceedings may be brought before the courts of the Member State where the data subject has his or her habitual residence, unless the controller or processor is a public authority of a Member State acting in the exercise of its public powers. Article 80 Representation of data subjects 108",
        "1.The data subject shall have the right to mandate a not-for-proﬁt body, organisation or association which has been properly constituted in accor- dance with the law of a Member State, has statutory objectives which are in the public interest, and is active in the ﬁeld of the protection of data subjects’ rights and freedoms with regard to the protection of their personal data to lodge the complaint on his or her behalf, to exercise the rights referred to in Articles 77, 78 and 79 on his or her behalf, and to exercise the right to receive compensation referred to in Article 82 on his or her behalf where provided for by Member State law. 2.Member States may provide that any body, organisation or association referred to in paragraph 1 of this Article, independently of a data subject’s mandate, has the right to lodge, in that Member State, a complaint with the supervisory authority which is competent pursuant to Article 77 and to exercise the rights referred to in Articles 78 and 79 if it considers that the rights of a data subject under this Regulation have been infringed as a result of the processing. Article 81 Suspension of proceedings 1.Where a competent court of a Member State has information on proceed- ings, concerning the same subject matter as regards processing by the same controller or processor, that are pending in a court in another Member State, it shall contact that court in the other Member State to conﬁrm the existence of such proceedings. 2.Where proceedings concerning the same subject matter as regards process- ing of the same controller or processor are pending in a court in another Member State, any competent court other than the court ﬁrst seized may suspend its proceedings. 3.Where those proceedings are pending at ﬁrst instance, any court other than the court ﬁrst seized may also, on the application of one of the parties, decline jurisdiction if the court ﬁrst seized has jurisdiction over the actions in question and its law permits the consolidation thereof. Article 82 Right to compensation and liability 1.Any person who has suﬀered material or non-material damage as a re- sult of an infringement of this Regulation shall have the right to receive compensation from the controller or processor for the damage suﬀered. 2.Any controller involved in processing shall be liable for the damage caused by processing which infringes this Regulation. A processor shall be liable for the damage caused by processing only where it has not complied with obligations of this Regulation speciﬁcally directed to processors or where it has acted outside or contrary to lawful instructions of the controller. 3.A controller or processor shall be exempt from liability under paragraph 2 if it proves that it is not in any way responsible for the event giving rise to the damage. 4.Where more than one controller or processor, or both a controller and a processor, are involved in the same processing and where they are, under paragraphs 2 and 3, responsible for any damage caused by processing, each 109",
        "controller or processor shall be held liable for the entire damage in order to ensure eﬀective compensation of the data subject. 5.Where a controller or processor has, in accordance with paragraph 4, paid full compensation for the damage suﬀered, that controller or processor shall be entitled to claim back from the other controllers or processors involved in the same processing that part of the compensation corresponding to their part of responsibility for the damage, in accordance with the conditions set out in paragraph 2. 6.Court proceedings for exercising the right to receive compensation shall be brought before the courts competent under the law of the Member State referred to in Article 79(2). Article 83 General conditions for imposing administrative ﬁnes 1.Each supervisory authority shall ensure that the imposition of adminis- trative ﬁnes pursuant to this Article in respect of infringements of this Regulation referred to in paragraphs 4, 5 and 6 shall in each individual case be eﬀective, proportionate and dissuasive. 2.Administrative ﬁnes shall, depending on the circumstances of each individ- ual case, be imposed in addition to, or instead of, measures referred to in points (a) to (h) and (j) of Article 58(2). When deciding whether to impose an administrative ﬁne and deciding on the amount of the administrative ﬁne in each individual case due regard shall be given to the following: (a)the nature, gravity and duration of the infringement taking into account the nature scope or purpose of the processing concerned as well as the number of data subjects aﬀected and the level of damage suﬀered by them; (b) the intentional or negligent character of the infringement; (c)any action taken by the controller or processor to mitigate the damage suﬀered by data subjects; (d)the degree of responsibility of the controller or processor taking into account technical and organisational measures implemented by them pursuant to Articles 25 and 32; (e) any relevant previous infringements by the controller or processor; (f)the degree of cooperation with the supervisory authority, in order to remedy the infringement and mitigate the possible adverse eﬀects of the infringement; (g) the categories of personal data aﬀected by the infringement; (h)the manner in which the infringement became known to the supervi- sory authority, in particular whether, and if so to what extent, the controller or processor notiﬁed the infringement; (i)where measures referred to in Article 58(2) have previously been ordered against the controller or processor concerned with regard to the same subject-matter, compliance with those measures; 110",
        "(j)adherence to approved codes of conduct pursuant to Article 40 or approved certiﬁcation mechanisms pursuant to Article 42; and (k)any other aggravating or mitigating factor applicable to the circum- stances of the case, such as ﬁnancial beneﬁts gained, or losses avoided, directly or indirectly, from the infringement. 3.If a controller or processor intentionally or negligently, for the same or linked processing operations, infringes several provisions of this Regulation, the total amount of the administrative ﬁne shall not exceed the amount speciﬁed for the gravest infringement. 4.Infringements of the following provisions shall, in accordance with para- graph 2, be subject to administrative ﬁnes up to 10 000 000 EUR, or in the case of an undertaking, up to 2 % of the total worldwide annual turnover of the preceding ﬁnancial year, whichever is higher: (a)the obligations of the controller and the processor pursuant to Articles 8, 11, 25 to 39 and 42 and 43; (b)the obligations of the certiﬁcation body pursuant to Articles 42 and 43; (c) the obligations of the monitoring body pursuant to Article 41(4). 5.Infringements of the following provisions shall, in accordance with para- graph 2, be subject to administrative ﬁnes up to 20 000 000 EUR, or in the case of an undertaking, up to 4 % of the total worldwide annual turnover of the preceding ﬁnancial year, whichever is higher: (a)the basic principles for processing, including conditions for consent, pursuant to Articles 5, 6, 7 and 9; (b) the data subjects’ rights pursuant to Articles 12 to 22; (c)the transfers of personal data to a recipient in a third country or an international organisation pursuant to Articles 44 to 49; (d)any obligations pursuant to Member State law adopted under Chapter IX; (e)non-compliance with an order or a temporary or deﬁnitive limitation on processing or the suspension of data ﬂows by the supervisory authority pursuant to Article 58(2) or failure to provide access in violation of Article 58(1). 6.Non-compliance with an order by the supervisory authority as referred to in Article 58(2) shall, in accordance with paragraph 2 of this Article, be subject to administrative ﬁnes up to 20 000 000 EUR, or in the case of an undertaking, up to 4 % of the total worldwide annual turnover of the preceding ﬁnancial year, whichever is higher. 7.Without prejudice to the corrective powers of supervisory authorities pursuant to Article 58(2), each Member State may lay down the rules on whether and to what extent administrative ﬁnes may be imposed on public authorities and bodies established in that Member State. 111",
        "8.The exercise by the supervisory authority of its powers under this Article shall be subject to appropriate procedural safeguards in accordance with Union and Member State law, including eﬀective judicial remedy and due process. 9.Where the legal system of the Member State does not provide for ad- ministrative ﬁnes, this Article may be applied in such a manner that the ﬁne is initiated by the competent supervisory authority and imposed by competent national courts, while ensuring that those legal remedies are ef- fective and have an equivalent eﬀect to the administrative ﬁnes imposed by supervisory authorities. In any event, the ﬁnes imposed shall be eﬀective, proportionate and dissuasive. Those Member States shall notify to the Commission the provisions of their laws which they adopt pursuant to this paragraph by 25 May 2018 and, without delay, any subsequent amendment law or amendment aﬀecting them. Article 84 Penalties 1.Member States shall lay down the rules on other penalties applicable to infringements of this Regulation in particular for infringements which are not subject to administrative ﬁnes pursuant to Article 83, and shall take all measures necessary to ensure that they are implemented. Such penalties shall be eﬀective, proportionate and dissuasive. 2.Each Member State shall notify to the Commission the provisions of its law which it adopts pursuant to paragraph 1, by 25 May 2018 and, without delay, any subsequent amendment aﬀecting them. CHAPTER IX Provisions relating to speciﬁc processing situations Article 85 Processing and freedom of expression and information 1.Member States shall by law reconcile the right to the protection of personal data pursuant to this Regulation with the right to freedom of expression and information, including processing for journalistic purposes and the purposes of academic, artistic or literary expression. 2.For processing carried out for journalistic purposes or the purpose of academic artistic or literary expression, Member States shall provide for exemptions or derogations from Chapter II (principles), Chapter III (rights of the data subject), Chapter IV (controller and processor), Chapter V (transfer of personal data to third countries or international organisations), Chapter VI (independent supervisory authorities), Chapter VII (coopera- tion and consistency) and Chapter IX (speciﬁc data processing situations) if they are necessary to reconcile the right to the protection of personal data with the freedom of expression and information. 3.Each Member State shall notify to the Commission the provisions of its law which it has adopted pursuant to paragraph 2 and, without delay, any subsequent amendment law or amendment aﬀecting them. Article 86 Processing and public access to oﬃcial documents 112",
        "Personal data in oﬃcial documents held by a public authority or a public body or a private body for the performance of a task carried out in the public interest may be disclosed by the authority or body in accordance with Union or Member State law to which the public authority or body is subject in order to reconcile public access to oﬃcial documents with the right to the protection of personal data pursuant to this Regulation. Article 87 Processing of the national identiﬁcation number Member States may further determine the speciﬁc conditions for the processing of a national identiﬁcation number or any other identiﬁer of general application. In that case the national identiﬁcation number or any other identiﬁer of general application shall be used only under appropriate safeguards for the rights and freedoms of the data subject pursuant to this Regulation. Article 88 Processing in the context of employment 1.Member States may, by law or by collective agreements, provide for more speciﬁc rules to ensure the protection of the rights and freedoms in respect of the processing of employees’ personal data in the employment context, in particular for the purposes of the recruitment, the performance of the contract of employment, including discharge of obligations laid down by law or by collective agreements, management, planning and organisation of work, equality and diversity in the workplace, health and safety at work, protection of employer’s or customer’s property and for the purposes of the exercise and enjoyment, on an individual or collective basis, of rights and beneﬁts related to employment, and for the purpose of the termination of the employment relationship. 2.Those rules shall include suitable and speciﬁc measures to safeguard the data subject’s human dignity, legitimate interests and fundamental rights, with particular regard to the transparency of processing, the transfer of personal data within a group of undertakings, or a group of enterprises engaged in a joint economic activity and monitoring systems at the work place. 3.Each Member State shall notify to the Commission those provisions of its law which it adopts pursuant to paragraph 1, by 25 May 2018 and, without delay, any subsequent amendment aﬀecting them. Article 89 Safeguards and derogations relating to processing for archiving pur- poses in the public interest, scientiﬁc or historical research purposes or statistical purposes 1.Processing for archiving purposes in the public interest, scientiﬁc or histori- cal research purposes or statistical purposes, shall be subject to appropriate safeguards, in accordance with this Regulation, for the rights and free- doms of the data subject. Those safeguards shall ensure that technical and organisational measures are in place in particular in order to ensure respect for the principle of data minimisation. Those measures may include pseudonymisation provided that those purposes can be fulﬁlled in that manner. Where those purposes can be fulﬁlled by further processing which does not permit or no longer permits the identiﬁcation of data subjects, those purposes shall be fulﬁlled in that manner. 113",
        "2.Where personal data are processed for scientiﬁc or historical research purposes or statistical purposes, Union or Member State law may provide for derogations from the rights referred to in Articles 15, 16, 18 and 21 subject to the conditions and safeguards referred to in paragraph 1 of this Article in so far as such rights are likely to render impossible or seriously impair the achievement of the speciﬁc purposes, and such derogations are necessary for the fulﬁlment of those purposes. 3.Where personal data are processed for archiving purposes in the public interest, Union or Member State law may provide for derogations from the rights referred to in Articles 15, 16, 18, 19, 20 and 21 subject to the conditions and safeguards referred to in paragraph 1 of this Article in so far as such rights are likely to render impossible or seriously impair the achievement of the speciﬁc purposes, and such derogations are necessary for the fulﬁlment of those purposes. 4.Where processing referred to in paragraphs 2 and 3 serves at the same time another purpose, the derogations shall apply only to processing for the purposes referred to in those paragraphs. Article 90 Obligations of secrecy 1.Member States may adopt speciﬁc rules to set out the powers of the supervisory authorities laid down in points (e) and (f) of Article 58(1) in relation to controllers or processors that are subject, under Union or Member State law or rules established by national competent bodies, to an obligation of professional secrecy or other equivalent obligations of secrecy where this is necessary and proportionate to reconcile the right of the protection of personal data with the obligation of secrecy. Those rules shall apply only with regard to personal data which the controller or processor has received as a result of or has obtained in an activity covered by that obligation of secrecy. 2.Each Member State shall notify to the Commission the rules adopted pur- suant to paragraph 1, by 25 May 2018 and, without delay, any subsequent amendment aﬀecting them. Article 91 Existing data protection rules of churches and religious associations 1.Where in a Member State, churches and religious associations or communi- ties apply, at the time of entry into force of this Regulation, comprehensive rules relating to the protection of natural persons with regard to processing, such rules may continue to apply, provided that they are brought into line with this Regulation. 2.Churches and religious associations which apply comprehensive rules in accordance with paragraph 1 of this Article shall be subject to the su- pervision of an independent supervisory authority, which may be speciﬁc, provided that it fulﬁls the conditions laid down in Chapter VI of this Regulation. CHAPTER X Delegated acts and implementing acts Article 92 Exercise of the delegation 114",
        "1.The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article. 2.The delegation of power referred to in Article 12(8) and Article 43(8) shall be conferred on the Commission for an indeterminate period of time from 24 May 2016. 3.The delegation of power referred to in Article 12(8) and Article 43(8) may be revoked at any time by the European Parliament or by the Council. A decision of revocation shall put an end to the delegation of power speciﬁed in that decision. It shall take eﬀect the day following that of its publication in the Oﬃcial Journal of the European Union or at a later date speciﬁed therein. It shall not aﬀect the validity of any delegated acts already in force. 4.As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the European Parliament and to the Council. 5.A delegated act adopted pursuant to Article 12(8) and Article 43(8) shall enter into force only if no objection has been expressed by either the European Parliament or the Council within a period of three months of notiﬁcation of that act to the European Parliament and the Council or if, before the expiry of that period, the European Parliament and the Council have both informed the Commission that they will not object. That period shall be extended by three months at the initiative of the European Parliament or of the Council. Article 93 Committee procedure 1.The Commission shall be assisted by a committee. That committee shall be a committee within the meaning of Regulation (EU) No 182/2011. 2.Where reference is made to this paragraph, Article 5 of Regulation (EU) No 182/2011 shall apply. 3.Where reference is made to this paragraph, Article 8 of Regulation (EU) No 182/2011, in conjunction with Article 5 thereof, shall apply. CHAPTER XI Final provisions Article 94 Repeal of Directive 95/46/EC 1. Directive 95/46/EC is repealed with eﬀect from 25 May 2018. 2.References to the repealed Directive shall be construed as references to this Regulation. References to the Working Party on the Protection of Individuals with regard to the Processing of Personal Data established by Article 29 of Directive 95/46/EC shall be construed as references to the European Data Protection Board established by this Regulation. Article 95 Relationship with Directive 2002/58/EC This Regulation shall not impose additional obligations on natural or legal persons in relation to processing in connection with the provision of publicly available electronic communications services in public communication networks in 115",
        "the Union in relation to matters for which they are subject to speciﬁc obligations with the same objective set out in Directive 2002/58/EC. Article 96 Relationship with previously concluded Agreements International agreements involving the transfer of personal data to third countries or international organisations which were concluded by Member States prior to 24 May 2016, and which comply with Union law as applicable prior to that date, shall remain in force until amended, replaced or revoked. Article 97 Commission reports 1.By 25 May 2020 and every four years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council. The reports shall be made public. 2.In the context of the evaluations and reviews referred to in paragraph 1, the Commission shall examine, in particular, the application and functioning of: (a)Chapter V on the transfer of personal data to third countries or international organisations with particular regard to decisions adopted pursuant to Article 45(3) of this Regulation and decisions adopted on the basis of Article 25(6) of Directive 95/46/EC; (b) Chapter VII on cooperation and consistency. 3.For the purpose of paragraph 1, the Commission may request information from Member States and supervisory authorities. 4.In carrying out the evaluations and reviews referred to in paragraphs 1 and 2, the Commission shall take into account the positions and ﬁndings of the European Parliament, of the Council, and of other relevant bodies or sources. 5.The Commission shall, if necessary, submit appropriate proposals to amend this Regulation, in particular taking into account of developments in information technology and in the light of the state of progress in the information society. Article 98 Review of other Union legal acts on data protection The Commission shall, if appropriate, submit legislative proposals with a view to amending other Union legal acts on the protection of personal data, in order to ensure uniform and consistent protection of natural persons with regard to processing. This shall in particular concern the rules relating to the protection of natural persons with regard to processing by Union institutions, bodies, oﬃces and agencies and on the free movement of such data. Article 99 Entry into force and application 1.This Regulation shall enter into force on the twentieth day following that of its publication in the Oﬃcial Journal 2. It shall apply from 25 May 2018. 116",
        "This Regulation shall be binding in its entirety and directly applicable in all Member States. Done at Brussels, 27 April 2016. For the European Parliament The President M. SCHULZ For the Council The President J.A. HENNIS-PLASSCHAERT 117"
      ]
    },
    "IDGC.pdf": {
      "fingerprint": "IDGC.pdf|1771998852|1512927",
      "pages": [
        "The Data Governance Institute www.DataGovernance.com telephone: 1.321.438. 0774 Abstract Data Governance can mean different things to different people. Adding to this ambiguity, governance and stewardship can be perceived as complicated endeavors. Frameworks help us organize how we think and communicate about complicated or ambiguous concepts . If your organization employs a framework, your people can more easily achieve clarity of thought and purpose. A fram ework can also help you succeed in realizing value from your program and efforts and data. The DGI Data Governance Framework was designed to help you: § Achieve clarity § Ensure value from your efforts § Create a clear mission § Maintain scope and focus § Establis h accountabilities § Define measurable successes This paper describes core concepts, the components of the DGI Data Governance Framework, and typical steps in implementing a program. TThhee DDGGII DDaattaa GGoovveerrnnaannccee FFrraammeewwoorrkk Gwen Thomas, The Data Governance Institute Focus AreasGoals Metrics / Success Measures Funding Business / IT Processes that touch datato achieve ManagementData Governance Decision RightsData StakeholdersData StewardsData Governance Office (DGO) WHO WHY WHAT Data Governance Pro cesses7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement7 Monitor, Measure, Report7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statementHOW WHENData Rules and DefinitionsMissionAccountabilities Controls10 Components of a Data Governance Program Rules & Rules of Engagement People & Organizational Bodies Processes 2 31 4 5 67 98 10The DGI Data Governance Frameworkfrom The Data Governance Institute The DGI Data Governance Frameworkfrom The Data Governance Institute Focus AreasGoals Metrics / Success Measures Funding Business / IT Processes that touch datato achieve ManagementData Governance Decision RightsData StakeholdersData StewardsData Governance Office (DGO) WHO WHY WHAT Data Governance Pro cesses7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement7 Monitor, Measure, Report7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statementHOW WHENData Rules and DefinitionsMissionAccountabilities Controls10 Components of a Data Governance Program Rules & Rules of Engagement People & Organizational Bodies Processes 2 31 4 5 67 98 10Focus AreasGoals Metrics / Success Measures Funding Business / IT Processes that touch datato achieve ManagementData Governance Decision RightsData StakeholdersData StewardsData Governance Office (DGO) WHO WHY WHAT Data Governance Pro cesses7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement7 Monitor, Measure, Report7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statementHOW WHENData Rules and DefinitionsMissionAccountabilities Controls10 Components of a Data Governance Program Rules & Rules of Engagement People & Organizational Bodies Processes 2 31 4 5 67 98 10The DGI Data Governance Frameworkfrom The Data Governance Institute The DGI Data Governance Frameworkfrom The Data Governance Institute Become a member via the Data Governance & Stewardship Community of Practice at ww.Data Stewardship. com",
        "The DGI Data Governance Framework © The Data Governance Institute Page 2 of 20 TTaabbllee ooff CCoonntteennttss What is Data Governance? ................................ ................................ ................................ .......... 3 Data Governance and IT Governance ................................ ................................ ...................... 4 Why Use the DGI Data Governance Framework? ................................ ................................ ....... 5 What Do You Want Data Governance to Accomplish? ................................ ............................... 6 Typical Data Governance Focus Areas ................................ ................................ .................... 6 Data Governance With a Focus on Policy, Standards, Strategy ................................ ............ 7 Data Governance With a Focus on Data Quality ................................ ................................ ..8 Data Governance With a Focus on Privacy / Compliance / Security ................................ ....8 Data Governance With a Focus on Architecture / Integration ................................ .............. 9 Data Governance With a Focus on Data Warehouses and Business Intelligence (BI) ........... 9 Data Governance With a Focus on Management Support ................................ .................. 10 Your Focus and Your Stakeholders ................................ ................................ ....................... 10 Working Toward Your Goals With a Data Governance Life Cycle Methodology ...................... 11 Framework Components in Detail ................................ ................................ ............................. 12 Data Governance Components that Deal With Rules and “Rules of Engagement ”................ 13 Component #1: Mission and Vision ................................ ................................ ................... 13 Component #2: Goals, Governance Metrics / Success Measures, Funding Strategies ......... 14 Component #3: Data Rules and Definitions ................................ ................................ ....... 15 Component #4: Decision Rights ................................ ................................ ........................ 15 Component #5: Accountabilities ................................ ................................ ....................... 16 Component #6: Controls ................................ ................................ ................................ ...17 Data Governance Components that Deal With People and Organizational Bodies ................ 17 Component #7: Data Stakeholders ................................ ................................ ..................... 17 Component #8: A Data Governance Office (DGO) ................................ ............................ 18 Component #9: Data Stewards ................................ ................................ .......................... 18 The Process of Governing Data ................................ ................................ ............................. 18 Component #10: Proactive, Reactive, and Ongoing Data Governance Processes ............... 18 Challenges ................................ ................................ ................................ ................................ 19 Getting Started ................................ ................................ ................................ .......................... 20",
        "The DGI Data Governance Framework © The Data Governance Institute Page 3 of 20 TThhee DDGGII DDaattaa GGoovveerrnnaannccee FFrraammeewwoorrkk WWhhaatt iiss DDaattaa GGoovveerrnnaannccee?? Here’s a s hort definition of Data Governance : t Data Governance is the exercise of decision -making and authority for data -related matters . Here’s a bit l onger definition: t Data Governance is a system of decision rights and accountabilities for information -related processes, executed according to agreed -upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods. What will Data Governance look like in your organization? Obviously, a program that focuses on Privacy / Compliance / Security may look different from one that exists to support Data Warehouses and Business I ntelligence. A nd, a program concentra ting on Architecture / Integration may involve different participants than one whose goals involve Data Quality. (We’ll discuss typical focus areas for Data Governance later.) Whatever you focus your efforts on, you’ll need to define for your participants exactly what you mean by Data Governance . Choose your words carefully; they must resonate in your culture and environment if your program is to be accepted . t Does your organization have strict hierarchical, command -and-controls protocols? Are you operat ing in a compliance -driven, “black -and-white” environment? Do you know what you want, but the challenge is getting people to obey existing rules? If so, your definition may emphasize “exercise of authority.” t What if you’re still trying to create collecti ons of policies, rules, and data definitions? What i f your issues fall more into a “shades -of-gray” area, where the challenge is getting the right participants to collaboratively agree on the right set of rules, and to work together to monitor and enforce them? In that case, your definition may emphasize “decision rights.” It’s important to present a definition that implies what you are trying to accomplish with your program. You don’t want to send the wrong message to those who are helping to govern your data, or to those who are being governed. Definition Data Governance is the exercise of decision - making and authority for data -related matters. Alternate Definition “Management is the decisions you make. Governance is the structure for making them.” - CIO Magazine",
        "The DGI Data Governance Framework © The Data Governance Institute Page 4 of 20 Here are s ome other definitions and sound bites that may be useful: t Data Governance refers to the organizational bodies, rules, decision rights, and accountabilities of people and information systems as they perfo rm information - related processes. t Data Governance is how we “decide how to decide .” Have you heard people at your organization claim they have “no governance?” That would be impossible. After all, anarchy is a form of governance! So is a dictatorship. C hances are, your organization wants a form of governance that falls somewhere between these two extremes. You probably want to move from an informal, unrecognized form of governance to a more formal, recognized, and accepted form of governance. Of course, Data Governance never replaces management. It complements it. Governance comes into play when individual managers find that they cannot – or should not – make independent decisions. Governance brings together cross -functional teams to make interdependent rules or to resolve issues or to provide services to data stakeholders. These cross -functional teams – Data Stewards and/or Data Governors – generally come from the Business side of operations. They set policy that IT and Data groups will follow as they e stablish their architectures, implement their own best practices, and address requirements. Governance can be considered the overall process of making this work. DDaattaa GGoovveerrnnaannccee aanndd IITT GGoovveerrnnaannccee What’s the difference between Data Governance and IT Gover nance? Let’s start with another question: What’s the difference between data /information and information technology (IT)? Consider a plumbing analogy: IT is like the pipes and pumps and storage tanks in a plumbing system. Data is like the water flowing t hrough those pipes. Suppose you were afraid that the water flowing through your pipes was poisoned. What type of plumber would you call? None, of course ! Plumbers are specialists in the pipes and pumps and storage tanks – not in what’s flowing through them . You’d call in specialists who know how to test for water quality – specialists who could tell the difference between clean water and other types of clear liquids. Large organizations have to make many decisions about their IT systems and the data that fl ows through them. Many of these Soundbi te IT is like the pipes and pumps and storage tanks in a plumbing system. Data is like the water flowing through those pipes. If you suspected your water was poisoned, would you call a plumber? Soundbite Governance comes into play when individual managers find that they cannot – or should not – make independent decisions.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 5 of 20 decisions are made by IT Governance groups focusing on IT Portfolio Management issues, such as deciding when it’s time to put in a new application, with all its new pipes and pumps an d storage tanks. Some concerns will neve r find their way to such a group, however. Issues specific to what’s “flowing through the pipes” require a cross -functional group that brings the expertise needed to make data -related decisions. This is not to say that your existing IT Governance Board ca n’t address issues that escalate up to them. But along the way, as issues are analyzed and recommendations are formed, Data Governance will require input from Subject Matter Experts and management representatives that understand data. Effective Data Gover nance will require ongoing contributions from Data Stewards. It will probably warrant its own organizational bodies, its own focused attention, and its own conceptual framework. WWhhyy UUssee aa DDaattaa GGoovveerrnnaannccee FFrraammeewwoorrkk?? Everything a company does should tie to one of three universal executive drivers t Increase revenue and value t Manage cost and complexity t Ensure survival through attention to risk and vulnerabilities: compliance, security, privacy, etc. This is certainly true of Data Governance; all efforts should tie back to one or more of these universal value mandates . Maintaining a focus on value can be difficult, though, if participants are uncertain about goals and strategies. Frameworks help us organize how we think and communicate about complicated or ambigu ous concepts. If your organization settles on a framework, your people can more easily achieve clarity of thought and purpose. The DGI Data Governance Fram ework was designed to help t Achieve clarity t Ensure value from your efforts t Create a clear mission t Main tain scope and focus t Establish accounta bilities t Define measurable successes Goals The DGI Data Governance Framework was designed to help • Achieve clarity • Ensure value from your efforts • Create a clear mission • Maintain scope and focus • Establish accounta - bilities • Define measurable successes",
        "The DGI Data Governance Framework © The Data Governance Institute Page 6 of 20 WWhhaatt DDoo YYoouu WWaanntt DDaattaa GGoovveerrnnaannccee ttoo AAccccoommpplliisshh?? Regardless of the focus of your program, chances are you hope to accomplish the following universal goals for Data Governance programs: 1. Enable better decision -making 2. Reduce operational friction 3. Protect the needs of data stakeholders 4. Train management and staff to adopt common approaches to data issues 5. Build standard, repeatable processes 6. Reduce costs and in crease effectiveness through coordination of efforts 7. Ensure transparency of processes What else are you trying to accomplish? The most common objective of Data Governance programs is to standardize data definitions across an enterprise or initiative . Other goals and objectives depend on the focus of a particular Data Governance program. TTyyppiiccaall DDaattaa GGoovveerrnnaannccee FFooccuuss AArreeaass It’s important to note that most programs don’t limit themselves to a single focus area. Some efforts – such as Compliance and Data Quality – naturally fit together. While many programs will address goals in two or three areas, then, most newly -formalized programs don’t try to address every focus area. Following are descriptions of Data Governance programs in six common focus area s. A single framework can help organize efforts for all of the se focus areas because of what all Data Governance programs have in common: t They all have activities that address a three -part governance mission : to create rules, resolve conflicts, and provide ongoing services. t They a ll employ most or all of the universal components of a Data Governance program. t They all address universal governance pr ocesses and services, such as Issue Resolution and Stakeholder Care. Benchmark The most common objective of Data Governance programs is to standardize data definitions across an enterprise. Perspective The type of policies you address and the level of involvement required of data stakeholder groups will depend on the focus of your Data Governance Program.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 7 of 20 Data Governance programs with different focus areas will, however, differ in the type of rules and issues they’ll address. They’ll differ in the emphasis they give to certain data -related decisions and actions. And, they’ll differ in the level of involvement required of types of data stakeholde rs. Data Governance With a Focus on Policy, Standards, Strategy This type of program t ypically comes into existence because some group within the organization needs support from a cross -funct ional leadership body. For exam ple, companies moving from silo development to enterprise systems may find their application development teams resisting the guidance of Data Architect s and Modelers. Formal Data Governance policies, backed by cross -functional Stewards, can give needed weight to architect ural positions. Enterprise initiatives such as Enterprise Data Management (EDM), Busine ss Process Reengineering (BPR), standardization on platforms, and acquisition of data sets and systems can also benefit from such a program focus. Often these types of programs start by concentrating on sets of Master Data and/or Metadata . Policy, Standards, Strategy Data Quality Privacy / Compliance / Security Architecture / Integration Data Warehouses and BI Management Support Soundbite Formal Data Governance policies, backed by cross - functional Stewards, can give needed weight to architectural positions. Alig nment",
        "The DGI Data Governance Framework © The Data Governance Institute Page 8 of 20 A charter for this type of program may hold Data Governance and Stewardship participants accountable to: § Review, approve, monitor policy § Collect, choose, review, approv e, monitor standards § Align sets of policies and standards § Contribute to Business Rules § Contribute to Data Strategies § Identify stakeholders and establish decision rights Data Governance With a Focus on Data Quality This type of program typically comes into existence because of issues around the q uality, integrity, or usability of data. It may be sponsored by a Data Quality group or a business team that needs better quality data. ( For e xample: Data Acquisition or Mergers & Acquisitions.) Often , quality eff orts are initially applied to Master Data. These types of programs almost always involve Data Quality software. They may begin with an enterprise focus, or efforts may be local to a department or a project. A charter for this type of program may hold Data Governance and Stewardship participants accountable to: § Set direction for Data Quality § Monitor Data Quality § Report status for quality -focused initiatives § Identify stakeholders, establish decision rights, clarify accountabilities Data Governance With a Foc us on Privacy / Compliance / Security This type of program typically comes into existence because of concerns about Data Privacy, Access Management / Permissions , Information Security controls, or compliance with regulatory, contractual, or internal requir ements. The program may be sponsored by B usiness or IT or be an outgrowth of a Governance, Risk, and Compliance (GRC) program . Often , it results from a senior management mandate. These programs generally begin with an enterprise scope , but often efforts are limited to specific types of data. They almost always include technologies to locate sensitive data, to protect data, and/or to manage policies or controls. A charter for this type of program may hold Data Governance and Stewardship participants accoun table to: Soundbite Data Governance often sets direction for Data Quality and then monitors the success of Data Quality efforts. Soundbite Programs that focus on Privacy / Compliance / Security often stem from a management mandate.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 9 of 20 § Help protect s ensitive data through support for Access Management and Security r equirements § Align frameworks and initiatives § Help assess risk and define controls to manage risk § Help enforce regulatory, contractual, architectural compliance requir ements § Identify stakeholders, establish decision rights, clarify accountabilities Data Governance With a Focus on Architecture / Integration This type of program typic ally comes into existence in conjunction with a major system acquisition, development effort, or update that requires new levels of cross -functional decision -making and accountabilities. Another driver for such a program would be a move to Service Oriented Architecture (SOA), with its need for well-governed data or a new focus on Metadata, Master Data Management (MDM) , or Enterprise Data Management (EDM) A charter for this type of program may hold Data Governance and Stewardship participants accountable to: § Ensure consistent data definitions § Support a rchitectural policies and s tandards § Supp ort Metadata Programs, SOA, Master Data Management , Enterprise Data Management (EDM) § Bring cross -functional attention to integration challenges § Identify stakeholders, establish decision rights, clarify accountabilities Data Governance With a Focus on Data Warehouses and Business Intelligence (BI) This type of program typic ally comes into existence in conjunction with a specific data warehouse, data mart, or BI tool. These types of efforts require tough data -related decisions, and organizations often implem ent governance to help make initial decisions, to support follow -on decisions, and to enforce standards and rules after the new system becomes operational. The scope may be initially limited to rules, roles , and responsibilities for the new system, but so metimes this type of program serves as a prototype for an enterprise Data Governance / Stewardship program . Soundbite A move to Service - Oriented Architecture (SOA) , with its need for well - governed data , could be a driver for a program with a focus on Architecture / Integration . Soundbite Data Governance programs w ith a focus on Data Warehouses / BI often start small, but may scale to other efforts The idea is to “act locally but think globally.”",
        "The DGI Data Governance Framework © The Data Governance Institute Page 10 of 20 A charter for this type of program may hold Data Governance and Stewardship participants accountable to: § Establish rules for data usage and data def initions. § Identify stakeholders, establish decision rights, clarify accountabilities § Identify SDLC embedded governance steps and loop-outs for projects § Clarify the value of data assets and data -related projects. Data Governance With a Focus on Managemen t Alignment This type of program typic ally comes into existence when managers find it difficult to make “routine” data -related management decisions because of th eir potential e ffect on operations or compliance efforts. Managers realize they need to come toge ther to make collaborative decisions but either don’t know all the stakeholders to involve or have an obstacle to assembling them. In such cases, a formal Data Governance program can help managers make decisions with confidence. Sometimes such programs co nsist primarily of councils that come together to analyze interdependencies, make decisions, and issue policies. Other times, the Data Governance program will have multiple focuses, such as supporting management and also addressing Compliance. A charter for this type of program may hold Data Governance and Stewardship participants accountable to: § Measure the value of data and data -related efforts. § Align frameworks and initiatives § Identify stakeholders, establish decision rights, clarify accountabilities § Identify SDLC embedded governance steps and loop-outs for projects § Monitor and report on data -related projects § Promote data -related messages and positions YYoouurr FFooccuuss aanndd YYoouurr SSttaakkeehhoollddeerrss What will your Data Governance program focus on? This decision will d etermine which type of rules and concerns your participants will address. It will influence the mix of data stakeholders involved in your data-related decisions and actions , as well as the amount of effort required of your stakeholders. Soundbite Data Governance programs w ith a focus on Management Alignment typically come into existence when managers find it difficult to make “routine” data -related management decisi ons because of their potential effect on operations or compliance efforts.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 11 of 20 Who is a data stake holder? Any individual or group that could affect or be affected by the data under discussion. Some stakeholders are obvious – business groups, IT teams, Data Architects, and DBAs. Other stakeholders may not be so obvious for a given decision or situation. Knowing which stakeholder to bring to the table – and when – is the responsibility of the Data Governance team. WWoorrkkiinngg TToowwaarrdd YYoouurr GGooaallss WWiitthh aa DDaattaa GGoovveerrnnaannccee LLiiffee CCyyccllee MMeetthhooddoollooggyy All p rograms have lifecycles. Here are the 7 Phases in the Data Gove rnance Life Cycle . 7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement7 Monitor, Measure, Report7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement Note that Data Governance does not begin with the design of the program! t Before you start deciding who goes on what committee, you should be clear about your program’s value statement . t You should have develop ed a roadmap to share wi th stakeholders. t Those stakeholders will want to know the WHO / WHAT / WHEN / WHERE / WHY / HOW of your program b efore they decide to support it, so y ou need to anticipate the ir questions. You’ll need preliminary answers, even if they’re only assumptions until you do your actual program design. These first three phases of your Data Governance Life Cycle are difficult. We’ve all heard the saying that it’s hard to see the forest for the trees. What if you ARE one of the trees? If you’re just getting starte d, consider asking for assistance from another group within your organization that has successfully launched a new program. What lessons did they learn about value statements and funding? Also, consider receiving input from other organizations that have s uccessfully launched Data Governance programs. What metrics did their executives want to see? What value statements resonated with their stakeholders? What funding models did they use, and were they successful? Best Practice Consider receiving input from other organizations that have successfully launched Data Governance programs.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 12 of 20 As you perform the activities needed to gain support and funding, remember that your program may plan to address multiple focus areas. Each new effort should be introduced using the seven steps of the life cycle . Even specif ic governance -led projects, such as creating a set of data standards, will want to follow the Data Governance Life Cycle steps: 1. Develop a value statement 2. Prepare a roadmap 3. Plan and fund 4. Design 5. Deploy 6. Govern 7. Monitor, measure, report . A note about the final phase in the Data Governance Life Cycle: Each time you consider a new se t of activities, you’ll want to anticipate stakeholders’ expectations for monitoring efforts, measuring success, and reporting status. Your ability to deliver industry -standard metrics that satisfy stakeholders can be the difference between program activit ies that are chronically painful and those that become routine. FFrraammeewwoorrkk CCoommppoonneennttss iinn DDeettaaiill Following are descriptions of each of the 10 universal components of a Data Governance program. Rules and Rules of Engagement 1. Mission and Vision 2. Goals, Go vernance Metrics and Success Measures, and Funding Strategies 3. Data Rules and Definitions 4. Decision Rights 5. Accountabilities 6. Controls People and Organizational Bodies 7. Data Stakeholders 8. A Data Governance Office 9. Data Stewards Processes 10. Proactive, Reactive, and Ongoing Data Governance Processes Benchmark Each time you consider a new set of activities, you’ll want to anticipate stakeholders’ expectations for monitoring efforts, measuring success, and reporting status. Your ability to deliver industry - standard metrics that satisfy stakeholders can be the difference between program activities that are chronically painful and those that become routine.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 13 of 20 Focus AreasGoals Metrics / Success Measures Funding Business / IT Processes that touch datato achieve ManagementData Governance Decision RightsData StakeholdersData StewardsData Governance Office (DGO) WHO WHY WHAT Data Governance Pro cesses7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement7 Monitor, Measure, Report7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statementHOW WHENData Rules and DefinitionsMissionAccountabilities Controls10 Components of a Data Governance Program Rules & Rules of Engagement People & Organizational Bodies Processes 2 31 4 5 67 98 10The DGI Data Governance Frameworkfrom The Data Governance Institute The DGI Data Governance Frameworkfrom The Data Governance Institute Focus AreasGoals Metrics / Success Measures Funding Business / IT Processes that touch datato achieve ManagementData Governance Decision RightsData StakeholdersData StewardsData Governance Office (DGO) WHO WHY WHAT Data Governance Pro cesses7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement7 Monitor, Measure, Report7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statementHOW WHENData Rules and DefinitionsMissionAccountabilities Controls10 Components of a Data Governance Program Rules & Rules of Engagement People & Organizational Bodies Processes 2 31 4 5 67 98 10Focus AreasGoals Metrics / Success Measures Funding Business / IT Processes that touch datato achieve ManagementData Governance Decision RightsData StakeholdersData StewardsData Governance Office (DGO) WHO WHY WHAT Data Governance Pro cesses7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statement7 Monitor, Measure, Report7 Monitor, Measure, Report6 Govern the data5 Deploy the program4 Design the program3 Plan and Fund2 Prepare a roadmap 1 Develop a value statementHOW WHENData Rules and DefinitionsMissionAccountabilities Controls10 Components of a Data Governance Program Rules & Rules of Engagement People & Organizational Bodies Processes 2 31 4 5 67 98 10The DGI Data Governance Frameworkfrom The Data Governance Institute The DGI Data Governance Frameworkfrom The Data Governance Institute DDaattaa GGoovveerrnnaannccee CCoommppoonneennttss tthhaatt DDeeaall WWiitthh RRuulleess aanndd ““RRuulleess ooff EEnnggaaggeemmeenntt”” Component #1: Mission and Vision At its highest level, Data Governance typically has a three -part mission: 1) Proactively define/align rules . 2) Provide ongoing, boundary -spanning protection and services to data stakeholders. 3) React to and resolve issues arising from non-compliance with rules . This is similar to the functions provided by the three bra nches of many representative forms of political governance. One branch of a government – the legislative branch – establishes rules in the form of laws. Another branch – the executive branch – executes those rules, while providing ongoing services to cons tituents. A third branch – the judicial branch – deals with rule -breakers and, when necessary, interprets laws and resolves inconsistencies between different sets of rules and regulations. Data Governance Data Governance",
        "The DGI Data Governance Framework © The Data Governance Institute Page 14 of 20 This is not to suggest your Data Governance program needs three “ branches.” After all, your Business and IT management structures are already functioning in the “executive branch” role. And often, the same group of Data Stewards that establish es rules will also resolve conflicts. However, as you build your Data Governa nce bodies, roles, and responsibilities, it’s useful to remember that Senators and Judges require support staff. To be successful in establishing and enforcing policies, standards, and other types of rules, your Data Governance program will need to provide the type of support that’s built into the DGI Data Governance Framework. Along with your mission, be sure to develop a clear vision. What could your organization look like with a mature Data Governance program? How about without one? A note: Mission stat ements can be dry. On the other hand, the language you use to paint your vision should be rich, evocative, compelling. Your vision should be able to inspire stakeholders, to help them envision possibilities, to encourage them to set data -related goals. Component #2: Goals, Governance Metrics / Success Measures, Funding Strategies Some of your program’s goals may result in “soft” results that are anecdotal, or hard to measure. Others should be SMART: Specific, Measurable, Actionable, Relevant, and Timely . How d o you decide which goals you should pursue? Start by anticipating the effect of governance efforts on the “4 Ps”: Programs, Projects, Professional Disciplines, and People as individuals. Ask how you efforts could help enterprise programs (or high -profile projects) t Increase revenue and value t Manage cost and complexity t Ensure survival through attention to risk and vulnerabilities: compliance, security, privacy, etc. Ask how the program could support the efforts of Architecture, Quality, Application De velopment, or other professional disciplines. Ask yourself what pains or wished -for gains of key individuals could be addressed by a strong Data Governance program. And, don’t forget to look at the data itself . How can you affect the amount of or quality of or protection of data and metadata ? Ask how you can measure that effect . Best Practice To be successful in establishing and enforcing policies, standards, and other types of rules, your Data Governance program will need to provide the type of support that’s built into the DGI Data Governance Framework.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 15 of 20 Metrics – just like goals – should be SMART. Everyone involved in Data Governance should know what success looks like, and how it’s being measured. Consider creating value statem ents with the following formula: If we do A, then we should expect B, with a result of C; otherwise, we should expect D, with a result of E. Such clarity around value helps as you consider funding options available for your program. With your key stakehol ders, you’ll want to explore t How you could fund your Data Go vernance Office (or its equivalent) t How you could fund Data Analyst/Architecture time needed to help define rules, define data, and research issues that must be resolved t How you could fund Stewar dship activities t What protocols need to be establishe d for Business and IT staff who § Help defin e data § Analyze data issues § Help resolve data issues Component #3: Data Rules and Definitions This component refers to data -related policies, standards, compl iance requirem ents, business rules , and data definitions . Depending on your focus areas, your program may work to § Create new rules/definitions § Gather existing rules/definitions § Address gaps and overlaps § Align and prioritize conflicting rules/definitions § Establish or formalize rules for when certain definitions apply . Component #4: Decision Rights Before any rule is created or any data -related decision is made, a prior decision must be addressed: who gets to make the decision, and when, and using what proc ess? It is the responsibility of the Data Governance program to facilitate (and to sometimes document and store) the collection of decision rights that are the “metadata” of data -related decisions. Decision rights for compliance -based programs are often simple to define. For example, s hould the decision about whether to comply with a federal law be left to a vote of those Best Practice Bring discipline to your program by translating goals to formal value statements.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 16 of 20 who will have to live with the decision? Of course not – that decision needs to be made by the organization’s Board of Directors, with i nput from the Legal department. For other types of programs, decision rights may require more negotiation. For example, who should decide the length of a data field in a new system ? Hmmm… maybe the decision should be made by Data Architecture . But maybe i t requires input from many stakeholders. Maybe one of them has a constraint that needs to drive the decision. Component #5: Accountabilit ies Once a rule is created or a data -related decision is made, the organization will be ready to act on it. Who should do what, and when? For activities that do not neatly map to departmental responsibilities, the Data Governance program may be expected to define accountabilities that can be baked into everyday processes and the organization’s software development life cy cle (SDLC) . This is especially true of programs with a focus on compliance. Organizations sometimes struggle with assigning responsibilities for compliance activities. Why? First, because compliance – like governance – is generally a boundary -spanning i nitiative that requires cross -functional alignment. Second, many managers who are adept at planning for the management of their specific areas have less experience with the r equirements that come with the P ost-Compliance P aradigm. This new paradigm says t hat, for efforts with a compliance requirement, the work is not finished until you 1) Do it, 2) Control it, 3) Document it, and 4) Prove compliance. Individual managers are often not prepared to identify all the tasks and integration points for designing and implementing controls, documentation, and auditable proof of compliance. Indeed, in a compliance environment, individual managers may not be allowed to interpret requirements independently. Instead, companies often move to a model where a centralized group develops these requirements and then disseminates them to stakeholders. Sometimes, Data Governance is asked to assist with developing requirements and accountabilities for such data-related efforts. A note: Whether you’re in a compliance environmen t or not, most governance efforts involve cross -functional teams . Your governance coordinators will need to understand and follow your organization’s protocols for engaging staff , assigning tasks , and providing status to management . The Post - Compliance Paradigm This new paradigm says that, for efforts with a compliance requirement, work is not finished until you 1) Do it, 2) Control it, 3) Document it, and 4) Prove compliance.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 17 of 20 Component #6: Controls It’s well established that data is constantly at risk. With the proliferation of sensitive data breaches – and the consequences for those who were entrusted with the data – it is becoming clear that data can also represent risk. How do we deal with risk ? We manage it, preferably by preventing the events that we don’t want to occ ur. Those we can’t be sure of preventing, we at least detect, so we can then correct the problem. O rganization’s risk management strategies are made operational through controls. Often the Data Governance program is asked to recommend data-related controls that could be applied at multiple levels of the controls stack (network / operating system; database; application; user processes) to support governance goals. Data Governance may also be asked to recommend ways that existing general controls (Change Management, policies, training, SDLCs and Project Management , etc. ) could be modified to support governance goals or enterprise goals . DDaattaa GGoovveerrnnaannccee CCoommppoonneennttss tthhaatt DDeeaall WWiitthh PPeeooppllee aanndd OOrrggaanniizzaattiioonnaall BBooddiieess Data Governance programs generally include several organizational bodies. At the least, Data Governance involves Data Stakeholders, Data Stewards, and a Data Governance Office (or its equivalent). Some programs also include Da ta Quality Stewards with specific responsibilities for monitoring and addressing certain quality characteristics. Component #7: Data Stakeholders Data Stakeholders come from across the organization. They include groups who create data, those who use dat a, and those who set rules and requirements for data. Because Data Stakeholders affect and are affected by data -related decisions, they will have expectations that must be addressed by the Data Governance program. Some will expect to be included in some k inds of data -related decisions. Some will be expected to be consulted before decisions are formalized, and others will be satisfied to be informed of decisions after they are made. Often, a subset of executive stakeholders will form a Data Governance Board to provide oversight to the program, issue policies , and resolve issues . Other times, governance oversight is provided by an existing organizational body, such as an IT Steering Committee or an Executive team. Best Practice Consider implementing a Data Governance Board made up of executive - level stakeholders. Background Controls can be: • Preventative • Detective • Corrective. They can manual, technology - aided, or completely automated. Data Governance tools bake controls into processes.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 18 of 20 Component # 8: A Data Governance Office (DGO) Earlier, we remarked that in federal governments, Senators and Judges need support staff. So do the people in your organization who are making data -related decisions, defining data, monitoring compliance to rules, and resolving issues. The Data Governan ce Office (DGO) facilitates and supports these governance activities. It collects metrics and success measures and reports on them to data stakeholders. It provides ongoing S takeholder C are in the form of communication, access to information, record -keepin g, and education/support. Component #9: Data Stewards The Data Stewardship (or Governance) Council consists of a set of Data Stakeholders who come together to make data- related decision s. They may set pol icy and specify standards, or they may craft recomm endations that are acted on by a higher -level g overnance board. Sometimes – especially for large organizations – a single level of stewards is inadequate. In this case, a hierarchy of stewards may exist. With large or small organizations, the Data Stewar dship Council may break out into teams or working groups that address specific data issues or decisions. Data Governance programs with a focus on Data Quality may also include Data Quality Stewards. These roles typically report to a business function or Da ta Quality team, with dotted -line accountabilities to Data Governance. These stewards examine sets of data against criteria for completeness, correctness, and integrity. Other types of stewards may have other, specific data -related responsibilities . TThhee PPrroocceessss ooff GGoovveerrnniinngg DDaattaa Component #10: Proactive, Reactive, and Ongoing Data Governance Processes Components 1-6 of the DGI Data Governance Framework deal with r ules. They also describe the “rules of engagement” employed by components 7 -9 (People and Org anizational Bodies) during governance . This last component – Processes – describes the methods used to govern data. Ideally, these processes should be standardized, documented, and repeatable. They should be crafted in such a way to support regulatory and compliance requirements for Data Management, Privacy, Security, and Access Management. Definition Data Stewardship: the set of activities that ensure data -related work is performed according to policies and practices as established through governance.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 19 of 20 Every organization will decide how much structure and formality to bring to the process of governing data. The Data Governance Institute recommends (and routinely imple ments) formal, documented, repeatable procedures for: 1. Aligning Policies, Requirements, and Controls 2. Establishing Decision Rights 3. Establishing Accountability 4. Performing Stewardship 5. Managing Change 6. Defining Data 7. Resolving Issues 8. Specifying Data Quality Requirements 9. Building Governance Into Technology 10. Stakeholder Care 11. Communications 12. Measuring and Reporting Value CChhaalllleennggeess At the industry’s first Data Governance Conference , in Orlando , Florida USA in December of 2006, leaders of successful Data Governance programs declared that, in their experience, Data Governance is between 80% and 95% communications! They uniformly expressed surprise at the amount of written and verbal communication required to successfully bring tog ether stake holders to achieve their goals . Is communication not your fort é? That’s OK. C ommunication plans and supporting tools can be developed by those with expertise and then administered by your program staff. For example, m ost DGOs employ customer se gmentation to understand stakeholders’ information needs, preferred terminology, and special interests. They develop layers of communication pieces – elevator speeches, value statements, impact statements, presentations, and other documentation – so they c an deliver the right versions of governance messages to the right people with the right level of detail. They employ stakeholder participation matrices to make sure stakeholder s aren’t overlooked and that the right people get their part of the message in t he right sequence . And, they employ email template s and templates for presenta tions and reports . Benchmark At the industry’s first Data Governance Conference, in Orlando, Florida USA in December of 2006, leaders of successful Data Governa nce programs declared that, in their experience, Data Governance is between 80% and 95% communications! They uniformly expressed surprise at the amount of written and verbal communication required to successfully bring together stakeholders to achieve t heir goals. Is communication not your forté? That’s OK. Communication plans and supporting tools can be developed by those with expertise and then administered by your program staff.",
        "The DGI Data Governance Framework © The Data Governance Institute Page 20 of 20 GGeettttiinngg SSttaarrtteedd What type of Data Governance is right for your organization? How should you begin your efforts? It’s tempting to start right in designing your Data Governance and Data Stewardship organizational bodies, assigning roles and responsibilities, and developing policy. And it ’s true: this is important work. But remember that such program design work is actually your fourth step of seven in implementin g a Data Governance program. Establishing your focus and value proposition should be your first priority. Be sure you understand how your efforts can contribute to your stakeholders’ need to increase revenue and value , manage cost and complexity , and e nsure survival through attention to risk , compliance, and vulnerabilities. Not sure? That’s understandable – as we said before, i t’s hard to see the forest when you’re one of the trees! Consider asking someone else from within your organization (or from witho ut) to help you understand your value statement and to develop a plan to clearly and unambiguously communicate that value . When you can clearly describe your organization’s data -related problems, how you’re going to address them, and how success can be m easured, then you’ll be ready to reap the benefits of a value -based Data Governance program. About the Data Governance Institute The Data Governance Institute is the premier provider of in-depth, vendor -neutral information about – and assistance with – tools, techniques, models, and best practices for the governance of data and information. The Institute provides a wealth of resources: the free DGI Data Governance Framework, information on data laws, regulations, and standards, whitepapers, cas e studies, best practices, data humor, and more . These are available from the Institute’s website at www.DataGovernance.com. The Institute also provides assist ance for clients through training, issue analysis, and program assistance as they roadmap, desig n, and implement governance programs. The membership arm of the Data Governance Institute is the Data Governance and Stewardship Community of Practice. Information is available at www.DataStewardship.com. The Data Governance Institute www.DataGov ernance.com telephone: +1.321.438. 0774 Become a member via the Data Governance & Stewardship Community of Practice at ww.Data Stewardship. com"
      ]
    }
  },
  "_meta": {
    "updated_utc": "2026-02-26T10:53:08Z"
  }
}